# Arxiv Daily AIQBIO-SA

This is an automated project designed to fetch the latest papers from the Quantitative Biology (q-bio) field on arXiv daily, use AI to filter papers related to machine learning for peptides aggregates/self-assembly, generate structured JSON data and aesthetically pleasing HTML pages, and finally automatically deploy the results to GitHub Pages via GitHub Actions. Forked from: https://github.com/onion-liu/arxiv_daily_aigc 

## Tech Stack

*   **Backend/Script**: Python 3.x (`arxiv`, `requests`, `jinja2`)
*   **Frontend**: HTML5, TailwindCSS (CDN), JavaScript, Framer Motion (CDN)
*   **Automation**: GitHub Actions
*   **Deployment**: GitHub Pages

## Installation

1.  **Clone Repository**:
    ```bash
    git clone <your-repository-url>
    cd arxiv_daily_qbiosa
    ```

2.  **Create and Activate Virtual Environment** (Recommended):
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate  # macOS/Linux
    # Or .\\.venv\\Scripts\\activate # Windows
    ```

3.  **Install Dependencies**: All required Python libraries are listed in the `requirements.txt` file.
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configure API Key**: This project requires an OpenRouter API Key for AI filtering. You can also modify `src/filter.py` to use other LLM APIs. For security, do not hardcode the key in the code. Set it as an environment variable when running locally. In GitHub Actions, set it as a Secret named `OPENROUTER_API_KEY`.

## Usage

### Local Run

You can directly run the main script `main.py` to manually trigger a complete process (fetch, filter, generate).

```bash
# Ensure the OPENROUTER_API_KEY environment variable is set
export OPENROUTER_API_KEY='your_openrouter_api_key'

# Run the main script (processes today's papers by default)
python src/main.py

# (Optional) Run for a specific date
# python src/main.py --date YYYY-MM-DD
```

After successful execution:
*   The JSON data for the day will be saved in `daily_json/YYYY-MM-DD.json`.
*   The HTML report for the day will be saved in `daily_html/YYYY_MM_DD.html`.
*   The main entry page `index.html` will be updated to include the link to the latest report.

You can open `index.html` directly in your browser to view the results.

### GitHub Actions Automation

The repository is configured with a GitHub Actions workflow (`.github/workflows/daily_arxiv.yml`).

*   **Scheduled Trigger**: The workflow is set to run automatically at a scheduled time daily by default.
*   **Manual Trigger**: You can also manually trigger this workflow from the Actions page of your GitHub repository.

The workflow automatically completes all steps and deploys the generated `index.html`, `daily_json/`, and `daily_html/` directory files to GitHub Pages.

## Viewing Deployment Results

The project is configured to display results via GitHub Pages. Please visit your GitHub Pages URL (usually `https://<your-username>.github.io/<repository-name>/`) to view the daily updated paper reports.

## File Structure

```
.
â”œâ”€â”€ .github/workflows/daily_arxiv.yml  # GitHub Actions configuration file
â”œâ”€â”€ src/                     # Python script directory
â”‚   â”œâ”€â”€ main.py              # Main execution script
â”‚   â”œâ”€â”€ scraper.py           # ArXiv scraper module
â”‚   â”œâ”€â”€ filter.py            # OpenRouter filter module
â”‚   â””â”€â”€ html_generator.py    # HTML generator module
â”œâ”€â”€ templates/               # HTML template directory
â”‚   â””â”€â”€ paper_template.html
â”œâ”€â”€ daily_json/              # Stores daily JSON results
â”œâ”€â”€ daily_html/              # Stores daily HTML results
â”œâ”€â”€ index.html               # GitHub Pages entry page
â”œâ”€â”€ requirements.txt         # Python dependency list
â”œâ”€â”€ README.md                # Project description file (This file)
â”œâ”€â”€ README_ZH.md             # Project description file (Chinese)
â””â”€â”€ TODO.md                  # Project TODO list
```

## Acknowledgements
- The inspiration for this project initially came from a share by [fortunechen](https://github.com/fortunechen)
- The vast majority of the code in this project was generated by Trae/Cursor, thanks for their hard work and diligence ðŸ˜„
