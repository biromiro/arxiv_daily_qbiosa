# Abstract—In antimicrobial peptide development, red-blood-cell lysis (HC50) is the principal safety barrier, but existing in silico tools stop at a binary toxicity classification. Here we propose a new method, AmpLyze, that closes this gap by predicting the actual HC50 value from protein sequence alone and explaining the residues that drive toxicity. The model couples residue-level ProtT5/ESM2 embeddings with sequence-level descriptors in dual local and global branches, aligned by a cross-attention module and trained with log-cosh loss for robustness to assay noise. The optimal AmpLyze model reaches a PCC of 0.756 and an MSE of 0.987, outperforming classical regressors and the state-of-the-art. Ablations confirm that both branches are essential, and cross-attention adds a further 1% PCC and 3% MSE improvement. Expected-Gradients attributions reveal known toxicity hotspots and suggest safer substitutions. By turning hemolysis assessment into a quantitative, sequence-based, and interpretable prediction, AmpLyze facilitates AMP design and offers a practical tool for early-stage toxicity screening.

Index Terms—Hemolysis Prediction, Model Interpretability,

Peptide Engineering

I. INTRODUCTION

Escalating antimicrobial resistance since the widespread use of antibiotics has undermined many antibiotics’ efficacy [1], driving the search for novel anti-infective approaches. Antimicrobial peptides (AMPs) are short and typically cationic molecules that form a first-line defense in the innate immune system. They have recently emerged as compelling candidates [2] for anti-infective approaches. They kill a broad range of microorganisms by preferentially disrupting microbial mem- branes or interfering with essential intracellular targets, and this biophysical mode of action places a comparatively low evolutionary pressure on pathogens to develop resistance [3]. Several candidates are already in clinical trials; notably, the magainin analogue pexiganan advanced to Phase III trials as a topical treatment for mildly infected diabetic foot ulcers [4]. However, conventional screening of AMPs is resource- consuming and labor-intensive, requiring extensive library de- sign and numerous assays. Over recent decades, considerable research effort has focused on developing in silico methods for AMP discovery and optimization. Early computational approaches aimed at classifying peptides into non-AMPs and

*Equal contribution. †Corresponding author.

AMPs. Initial machine learning (ML) predictors such as An- tiBP [5], AMPScanner vr.1 [6], and AMPfun [7] primarily employed algorithms including Support Vector Machines and Random Forests on sequence-derived features like one-hot encodings of amino acids and hand-crafted protein descriptors such as hydrophobicity and net charge. The advent of deep learning models eliminated the need for tedious feature hand- crafting and selection. In 2018, newer version AMPScanner vr.2 [8] was the first attempt to apply a deep neural network to AMP classification. Subsequent deep-learning models such as AI4AMP [9] and iAMPCN [10] built on this initial work. More recently, instead of relying solely on hand-crafted protein descriptors, advanced deep learning models like PepNet [11] have incorporated embeddings from large pre-trained protein language models to capture rich representations of long-range dependencies, structural motifs, and evolutionary signals. No- tably, AMP-Identifier [12] adapted the pre-trained ProtBERT architecture, achieving an impressive 99.18% accuracy in AMP classification.

Beyond basic classification, the field has progressed toward predicting quantitative potency metrics such as the minimum inhibitory concentration (MIC), which is the lowest concen- tration that stops microbial growth. Accurate MIC predic- tion can further narrow down promising candidates before costly assays. Yan et al. (2023) developed a multi-branch CNN-attention model for E. coli [13]. Chung et al. then com- bined sequence features with protein-language-model embed- dings using an ensemble method to improve MIC prediction for E. coli, S. aureus and P. aeruginosa [14]. Most recently, Cai et al. applied BERT-based transfer learning that further improved the Pearson correlation coefficient for E. coli to around 0.8 [15]. These advances in potency prediction move beyond binary activity classification toward models that can guide dosage and efficacy considerations.

While the modeling of antimicrobial efficacy is advancing, an equally critical aspect of AMP development is toxicity, par- ticularly hemolytic toxicity. Hemolysis is a primary indicator of an AMP’s safety, which is usually measured as hemolytic concentration (HC50) representing the concentration of peptide required to lyse 50% of red blood cells under defined assay conditions [16]. A peptide, even with great potential in antimi- crobial activity, will have a narrow therapeutic application if it













lyses red blood cells at low concentration. Unfortunately, many potent AMPs, estimated at 70% of known AMPs, also exhibit toxicity toward mammalian cells, particularly red blood cells, leading to hemolysis due to their amphipathic nature and high hydrophobicity [17].

Only limited studies have been conducted from this toxicity perspective, and almost all of them focus primarily on binary classification of hemolytic and non-hemolytic AMPs. An early example is HemoPI [18] that used multiple machine learning methods using amino acid composition features. Using the same dataset, HemoPred [19] employed Random Forest to achieve better accuracy. More advanced techniques have since been explored. AMPDeep (2022) [20] used transfer learning by fine-tuning large protein language models to capture the complex sequence patterns associated with hemolytic activ- ity. Similarly, ToxIBTL [21] fused transfer learning with an information bottleneck to learn compact, toxicity-specific embeddings for enhanced peptide toxicity prediction. Recently, tAMPer (2024) [22] introduced a multimodal architecture combining ESM2-derived embeddings and graph neural net- works on predicted peptide structures, taking advantage of both sequence and structural information to make more ac- curate toxicity predictions.

Until very recently, only very few methods existed to predict the hemolytic concentration of a peptide. From a drug optimization and development perspective, a binary decision whether a candidate AMP is hemolytic or not is usually insuffi- cient as there are substantial differences between a highly toxic peptide and one that’s just above the hemolytic threshold. A predictor of hemolytic concentration can effectively guide the drug development process by allowing scientists to precisely tune peptide sequences. The most significant advance to date in this area came in 2025, where Rathore et al. trained a random forest regressor on Pfeature-derived physicochemical features achieving a Pearson correlation of 0.739 (R2 = 0.543) on an independent test set [23].

Despite this progress, significant challenges remain in the accurate prediction of HC50 due to limited size of available data and its inherently noisy nature. Experimental HC50 deter- minations are sensitive to multiple assay conditions that com- plicate model training [24], [25]. Furthermore, the relationship between sequence and hemolytic activity is highly non-linear, where even single amino acid substitutions can dramatically alter the toxicity through complex effects on peptide folding, aggregation, and membrane interactions [26], [27]. There is also an urgent need for model interpretability such that the importance of each amino acid position can guide the AMP optimization process.

the To address these challenges, we present AmpLyze, first end-to-end deep learning model for HC50 prediction. Our approach achieves state-of-the-art prediction performance while providing residue-level insights into the determinants of hemolytic toxicity. We evaluated AmpLyze using stratified 5-fold cross-validation. In each fold, one subset was held out as the test set, and the remaining data were split into training and validation sets. The model was trained on the training

set and its hyperparameters tuned on the validation set before final evaluation on the test fold. Average performance across all five folds is reported.

II. PROPOSED METHODOLOGY

that

Our architecture builds on the insight

large, self- supervised protein language models (pLMs) can learn rich biophysical and evolutionary context directly from sequence corpora, greatly reducing the need for handcrafted features. In this work, we employed two state-of-the-art pre-trained pLMs, ESM2-3B [28] and ProtT5-XL-UniRef50 [29], to encode the raw amino acid sequences. AmpLyze’s dual-branch architec- ture extracts fine-grained residue embeddings from ESM2-3B alongside coarse-grained sequence embeddings from ProtT5- XL-UniRef50, then fuses these complementary representations dynamically through a learnable cross-attention module. By jointly modeling detailed local residue context and global sequence semantics, AmpLyze captures a richer biochemical embedding than either model alone.

A. Local Encoding

For each amino acid position i, we first extract two per- residue embeddings from pre-trained protein language models: one from ProtT5-XL-UniRef50 and the other from ESM2-3B, and then concatenate them into

ri = (cid:2)eT5

i

; eESM i

(cid:3) ∈ Rd.

(1)

During training random feature masking is applied, randomly zeroing out channels of ri via an element-wise mask m. The masked vectors are projected into a lower-dimensional space and fed into a bidirectional LSTM, which integrates informa- tion from both N-terminus→C-terminus and C-terminus→N- terminus directions. Finally, a multi-head self-attention layer refines the sequence of LSTM outputs, yielding a contextual- ized representation matrix

Hseq = MHSA

(cid:16)

BiLSTM(cid:0)Wp (m ⊙ [ eT5; eESM ]) + bp

(cid:1)(cid:17)

∈ RL×dlocal

where L is the peptide length.

B. Global Encoding

(2)

In parallel, we take the per-sequence ProtT5-XL-UniRef50 embedding, which is a single 1,024-dim vector summarizing the entire peptide, and pass it through a small two-layer MLP, giving the processed feature map g ∈ Rdmodel.

C. Cross-attention Fusion

To let the model decide which residues matter most given the overall peptide context, we use a cross-attention block where the global vector g serves as the Query, and the per-residue features Hseq serve as both Keys and Values. Concretely, Key K = Hseq W K, and value V = Hseq W V

Fig. 1. The proposed AmpLyze architecture based on local and global encoding branches and enhanced with a cross-attention fusion module.

are constructed with learnable weights K, V ∈ Rdlocal×dmodel. Attention scores are then computed as

α = softmax

√

(cid:16) q K⊤

dmodel

(cid:17)

+ M

(α ∈ R1×L)

(3)

where M is mask to prevent the model attending to padded positions. The attended residue summary Y is computed as

X = α V ∈ R1×dmodel,

Y = g + X ∈ R1×dmodel.

(4)

A scalar length feature is appended to Y . This vector is then passed through a fully connected layer to predict the hemolytic concentration.

III. EXPERIMENTS

A. Setup

a) Dataset: We employed the same dataset as HemoPI2 to ensure a fair comparison of model performance. All peptide sequences were obtained from two publicly available sources: Hemolytik [30] and DBAASP v3 [31]. Hemolytik is a com- prehensive resource that aggregates experimentally validated hemolytic and non-hemolytic peptides from several well- established antimicrobial peptide (AMP) databases, including APD2 and DAMPD, while DBAASP also contains detailed activity profiles that includes hemolytic and cytotoxic activity. Only AMP entries with available and numeric HC50 values were retained; for peptides with multiple HC50 measurements or reported ranges, the mean value was used to represent overall hemolytic activity. Sequences containing non-standard residues or shorter than six amino acids were removed. After filtering, the final dataset comprised 1926 peptides.

To train and evaluate our models, the entire dataset was par- titioned into five mutually exclusive using stratified sampling that preserved the peptide-length and HC50 distribution to limit

potential distribution shift. In each iteration, four folds were used to train the model and tune hyper-parameters, while the remaining fold served as an unseen validation set to prevent data leakage. Model performance is then reported as the mean ± standard deviation across the five runs.

Logarithmic transformation is widely used to stabilise vari- ance and improve the performance of regression models [32]. Accordingly, we converted the raw HC50 measurements to their negative natural logarithm (Eq. 5) and used this value as the regression target. The transformation reduces the influence of extreme values and mitigates heteroscedasticity.

pHC50 = − ln(cid:0)HC50

(cid:1)

(5)

b) Data augmentation: To combat overfitting and also encourage the model to learn contextual information from the peptide sequences, we applied random feature masking during training. For each peptide, a randomly chosen subset of 0–50 features from each per-residue protein-language-model embedding was masked. By forcing the model to reconstruct the missing information from neighboring features, this regu- larization promotes more context-aware representations.

B. Comparison Experiments

To obtain reliable estimates that are not biased by a single random split, we assessed every model under a stratified 5-fold cross-validation protocol. The entire dataset was first shuffled once and then partitioned into five mutually exclusive folds such that the distribution of peptide length and pHC50 was preserved in each fold. Table I presents our results using various metrics: the mean ± standard deviation across the five test folds for Pearson’s correlation coefficient (PCC), mean squared error (MSE), mean absolute error (MAE), and the coefficient of determination (R2).

LLMsProtT5⨁ESMProtT5(Global)InputFeature mapFeature MaskFeatureConcatenationBiLSTM LayerDropoutMulti-Head Self-AttentionLinear ProjectionConcatenatedfeature map. . .. . .LocalLinearProjectionBNLinearProjectionFeed Forward LayerLocal-Global Base Neural Network ModuleLocalFeatureGlobalFeatureLinear ProjectionMatmulDropoutKeyQueryMaskSoftMaxMatmul⨁LocalBranchGlobalBranchGlobalCross AttentionReLUReLUValueQKVTABLE I COMPARISON OF PERFORMANCE OF DIFFERENT MODELS

Model SVR XGB AdaBoost HemoPI2

SVR XGB AdaBoost RF

Features ALLCOMP ex SPCa ALLCOMP ex SPC ALLCOMP ex SPC ALLCOMP ex SPC

ProtT5+ESMb ProtT5+ESM ProtT5+ESM ProtT5+ESM

PCC ↑ 0.302 ± 0.033 0.677 ± 0.017 0.600 ± 0.027 0.730 ± 0.014

0.646 ± 0.027 0.690 ± 0.022 0.627 ± 0.028 0.705 ± 0.025

MSE ↓ 2.152 ± 0.168 1.267 ± 0.064 1.550 ± 0.076 1.074 ± 0.058

1.351 ± 0.073 1.231 ± 0.036 1.422 ± 0.081 1.200 ± 0.043

MAE ↓ 1.096 ± 0.044 0.858 ± 0.027 0.993 ± 0.028 0.753 ± 0.026

0.840 ± 0.018 0.846 ± 0.021 0.943 ± 0.028 0.816 ± 0.029

R2 ↑ 0.052 ± 0.021 0.441 ± 0.018 0.316 ± 0.024 0.526 ± 0.019

0.404 ± 0.028 0.456 ± 0.024 0.372 ± 0.032 0.470 ± 0.028

Fusion Embeddingsc

AmpLyze a ALLCOMP ex SPC: all compositional descriptors except the property-level Shannon entropy (SPC) features. b ProtT5+ESM: concatenation of per-sequence embeddings from ProtT5 and mean-pooled per-residue embeddings from the ESM protein language model. c Fusion Embeddings: use of per-residue embeddings from ProtT5 and ESM with the per-sequence ProtT5 embedding.

0.987 ± 0.095

0.703 ± 0.029

0.570 ± 0.033

0.756 ± 0.019

We benchmarked AmpLyze against the classical regressors most frequently used in AMP property predictions—Support Vector Regressor (SVR), Random Forest (RF), AdaBoost and XGBoost—as well as the state-of-the-art model HemoPI2 [23]. HemoPI2 is an RF model trained on the ALLCOMP ex SPC physicochemical descriptors. Among ML models trained on the same hand-crafted features, HemoPI2 reproduced its reported strong performance, achieving a PCC of 0.705 ± 0.025 and an R2 of 0.470 ± 0.028. Switching to embeddings from the large protein language models, ProtT5 and ESM2, degraded HemoPI2’s performance, consistent with observa- tions in its original study. By contrast, it is interesting that the SVR benefited substantially from these PLM embeddings, with PCC rising from 0.302 to 0.646, suggesting that kernel-based models are effective in high-dimensional spaces. Among these models, our model outperforms every baseline across all metrics, achieving the highest PCC (0.756 ± 0.019) and R2 (0.570 ± 0.033), alongside the lowest MSE (0.987 ± 0.095) and MAE (0.703 ± 0.029). This indicates our model’s ability to capture the nonlinear relationship between the sequence and toxicity, and generalize well for more reliable results.

Fig. 2A visualizes the performance of AmpLyze on the test set comparing the predicted and experimental pHC50. The dashed blue line shows the least-squares regression, which lies close to the identity line (grey dotted), indicating promising systematic bias. The margin histograms of the predicted (right) and observed (top) values exhibit similar shapes, indicating that AmpLyze recovers the empirical distribution of toxicity across the range rather than concentrating the accuracy in any particular region. Scatter points colored by absolute error further show that the large deviations are well distributed. Fig. 2B presents the distribution of residuals. Errors are tightly centered around zero and follow an approximate Gaussian distribution with light tails, with 73.3% of predictions falling within ± 1 ln µM.

C. Model application and model residual importance.

interpretability for per-

Deep learning models have been widely used for their ability to tackle various complex tasks and learn from high- dimensional data, but they also suffer from the black-box

Fig. 2. Performance of AmpLyze. (A) Scatter of predicted versus experimental pHC50 values. (B) Histogram of residuals.

nature due to over-parameterization that impedes model inter- pretation [33]. In recent years, extensive research has focused on illuminating these model’s decision-making processes. In our context, decoding per-residue influence on the model’s pre- diction of pHC50 yields practical, high-resolution insights that could significantly facilitate peptide design and engineering. Among the methods and tools designed for model inter- pretability, Integrated Gradients (IG) enjoys several desirable properties such as implementation invariance, completeness and sensitivity [34]. These properties make IG the natural choice for residue-level interpretation that should capture even subtle changes in a single amino acid token in a robust manner, unaffected by model implementation while reflective of the learned biology. It attributes the prediction F (x) to each input feature xi by integrating the model’s gradient over the path from the baseline x to the input x, as described by equation 6. Choosing the proper baseline is therefore a crucial hyperparameter. The conventional approach is to use the zero embedding vector as the baseline. However, recent work has argued that the zero vector typically lies outside the data distribution representing an impossible state and thus produces less meaningful attributions [35].

IGi(x) = (xi − x′ i)

(cid:90) 1

0

∂F (cid:0)x′ + α (x − x′)(cid:1) ∂xi

dα (6)

In this study, we adopted a multi-baseline Expected Gra- dients (EG) strategy that replaces a single baseline with the expectation of IG over a distribution of baselines (Eq. 7) to reduce attribution variance and sensitivity to arbitrary baseline choices [36]. We picked those peptides from the training set with pHC50 larger than 6 µM. Such sequences are essentially non-hemolytic, so they formed the reference pool that remains on-manifold and preserves natural length and amino-acid statistics. IG was then approximated with 50 Riemann steps and internal batch size of 32. We monitored convergence delta and discarded any attribution whose mean |∆| exceeded 0.01 to ensure the integral faithfully reconstructs the prediction gap. Finally, per-token attributions were averaged across the ProtT5 and ESM embedding channels and summed along the feature dimension to yield a single importance score for every residue.

(cid:104) EGi(x) = Ex′

(xi − x′ i)

(cid:90) 1

0

∂F(cid:0)x′ + α (x − x′)(cid:1) ∂xi

(cid:105)

dα

(7)

To show how interpretability can guide in silico AMP design and to further validate our model’s predictive robustness, we compiled several AMP optimization examples from the liter- ature focused on reducing hemolysis. The prediction results are summarized in Table II. Hemolytic activity depends on complex relationships of residue physicochemistry, peptide structures and membrane interactions, and no simple rule has been established yet for the design [37], [38]. As a result, targeted amino acid substitutions remain the predominant optimization strategy. In this challenging setting, our model continues to perform strongly and accurately captures the ef- fect of substitution of amino acids in most cases (PCC=0.844, MAE=0.863), reflecting that the model has indeed learned the underlying relationships. For instance, substituting two positions with lysine residues in Dermaseptin yields a dramatic increase in predicted pHC50, which is in close agreement with experimental measurements.

Information about the influence of each amino acid provides crucial initial optimization directions, and saves work from tedious permutations. We performed a case study on the Temporin series. Temporins are a family of short, hydrophobic peptides secreted by frog skin, and hold high value as a template for engineering novel antimicrobial therapeutics. The IG map for the original temporin (Fig. 3) indicates that most positions 1-13 show negative contributions, with only the tail showing a positive contribution. This corresponds to the floppy nature of the C-terminus as shown in Fig. 4A. Such structure usually impedes deep and stable insertion into the lipid bilayer, thereby limiting pore formation and reducing the peptide’s hemolytic activity. For its mutants, our model correctly predicts the effect of each lysine substitution, and the IG profile clearly confirms the causal link. For example, substituting Leu12 with Lys (Temporin-K12) reduces hemolytic activity into the safe range, and the IG profile indicates its neg- ative contribution is halved, while the surrounding negatively contributing residues are damped. This reflects the mechanism whereby this added positively charged residue damages the

hydrophobic face (Fig. 4B), with the hydrophobic moment dropping from 0.490 µH to 0.443 µH. Substituting two more lysines at Leu4 and Ala8 (Temporin-4K) further perforates the non-polar face (Fig. 4C). In the IG profile, the prominent negative bars that dominated the N-terminal half of the parent peptide are greatly abolished or even flip slightly positive. Experimentally, this cumulative disruption pushes the HC50 up by another order of magnitude (> 1000 µM).

Hindsight is 20/20: once the data are in, one can quickly credit Temporin-4K’s reduced hemolysis to its lower hy- is difficult, as drophobicity. Yet making that call a priori physicochemical properties interact in a highly non-additive way. Temporin 4K actually displays a higher hydrophobic moment than the wild-type, which is normally linked to greater toxicity, while its diminished hydrophobicity drives the opposite outcome. With no universally accepted guideline for reconciling these competing factors, rational optimisation of AMPs towards lower hemolysis remains challenging. How- ever, AmpLyze helps close this gap by offering an informative prior that could guide and accelerate AMP optimization.

D. Ablation Study

To understand the importance of each component of the architecture (Fig. 1), we performed an ablation study. After removing one module at a time, we retrained the network while keeping all other settings unchanged, such as dataset splits, learning-rate schedule and optimizer, and compared the results with the performance of the original model. The three variants are: (1) Global-only, which removes the local branch. The model bases its prediction exclusively on the per-sequence information. (2) Local-only, which strips the global branch, forcing the model to rely solely on per-residue information and ignoring the coarse sequence-level context information. (3) No Cross-attention, which retains both branches but the global- to-local cross-attention mechanism is replaced with simple mean-pooling over the local features. All ablated models were trained from scratch under identical hyper-parameter settings to ensure fair comparison. As summarized in Table III, dropping either representation branch degrades performance: Local-only (PCC 0.711, MSE 1.195) and Global-only (0.725, 1.107) each lose around 4-6% PCC relative to the full model, confirming that residue-level and sequence-level cues are com- plementary. Adding cross-attention on top of the dual branches in place of simple pooling elevates performance to PCC 0.756 and MSE 0.987, providing a further 3% reduction in MSE and 1% gain in PCC achieved by dynamically aligning global context with critical residues.

E. Hyperparameter Tuning

Hyperparameters have a direct impact on model perfor- mance, so careful tuning is essential. We performed a grid search across the hyperparameter ranges listed in Table IV. In experiments, we discovered that the loss function had the greatest influence on the results. Because the loss guides gradient descent and the learning process, it is arguably the most critical hyperparameter in deep-learning models. In our

Fig. 3. Per-residue IG attributions and helical-wheel representations for wild-type and mutant Temporin peptides. Bar charts show per-residue IG importance scores with higher values indiciating positive contributions to reduced hemolytic activity and lower values indicating contributions to increased hemolysis. Helical wheels color residues by polarity: red for basic/polar; green for polar/uncharged; yellow or nonpolar.

TABLE II PREDICTION OF HEMOLYSIS (PHC50 ) IN PARENT AND MUTANT AMPS

Sequence ALWMTLLKKVLKAAAKALNAVLVGANA ALWMTLKKKVLKAKAKALNAVLVGANA FLPLIIGALSSLLPKIF FLPLIIGKLSSLLPKIF FLPKIIGKLSSLLPKIF FLPLIIGALSSKLPKIF FLPKIIGKLSSKLPKIF KWKLFKKIGAVLKVL KWKLFKKIGKVLKKL VWKLFKKIGKVLKKL VWKLFKKIGAVLKKL INWLKLGKAVIDAL INWLKLGKKVIDAL INWLKLGKAVSDAL INWLKLGKKVIAAL INWLKLGKAVSAAL INWLKLGKAVSDIL INWLKLGKAVSAIL INWLKLGKKVIAIL

Predicted 1.049 4.660 2.293 3.865 4.540 4.592 5.210 4.145 4.922 4.346 3.994 2.829 4.395 4.617 4.598 4.136 3.876 3.273 4.930

Experiment -0.511 5.485 1.884 2.989 4.471 4.810 7.421 3.916 ± 0.052 5.713 ± 0.157 4.175 ± 0.069 4.310 ± 0.070 3.135 ± 0.052 5.173 ± 0.006 6.512 ± 0.281 4.700 ± 0.023 5.729 ± 0.020 5.120 ± 0.016 4.946 ± 0.017 3.782 ± 0.082

Peptide Dermaseptin S4 a Dermaseptin S4-1 a Temporin b Temporin-2K b Temporin-3K b b Temporin-K12 Temporin-4K b CM c CM-10K14K c CM-1V10K14K c CM-1V14K c MPIII d MPIII-1 d MPIII-2 d MPIII-6 d MPIII-8 d MPIII-9 d MPIII-11 d MPIII-12 d a Data from [39]. b Data from [40]. c Data from [41]. d Data from [42].

TABLE IV HYPERPARAMETER SEARCH SPACE FOR AMPLYZE.

Hyperparameter loss function frequency mask # heads (MSA) dropout hidden dim (LSTM) hidden dim (MLP) dmodel

Candidates

Best

MAE, MSE, Huber, Log-Cosh Log-Cosh

0, 50, 100, . . . , 300 1, 3, 5 0.1, 0.2, . . . , 0.8 300, 400, . . . , 800 300, 400, . . . , 800 60, 128, 200, 300

100 5 0.3 500 500 128

Fig. 4. Surface hydrophobicity and backbone representation of wild-type and mutant Temporin peptides. (A) Wild-type Temporin shows a continuous hydrophobicity surface with the flexible C-terminus. (B) Single mutant Temporin-K12 (substitution in magenta) disrupts the hydrophobicity surface by introducing a positively charged side chain. (C) Triple mutant Temporin-4K breaks up nonpolar clustering with three lysine substitutions. Peptide 3D structures were predicted using PEP-FOLD 3.5 with default parameters [43].

TABLE III PERFORMANCE OF AMPLYZE ABLATION VARIANTS

Model − Global Branch − Local Branch − Cross Attention AmpLyze

PCC ↑

MSE ↓

MAE ↓

R2 ↑

0.725 ± 0.026 1.107 ± 0.108 0.744 ± 0.032 0.518 ± 0.04

0.711 ± 0.023 1.195 ± 0.101 0.773 ± 0.028 0.471 ± 0.045

0.748 ± 0.021 1.014 ± 0.116 0.716 ± 0.033 0.550 ± 0.046

0.756 ± 0.019 0.987 ± 0.095 0.703 ± 0.029 0.570 ± 0.033

context of hemolytic concentration, it is worth noting that HC50 values are notoriously protocol-sensitive. The same peptide, for example, can produce hemolysis ratios that vary by up to four-fold when the assay is run on mouse versus rab- bit erythrocytes, and simply substituting the positive-control

detergent can alter the value by a further approximately three-fold [24]. Our dataset comprises HC50 values measured with human, mouse, horse and rabbit erythrocytes, yet key protocol details are often left unspecified; consequently, sub- stantial inter-experiment variability is inevitable. To limit over- fitting and ensure generalization across these heterogeneous conditions, a robust loss function is fundamentally essential. We evaluated Mean Square Loss (MSE), Mean Absolute Loss (MAE), Huber Loss and Log-Cosh Loss (Eq. 8). As expected, MSE performed worst because of its pronounced sensitivity to outliers. Huber Loss comes with an additional hyperparameter to tune to control the switch from quadratic and linear loss. We experimented with δ = 0.5 and δ = 1.0, with both giving better results than MAE, but still slightly trailed Log-Cosh. Log-Cosh seamlessly integrates quadratic and linear penalties into a single smooth, twice-differentiable function, eliminating the need to hand-tune a δ hyperparameter while dampening the influence of large residuals.

Llog-cosh(y, ˆy) = log(cid:0)cosh(ˆy − y)(cid:1)

(8)

TemporinTemporin-k12Temporin-4kLeu4Ala8Leu12ABCC-terminusIV. CONCLUSION

In this work, we present AmpLyze, a sequence-based deep learning model to predict peptide hemolytic concen- tration (HC50) quantitatively and interpretably. By leverag- ing pre-trained protein language model embeddings and a fusion-embedding strategy, AmpLyze achieves superior perfor- mance over prior regression approaches: Pearson’s r improved from 0.739 to 0.756, R2 rose from 0.543 to 0.570, MSE fell from 1.074 to 0.987, and MAE declined from 0.753 to 0.703. These gains demonstrate that deep neural architectures can more effectively capture the nonlinear, high-dimensional relationships between sequence motifs and hemolytic activ- ity. We further show that robust loss functions are crucial for handling the inherent heterogeneity of hemolysis assays. Experimental HC50 values can vary by orders of magnitude across different erythrocyte sources, buffer compositions, and protocols. We found that losses such as Log-Cosh enables AmpLyze to generalize across noisy, multi-source datasets. Importantly, AmpLyze offers residue-level interpretability via Expected Gradients with a multi-baseline scheme anchored by non-hemolytic references. This approach successfully recapit- ulates literature-reported mutational effects, shedding light on rational peptide design. Looking ahead,

the next step is to integrate AmpLyze with in-silico MIC predictors to create a unified HC50–MIC framework. Both hemolytic and antimicrobial activities depend on peptide–membrane interactions: increased cationicity and amphipathicity promote binding to negatively charged bacte- rial membranes but also enhance insertion into zwitterionic erythrocyte bilayers. Future work will focus on training a multi-task model to capture subtle sequence features that maximize bacterial pore formation while minimizing red-cell lysis. Such a joint optimization strategy promises to accelerate the design of peptides with balanced safety and efficacy.

REFERENCES

[1] Centers for Disease Control and Prevention, “Antibiotic resistance threats in the united states, 2019,” U.S. Department of Health and Human Services, CDC, Atlanta, GA, Tech. Rep., 2019, cDC Stacks #82532. [Online]. Available: https://stacks.cdc.gov/view/cdc/82532 [2] I. E. Mba and E. I. Nweze, “Antimicrobial peptides therapy: an emerging alternative for treating drug-resistant bacteria,” The Yale journal of biology and medicine, vol. 95, no. 4, p. 445, 2022.

[3] C. Bucataru and C. Ciobanasu, “Antimicrobial peptides: Opportunities and challenges in overcoming resistance,” Microbiological Research, p. 127822, 2024.

[4] H. M. Lamb and L. R. Wiseman, “Pexiganan acetate,” Drugs, vol. 56,

pp. 1047–1052, 1998.

[5] S. Lata, B. Sharma, and G. P. Raghava, “Analysis and prediction of antibacterial peptides,” BMC bioinformatics, vol. 8, pp. 1–10, 2007. [6] D. P. Veltri, “A computational and statistical framework for screening novel antimicrobial peptides,” Ph.D. dissertation, George Mason Uni- versity, 2015.

[7] C.-R. Chung, T.-R. Kuo, L.-C. Wu, T.-Y. Lee, and J.-T. Horng, “Char- acterization and identification of antimicrobial peptides with different functional activities,” Briefings in bioinformatics, vol. 21, no. 3, pp. 1098–1114, 2020.

[8] D. Veltri, U. Kamath, and A. Shehu, “Deep learning improves antimicro- bial peptide recognition,” Bioinformatics, vol. 34, no. 16, pp. 2740–2747, 2018.

[9] T.-T. Lin, L.-Y. Yang, I.-H. Lu, W.-C. Cheng, Z.-R. Hsu, S.-H. Chen, and C.-Y. Lin, “Ai4amp: an antimicrobial peptide predictor using physicochemical property-based encoding method and deep learning,” Msystems, vol. 6, no. 6, pp. e00 299–21, 2021.

[10] J. Xu, F. Li, C. Li, X. Guo, C. Landersdorfer, H.-H. Shen, A. Y. Peleg, J. Li, S. Imoto, J. Yao et al., “iampcn: a deep-learning approach for identifying antimicrobial peptides and their functional activities,” Briefings in Bioinformatics, vol. 24, no. 4, p. bbad240, 2023.

[11] J. Han, T. Kong, and J. Liu, “Pepnet: an interpretable neural network for anti-inflammatory and antimicrobial peptides prediction using a pre- trained protein language model,” Communications Biology, vol. 7, no. 1, p. 1198, 2024.

[12] C. Pian, J. Huang, Y. Yang, Y. Li, L. Gao, W. Zhao, Z. Wang, X. Xu, J. Ji, Y. Zhang et al., “Aipampds: an ai platform for antimicrobial peptide design and screening,” bioRxiv, pp. 2025–03, 2025.

[13] J. Yan, B. Zhang, M. Zhou, F.-X. Campbell-Valois, and S. W. Siu, “A deep learning method for predicting the minimum inhibitory concen- tration of antimicrobial peptides against escherichia coli using multi- branch-cnn and attention,” Msystems, vol. 8, no. 4, pp. e00 345–23, 2023. [14] C.-R. Chung, C.-Y. Chien, Y. Tang, L.-C. Wu, J. B.-K. Hsu, J.-J. Lu, T.- Y. Lee, C. Bai, and J.-T. Horng, “An ensemble deep learning model for predicting minimum inhibitory concentrations of antimicrobial peptides against pathogenic bacteria,” Iscience, vol. 27, no. 9, 2024.

[15] J. Cai, J. Yan, C. Un, Y. Wang, F.-X. Campbell-Valois, and S. W. Siu, “Bert-ampep60: A bert-based transfer learning approach to pre- dict the minimum inhibitory concentrations of antimicrobial peptides for escherichia coli and staphylococcus aureus,” Journal of Chemical Information and Modeling, vol. 65, no. 7, pp. 3186–3202, 2025. [16] J. Ruiz, J. Calderon, P. Rond´on-Villarreal, and R. Torres, “Analysis of structure and hemolytic activity relationships of antimicrobial peptides (amps),” in Advances in computational biology: proceedings of the 2nd colombian congress on computational biology and bioinformatics (CCBCOL). Springer, 2014, pp. 253–258.

[17] F. Plisson, O. Ram´ırez-S´anchez, and C. Mart´ınez-Hern´andez, “Machine learning-guided discovery and design of non-hemolytic peptides,” Sci- entific reports, vol. 10, no. 1, p. 16581, 2020.

[18] K. Chaudhary, R. Kumar, S. Singh, A. Tuknait, A. Gautam, D. Mathur, P. Anand, G. C. Varshney, and G. P. Raghava, “A web server and mobile app for computing hemolytic potency of peptides,” Scientific reports, vol. 6, no. 1, p. 22843, 2016.

[19] T. S. Win, A. A. Malik, V. Prachayasittikul, J. E. S Wikberg, C. Nantase- namat, and W. Shoombuatong, “Hemopred: a web server for predicting the hemolytic activity of peptides,” Future medicinal chemistry, vol. 9, no. 3, pp. 275–291, 2017.

[20] M. Salem, A. Keshavarzi Arshadi, and J. S. Yuan, “Ampdeep: hemolytic activity prediction of antimicrobial peptides using transfer learning,” BMC bioinformatics, vol. 23, no. 1, p. 389, 2022.

[21] L. Wei, X. Ye, T. Sakurai, Z. Mu, and L. Wei, “Toxibtl: prediction of peptide toxicity based on information bottleneck and transfer learning,” Bioinformatics, vol. 38, no. 6, pp. 1514–1524, 2022.

[22] H. Ebrahimikondori, D. Sutherland, A. Yanai, A. Richter, A. Salehi, C. Li, L. Coombe, M. Kotkoff, R. L. Warren, and I. Birol, “Structure- aware deep learning model for peptide toxicity prediction,” Protein Science, vol. 33, no. 7, p. e5076, 2024.

[23] A. S. Rathore, N. Kumar, S. Choudhury, N. K. Mehta, and G. P. Raghava, “Prediction of hemolytic peptides and their hemolytic concentration,” Communications Biology, vol. 8, no. 1, p. 176, 2025.

[24] I. P. Sæbø, M. Bjør˚as, H. Franzyk, E. Helgesen, and J. A. Booth, “Optimization of the hemolysis assay for the assessment of cytotoxicity,” International journal of molecular sciences, vol. 24, no. 3, p. 2914, 2023. [25] I. Greco, N. Molchanova, E. Holmedal, H. Jenssen, B. D. Hummel, J. L. Watts, J. H˚akansson, P. R. Hansen, and J. Svenson, “Correlation between hemolytic activity, cytotoxicity and systemic in vivo toxicity of synthetic antimicrobial peptides,” Scientific reports, vol. 10, no. 1, p. 13206, 2020.

[26] A. Kumar, A. K. Tripathi, M. Kathuria, S. Shree, J. K. Tripathi, R. Purshottam, R. Ramachandran, K. Mitra, and J. K. Ghosh, “Single amino acid substitutions at specific positions of the heptad repeat sequence of piscidin-1 yielded novel analogs that show low cytotoxicity and in vitro and in vivo antiendotoxin activity,” Antimicrobial Agents and Chemotherapy, vol. 60, no. 6, pp. 3687–3699, 2016.

[27] M. Pirtskhalava, B. Vishnepolsky, M. Grigolava, and G. Managadze, “Physicochemical features and peculiarities of interaction of amp with the membrane,” Pharmaceuticals, vol. 14, no. 5, p. 471, 2021.

[28] Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, N. Smetanin, A. dos Santos Costa, M. Fazel-Zarandi, T. Sercu, S. Candido et al., “Language models of protein sequences at the scale of evolution enable accurate structure prediction,” bioRxiv, 2022.

[29] A. Elnaggar, M. Heinzinger, C. Dallago, G. Rehawi, Y. Wang, L. Jones, T. Gibbs, T. Feher, C. Angerer, M. Steinegger, D. BHOWMIK, and B. Rost, “Prottrans: Towards cracking the language of life’s code through self-supervised deep learning and high performance computing,” bioRxiv, 2020. [Online]. Available: https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554 [30] A. Gautam, K. Chaudhary, S. Singh, A. Joshi, P. Anand, A. Tuknait, D. Mathur, G. C. Varshney, and G. P. Raghava, “Hemolytik: a database of experimentally determined hemolytic and non-hemolytic peptides,” Nucleic acids research, vol. 42, no. D1, pp. D444–D449, 2014. [31] M. Pirtskhalava, A. A. Amstrong, M. Grigolava, M. Chubinidze, E. Al- imbarashvili, B. Vishnepolsky, A. Gabrielian, A. Rosenthal, D. E. Hurt, and M. Tartakovsky, “Dbaasp v3: database of antimicrobial/cytotoxic activity and structure of peptides as a resource for development of new therapeutics,” Nucleic acids research, vol. 49, no. D1, pp. D288–D297, 2021.

[32] F. Changyong, W. Hongyue, L. Naiji, C. Tian, H. Hua, L. Ying, and M. T. Xin, “Log-transformation and its implications for data analysis,” Shanghai archives of psychiatry, vol. 26, no. 2, p. 105, 2014.

[33] X. Li, H. Xiong, X. Li, X. Wu, X. Zhang, J. Liu, J. Bian, and D. Dou, “Interpretable deep learning: Interpretation, interpretability, trustworthi- ness, and beyond,” Knowledge and Information Systems, vol. 64, no. 12, pp. 3197–3234, 2022.

[34] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep PMLR,

networks,” in International conference on machine learning. 2017, pp. 3319–3328.

[35] J. Bardhan, C. Neeraj, M. Rawat, and S. Mitra, “Constructing sensible baselines for integrated gradients,” arXiv preprint arXiv:2412.13864, 2024.

[36] G. Erion, J. D. Janizek, P. Sturmfels, S. M. Lundberg, and S.-I. Lee, “Improving performance of deep learning models with axiomatic attribution priors and expected gradients,” Nature machine intelligence, vol. 3, no. 7, pp. 620–631, 2021.

[37] M. R. Yeaman and N. Y. Yount, “Mechanisms of antimicrobial peptide action and resistance,” Pharmacological reviews, vol. 55, no. 1, pp. 27– 55, 2003.

[38] A. Oddo and P. R. Hansen, “Hemolytic activity of antimicrobial pep- Springer,

tides,” in Antimicrobial peptides: methods and protocols. 2016, pp. 427–435.

[39] Z. Jiang, L. Gera, C. T. Mant, and R. S. Hodges, “Design of new antimicrobial peptides (amps) with “specificity determinants” that en- code selectivity for gram negative pathogens and remove both gram- positive activity and hemolytic activity from broad-spectrum amps,” in Proceedings of the 24th American Peptide Symposium, 2015, pp. 245– 248.

[40] Y. Lin, Y. Jiang, Z. Zhao, Y. Lu, X. Xi, C. Ma, X. Chen, M. Zhou, T. Chen, C. Shaw et al., “Discovery of a novel antimicrobial peptide, temporin-pke, from the skin secretion of pelophylax kl. esculentus, and evaluation of its structure-activity relationships,” Biomolecules, vol. 12, no. 6, p. 759, 2022.

[41] N. Klubthawee, M. Wongchai, and R. Aunpad, “The bactericidal and antibiofilm effects of a lysine-substituted hybrid peptide, cm-10k14k, on biofilm-forming staphylococcus epidermidis,” Scientific Reports, vol. 13, no. 1, p. 22262, 2023.

[42] X. Ye, H. Zhang, X. Luo, F. Huang, F. Sun, L. Zhou, C. Qin, L. Ding, H. Zhou, X. Liu et al., “Characterization of the hemolytic activity of mastoparan family peptides from wasp venoms,” Toxins, vol. 15, no. 10, p. 591, 2023.

[43] A. Lamiable, P. Th´evenet, J. Rey, M. Vavrusa, P. Derreumaux, and P. Tuff´ery, “Pep-fold3: faster de novo structure prediction for linear peptides in solution and in complex,” Nucleic acids research, vol. 44, no. W1, pp. W449–W454, 2016.

# 