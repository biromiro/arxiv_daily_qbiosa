# Abstract

Peptide self-assembly prediction offers a powerful bottom-up strategy for designing biocompatible, low-toxicity materials for large-scale synthesis in a broad range of biomedical and energy applications. However, screening the vast sequence space for categorization of aggregate morphology remains intractable. We introduce PepMorph, an end-to-end peptide discovery pipeline that generates novel sequences that are not only prone to aggregate but self-assemble into a specified fibrillar or spherical morphology. We compiled a new dataset by leveraging existing aggregation propensity datasets and extracting geometric and physicochemical isolated peptide descriptors that act as proxies for aggregate morphology. This dataset is then used to train a Transformer-based Conditional Vari- ational Autoencoder with a masking mechanism, which generates novel peptides under arbitrary conditioning. After filtering to ensure design specifications and validation of generated sequences through coarse-grained molec- ular dynamics simulations, PepMorph yielded 83% accuracy in intended morphology generation, showcasing its promise as a framework for application-driven peptide discovery.

# Introduction

Supramolecular self-assembly is a powerful bottom-up strategy for designing functional materials. Small molecular building blocks can spontaneously organize into well-ordered architectures through weak non-covalent interactions (hydrogen bonding, aromatic π-π stacking, hydrophobic effects, electrostatics, metal coordination, etc.). Given the dynamic and reversible nature of these interactions, supramolecular assemblies often exhibit adaptive, self-healing, and stimuli-responsive behaviors [1]. Such processes are ubiquitous in nature; for example, phospholipid molecules assemble into cell membranes, and complementary strands of nucleotides form the DNA double helix via hydrogen bonding. These biological motifs have inspired a variety of synthetic supramolecular materials.

Among the various supramolecular building blocks, peptides stand out for their inherent biocompatibility, chem- ical tunability, and straightforward synthesis [2, 3]. The combinatorial diversity afforded by the 20 canonical amino acids defines a vast possible design space for peptide assemblies, as even minor modifications in sequence often yield markedly different supramolecular outcomes, highlighting the critical role of sequence design in defining material structure and function. Indeed, different peptide sequences can form a rich variety of nanostructures — including fibers, tubes, rods, sheets, vesicles, and micelles —, depending on their primary sequence [4]. Moreover, environ- mental factors such as pH, temperature, and solvent composition can also steer the assembly trajectory and thereby dictate the final morphology [5].

Peptide-based supramolecular materials have found utility across a diverse array of applications. In biomedicine, for example, peptide assemblies have been employed as drug-delivery vehicles, tissue-engineering scaffolds, biosen- sors, and theranostic agents [6–10]. These examples emphasize the potential of peptide self-assembly in life-science

1













and materials-science contexts. However, realizing this potential depends on discovering sequences that adopt target morphologies and satisfy functional requirements, a highly complex challenge given the vast combinatorial sequence space.

Computational design strategies have therefore become necessary for exploring this sequence space. While trial- and-error synthesis is impractical for large-scale screening, conventional rational design, relying on expert intuition and existing heuristics, can be biased and may fail to capture unexpected solutions. In the past few years, machine learning has emerged as a powerful alternative for peptide sequence design, particularly through generative models. Variational autoencoders (VAEs) [11] learn continuous, low-dimensional embeddings of sequences, while condi- tional VAEs (CVAEs) [12] further enable targeted generation by incorporating property labels or descriptors as conditioning inputs. For instance, PepCVAE demonstrated semi-supervised generation of antimicrobial peptides by conditioning on activity labels [13], as well as the HydrAMP framework built upon this approach to optimize peptide potency and hydrophobic balance for antimicrobial activity [14].

Although developed for images or general tabular data rather than for peptide design, recent work has also shown that CVAEs can condition on an arbitrary subset of attributes by explicitly modeling a mask of observed covariates. In VAEAC (Variational Autoencoder with Arbitrary Conditioning), a conditional prior p(z | c, m) processes the observed conditions c and a binary mask m. In contrast, the decoder is conditioned on (c,m) and trained under random masks to handle any subset at test time [15]. Related approaches extend CVAEs to missing-covariate settings by marginalizing unobserved conditions during training [16]. These approaches highlight the ability of CVAE-based frameworks to efficiently explore complex design spaces and generate samples conditioned on fully specified or arbitrary partial sets of attributes, laying the groundwork for extending generative design to peptide self-assembly and morphology control, given that the labeled search space is still not extensive.

Indeed, peptide study via machine learning models is only beginning to take hold when applied to self-assembly design, especially given the lack of data. Recently, larger-scale in silico screening of the peptide sequence space has enabled the creation of the first extensive aggregation-propensity datasets [17–19]. Aggregation Propensity (AP) quantifies the ratio of solvent-accessible surface area (SASA) at the end versus the beginning of a Molecular Dynamics (MD) trajectory [20]. This metric is a proxy for a peptide’s inherent tendency to self-associate and has been used to build classifiers for aggregation given solely the peptide sequences in FASTA format [18]. Despite recent progress, peptide generation for self-assembly remains largely unexplored. Njirjak et al. tackle this problem by first training a supervised recurrent neural network classifier on a curated set of 368 peptides and then using it as a fitness oracle within a genetic algorithm to propose sequences with high predicted self-assembly propensity [21]. Candidates are validated by coarse-grained MD (CG-MD) and, for a subset, experimentally, yielding a reported discovery accuracy of 80 − 95%. However, genetic algorithm search explores only a narrow region of sequence space and produces high redundancy — generated sequences show > 40% similarity — limiting diversity and coverage of the peptide aggregate space.

Beyond these limitations, morphology remains a critical determinant of material function even when aggregation- prone sequences are identified. Applications ranging from drug delivery to biosensors demand specific aggregate geometries (fibers, tubes, spheres, etc.), yet datasets linking peptide sequence to computationally or experimentally validated self-assembly morphology are scarce. To date, efforts for data gathering concerning morphology have primarily assembled experimental databases such as SAPdb [22]; however, these are small and very sparse datasets, as simulation setups differ substantially across the aggregated studies. This lack of scale and consistency renders the available data unsuitable for training machine learning models. Without direct morphology labels, generative design must rely on indirect proxies as stand-ins for the desired assembly shape; for example, predicted descriptors derived from the peptide sequence and the isolated peptide structure model. Features such as β-sheet formation, amphiphilicity, and charge distribution were postulated to directly influence whether peptides self-assemble into fibrils, sheets, or globular micelles [23]. For example, sequences dominated by hydrophobic residues may form compact, spherical aggregates (minimizing exposed hydrophobic surface). Tools like PEP-FOLD [24] allow the effective generation of the most likely 3D conformations of isolated peptides in aqueous solution, which is one of the most common target solvents for supramolecular peptide assemblies.

Computationally derived peptide-level descriptors may, therefore, effectively predict a peptide’s assembly behav- ior. To validate this hypothesis, we introduce PepMorph, a framework designed to guide peptide discovery through aggregation propensity and morphology awareness. At its core, PepMorph employs a transformer-based CVAE with a masking mechanism, akin to VAEAC, allowing peptide generation to be flexibly conditioned on descriptors of iso- lated peptides that serve as morphology proxies. We demonstrate that the model generates highly novel and diverse peptides, with < 10% similarity, whose properties match all their targets in 55% of cases. All the generated candi- dates are then passed through a dual-stage filtering pipeline, ensuring that only sequences meeting aggregation and sequence or structure requirements are retained. In end-to-end validation with CG-MD simulations, all sequences

2

formed aggregates, and 83% matched the intended morphology by visual inspection, supporting morphology-aware sequence discovery. PepMorph bridges the gap between unguided sequence exploration and fully constrained design, offering a versatile route toward developing functional supramolecular materials.

Results

We begin by forming the PepMorph dataset, an extensive aggregation and isolated peptide descriptor dataset. Then, we implement and rigorously evaluate the PepMorph pipeline on tri-to-decapeptide morphology-aware aggregate discovery tasks. To do so, we train a transformer-based CVAE with a masking mechanism on an augmented dataset comprising aggregation metrics and computed descriptors for each peptide sequence. We then demonstrate the model’s generative performance by sampling sequences conditioned on assembly and structural metrics. Next, we apply our post-generation filters, selecting fifteen candidates each for fibrillar and spherical morphologies. Finally, we subject these peptides to CG-MD simulations in aqueous solution, quantifying their AP and examining the resulting aggregate morphologies quantitatively and visually.

Dataset Curation and Generation

One of the main issues with predictive models for short peptides is the lack of extensive datasets like the Protein Data Bank, which provides a comprehensive and standardized structural reference for proteins [25]. Nevertheless, we can move towards machine learning models for assembly of short peptides by enriching existing datasets with additional sequences and morphology proxy features (Figure 1a).

We curated continuous AP values from Wang et al. [17] and Teijlingen & Tuttle [19], where AP follows the CG-MD-derived SASA ratio [17]. Both studies used the MARTINI 2.2 force field. For the 56 peptides common to both datasets, we observed a mean AP difference of 0.089 ± 0.13, supporting comparability, and thus mergeability, of the two AP sources. During deduplication, we retained the values reported by Wang et al. and, following their procedure, subsequently assigned peptide a self-assembly/no-self-assembly (SA/no-SA) flag based on AP: ≥ 1.8 indicates aggregation, ≤ 1.65 indicates no aggregation, and values in between remain undefined (Figure 1b) [17]. This yielded a merged dataset with AP for 121, 652 peptides, 93, 668 with a SA/no-SA label.

Given the masking mechanism of our model, further explained in the next section, PepMorph does not require any specific target to be present to generate sequences (e.g., AP can be absent). As such, we also added 39, 468 sequences not present in the literature–derived set to expand the conditioning space. For these and for the set of peptides from Wang et al.’s work, we further processed the peptides through PEP-FOLD [24] to generate isolated 3D conformations and extract, for each, key properties that may drive specific assemblies. As we will elaborate in the validation pipeline, we focus on three biophysical metrics that plausibly steer spherical versus fibrillar assembly: • β-sheet content, referring to whether or not any of the amino acid residues adopt a β-strand conformation; peptides with β-sheet propensity tend to stack via backbone hydrogen bonds into elongated, fibrillar (cylindrical) assemblies [26].

• hydrophobic moment, measuring the peptide’s amphiphilicity; that is, the segregation of hydrophobic and

hydrophilic residues in space.

• net charge, measuring the peptide’s overall charge at physiological pH; for example, highly charged peptides

experience strong electrostatic repulsion that impedes self-assembly [26].

We obtained valid 3D conformations (and thus retrievable descriptors) for 80, 870 sequences (≈ 50%). The result- ing corpus contains 161, 120 unique peptide records with their composition described in Table 1. The distribution across sequence lengths is relatively balanced, with between 15, 000 and 35, 000 peptides represented at each length (Figure 1e). All possible tripeptides are included, but for longer lengths the dataset covers a smaller fraction of the exponentially expanding theoretical space of 20L sequences (see Supplementary Material).

The resulting PepMorph dataset also exhibits notable imbalances across descriptors. In particular, SA labels are roughly double that of no-SA (Figure 1c), and β-sheet content is exceedingly rare (Figure 1f). The AP distribution itself is continuous, with many sequences clustering around the cutoff of ≥ 1.8 (Figure 1b); as such, this threshold should be regarded as an approximate separator. By contrast, continuous descriptors such as the hydrophobic moment (Figure 1d) and net charge (Figure 1g) show a broad spread. However, most peptides lie in the low- to-moderate regime for the former. These patterns emphasize the potential bias and structural diversity in the PepMorph dataset. Its associated sparsity is, however, naturally handled by the aforementioned masking mechanism. Furthermore, extending the dataset with additional descriptors from the generated peptide conformations would also be trivial to integrate into the pipeline.

3

Fig. 1: PepMorph dataset (a) Data curation and feature-extraction workflow: we merge three sources: Wang et al. [17] (∼62k peptides, 5-10 amino acids (aa)), Teijlingen & Tuttle [19] (∼60k, 3-8 aa), and a set of ∼39k random peptides (4-10 aa). After deduplication, self-assembly (SA) labels are assigned, and peptide conformations are predicted with PEP-FOLD to derive all descriptors (sequence length, β-sheet content, net charge and hydrophobic moment). The resulting PepMorph corpus contains 161k unique peptides spanning 3-10 aa with aggregation- propensity (AP) values and SA/no-SA labels, as well the calculated peptide–level descriptors. Univariate summaries of the PepMorph dataset are shown, specifically of AP density (b), assembly vs no assembly (c), hydrophobic moment (d), peptide length (e), presence of β-sheet content (f) and net-charge (g). Regions regarding no-assembly and assembly are highlighted in (b), and condition regions used when targeting specific morphologies are highlighted in the remaining summaries (d-g).

PepMorph Model

Incorporating available descriptors into the generative loop introduces a new challenge to a typical Conditional Variational Autoencoder (CVAE). Existing conditional generative models typically require full specification of all conditioning variables during sequence generation. In contrast, a peptide-design expert may wish to constrain only a subset of properties and leave the rest free. For example, one might enforce a target secondary–structure propensity yet remain agnostic about sequence length or net charge. Rigidly conditioning on every parameter can incorrectly restrict output diversity or prevent generation when specific target attributes are undefined.

To address this limitation and fully leverage our dataset, we develop a transformer-based CVAE that supports partial conditioning via an explicit masking mechanism, similarly to the arbitrary–conditioning paradigm of VAEAC — although in VAEAC this is used for generic feature imputation and image inpainting, rather than targeted masking of descriptors [15] (Figure 2a). Concretely, for d normalized descriptors (min-max scaling using training

4

Table 1: PepMorph dataset composition (161, 120 entries), comprising the peptide sequence and length, two aggregation descriptors, and three morphology–proxy descriptors. Coverage counts non- null entries.

Descriptor

sequence

length

ap is assembled

Description

Provenance

Coverage

Peptide in FASTA format (uppercase single-letter codes) [27]. Number of residues in sequence.

Aggregation propensity (AP) Self-assembly label (1=SA, 0=no-SA).

[17, 19] + random

derived

[17, 19]

161, 120 (100.0%)

121, 652 (75.5%) 93, 668 (58.1%)

has beta sheet content Whether any residue is assigned to β-strand in

hydrophobic moment

net charge

the predicted peptide 3D conformation. Magnitude of hydrophobic moment from predicted peptide 3D conformation. Peptide net charge at neutral pH.

computed

80, 870 (50.2%)

set statistics) we attach a binary mask m ∈ {0, 1}d to the descriptor vector c ∈ Rd and compute a compact context summary

s = ϕ(cid:0)[ c ⊙ m, m ](cid:1), (1) where ⊙ is the element-wise product and ϕ(·) is a multilayer perceptron. This summary parameterizes a masked conditional prior p(z | s) over the latent variable z. Intuitively, users fill only the entries they care about; unspecified fields i are masked (mi = 0) and are implicitly marginalized by the conditional prior. The decoder then generates the peptide sequence y from p(y | z, s). We implement p with an autoregressive Transformer whose cross-attention ”memory” contains a latent token (from z) and a condition token (from s), allowing the model to honor provided constraints while preserving variability (see Methods). Our approach mirrors the VAEAC’s recipe, conditioning the prior and decoder on the observed context, but uses a learned summary token s instead of injecting raw [c ⊙ m, m] directly.

During training, we apply stochastic masking on top of naturally missing descriptors so the model sees arbitrary subsets of observed fields. For each dataset sample, we randomly form a mask m, compute s using Eq. 1, and optimize a CVAE objective that encourages the encoder’s posterior distribution q(z | y) to match the masked conditional prior p(z | s) while simultaneously maximizing the likelihood of reconstructing the sequence under the decoder p(y | z, s). To ensure that s faithfully encodes the provided context (and only the provided context), we add light auxiliary reconstruction terms: a mask-reconstruction head supervises m with a binary cross-entropy (BCE) loss, two heads supervise the binary descriptors with BCE evaluated only where mi = 1, and a small regressor supervises continuous descriptors with a masked mean-squared error (MSE) loss (again only on unmasked dimensions). These terms regularize ϕ to capture the observed specifications without imputing missing entries, and empirically improve constraint satisfaction while mitigating posterior collapse. The full loss formulation is shown in the Methods section. At inference, the user specifies any subset of descriptors (e.g., target length and net charge), sets the corre- sponding mask entries to 1, forms s, samples z ∼ p(z | s), and decodes tokens left-to-right until the end-of-sequence marker (<eos>) is yielded.

Novelty, Diversity and Condition Matching

After training, we can discard the encoder component for sampling and leverage the trained mapping of the partial condition sets to the prior distribution and the decoder component as an autoregressive generator. While the generator alone cannot certify assembly or morphology, we can rigorously evaluate (i) novelty, measured relative to the training set, (ii) diversity and the similarity structure of the generated set (to verify guidance without collapse), and (iii) condition matching against the requested descriptors.

To this end, we must generate several peptides with plausible condition sets. Sampling entirely random descriptor combinations is, however, a poor validation strategy, as arbitrary tuples of values (e.g., conditioning on having β-sheet content with sequence length of four) often lie far off the empirical manifold and are either physically inconsistent or statistically implausible, rendering them effectively impossible targets for PepMorph. Instead, we fit a Gaussian Mixture Model per length (L ∈ {5, . . . , 10} for comparison with Njirjak et al. [21]) on the training descriptor space and sample condition vectors from these Gaussian Mixture Models, respecting observed correlations and yielding realistic conditions. Because our model supports partial conditioning, we explicitly mask subsets of descriptors at generation time: for each length L, we generate 20 conditions, distributing them evenly across

5

Fig. 2: PepMorph model and generation validation. (a) Schematic of the Transformer-based Conditional Variational Autoencoder with the masking mechanism: a descriptor vector c and mask m are summarized into a condition summary that conditions both the latent prior and the autoregressive Transformer decoder, enabling gen- eration under arbitrary subsets of constraints. (b) Amino acid frequency in generated peptides closely follows the training distribution for both common and rare condition sets. (c) Novelty relative to the training set, quantified by the nearest-neighbour normalized edit distance (NED), showing similar novelty ability in common and rare con- ditions. (d) Condition-matching as a function of the number of conditioned descriptors k: the fraction of peptides meeting their targets declines as constraints tighten. (e) Similarity via Needleman-Wunsch percent identity of gen- erated sequences (points) to the training set (Simtrain) vs. generated sequences within the same common conditions set (Simwithin ), color coded by the number of conditions k; values remain near low-identity baselines. (f) Summary table of generation metrics: high exact novelty, broad diversity, low global similarity, and strong condition fidelity.

gen

k ∈ {1, . . . , 6} used descriptors. We also evaluate 10 rare conditions, randomly split across lengths: five with positive β-sheet content and five with rare hydrophobic-moment extremes (> 0.8), with all other descriptors set near per-length medians. For each of the 130 conditions, we decode 100 peptides autoregressively.

We quantify novelty in two ways: as exact sequence matching (fraction of generated sequences present in the training set), and as the nearest-neighbor normalized edit distance (NED) (Levenshtein distance divided by the longer length) to the closest training peptide. The PepMorph model generated highly novel sequences (only ∼ 300 of the 13k generated sequences are present in the training dataset) that require, on average, roughly a third of their sequence to be edited to match the closest neighbor on the training set (0.375 NED, Figure 2c). Additionally, the generated peptides’ amino acid frequency distribution still closely follows the training set’s, and shows no collapse when sampling for rarer conditions (Figure 2b).

As for diversity, we assess it with exact sequence matching within the 100 peptides for each generated condition — showing how well the model explores the space for the same condition set —, but also from the distribution of pairwise NED within each condition (all pairs among the 100 samples) and all conditions (100k random pairs of the

6

PepMorphFAWLMTFASTA Sequence1.961.5101001maskconditionMLPMLPMLPcondition summarycondition summaryESM tokenizerinput embeddinglatent embeddingTransformer EncoderAutoregressive Transformer DecodersampleFAWLMTad# of Conditions Used (

)FrequencyfbAminoacidFrequencyMean NEDDensitycSim



(%)trainSim





 (%) withingene13k generated peptides). The results show that the diversity, like the novelty, is also very high (Figure 2f): generated peptides are considerably different from each other, evidenced by very high pairwise NED but also extremely low exact sequence matching (0.02%), which indicates that the exploration of the search space can be comprehensive. To characterize similarity among sequences in a way that is sensitive to conditional structure (and not washed out by an extensive training set), we compute the Needleman–Wunsch global alignment with percent identity [21] regarding different sets. We report Simtrain, representing the similarity to all training peptides, Simall gen, representing the similarity to all other generated peptides, and Simwithin , representing the similarity to peptides generated from the same condition only. The results (Figure 2f) reinforce our novelty and diversity claims, as all similarity means are below 10% — comparatively, Njirjak et al.’s genetic algorithm-based model reports > 40% similarity across all generated sequences. From these results, two observations follow: first, Simtrain staying low argues against memorization; second, Simwithin gen, implying that partial conditioning preserves substantial sequence variability within each query rather than collapsing to a few templates. The scatter plot in Figure 2e also shows a broad cloud of values: we see no significant difference regarding similarity as the number of conditions increases, but we do see some sets of generated peptides with considerably higher similarity (some reaching close to 20% similarity within conditions), indicating that the type of descriptor that it was conditioned on greatly influences the restriction of the manifold.

remains low when compared to Simall

gen

gen

Lastly, we quantify condition matching (”effectiveness”, Lim et al. [28]) separately for binary and continuous descriptors. For binary and discrete descriptors, such as SA/no-SA, a sample is a match if the predicted label equals the target. For continuous descriptors, such as AP, a match requires the predicted value to lie within ±10% of the target. We report effectiveness in Figure 2f both per components — (i) length only, (ii) AP only, (iii) SA/no-SA only and (iv) all other descriptors — and as an aggregate score that requires all targeted descriptors to meet their respective criteria simultaneously. This aggregate reflects the model’s ability to honor arbitrary subsets of conditioned descriptors. To be able to evaluate the aggregation description matching, we train a separate AP regressor and self-assembly classifier using the PepMorph aggregation-related subset. As detailed in Methods, we follow Liu et al. [18]’s approach, but use a single ESM-based transformer backbone [29] with the two task-specific heads: an AP regression head and a self-assembly (SA/no-SA) classification head. This unified model yields state- of-the-art performance on both tasks: the AP head attains an MAE of 0.0393 (matching Liu et al.’s 0.0391), and the SA head reaches 96.72% accuracy (vs. their reported 94.49%). We predict 3D structures with PEP-FOLD for morphology descriptors and compute the corresponding properties. Overall, condition fidelity is strong (55% general matching) but descriptor dependent. We see a very high matching rate of length, but considerably lower when measuring against the net charge, β-sheet content, or hydrophobic moment. One can also see the expected monotonic decline of the all-target rate with increasing k used descriptors in Figure 2d (from 84.58% at k = 1 down to 24.55% at k = 6). As more constraints are enforced, any single miss causes failure, and rare settings are intrinsically harder to satisfy. Even so, high length fidelity alongside solid AP/SA matching indicates that the model is not trading constraint satisfaction for trivial length control; rather, PepMorph can juggle multiple design descriptors without collapsing to a single one.

Morphology Validation via MD Simulations

To evaluate our framework against the central hypothesis, we deliberately restrict the conditioning descriptors to the minimum needed to target two morphologies — spherical and fibrillar. As such, we perform an end-to-end quantitative evaluation of morphology control by fixing the condition vector to values intended to generate one of the two target morphologies. These values are duly explained in Table 2. Because the generator only accepts exact descriptor vectors c but peptide space screening requires ranges, we sweep each descriptor over its specified interval with the given step size and evaluate all combinations, collecting the corresponding generated peptides. The only aggregation-related input during generation was the binary aggregation label, which we fixed to 1 for all conditions. Despite the flexibility afforded by partial conditioning, the stochasticity of generative sampling and the additional noise introduced by masking imply that the generated sequences may still deviate from their target properties. To enforce fidelity to the design criteria, we incorporate a post-generation filtering stage using established predictors (Figure 3a). First, sequences are screened with the aforementioned AP regressor and SA/no-SA classifier to remove sequences unlikely to aggregate. Next, each candidate peptide is fed into PEP-FOLD to obtain its 3D conformation; from it, together with the sequence, we can recompute all peptide-level descriptors. Only those whose predicted metrics fall within the predefined tolerance (±10%) of the conditioned values are retained. This dual filtering step transforms our pipeline into a reliable end-to-end framework that yields peptides meeting both aggregation and conformation specifications.

7

Table 2: Peptide generation target properties used to direct spherical vs. fibrillar aggregate morphologies. Hydrophobic moment and net charge are shown in their min-max scaled units; targets correspond to high hydrophobic moment and near- neutral net charge. Combinatorial condition grids were sampled with increments of 1 (length), 0.1 (hydrophobic moment), and 0.05 (net charge).

Target Assembly

Spherical Aggregates

Fibrillar Aggregates

Property

Range Rationale

Peptide Length

4 − 7aa

Short peptides hardly adopt extended conformations that align into β-sheets [30].

Hydrophobic Moment

0.6 − 1.0 Moderate amphipathicity favors the

Net Charge

Peptide Length

Has β-sheet content

Net Charge

burial of hydrophobic faces leading to amphiphiles like vesicles or micelles [31].

0.4 − 0.6 Near-neutral charge reduces repulsion, enabling self-assembly [26].

7 − 10aa

Longer peptides may adopt extended conformations that can align into β-sheets [30]. High strand content drives backbone H-bond stacking into longer fibrils [26]. 0.4 − 0.6 Near-neutral charge reduces repulsion, enabling self-assembly [26].

Yes

Given the defined conditions, we generated 60 distinct peptides per condition for spherical targets and 300 for fibrillar targets, yielding 4, 800 peptides for each morphology. After deduplication, the spherical set decreased to 4, 697 sequences, whereas the fibrillar set remained at 4, 800 (no duplicates). We then applied our post-processing filter to prioritize highly aggregating candidates: we retained only peptides with predicted aggregation probability ≥ 75% from the SA/no-SA classifier, and with predicted aggregation propensity (AP) ≥ 1.8 from the regressor. Following this initial screening, 3, 986 spherical and 3, 794 fibrillar sequences remained. After the aggregation- based screening, we assess peptide-level descriptor compliance and predict structures for all remaining sequences with PEP-FOLD, discarding any peptide that fails to satisfy all targeted/conditioned descriptors within the 10% threshold. This yields 64 spherical candidates and 29 fibrillar candidates. From the combined candidate set (both morphologies), we select the top 15 sequences for each morphology by predicted aggregation propensity (AP) from our trained AP regressor, and proceed with validation via CG-MD simulations.

For this particular set of peptides, we can quantitatively distinguish both morphologies from the resulting MD trajectory via the ratio of principal moments of inertia (RMOI) introduced by Wang et al. [23]. For the largest aggregate cluster, let λ1 ≤ λ2 ≤ λ3 denote the eigenvalues of its inertia tensor (see Methods); then

RMOI =

λ1 λ3

.

(2)

By construction, RMOI ∈ (0, 1]: elongated, fibrillar aggregates yield values closer to 0, whereas compact, spher- ical aggregates approach 1. Following Wang et al., RMOI ≤ 0.35 can be classified as fibrillar or tubular, while RMOI ≥ 0.75 denotes spherical assemblies. Intermediate values fall into an undefined regime that may correspond to amorphous aggregates or diverse morphologies such as sheets or nets.

All peptides selected for simulation-based validation were run in triplicate (90 simulations in total). In every run, the peptides formed aggregates: each exceeded the aggregation-propensity threshold (AP ≥ 1.8, computed from the CG-MD SASA ratio) and showed visible assembly upon trajectory inspection. With respect to morphology targeting, the RMOI distributions show distinct tendencies for the two classes: spherical assemblies cluster around high values (∼ 0.8), while fibrillar assemblies concentrate at low values (∼ 0.3), as seen in Figure 3d,e. This separation highlights that RMOI captures the expected contrast between compact and elongated morphologies, albeit only as a proxy. Nevertheless, the cutoff values were empirically defined, and the metric has significant limitations. We observed several borderline cases where RMOI classified assemblies incorrectly, despite visual inspection confirming the target morphology. Because RMOI considers only the longest and shortest inertia directions, it neglects how mass is distributed in the simulation box — for instance, aggregates aligned along box boundaries in all directions sometimes yielded high RMOI despite being fiber-like. More broadly, RMOI struggles to reliably classify fibrils: slightly wider fibers tend to exhibit intermediate values, and the metric inherently overemphasizes tubular/rod

8

Fig. 3: PepMorph pipeline for spherical vs. fibrillar aggregate generation: screening and Molecular Dynamics (MD) visualization. (a) Screening funnel for the two targeted morphologies (left values refer to spheres, right values to fibers). Amino acid occurrence across the funnel for (b) spheres and (c) fibers. For spheres, the validated set collapses to a narrow alphabet dominated by F/I/L/V, whereas fibers remain compositionally closer to the pre-filter pool. (d) Distributions of the RMOI for all MD runs (3 per selected peptide, leading to 90 runs); dashed lines mark success thresholds (spheres ≥ 0.75, fibers ≤ 0.35), with individual peptide points overlaid. Representative MD snapshots are circled in (d) and shown in (e), with the corresponding sequence and RMOI above each panel illustrating the progression from fibrillar to spherical aggregates.

geometries rather than fibrillar ones. By visually inspecting the final frame of each trajectory (see Supplementary Material), we determine a morphology success rate of 83%.

Compared with validation under training-like conditions using our Gaussian Mixture Model procedure, enforcing the target morphology descriptors leads to a substantially larger drop in valid generated peptides. This suggests that the chosen condition combinations are both underrepresented in the training distribution and intrinsically difficult, yielding low data support and fewer sequences that satisfy all constraints. The effect is particularly pronounced for fibrillar targets: the required β-sheet fraction occurs in < 0.1% of training peptides (Figure 1f), which further depresses the hit rate.

We also fit a Uniform Manifold Approximation and Projection (UMAP) on the conditional prior centers and project all generated peptide embeddings (from the encoder) into this latent space (Figure 4). This visualization contextualizes the morphology-specific conditioning: fiber-targeted conditions collapse into a compact, sparsely pop- ulated island, consistent with the rarity of β-sheet-positive peptides, whereas sphere-targeted conditions span several sub-modes, reflecting broader but structured support. Even so, it is important to recall that the validated sphere set remains compositionally narrow, with amino acid usage dominated by F/I/L/V (Figure 3b), suggesting that the descriptor choices together with the AP ≥ 1.8 and structural filters bias toward a restricted set of chemistries. A closer look at the fiber samples reveals a connection between the sequence length of seven in the region correspond- ing to spheres and the island corresponding to true fiber conditions. This suggests that conditioning for shorter sequences with β-sheet content, which was used as a target for fibers, is likely an overconstrained requirement (pos- sibly unrealistic), as the resulting sequences do not cluster near their intended condition centers. This reinforces

9

eIncreasing RMOIILSCPGWPFYFTFIPGWVDLIVLFIFVFFIF0.2580.3900.7540.907aAll generated sequencesUnique filteringAP > 1.8 and SA > 75%< 10% descriptor mismatchHighest APMD48004697398664151529379448004800PepMorphbcFrequency (%)AminoacidsSpheresAminoacidsFrequency (%)FibersdRMOIFig. 4: Latent map of morphology conditioning and morphology matching results. A UMAP is fit on the conditional prior centers with encoder posteriors of generated peptides projected into the same space (dots for spheres, stars for fibers). (a) Condition center embeddings are colored by conditioned sequence length, revealing an ordered length gradient along the sphere branch and a single, compact, and distinct fiber island; notably, fiber targets of length 7 cluster closer to the sphere branch, consistent with over-constrained fiber conditions claims. (b) Same embedding with MD-simulated peptides highlighted according to target morphology and outcome (success/failure under the RMOI criterion); stars mark the sphere/fiber prior centers. (c) Morphology success rates for the 30 candidates under the RMOI rule and independent visual assessment were reported by target morphology. In total, RMOI success is based on the mean across three runs, with visual success on the majority vote.

both the challenges of overconstraining and the importance of masking and range exploration in maintaining valid generative support.

Discussion

We have developed PepMorph, an end-to-end framework for generating novel, diverse, aggregation-prone peptides, explicitly conditioned on geometric and physicochemical descriptors that govern their assembly morphology. Our pipeline is extremely successful at generating highly novel and diverse sequences, with 83% of them adopting the intended morphology under targeted conditioning. PepMorph addresses a gap in peptide generative modeling by enabling user-specified morphology control through a transformer-based CVAE with a masking mechanism for flexible conditioning on peptide descriptors, whereas prior peptide generators have not explicitly modeled self- assembly [13, 14] or provided any morphology goal or conditioning [21].

A key advantage of PepMorph is that its conditioning mechanism is both descriptor-agnostic and mask-aware: any peptide descriptor with an assignable value (and, optionally, a predictor for validation) can be integrated without changing the core architecture. This makes the framework naturally extensible along two axes. First, it can easily scale in the sequence space: extending to longer peptides primarily requires additional data and model capacity, while the masked conditioning and autoregressive decoding remain directly applicable. Second, it can expand in the morphology space: the same interface can accommodate alternative morphologies by using different and possibly richer descriptors, enabling targets beyond spheres and fibrils (e.g., sheets or nets). The main challenge here lies in quantitatively characterizing more complex morphologies — an inherently difficult task, as illustrated by the limitations of RMOI. Beyond morphology, application-driven properties such as antimicrobial activity, previously used in generative peptide design [14, 32], can also be seamlessly incorporated. In short, PepMorph provides a reusable scaffold for expanding from short peptides and two morphologies to broader sequence lengths and richer structural or functional targets.

10

increasing lengthUMAP-1UMAP-2aUMAP-1UMAP-2bcWhile the current descriptor set works well for the chosen targeted conditioning, two practical issues are worth improving. First, PEP-FOLD occasionally fails to produce conformations for specific sequences, leading to missing 3D-derived descriptors and, consequently, discarded candidates. Further adjustments on PEP-FOLD or even entirely replacing it with more robust structure predictors would reduce the amount of discarded candidates. Second, some regions of descriptor space are considerably underrepresented (e.g., sequences with any β-sheet content are ∼ 0.1%, with almost all of them having 10 residues — see Supplementary Material), which can bias learning and make those targets harder to satisfy. Curating additional data in these sparse regimes and performing expert-guided descriptor selection are straightforward, high-yield next steps that complement the existing pipeline.

Regarding the model, the simple approach of modeling the conditional prior as a single diagonal Gaussian works well in our setting, as reflected by strong condition matching performance in validation. However, under partial conditioning with a sparse mask m, the mapping from descriptors to valid sequences is inherently multimodal. A unimodal prior averages across disparate modes, which weakens constraint satisfaction and can cause the decoder to default to frequent training patterns. A promising direction for future work would be to develop architectures that adopt a richer masked prior, such as explicitly multimodal or mixture-based formulations [33, 34], making the uncertainty induced by masked descriptors explicit, rather than collapsing it into a single Gaussian.

Since the aggregation labels in our dataset were obtained with CG-MD, we validated candidate morphologies in silico, using a similar CG-MD setup, rather than experiments. This choice ensures methodological consistency, but MD predictions are known to be sensitive to the employed force field and simulation setting, and do not always match the experimental predictions. Thus, re-labeling the aggregation labels with more accurate molecular models would be very impactful. Particularly promising in this respect are machine-learning interatomic potentials [35, 36] — especially implicit-solvent variants [37–39] — which can approach ab-initio accuracy while retaining near-linear scaling with system size. Integrating such models would yield higher-fidelity training and validation labels while keeping the end-to-end design cycle computationally tractable. In addition, the simulation–generation loop could be tightened by calculating and conditioning directly on morphology descriptors such as RMOI.

In summary, PepMorph represents a versatile generative framework for peptide self-assembly that enables treat- ing morphology as an explicit design objective through partial conditioning. Coupled with an MD-validated screening loop, it yields low-redundancy, morphology-specific candidates with measurable shape outcomes, indicating its capa- bility of navigating the vast peptide sequence space. We see this work as a stepping stone toward truly designable peptide assemblies, bridging the gap between sequence specification and material form.

Methods

Descriptor Calculation

We consider three classes: (i) sequence–derived descriptors, (ii) aggregation descriptors, and (iii) peptide–level 3D descriptors. Sequence–derived descriptors — sequence length (number of amino acids L) and net charge — are computed directly from the amino acid sequence in FASTA format. At neutral pH, the net charge is calculated as

q =

L (cid:88)

l=1

ql,

ql =

  

+1,

−1,

0,

if residue l ∈ {Lys, Arg},

if residue l ∈ {Asp, Glu},

otherwise.

(3)

Aggregation descriptors — aggregation propensity and SA/no-SA labels — are taken as provided by the merged source datasets.

All peptide–level 3D descriptors are computed from the lowest-energy structure predicted by PEP-FOLD [24]. The code was executed in parallel and returns the top five models per peptide; it then selects the model with the best internal score in PDB format. Failed predictions were retried up to three times. From each selected PDB, we parsed backbone and side-chain coordinates using Biopython’s PDBParser [40]. Residue secondary structure is assigned with the DSSP algorithm [41]. Peptides for which DSSP or PDB coordinate parsing failed were omitted from conditioning and, consequently, from model training/validation. All descriptor–extraction steps were parallelized. We compute the β-sheet fraction as

fβ =

,

(4)

LE L

where LE is the number of residues labeled E (extended strand) by DSSP. Because nonzero fβ values are rare in our dataset, we binarized this feature as a flag indicating β-sheet content if fβ > 0.

11

For the hydrophobic moment, we use the Eisenberg hydrophobicity scale hl for residue l [42] and define ˆvl as the vector along Cα → Cβ for residue l (for Gly or missing Cβ, a unit vector is used). The hydrophobic-moment vector and its magnitude are

M =

1 L

L (cid:88)

l=1

hl ˆvl,

M =

(cid:113)

M 2

x + M 2

y + M 2 z ,

(5)

where hl weights each directional contribution ˆvl. Normalizing by L yields a length-independent measure. The magnitude of the vector is then used as the final descriptor.

Aggregation Classifier and Regressor

We fine-tune a lightweight, two-head predictor on top of an ESM-2 encoder (t12/35M) [29]. Given an input peptide y = (y1, . . . , yL), with Lmax = 10, where each yl is an amino acid symbol, we tokenize the input for ESM-2 and take the final ESM layer representation, mask out special and padding tokens, and compute a masked mean sequence representation. A linear projection maps this representation to a shared hidden state, from which two heads produce (i) a scalar aggregation propensity ˆa and (ii) a probability ˆp for assembly. The objective loss of the model is a sum of a binary cross-entropy term on ˆp and a mean-squared error term on ˆa. We do training in two stages: (i) firstly, we freeze the ESM-2 encoder and train only the heads; (ii) then, we unfreeze and fine-tune end-to-end with discriminative learning rates. As such, we firstly train the heads with AdamW (lr 10−3) for 5 epochs; we then unfreeze the ESM-2 encoder and fine-tune with discriminative learning rates (encoder 10−5, heads 10−3) and an exponential learning rate decay for 6 more epochs.

PepMorph Generative Model

We model peptide sequences via a conditional variational autoencoder with a masking mechanism that supports partial specification of design descriptors. With y = (y1, . . . , yL) denoting the peptide, where each yl is an amino acid symbol, we draw the tokenized input from the ESM-2 alphabet (20 residues plus special <bos>, <eos>, and <pad> tokens). We cap peptide length at Lmax residues and form fixed-length token sequences by framing them with <bos> and <eos> and right-padding with <pad>. Let c ∈ Rd be the vector of d = 6 normalized descriptors, and let m ∈ {0, 1}d indicate which descriptors are available for a given sample.

The model then leverages an encoder, a masked conditional prior, and a decoder. First, tokens are embedded and summed with sinusoidal positional encodings, then passed through a 2-layer Transformer encoder (hidden size 256, 8 heads). We apply padding masks and average the contextual embeddings across non-<pad> positions to obtain a fixed-dimensional sequence representation, from which two linear heads produce the mean and log-variance of a diagonal-Gaussian posterior q(z | y) over the latent z. The masked conditional prior forms the condition summary s as described previously. Finally, a 2-layer Transformer decoder (hidden size 256, 8 heads) autoregressively models p(y | z, s): its cross-attention memory concatenates two tokens — a latent token obtained by projecting z and a condition token obtained by projecting s — and at each step the decoder attends to both while predicting the next amino acid symbol until emitting <eos>.

For a mini-batch of size N , we seek to minimize

L = Lrec + β LKL + λmaskLmask + λbinLbin + λcontLcont

(6)

with β a cyclic Kullback–Leibler (KL) weight and λmask, λbin, λcont scalar hyperparameters. The reconstruction loss Lrec is the negative log–likelihood of the ground–truth amino–acid at each non-<pad> position under the decoder’s categorical distribution (token–level cross–entropy with label smoothing and teacher forcing).

For the divergence term, we follow the VAEAC approach [15] and use the closed-form KL divergence between n)) and the masked

diagonal Gaussians. Let the encoder posterior for sample n be qn(z | y) = N (µn, diag(σ2 conditional prior be pn(z | s) = N (µprior,n, diag(σ2

prior,n)), both in latent dimension K. The KL loss is then

LKL =

1 N

N (cid:88)

n=1

1 2

(cid:34)

K (cid:88)

k=1

log

σ2

prior,n,k σ2

n,k

+

n,k + (cid:0)µn,k − µprior,n,k σ2

(cid:1)2

σ2

prior,n,k

(cid:35)

− 1

.

(7)

To encourage the summary s to encode which descriptors are present and their values, we add three reconstruc- tion terms. For the mask head, let ˆmn,i be the predicted logit for mask entry mn,i ∈ {0, 1} (for descriptor index

12

i = 1, . . . , d). We define

Lmask =

1 N

N (cid:88)

n=1

1 d

d (cid:88)

(cid:16)

i=1

− mn,i log σ( ˆmn,i) − (1 − mn,i) log(cid:0)1 − σ( ˆmn,i)(cid:1)(cid:17)

.

(8)

For the binary descriptors, let Bn = {i ∈ {1, . . . , d} : mn,i = 1} denote the binary descriptors that are observed for sample n, with target bn,i ∈ {0, 1} and predicted logit ˆbn,i. We define

Lbin =

1 N

N (cid:88)

n=1

1 |Bn|

(cid:88)

(cid:16)

− bn,i log σ(ˆbn,i) − (1 − bn,i) log(cid:0)1 − σ(ˆbn,i)(cid:1)(cid:17)

,

(9)

i∈Bn

i.e., the same binary cross–entropy, but applied only to the observed binary descriptors for each sample. For the continuous descriptors, let Cn = {i ∈ {1, . . . , d} : mn,i = 1} be the continuous descriptors observed for sample n, with target value cn,i and prediction ˆcn,i. We define

Lcont =

1 N

N (cid:88)

n=1

1 |Cn|

(cid:88)

i∈Cn

(cid:0)ˆcn,i − cn,i

(cid:1)2

.

(10)

To maximize the performance of the model and better attain the main goals of generation — broad exploration of sequence space and strong compliance with arbitrary partial conditions —, we introduce three adjustments to the training data usage. Firstly, we attenuate class imbalance in the binary descriptors with weighted sampling (×2 for positive is assembled, ×10 for positive has beta sheet content). Adding to this, in order to expose the model to partial conditions, we apply stochastic masking during training: for each example, a random subset of currently available descriptors is set to unobserved (mi = 0), ensuring at least one descriptor remains observed. Finally, each epoch is augmented with 5, 000 uniformly sampled short peptides (up to Lmax) that observe only the length descriptor, seeking to achieve robustness at encoding the entirety of the peptide space.

We train for 250 epochs using AdamW (learning rate 10−3, weight decay 10−4) and halve the learning rate on validation-loss plateaus (patience 30 epochs). All model training was implemented in PyTorch and run on NVIDIA RTX 3090 GPUs. The data was stratified by peptide length and split into train (80%), validation (10%), and test (10%) sets using scikit-learn’s train test split.

Coarse-Grained Molecular Dynamics Simulations

Individual PDB files were built from the FASTA sequences via Avogadro v1.2 [43] using geometry optimization. All CG-MD simulations used GROMACS [44] with MARTINI 3 [45] forcefield, applying bead substitutions (Q5 to Q4, TC5 to SC4) following Sasselli and Coluzza’s adaptation to small peptides [46]. Topologies were generated with martinize2 [47] from the all-atom PDBs to obtain the corresponding coarse-grained representations.

Each system comprised 300 peptides in a 15 × 15 × 15 nm3 cubic, periodic box [23]. The boxes were then solvated with the MARTINI water model and neutralized by adding Na+/Cl− when needed to achieve electroneutrality. Energy minimization employed the steepest-descent integrator for 5, 000 steps, with reaction-field electrostatics and 1.2 nm cutoffs for both Coulomb and Van der Waals (VdW) interactions. Production simulations consisted of 20 million steps (∆t = 25 fs, totalizing 500 ns) in the NPT ensemble at 303 K and 1 bar. A frame checkpoint of the trajectory is stored every 50k steps (1.25 ns). We used the velocity-rescale thermostat, C-rescale barostat and Particle–Mesh Ewald for electrostatics with a cutoff of 1.2 nm. VdW interactions used a 1.2 nm cutoff with the potential-shift-Verlet modifier. Neighbor search used the Verlet scheme with a 0.005 buffer tolerance. Each system was run 3 times with different initial velocities for statistical purposes. All simulation runs used GPU acceleration.

Aggregation and Morphology Metrics

With the full trajectories, we can compute the SASA per saved frame with GROMACS (gmx sasa) on the peptide group. Let St be the total SASA at frame t. We define the early-time mean ¯Sfirst2 = (S1 + S2)/2 and the late-time mean ¯Slast2 = (ST −1 + ST )/2, so that the simulation AP is

AP =

¯Slast2 ¯Sfirst2

.

13

(11)

For RMOI, we identify, for the final saved frame of the trajectory, the largest peptide aggregate using a periodic- boundary, surface-to-surface connectivity graph. Each coarse-grained bead j is assigned a radius rj and a mass mj (by MARTINI bead class, using defaults [45]). Two beads j, k are considered connected if their minimum-image center distance djk satisfies

djk − (rj + rk) < rcut, (12) with cutoff rcut of 0.6nm. Bead coordinates are unwrapped by the minimum-image convention and translated so the mass-weighted center of mass (COM) is at the origin

M =

(cid:88)

mj,

rCOM =

j

1 M

(cid:88)

j

mj rj,

˜rj = rj − rCOM.

The inertia tensor I about the COM is the standard mass-weighted second-moment matrix

I =

(cid:88)

j

mj

(cid:0) ∥˜rj∥2I − ˜rj˜r⊤

j

(cid:1) .

(13)

(14)

We then calculate RMOI as the ratio of the smallest and largest eigenvalues of I (Eq. 2).

Data Availability

The curated PepMorph dataset, as well as the resulting simulation trajectories used for in silico validation, are made publicly available at https://github.com/tummfm/pepmorph.

Code Availability

Weights for all trained and reported models, as well as code regarding the trained models and the simulation setups for in silico validation are made publicly available at https://github.com/tummfm/pepmorph.

Acknowledgements Funded by the European Union. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. This work was funded by the ERC (StG SupraModel) - 101077842.

Author contributions. Costa, N.: Conceptualization, Methodology, Software, Validation, Formal Analysis, Investigation, Resources, Data Curation, Writing - Original Draft, Visualization. Zavadlav, J.: Writing - Review & Editing, Supervision.

Competing interests The authors declare no competing interests.

References

[1] Aida, T., Meijer, E.W., Stupp, S.I.: Functional Supramolecular Polymers. Science (New York, N.y.) 335(6070),

813–817 (2012) https://doi.org/10.1126/science.1205962

[2] Okesola, B.O., Mata, A.: Multicomponent self-assembly as a tool to harness new properties from peptides and proteins in material design. Chemical Society Reviews 47(10), 3721–3736 (2018) https://doi.org/10.1039/ C8CS00121A

[3] Sheehan, F., Sementa, D., Jain, A., Kumar, M., Tayarani-Najjaran, M., Kroiss, D., Ulijn, R.V.: Peptide-Based Supramolecular Systems Chemistry. Chemical Reviews 121(22), 13869–13914 (2021) https://doi.org/10.1021/ acs.chemrev.1c00089

[4] Makam, P., Gazit, E.: Minimalistic peptide supramolecular co-assembly: Expanding the conformational space for nanotechnology. Chemical Society reviews 47(10), 3406–3420 (2018) https://doi.org/10.1039/c7cs00827a

[5] Wehner, M., W¨urthner, F.: Supramolecular polymerization through kinetic pathway control and living chain

growth. Nature Reviews Chemistry 4(1), 38–53 (2020) https://doi.org/10.1038/s41570-019-0153-8

14

[6] Gupta, S., Singh, I., Sharma, A.K., Kumar, P.: Ultrashort Peptide Self-Assembly: Front-Runners to Transport Drug and Gene Cargos. Frontiers in Bioengineering and Biotechnology 8 (2020) https://doi.org/10.3389/fbioe. 2020.00504

[7] Li, S., Zou, Q., Xing, R., Govindaraju, T., Fakhrullin, R., Yan, X.: Peptide-modulated self-assembly as a versatile strategy for tumor supramolecular nanotheranostics. Theranostics 9(11), 3249–3261 (2019) https: //doi.org/10.7150/thno.31814

[8] Ashworth, C.: Plastics from proteins. Nature Reviews Chemistry 6(3), 165–165 (2022) https://doi.org/10.1038/

s41570-022-00367-9

[9] Boddula, R., Singh, S.P.: Peptide-based novel small molecules and polymers: Unexplored optoelectronic materials. Journal of Materials Chemistry C 9(37), 12462–12488 (2021) https://doi.org/10.1039/D1TC03375A

[10] Nguyen, V., Zhu, R., Jenkins, K., Yang, R.: Self-assembly of diphenylalanine peptide with controlled polariza- tion for power generation. Nature Communications 7(1), 13566 (2016) https://doi.org/10.1038/ncomms13566

[11] Kingma, D.P., Welling, M.: Auto-Encoding Variational Bayes. arXiv (2022). https://doi.org/10.48550/arXiv.

1312.6114

[12] Sohn, K., Yan, X., Lee, H.: Learning structured output representation using deep conditional generative models. In: Proceedings of the 29th International Conference on Neural Information Processing Systems - Volume 2. NIPS’15, vol. 2, pp. 3483–3491. MIT Press, Cambridge, MA, USA (2015)

[13] Das, P., Wadhawan, K., Chang, O., Sercu, T., Santos, C.D., Riemer, M., Chenthamarakshan, V., Padhi, I., Mojsilovic, A.: PepCVAE: Semi-Supervised Targeted Design of Antimicrobial Peptide Sequences. arXiv (2018). https://doi.org/10.48550/arXiv.1810.07743

[14] Szymczak, P., Mo˙zejko, M., Grzegorzek, T., Jurczak, R., Bauer, M., Neubauer, D., Sikora, K., Michal- ski, M., Sroka, J., Setny, P., Kamysz, W., Szczurek, E.: Discovering highly potent antimicrobial peptides with deep generative model HydrAMP. Nature Communications 14(1), 1453 (2023) https://doi.org/10.1038/ s41467-023-36994-z

[15] Ivanov, O., Figurnov, M., Vetrov, D.: Variational Autoencoder with Arbitrary Conditioning. In: International

Conference on Learning Representations (2018)

[16] Ramchandran, S., Tikhonov, G., L¨onnroth, O., Tiikkainen, P., L¨ahdesm¨aki, H.: Learning conditional varia- tional autoencoders with missing covariates. Pattern Recognition 147, 110113 (2024) https://doi.org/10.1016/ j.patcog.2023.110113

[17] Wang, J., Liu, Z., Zhao, S., Tengyan Xu, Wang, H., Li, S.Z., Li, W.: Deep Learning Empowers the Discovery of Self-Assembling Peptides with Over 10 Trillion Sequences. Advanced Science 10(31), 2301544 (2023) https: //doi.org/10.1002/advs.202301544

[18] Liu, Z., Wang, J., Luo, Y., Zhao, S., Li, W., Li, S.Z.: Efficient prediction of peptide self-assembly through sequential and graphical encoding. Briefings in Bioinformatics 24(6), 409 (2023) https://doi.org/10.1093/bib/ bbad409

[19] van Teijlingen, A., Tuttle, T.: Beyond Tripeptides Two-Step Active Machine Learning for Very Large Data sets. Journal of Chemical Theory and Computation 17(5), 3221–3232 (2021) https://doi.org/10.1021/acs.jctc. 1c00159

[20] Frederix, P.W.J.M., Scott, G.G., Abul-Haija, Y.M., Kalafatovic, D., Pappas, C.G., Javid, N., Hunt, N.T., Ulijn, R.V., Tuttle, T.: Exploring the sequence space for (tri-)peptide self-assembly to design and discover new hydrogels. Nature Chemistry 7(1), 30–37 (2015) https://doi.org/10.1038/nchem.2122

[21] Njirjak, M., ˇZuˇzi´c, L., Babi´c, M., Jankovi´c, P., Otovi´c, E., Kalafatovic, D., Mauˇsa, G.: Reshaping the discovery of self-assembling peptides with generative AI guided by hybrid deep learning. Nature Machine Intelligence 6(12), 1487–1500 (2024) https://doi.org/10.1038/s42256-024-00928-1

15

[22] Mathur, D., Kaur, H., Dhall, A., Sharma, N., Raghava, G.P.S.: SAPdb: A database of short peptides and the corresponding nanostructures formed by self-assembly. Computers in Biology and Medicine 133, 104391 (2021) https://doi.org/10.1016/j.compbiomed.2021.104391

[23] Wang, J., Liu, Z., Zhao, S., Zhang, Y., Xu, T., Li, S.Z., Li, W.: Aggregation Rules of Short Peptides. JACS

Au 4(9), 3567–3580 (2024) https://doi.org/10.1021/jacsau.4c00501

[24] Rey, J., Murail, S., de Vries, S., Derreumaux, P., Tuffery, P.: PEP-FOLD4: A pH-dependent force field for peptide structure prediction in aqueous solution. Nucleic Acids Research 51(W1), 432–437 (2023) https://doi. org/10.1093/nar/gkad376

[25] Zardecki, C., Dutta, S., Goodsell, D.S., Lowe, R., Voigt, M., Burley, S.K.: PDB-101: Educational resources supporting molecular explorations through biology and medicine. Protein Science 31(1), 129–140 (2022) https: //doi.org/10.1002/pro.4200

[26] Dehsorkhi, A., Castelletto, V., Hamley, I.W.: Self-assembling amphiphilic peptides. Journal of Peptide Science

20(7), 453–467 (2014) https://doi.org/10.1002/psc.2633

[27] Zhang Lab: FASTA Format. https://zhanggroup.org/FASTA/

[28] Lim, J., Ryu, S., Kim, J.W., Kim, W.Y.: Molecular generative model based on conditional variational autoen- coder for de novo molecular design. Journal of Cheminformatics 10(1), 31 (2018) https://doi.org/10.1186/ s13321-018-0286-7

[29] Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., Costa, A.d.S., Fazel-Zarandi, M., Sercu, T., Candido, S., Rives, A.: Language Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction. bioRxiv (2022). https://doi.org/10.1101/2022.07.20.500902

[30] Stanger, H.E., Syud, F.A., Espinosa, J.F., Giriat, I., Muir, T., Gellman, S.H.: Length-dependent stability and strand length limits in antiparallel beta -sheet secondary structure. Proceedings of the National Academy of Sciences of the United States of America 98(21), 12015–12020 (2001) https://doi.org/10.1073/pnas.211536998

[31] Fam´ılia, C., Dennison, S.R., Quintas, A., Phoenix, D.A.: Prediction of Peptide and Protein Propensity for

Amyloid Formation. PLoS ONE 10(8), 0134679 (2015) https://doi.org/10.1371/journal.pone.0134679

[32] Li, T., Ren, X., Luo, X., Wang, Z., Li, Z., Luo, X., Shen, J., Li, Y., Yuan, D., Nussinov, R., Zeng, X., Shi, J., Cheng, F.: A Foundation Model Identifies Broad-Spectrum Antimicrobial Peptides against Drug-Resistant Bacterial Infection. Nature Communications 15(1), 7538 (2024) https://doi.org/10.1038/s41467-024-51933-2

[33] Wang, L., Schwing, A.G., Lazebnik, S.: Diverse and Accurate Image Description Using a Variational Auto- Encoder with an Additive Gaussian Encoding Space. arXiv (2017). https://doi.org/10.48550/arXiv.1711.07068

[34] Lavda, F., Gregorov´a, M., Kalousis, A.: Improving VAE Generations of Multimodal Data through Data-

Dependent Conditional Priors. arXiv (2019). https://doi.org/10.48550/arXiv.1911.10885

[35] Thaler, S., Zavadlav, J.: Learning neural network potentials from experimental data via Differentiable

Trajectory Reweighting. Nature communications 12(1), 6884 (2021)

[36] R¨ocken, S., Zavadlav, J.: Accurate machine learning force fields via experimental and simulation data fusion. npj Computational Materials 10(1), 69 (2024) https://doi.org/10.1038/s41524-024-01251-4 arXiv:2308.09142 [physics]

[37] R¨ocken, S., Burnet, A.F., Zavadlav, J.: Predicting Solvation Free Energies with an Implicit Solvent Machine

Learning Potential. arXiv (2025). https://doi.org/10.48550/arXiv.2406.00183

[38] Coste, A., Slejko, E., Zavadlav, J., Praprotnik, M.: Developing an Implicit Solvation Machine Learning Model for Molecular Simulations of Ionic Media. Journal of Chemical Theory and Computation 20(1), 411–420 (2024) https://doi.org/10.1021/acs.jctc.3c00984

16

[39] Thaler, S., Stupp, M., Zavadlav, J.: Deep Coarse-grained Potentials via Relative Entropy Minimization. The Journal of Chemical Physics 157(24), 244103 (2022) https://doi.org/10.1063/5.0124538 arXiv:2208.10330 [physics]

[40] Cock, P.J.A., Antao, T., Chang, J.T., Chapman, B.A., Cox, C.J., Dalke, A., Friedberg, I., Hamelryck, T., Kauff, F., Wilczynski, B., de Hoon, M.J.L.: Biopython: Freely available Python tools for computational molecular biology and bioinformatics. Bioinformatics 25(11), 1422–1423 (2009) https://doi.org/10.1093/bioinformatics/ btp163

[41] Hekkelman, M.: Mkdssp: Calculate Secondary Structure for Proteins in a PDB File | Dssp Commands | Man

Pages | ManKier. https://www.mankier.com/1/mkdssp

[42] Eisenberg, D., Schwarz, E., Komaromy, M., Wall, R.: Analysis of membrane and surface protein sequences with the hydrophobic moment plot. Journal of Molecular Biology 179(1), 125–142 (1984) https://doi.org/10.1016/ 0022-2836(84)90309-7

[43] Hanwell, M.D., Curtis, D.E., Lonie, D.C., Vandermeersch, T., Zurek, E., Hutchison, G.R.: Avogadro: An advanced semantic chemical editor, visualization, and analysis platform. Journal of Cheminformatics 4(1), 17 (2012) https://doi.org/10.1186/1758-2946-4-17

[44] Abraham, M.J., Murtola, T., Schulz, R., P´all, S., Smith, J.C., Hess, B., Lindahl, E.: GROMACS: High perfor- mance molecular simulations through multi-level parallelism from laptops to supercomputers. SoftwareX 1–2, 19–25 (2015) https://doi.org/10.1016/j.softx.2015.06.001

[45] Souza, P.C.T., Alessandri, R., Barnoud, J., Thallmair, S., Faustino, I., Gr¨unewald, F., Patmanidis, I., Abdizadeh, H., Bruininks, B.M.H., Wassenaar, T.A., Kroon, P.C., Melcr, J., Nieto, V., Corradi, V., Khan, H.M., Doma´nski, J., Javanainen, M., Martinez-Seara, H., Reuter, N., Best, R.B., Vattulainen, I., Monticelli, L., Peri- ole, X., Tieleman, D.P., de Vries, A.H., Marrink, S.J.: Martini 3: A general purpose force field for coarse-grained molecular dynamics. Nature Methods 18(4), 382–388 (2021) https://doi.org/10.1038/s41592-021-01098-3

[46] Sasselli, I.R., Coluzza, I.: Assessment of the MARTINI 3 Performance for Short Peptide Self-Assembly. Journal

of Chemical Theory and Computation 20(1), 224–238 (2024) https://doi.org/10.1021/acs.jctc.3c01015

[47] Kroon, P.C., Grunewald, F., Barnoud, J., Tilburg, M., Brasnett, C., Souza, P.C.T., Wassenaar, T.A., Marrink, S.-J.J.: Martinize2 and Vermouth: Unified Framework for Topology Generation. eLife 12 (2025) https://doi. org/10.7554/eLife.90627.3

17