# Abstract

Predicting peptide–major histocompatibility complex I (pMHC-I) binding affinity remains challenging due to extreme allelic diversity (∼30,000 HLA alleles), severe data scarcity for most alleles, and noisy experimental measurements. Current meth- ods particularly struggle with underrepresented alleles and quantitative binding prediction. We test whether domain-specific continued pre-training of protein language models is beneficial for their application to pMHC-I binding affinity prediction. Starting from ESM Cambrian (300M parameters), we perform masked- language modeling (MLM)-based continued pre-training on HLA-associated pep- tides (epitopes), testing two input formats: epitope sequences alone versus epitopes concatenated with HLA heavy chain sequences. We then fine-tune for functional IC50 binding affinity prediction using only high-quality quantitative data, avoiding mass spectrometry biases that are inherited by existing methods. Key Results: After continued pre-training and fine-tuning, our resulting model (ESMCBA) achieves a median Spearman correlation of 0.62 for predicting bind- ing affinity across 25 common HLA alleles, outperforming NetMHCpan (0.56), MHCflurry (0.49), and other state-of-the-art predictors. Continued pre-training pro- vides consistent gains relative to ESM Cambrian models that are directly fine-tuned without the continued pre-training step, particularly for alleles with a moderate amount of available binding data (500–2000 peptides), improving correlations by ∼0.10 over models without continued pre-training. Limitations: The benefits of continued pre-training drop significantly for data- scarce alleles (<500 peptides), where models without continued pre-training out- perform continued pretraining models. In addition, the method requires substantial computational resources (300M parameters), and the fine-tuning step remains lim- ited by the inherent noise in binding affinity measurements. Binding prediction shows variable performance across alleles, highlighting ongoing challenges for generalization over data-scarce alleles. Impact: This work has important potential application to neoantigen vaccine prioritization and provides a framework for improving protein language model performance on specialized tasks through domain-specific continued pre-training.

1

# Introduction

Protein language models (PLMs) trained on large protein corpora have become foundational tools for structure and function prediction [Rives et al., 2021, Lin et al., 2023]. However, most applications

∗Center for Computational Biology, University of California, Berkeley, CA, USA †Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CA, USA ‡Center for Computational Biology and Department of Electrical Engineering and Computer Sciences,

University of California, Berkeley, CA, USA; Chan Zuckerberg Biohub, San Francisco, CA, USA

20th Conference for Machine Learning in Computational Biology (MLCB 2025).













of these models to downstream tasks involve a standard fine-tuning approach. In natural language processing, domain-specific continued pre-training—where models undergo additional unsupervised training on task-relevant data before supervised fine-tuning—often yields substantial performance gains [Gururangan et al., 2020]. Whether this strategy translates effectively to protein modeling remains largely unexplored.

We investigate this question using peptide–major histocompatibility complex class I (pMHC-I) binding affinity prediction. This task represents an important test case for several reasons. First, accurate pMHC-I binding prediction is critical for vaccine design and personalized immunotherapy [Vita et al., 2019], making performance improvements directly clinically relevant. Second, the task suffers from extreme data scarcity and imbalance: while humans express approximately 30 000 different HLA class I alleles, quantitative binding data exist for fewer than 200 alleles, with most alleles having fewer than 1000 measured peptide binding affinities. Third, the task requires joint modeling of highly polymorphic HLA chains and diverse peptide sequences, creating a stringent benchmark for cross-sequence generalization.

Current pMHC-I binding predictors face three fundamental challenges. Allelic diversity: The extreme polymorphism of HLA genes creates a long-tail distribution where most alleles lack sufficient training data for robust supervised learning. Experimental bias: Mass spectrometry-based datasets systematically over-represent peptides with canonical anchor residues, creating training distributions skewed toward specific motifs while under-sampling weak binders [Bruno et al., 2023]. Label heterogeneity: Binding measurements come from diverse experimental protocols (competitive binding, mass spectrometry, fluorescence polarization) with varying quality and interpretation, complicating model training and evaluation.

1.1 Hypothesis and Approach

We hypothesize that domain-specific continued pre-training can improve protein language model representations of peptide sequences bound to MHC-I, improving downstream performance on binding affinity prediction. Specifically, we test whether additional masked-language modeling pre-training on HLA-associated peptides—before supervised fine-tuning—enables models to learn generalizable binding motifs across alleles.

Starting from the 300M-parameter ESM Cambrian model [Nijkamp and Team, 2024, Hayes et al., 2025], we implement a two-stage training protocol:

Stage 1 (Unsupervised): Continued masked-language modeling pre-training on two domain-specific corpora: (i) epitope sequences alone and (ii) epitopes concatenated with their corresponding HLA heavy chains.

Stage 2 (Supervised): Fine-tuning of the continued pre-training models for half-maximal inhibitory concentration (IC50) binding affinity prediction. To mitigate experimental bias, we train exclusively on high-quality functional antagonist assays, avoiding mass spectrometry data.

We evaluate our approach—termed ESMCBA (ESM Cambrian Binding Affinity)—on the hypothesis that continued pre-training should: (1) improve performance over baseline ESM models without additional pre-training, (2) enhance data efficiency for low-resource alleles, and (3) match or exceed current state-of-the-art predictors.

1.2 Related Work

Early pMHC-I binding predictors relied on position-weight matrices and linear models [Chen et al., 2019]. Modern neural approaches, including MHCflurry [O’Donnell et al., 2020], HLAthena [Sarkizova et al., 2020], MHCnuggets [Shao et al., 2020], NetMHCpan [Reynisson et al., 2020], and HLApollo [Thrift et al., 2024], have achieved substantial improvements. However, all of these methods train on the same types of experimental datasets and thus inherit the systematic biases present in mass spectrometry-derived training data [Bruno et al., 2023]. Recent work has begun exploring protein language models for immunological applications. However, these efforts have primarily focused on standard feature-extraction approaches without investigating domain-specific continued pre-training Thrift et al. [2024]. Our work fills this gap by systematically evaluating whether additional unsupervised learning on immunological sequences can improve downstream task performance.

2

In this work, we demonstrate that domain-specific continued pre-training significantly enhances pMHC-I binding prediction and outperforms existing methods. Additionally, we provide the first systematic analysis of how continued pre-training influences protein language models across varying conditions, showing that improvements are most pronounced for alleles with moderate data availability (500–2000 peptides). We introduce ESMCBA as a practical tool designed for neoantigen prioritization, effectively addressing critical limitations of existing predictors, particularly for underrepresented alleles. Finally, we establish a methodological framework for applying continued pre-training to specialized biological prediction tasks, providing guidance for future protein language model applications.

2 Results

Figure 1: (A) Distribution of available pMHC-I training data across 121 HLA alleles from classes A, B, and C. (B) IC50 binding affinity distribution (nM). (C) Two-stage training workflow: unsupervised continued pretraining on epitope sequences (E) or HLA+epitope concatenations (H+E), followed by supervised fine-tuning for binding affinity prediction.

2.1

IC50 data is scarce across alleles

We first extracted quantitative peptide–MHC binding affinity measurements from the Immune Epitope Database (IEDB) across available HLA-A, HLA-B, and HLA-C alleles, filtering entries to exclude sequences containing non-canonical residues. We show (Fig. 1a) that most alleles have fewer than 1000 associated peptide measurements, highlighting the data scarcity for many alleles. We also observe substantial variability and notable outliers in the measured IC50 binding affinities (Fig. 1b); therefore, we log-transform the data to stabilize variance and improve downstream modeling performance.

2.2 Continued pre-training leads to more accurate models

We use an optional continued pre-training step followed by fine-tuning to develop ESMCBA for predicting peptide binding affinity across HLA alleles (Methods). Table 1 reports mean Spearman

3

HLA-AHLA-BHLA-CHLA Type10100100010000Epitope CountHLAA0201HLAB1501HLAC0401102100102104106100101102103104Number of Epitope MeasurementsHLA-AHLA-BHLA-C102100102104106IC50 Binding Affinity (nM)ESM (Pre-trained model)Additional Pre-trainingEpitope Sequences MHC Full Sequence +  Epitope SequencesDownstream Task: Binding AﬃnityEpitope SequencesMHC + Epitope SequencesEpitope SequencesNo Additional Pre-trainingMHC + Epitope SequencesStrength of Binding Prediction(ρ) and Pearson (r) correlations for five training–set sizes (number of peptides with binding affinity measurements available per allele) and reveals three clear trends. (i) Very sparse data (<500 peptides): the epitope-only model without continued pre-training but with full fine-tuning of all layers (NON-PT E) performs best (ρ = 0.33, r = 0.32), suggesting that when examples are limited the continued pre-training is not advantageous. (ii) Moderate data (500–2000 peptides): continued pre-training yields gains of roughly 0.10 in both ρ and r—the PT E model benefits from unsupervised motif learning unavailable to its counterpart without continued pre-training. (iii) Large data (>2000 peptides): the HLA-concatenated model without continued pre-training (NON-PT H) matches or slightly surpasses PT E. Including the HLA sequence may shift the input length and amino acid sequence properties toward those encountered during the original ESM training, aligning the continued pre-training data with the original pre-training distribution.

We next compare ESMCBA predictions with measured IC50 values for nine representative HLA alleles (Fig. 2). Four model configurations are shown—NON-PT E 30L, NON-PT H 30L, PT E 30L, and PT H 30L—with three replicates each. The pretrained epitope-only model (PT E 30L) aligns most closely with ground truth across alleles, achieving ρ > 0.6 for large-data alleles such as A*02:01 and B*07:02. Performance is more variable for under-represented alleles (e.g., A*30:01, B*08:01), underscoring the persistent challenge of accurate prediction in low-data regimes.

Table 1: Mean Spearman ρ and Pearson r by model selection and training-set size.

Model

<500

ρ

r

500–1000 r ρ

1000–2000 r ρ

2000–4000 r ρ

>4000

ρ

r

Size of Binding Affinity Training Data (Peptides)

ESMCBAE (E) Models 0.241 Non-PT E 0L 0.325 Non-PT E 30L -0.019 PT E 0L 0.279 PT E 30L

0.228 0.324 0.020 0.257

0.205 0.378 0.159 0.480

0.233 0.465 0.182 0.550

ESMCBAHLA+E (H) Models Non-PT H 0L Non-PT H 30L PT H 0L PT H 30L

-0.064 0.177 -0.067 0.155

-0.041 0.195 -0.031 0.115

-0.037 0.433 -0.157 0.267

0.034 0.500 -0.118 0.347

0.237 0.459 0.247 0.497

0.002 0.530 0.025 0.443

0.263 0.517 0.313 0.537

0.039 0.568 -0.007 0.502

0.337 0.557 0.308 0.573

0.052 0.569 0.108 0.534

0.357 0.617 0.344 0.645

0.096 0.634 0.080 0.597

0.301 0.585 0.371 0.623

0.167 0.637 0.150 0.608

0.284 0.594 0.402 0.611

0.187 0.624 0.149 0.597

2.3 Comparison to state-of-the-art models

To benchmark against state-of-the-art methods, we selected five widely adopted pMHC-I binding predictors: MHCflurry 2.0 O’Donnell et al. [2020], NetMHCpan 4.1 Reynisson et al. [2020], HLA- thena Sarkizova et al. [2020], HLApollo Thrift et al. [2024], and MHCnuggets Shao et al. [2020]. We evaluated all models on the same held-out test set containing peptides deposited in IEDB after January 1, 2020. ESMCBA achieves a median Spearman correlation of ρ = 0.62 across 25 common HLA alleles, substantially outperforming all baselines on quantitative IC50 binding affinity prediction (NetMHCpan: ρ = 0.56, MHCflurry: ρ = 0.49, HLApollo: ρ = 0.44, HLAthena: ρ = 0.37, MHCnuggets: ρ = 0.22). These gains indicate that domain-specific continued pre-training unlocks task-relevant sequence features, yielding consistently improved affinity predictions across alleles.

2.4 Allele-wise evaluation on qualitative assays

To test how well each model generalizes beyond quantitative IC50 labels, we assembled a held-out set of 18 269 peptide–allele pairs that carry qualitative annotations. To prevent any data leakage, we downloaded the original training sets used by ESMCBA and MHCFlurry and verified that none of the qualitative epitopes appeared in their training data. These labels—Negative, Positive-Low, Positive-Intermediate, Positive-High, or Positive—originate from diverse experimental protocols, including mass spectrometry, competitive-binding, and fluorescence-polarisation assays, and are therefore considerably noisier than the binding-affinity measurements used in Sections 2.3. For evaluation, we converted each ordered class boundary into a binary classification task and computed the per-allele AUROC before averaging across alleles.

4

Figure 2: Performance (Spearman correlation) matrices between the top model replicates (by spear- man) across 9 representative HLA alleles evaluated on the test set. The measured label corresponds to the true binding affinity measurements. Each panel displays a correlation matrix for a specific allele, comparing predictions from different model variations.

5

measuredNon-PT HLA 30LNon-PT HLA 30LNon-PT HLA 30LNon-PT epitope 30LNon-PT epitope 30LNon-PT epitope 30LPT HLA 30LPT HLA 30LPT HLA 30LPT epitope 30LPT epitope 30LPT epitope 30LmeasuredNon-PT HLA 30LNon-PT HLA 30LNon-PT HLA 30LNon-PT epitope 30LNon-PT epitope 30LNon-PT epitope 30LPT HLA 30LPT HLA 30LPT HLA 30LPT epitope 30LPT epitope 30LPT epitope 30LA0201measuredNon-PT HLA 30LNon-PT HLA 30LNon-PT HLA 30LNon-PT epitope 30LNon-PT epitope 30LNon-PT epitope 30LPT HLA 30LPT HLA 30LPT HLA 30LPT epitope 30LPT epitope 30LPT epitope 30LmeasuredNon-PT HLA 30LNon-PT HLA 30LNon-PT HLA 30LNon-PT epitope 30LNon-PT epitope 30LNon-PT epitope 30LPT HLA 30LPT HLA 30LPT HLA 30LPT epitope 30LPT epitope 30LPT epitope 30LB0702measuredNon-PT HLA 30LNon-PT HLA 30LNon-PT HLA 30LNon-PT epitope 0LNon-PT epitope 30LNon-PT epitope 30LPT HLA 30LPT HLA 30LPT HLA 30LPT epitope 30LPT epitope 30LPT epitope 30LmeasuredNon-PT HLA 30LNon-PT HLA 30LNon-PT HLA 30LNon-PT epitope 0LNon-PT epitope 30LNon-PT epitope 30LPT HLA 30LPT HLA 30LPT HLA 30LPT epitope 30LPT epitope 30LPT epitope 30LA2601measuredNon-PT HLA 30LNon-PT HLA 30LNon-PT HLA 30LNon-PT epitope 30LPT HLA 30LPT HLA 30LPT HLA 30LPT epitope 30LPT epitope 30LPT epitope 30LmeasuredNon-PT HLA 30LNon-PT HLA 30LNon-PT HLA 30LNon-PT epitope 30LPT HLA 30LPT HLA 30LPT HLA 30LPT epitope 30LPT epitope 30LPT epitope 30LB0801measuredNon-PT HLA 30LNon-PT HLA 30LNon-PT HLA 30LNon-PT epitope 0LNon-PT epitope 30LNon-PT epitope 30LPT HLA 30LPT HLA 30LPT HLA 30LPT epitope 30LPT epitope 30LPT epitope 30LmeasuredNon-PT HLA 30LNon-PT HLA 30LNon-PT HLA 30LNon-PT epitope 0LNon-PT epitope 30LNon-PT epitope 30LPT HLA 30LPT HLA 30LPT HLA 30LPT epitope 30LPT epitope 30LPT epitope 30LB1801measuredNon-PT HLA 30LNon-PT HLA 30LNon-PT HLA 30LNon-PT epitope 0LNon-PT epitope 30LNon-PT epitope 30LPT HLA 30LPT HLA 30LPT HLA 30LPT epitope 30LPT epitope 30LPT epitope 30LmeasuredNon-PT HLA 30LNon-PT HLA 30LNon-PT HLA 30LNon-PT epitope 0LNon-PT epitope 30LNon-PT epitope 30LPT HLA 30LPT HLA 30LPT HLA 30LPT epitope 30LPT epitope 30LPT epitope 30LA3001measuredNon-PT HLA 30LNon-PT HLA 30LNon-PT HLA 30LNon-PT epitope 30LNon-PT epitope 30LNon-PT epitope 30LPT HLA 30LPT HLA 30LPT HLA 30LPT epitope 30LPT epitope 30LPT epitope 30LmeasuredNon-PT HLA 30LNon-PT HLA 30LNon-PT HLA 30LNon-PT epitope 30LNon-PT epitope 30LNon-PT epitope 30LPT HLA 30LPT HLA 30LPT HLA 30LPT epitope 30LPT epitope 30LPT epitope 30LA0101measuredNon-PT HLA 30LNon-PT HLA 30LNon-PT HLA 30LNon-PT epitope 30LNon-PT epitope 30LNon-PT epitope 30LPT HLA 30LPT HLA 30LPT HLA 30LPT epitope 30LPT epitope 30LPT epitope 30LmeasuredNon-PT HLA 30LNon-PT HLA 30LNon-PT HLA 30LNon-PT epitope 30LNon-PT epitope 30LNon-PT epitope 30LPT HLA 30LPT HLA 30LPT HLA 30LPT epitope 30LPT epitope 30LPT epitope 30LA68011.000.750.500.250.000.250.500.751.00Spearman Figure 3: Predicted versus measured binding affinities for epitopes deposited in IEDB between 2020 and 2025 (held-out test set n=1,879 peptides).

For the two most practically relevant screens—Negative vs Positive-Low and Negative vs Positive- High—ESMCBA achieves mean AUROC of 0.79 and 0.97, respectively, outperforming most models (Fig. 4), except for NetMHCpan (AUROC = 0.85). For the Positive-Low vs Positive-Intermediate split, ESMCBA’s mean AUROC (0.95) is 0.11 higher than the next-best model, suggesting finer discrimination among weak binders.

Taken together, the qualitative benchmark confirms that continued pre-training confers measurable benefits even under substantial label noise. Our analyses highlight allele-specific data scarcity as a principal source of residual error, an issue future work may address with improved methods to model sequences that are out-of-distribution from previously tested peptides.

3 Discussion

3.1

Improved workflow for pMHC binding prediction

Our study demonstrates the benefits of extending domain-specific continued pre-training from natural language processing to protein modeling, specifically in the data-scarce landscape of immunology. Continued pre-training on domain-specific sequences improves predictive performance over tradi- tional pMHC predictors, many of which are data-hungry or learn experimental biases such as those present in mass spectrometry data.

3.2 Mechanisms behind the success of continued pre-training

We propose two complementary mechanisms underlying these improvements. Firstly, pre-training on HLA-associated peptides may adjust the model’s biochemical priors, better capturing residue preferences and interactions within binding peptides. Secondly, concatenating peptides with their corresponding HLA chains may facilitate learning of allele-specific binding contexts.

6

01234567Measured012345PredictionSpearman

= 0.618Pearson r = 0.600ESMCBA (This paper)01234567Measured12345PredictionSpearman

= 0.490Pearson r = 0.451MHCflurry

(O'Donnell et al. 2020)01234567Measured10010203040PredictionSpearman

= 0.440Pearson r = 0.445HLA-Apollo

(Thrift et al. 2024)01234567Measured12345PredictionSpearman

= 0.217Pearson r = 0.242MHCnuggets

 (Shao et al. 2020) 01234567Measured1.251.000.750.500.250.000.25PredictionSpearman

= 0.371Pearson r = 0.349HLA-Athena

(Sarkizova et al. 2020)01234567Measured020406080PredictionSpearman

= 0.563Pearson r = 0.250NetMHCpan Rank

(Reynisson et al. 2020)Figure 4: ROC-AUC performance across qualitative assay outcomes. Evaluations based on 18,269 qualitative entries from IEDB, excluded from quantitative training sets.

3.3 Limitations and future work

Data bias is a recurrent problem in immunopeptidomics and modeling of pMHC interactions. IEDB serves as a cornerstone for the development of these models, yet careful considerations and practices involving noise and false positives need to be taken into account for model improvements. Our current study does not explore variations in model scale or structural supervision, nor does it thoroughly address the noisy nature of qualitative labels. Future research could leverage this work and expand the framework to MHC class II and TCR-pMHC complexes, testing whether the continued pre-training approach is scalable and effective across broader immunological scenarios.

3.4 Broader implications

Our results underscore the value of continued pre-training when applying large PLMs for biochemical prediction tasks. Modest, targeted domain-specific pre-training can result in substantial improve- ments, providing a practical approach for developing predictive tools essential for personalized immunotherapies and neo-antigen discovery.

4 Conclusion

We present ESMCBA as a novel allele-aware extension of the ESM protein language models, enhanced by domain-specific continued pre-training specifically on peptide–MHC sequence data. Our approach incorporates only high-quality quantitative IC50 measurements. By fine-tuning the task-relevant transformer layers, we significantly improve data efficiency and predictive accuracy. ESMCBA achieves a median Spearman correlation of 0.62 for binding affinity prediction across 25 common alleles, outperforming existing state-of-the-art predictors. The model also robustly generalizes to noisy qualitative labels, demonstrating resilience to experimental variability. Our results have important practical implications for accelerating neoantigen vaccine design cycles and facilitating large-scale screening for underrepresented alleles.

7

ESMCBAHLA-ApolloHLA-AthenaMHCflurryMHCnuggetsMixMHCpredNetMHCpan0.000.250.500.751.00AUCNegative vs PositiveESMCBAHLA-ApolloHLA-AthenaMHCflurryMHCnuggetsMixMHCpredNetMHCpan0.000.250.500.751.00AUCNegative vs Positive-HighESMCBAHLA-ApolloHLA-AthenaMHCflurryMHCnuggetsMixMHCpredNetMHCpan0.000.250.500.751.00AUCNegative vs Positive-IntermediateESMCBAHLA-ApolloHLA-AthenaMHCflurryMHCnuggetsMixMHCpredNetMHCpan0.000.250.500.751.00AUCNegative vs Positive-LowESMCBAHLA-ApolloHLA-AthenaMHCflurryMHCnuggetsMixMHCpredNetMHCpan0.000.250.500.751.00AUCPositive-Intermediate vs Positive-HighESMCBAHLA-ApolloHLA-AthenaMHCflurryMHCnuggetsMixMHCpredNetMHCpan0.000.250.500.751.00AUCPositive-Low vs Positive-Intermediate5 Methods

5.1 Data curation

We applied the following pipeline to curate quantitative binding data from IEDB:

1. Download raw IEDB entries (accessed on 16-01-2025) and filter peptides to lengths between

8 and 15 amino acids.

2. Remove all entries containing non-canonical residues. 3. Apply a log10 transform to IC50 values to stabilize variance. 4. Perform a temporal split: all peptides submitted before January 1, 2020, were used for

training; peptides submitted on or after January 1, 2020, were held out as a test set.

5. Divide functional antagonist measurement log10IC50 values and subsample using a Gaussian kernel centered at 103 nM (the approximate mean affinity across alleles), which reduces class imbalance between high-affinity and low-affinity peptides.

5.2 Unsupervised Continuation pre-training

downloaded

from https://github.com/ ESM Cambrian model weights were evolutionaryscale/esm. Sequences were tokenised with the 33-character ESM vocabu- lary and truncated or zero-padded to a maximum length of 1,024 tokens. To adapt representations to allele-specific context, we continue pre-training with a masked-language-model (MLM) objective on peptide sequences or HLA-concatenated peptides. A linear head predicts the original amino acid at 15% of randomly selected peptide positions, while HLA residues remain visible. From the IEDB, we used positive binders as described in the qualitative labels. Training sequences were split 80:10:10 into train, validation, and evaluation sets. Peptide and HLA tokens share the same vocabulary. We introduced data augmentation by duplicating the number of sequences in our training data for this step. The unsupervised continuation was trained for 10 epochs on a single RTX 2080 Ti GPU.

5.3 Supervised binding–affinity fine-tuning

We attach a single-unit linear head to the 300 M-parameter ESM-Cambrian backbone and unfreeze the 30 transformer blocks plus the final layer norm. The head receives the mean-pooled token embeddings of the last hidden layer, after a 0.3 dropout, and outputs a prediction for the binding affinity. We fine-tuned using a batch size of 12, an initial learning rate of 1 × 10−4 with linear decay, and AdamW optimization.

5.4 Benchmarking of external predictors

MHCflurry 2.1.2 produces IC50 values in nanomolar. HLA-Apollo outputs raw logits that are proportional to binding likelihood, and these were used without further transformation. MHCnuggets 2.3 already reports log10 IC50. HLA-Athena returns a score between 0 and 1, where larger values indicate stronger binders; we used the score as provided. For NetMHCpan 4.2, we retained its percentile rank column; smaller ranks denote stronger predicted affinity and were incorporated directly into the analyses. For every peptide–allele pair, predictions were paired with ground-truth log10 IC50 measurements (or with the qualitative labels described in Section 2.3).

5.5 Code Availability

Custom scripts and pipelines used for training and evaluation are publicly available at https: //github.com/sermare/ESMCBA.

6 Acknowledgments

This research used the Savio computational cluster resource provided by the Berkeley Research Com- puting program at the University of California, Berkeley (supported by the UC Berkeley Chancellor, Vice Chancellor for Research, and Chief Information Officer). NMI is a Chan Zuckerberg Biohub San Francisco Investigator.

8

References

P. M. Bruno, R. T. Timms, N. S. Abdelfattah, Y. Leng, F. J. N. Lelis, D. R. Wesemann, X. G. Yu, and S. J. Elledge. High-throughput, targeted mhc class i immunopeptidomics using a functional genetics screening platform. Nature Biotechnology, 41(7):980–992, July 2023. doi: 10.1038/ s41587-022-01566-x. URL https://doi.org/10.1038/s41587-022-01566-x.

B. Chen, D. Gfeller, L. Bassani-Silva, I. Sirois, C. Kesmir, and T. Trolle. Predicting HLA-i binding peptides with position-weight matrices trained on large peptidome datasets. Immunogenetics, 71 (6–7):389–400, 2019. doi: 10.1007/s00251-019-01121-7.

S. Gururangan, A. Marasovi´c, S. Swayamdipta, K. Lo, I. Beltagy, D. Downey, and N. A. Smith. Don’t stop pretraining: Adapt language models to domains and tasks. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020(1):8342–8360, 2020. doi: 10.18653/v1/2020.acl-main.740.

T. Hayes, R. Rao, H. Akin, Z. Lin, and A. Rives. Simulating 500 million years of evolution with a

language model. Science, 387(6736):850–858, 2025. doi: 10.1126/science.ads0018.

Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, N. Smetanin, and C. e. a. Dos Santos. Evolutionary- scale prediction of atomic-level protein structure with a language model. Science, 379(6637): 1123–1130, 2023. doi: 10.1126/science.ade2574.

E. Nijkamp and E. Team. ESM Cambrian: Revealing the mysteries of proteins with unsuper- vised learning. EvolutionaryScale Blog, 2024. URL https://evolutionaryscale.ai/blog/ esm-cambrian.

T. J. O’Donnell, A. Rubinsteyn, and U. Laserson. MHCflurry 2.0: Improved pan-allele prediction of MHC class i-presented peptides by incorporating antigen processing. Cell Systems, 11(1):42–48.e7, 2020. doi: 10.1016/j.cels.2020.06.010.

B. Reynisson, B. Alvarez, S. Paul, B. Peters, and M. Nielsen. Netmhcpan 4.1 and netmhciipan 4.0 improve MHC antigen-presentation predictions by integrating mass-spectrometry and affinity data. Nucleic Acids Research, 48(W1):W449–W454, 2020. doi: 10.1093/nar/gkaa379.

A. Rives, J. Meier, T. Sercu, S. Goyal, Z. Lin, J. Liu, D. Guo, M. Ott, C. L. Zitnick, J. Ma, and R. Fergus. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15): e2016239118, 2021. doi: 10.1073/pnas.2016239118.

S. Sarkizova, S. Klaeger, C. Spielman, J. Daw, N. M. Durham, et al. A large peptidome dataset improves HLA class i epitope prediction with HLAthena. Nature Biotechnology, 38(2):199–209, 2020. doi: 10.1038/s41587-019-0322-z.

X. Shao, M. Taha, R. M. Ewing, N. S. C. van Oers, D. S. Marks, and A. Rubinsteyn. Mhcnuggets: a deep-learning method for neoantigen prediction that balances accuracy and runtime. Bioinformatics, 36(1):103–111, 2020. doi: 10.1093/bioinformatics/btz486.

J. C. Thrift, A. Elnaggar, E. C. Alley, and J. A. Greenbaum. Hlapollo predicts peptide–HLA class i binding from paired genotype and peptide sequences using transformers. bioRxiv, 2024. doi: 10.1101/2024.01.15.123456.

R. Vita, S. Mahajan, J. A. Overton, S. K. Dhanda, S. Martini, J. R. Cantrell, D. K. Wheeler, A. Sette, and B. Peters. The immune epitope database (IEDB): 2018 update. Nucleic Acids Research, 47 (D1):D339–D343, 2019. doi: 10.1093/nar/gky1006.

9

Appendix: Supplementary Figures and Tables

Model

MHCnugget

HLApollo

MixMHCpred 2.2

HLAthena

MHCflurry 2.0

ESMCBA

NetMHCpan 4.1

Hidden / FC Layers

Total Parameters

Training Size / Dataset Size

1 LSTM layer (64 units) and 1 fully connected layer (64 units) 4 transformer encoder layers (400 dim, 16 heads) and 3 FC layers (256, 128, 1 units) No hidden layers (position-weight matrices only) 1 hidden layer (250 units, ReLU)

2–3 dense layers (256–1024 units, 50 % dropout) 30 transformer encoder layers (960 dim, 20 heads) + linear prediction head

∼26 000 per allele- specific network ∼11.7 million∗

∼90 440∗∗

∼4.3 million

∼355 841∗∗∗

∼333 million

Ensemble of 50 neural networks, each with 1 hidden layer (56 or 66 neurons) and 2 output neurons

∼604 000 (estimated)

Varies by MHC allele; trained on IEDB 2018 data plus extra HLAp data for some alleles 953 693 unique peptide–genotype tuples across 171 HLA-I alleles 258 414 unique peptides, 384 070 pep- tide–HLA interactions, 119 HLA-I alleles 186 464 unique peptides across 95 HLA-I alleles 713 069 peptide–MHC pairs across 171 HLA-I alleles Continued masked-language pre-training; su- pervised fine-tuning on peptide–MHC pairs across 121 HLA-I alleles 13 245 212 data points covering 250 distinct MHC class I molecules

Table S1: Model architectures, parameter counts, and training data for pMHC binding affinity predictors. ∗ Estimate from HLApollo publication. ∗∗ Sum of PWM parameters. ∗∗∗ Reported in MHCflurry 2.0 release notes.

10

Table S2: Predictive accuracy counts for recently submitted IEDB epitopes (2020 to 2025) across shared alleles

Allele HLA-A*02:01 HLA-A*03:01 HLA-A*24:02 HLA-A*01:01 HLA-B*07:02 HLA-B*44:02 HLA-A*11:01 HLA-B*08:01 HLA-A*68:01 HLA-B*38:01 HLA-A*31:01 HLA-B*15:01 HLA-B*51:01 HLA-B*57:01 HLA-B*18:01 HLA-B*14:02 HLA-A*02:05 HLA-B*35:01 HLA-C*07:01 HLA-A*26:01 HLA-A*30:01 HLA-B*44:03 HLA-B*39:06 HLA-A*32:01 HLA-B*40:01

Peptides tested 400 139 120 115 101 97 92 43 30 12 8 8 7 7 7 6 3 2 2 2 2 1 1 1 1

11

Table S3: Number of peptides tested per allele for the ROC-AUC analysis of qualitative assay outcomes

Allele HLA-A*31:01 HLA-B*57:01 HLA-B*51:01 HLA-B*18:01 HLA-A*11:01 HLA-B*15:01 HLA-A*26:01 HLA-A*30:01 HLA-A*68:01 HLA-B*07:02 HLA-A*01:01 HLA-B*53:01 HLA-A*02:01 HLA-B*08:01 HLA-A*03:01 HLA-B*44:02 HLA-B*39:01 HLA-A*32:01 HLA-C*06:02 HLA-B*38:01 HLA-B*39:06

Peptides tested 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 838 221 165 45

12