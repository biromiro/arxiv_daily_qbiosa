# Abstract—Over the past decade, advances in generative model- ing, such as generative adversarial networks, masked autoencoders, and diffusion models, have significantly transformed biological research and discovery, enabling breakthroughs in molecule design, protein generation, drug discovery, and beyond. At the same time, biological applications have served as valuable testbeds for evaluating the capabilities of generative models. Recently, flow matching has emerged as a powerful and efficient alternative to diffusion-based generative modeling, with growing interest in its application to problems in biology and life sciences. This paper presents the first comprehensive survey of recent developments in flow matching and its applications in biological domains. We begin by systematically reviewing the foundations and variants of flow matching, and then categorize its applications into three major areas: biological sequence modeling, molecule generation and design, and peptide and protein generation. For each, we provide an in-depth review of recent progress. We also summarize commonly used datasets and software tools, and conclude with a discussion of potential future directions. The corresponding curated resources are available at https: //github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.

Index Terms—Flow Matching, Generative Modeling, Molecule Generation, Protein Generation, Computational Biology, Artificial Intelligence, Survey

I. INTRODUCTION

Flow Matching (FM) [1] has recently emerged as a powerful paradigm for generative modeling, offering a flexible and scal- able framework applicable across a wide range of domains, such as computer vision [1, 2], and natural language processing [3, 4]. By constructing a continuous probability trajectory between simple and complex distributions, FM provides an efficient and principled method to model high-dimensional, structured data. While FM has demonstrated strong performance in conventional generative tasks such as image, video, and language synthesis, its potential extends far beyond these domains. In particular, its ability to model diverse modalities while preserving structural and geometric constraints makes it especially well-suited for applications in biology and life sciences.

At the same time, biological and life science applications present a natural testbed for FM (Figure 1). These tasks, ranging from genomic sequence modeling [5, 6, 7], molecular graph generation [8, 9, 10], and protein structure predic- tion [11, 12, 13], to biomedical image synthesis [14, 15, 16, 17], are often high-dimensional, multimodal, and governed by strict structural, physical, or biochemical constraints. In fact, they have already served as benchmarks for validating the performance of various generative modeling paradigms, such

∗Equal Contributions. ⋄Corresponding authors. Preprint. Released July 2025.

Fig. 1. Flow Matching Meets Biological and Life Sciences. Flow matching serves as a powerful generative modeling paradigm for a wide range of biological and life science applications. Conversely, these domains offer rich and diverse tasks for evaluating and advancing flow matching techniques. In this survey, we first present state-of-the-art flow matching models and their variants, then categorize their applications into four major areas: sequence modeling, molecule generation, protein design, and other emerging biological applications.

as Generative Adversarial Networks [18, 19, 20], Masked Au- toencoders [21, 22, 23, 24], and Diffusion Models [25, 26, 27]. Compared to traditional rule-based simulations [28, 29, 30, 31] and physics-driven models [32, 33, 34, 35], which often suffer from limited scalability and reliance on expert-crafted rules, these machine-learning-based generative models offer a data- driven alternative that can scale to complex biological systems, adapt to diverse modalities, and generalize beyond handcrafted constraints [36, 37, 38, 39, 40, 41, 42, 43, 44]. By learning directly from empirical data, they enable the generation of biologically plausible outputs while significantly reducing the need for domain-specific assumptions. FM, as a newer yet promising alternative, inherits key advantages from these models such as expressiveness, scalability, and data efficiency, while introducing a more stable training objective based on continuous probability flows. Its ability to generate high- quality samples with fewer inference steps makes it particularly appealing for biological applications, where modeling precision and computational efficiency are both critical.

Interest in applying FM to biological problems is growing rapidly. As illustrated in Figure 2, we have observed a steadily growing trend in the number of FM-related publications, with a visible rise in bio-related applications. The first biological applications appeared at NeurIPS 2023 [45, 46], both focusing on molecule generation. This momentum continued with the introduction of FM-based protein generation models at ICLR 2024 [47], followed by further progress in biological sequence and peptide generation at ICML 2024. Beyond these milestones, 2024 and 2025 have seen the emergence













PREPRINT. JULY 2025.

2

Fig. 2. Trend of published papers on flow matching (FM) and its applications in biology and life sciences across major ML conferences from 2023 to 2025. The blue line indicates the total number of FM papers, while the orange line shows the subset focused on biological applications. Annotations highlight key milestones in FM and its adoption for molecule, sequence, and protein generation, illustrating the rapid growth and expanding interest in this area.

of increasingly specialized FM variants, such as categorical FM [48], rectified FM [49], and non-Euclidean formulations including Riemannian [50] and Dirichlet [51] FM. Many of these have begun to find applications in structural biology, molecular conformation modeling, and biomedical imaging. This upward trajectory highlights not only the methodological innovation within FM, but also its growing relevance in life science domains that demand high-dimensional, structure-aware generative modeling.

As both FM and its biological applications evolve, the landscape has become increasingly fragmented, making it difficult to keep track of key developments and emerging trends. This survey addresses this gap by providing the first comprehensive review of FM in the context of biology and life sciences. We begin with a systematic overview of FM methods and variants, and then categorize their biological applications into three core areas: biological sequence modeling, molecule generation and design, and protein generation. We also review auxiliary topics such as bioimage modeling and spatial transcriptomics, summarize commonly used datasets and tools, and conclude with open challenges and future directions. Our goal is to offer an accessible entry point for newcomers, while equipping experienced researchers with a clear map of the field’s current trajectory.

A. Challenges of Generative Modeling for Biology

Biological systems are among the most intricate and multi- faceted systems in the natural world [106, 107, 108], shaped by billions of years of evolution and governed by deeply

intertwined physical, chemical, and informational processes. Modeling such systems has long been a grand challenge across scientific disciplines, demanding tools that can reconcile preci- sion with flexibility [109, 110, 111, 112, 112, 113, 114]. The complexity of biological data and phenomena stems from a con- fluence of factors, with some of the most formidable challenges including: (i) the necessity to embed rich domain knowledge, ranging from physical laws to biochemical constraints, into generative models in a way that ensures structural and functional validity; (ii) the scarcity, incompleteness, and noise characteristic of real-world biological datasets, often resulting from expensive or error-prone experimental procedures; (iii) the inherently multi-scale and multi-modal nature of biological processes, which span atomic interactions to cellular behavior, and integrate diverse data types such as sequences, structures, and spatial-temporal signals; (iv) the increasing demand for controllable and condition-aware generation, where outputs must satisfy explicit biological properties or therapeutic objec- tives; and (v) the pressing need for models that are not only ac- curate but also computationally scalable and sample-efficient, especially in applications such as drug discovery or protein design where inference speed can be critical. Together, these challenges make it challenging for biology models.

FM, as a recently introduced generative modeling paradigm, holds strong potential for addressing the unique challenges of biological data. It learns a deterministic vector field to map a simple base distribution directly to complex target data via continuous probability trajectories. This yields several advantages particularly relevant to biological applications, such as faster and more stable sampling, easier conditioning on

ICLR 2023ICML 2023NeurIPS 2023ICLR 2024ICML 2024NeurIPS 2024ICLR 2025ICML 2025Conference Year0246810121416182022242628Number of PapersFirst FM paperFM for moleculeFM for proteinFM for sequence and peptidesKeeps growingKeeps growingMultisample FMEquivariant FMFirst non-Euclidean FMDirichlet FM, Remannian FMCategorical FM, Rectified FM, Discrete FMTrend of Flow Matching and its Applications in Biology and Life Science ResearchTotal Flow Matching PapersBio-related FM PapersPREPRINT. JULY 2025.

3

Flow Matching Meets Biology & Life Science

Conditional FM: [52] / [53] / [54] / [55]

General FM

Rectified FM: [56] / [57] / [55] / [49]

Flow Matching Basics (Sec. III)

Non-Euclidean FM: [50] / [58] / [59] / [60] / [61] / [62]

Discrete FM

CTMC: [63] / [59] / [60] / [4] / [64] / [65]

Simplex: [66] / [51] / [67]

Bio Sequence Modeling (Sec. IV)

DNA Sequence

[60] / [51] / [67]

RNA Sequence

[68] / [69] / [70]

Whole-Genome

[71] / [72] / [73]

Antibody Sequence

[74] / [75]

2D

[76] / [77] / [78]

Molecule Generation and Design (Sec. V)

SE(3)-equivariant: [79] / [45] / [80] / [45] / [81]

3D Modeling

Efficiency: [82] / [83] / [84] / [85] / [86]

Guided Generation: [87] / [88] / [89] / [90] / [91]

Conditional Molecule Design and Applications

Unconditional Generation

[92] / [93] / [94] / [95]

Backbone Generation

Co-design Generation

Conditional Generation

Motif-scaffolding Generation

Pocket & Binder Design

Conformer Prediction

Structure Prediction

Side-chain Packing

Docking Prediction

Peptide and Antibody Generation

Dynamic Cell Trajectory

[96] / [71] / [97] / [98]

Bio-Image Generation and Enhancement

[99] / [100] / [101]

Cellular Microenvironments from Spatial Transcriptomics

[102] / [103]

Neural Activities

[104] / [105]

Protein Generation (Sec. VI)

Other Biology Applications (Sec. VII)

Fig. 3. Overview of the survey taxonomy. We begin by introducing the foundations of flow matching, including its core models and variants. Our taxonomy then categorizes flow matching applications into major biological domains and tasks.

structured inputs, and the ability to incorporate geometric or physical priors into the modeling process. Since its introduction, a growing number of studies have explored the use of FM in tackling biological tasks. These early successes demonstrate not only the method’s versatility but also its capacity to model the structured, multimodal, and constraint-rich nature of biological systems, positioning FM as a compelling alternative to conventional generative frameworks in the life sciences.

B. Our contributions

This survey presents the first comprehensive review of FM and its applications in biology and life sciences. Our key contributions are summarized as follows:

• A Unified Taxonomy of Flow Matching Variants. We introduce a structured taxonomy of FM methodologies, spanning general FM, conditional and rectified FM, non- Euclidean and discrete FM, and hybrid variants.

• In-depth Survey of Biological Applications. We sys-

PREPRINT. JULY 2025.

4

TABLE I EXISTING SURVEYS RELATED TO THIS WORK. WE PRESENT THE FIRST COMPREHENSIVE SURVEY DEDICATED TO FLOW MATCHING AND ITS APPLICATIONS IN BIOLOGY AND LIFE SCIENCES.

Reference

Generative Modeling

Task Domain

Jabbar et al. (2021) [115] Xia et al. (2022) [116] Greener et al. (2022) [117] Li et al. (2023) [118] Yang et al. (2023) [27] Croitoru et al. (2023) [119] Liang et al. (2024) [120] Cao et al. (2024) [121] Guo et al. (2024) [26] Saad et al. (2024) [122] Tang et al. (2024) [123] Du et al. (2024) [124] Zhang et al. (2025) [125] Ours

Generative Adversarial Network Generative Adversarial Network Various ML and Generative Modeling Methods Autoencoder Diffusion Model Diffusion Model Variational Autoencoder Diffusion Model Diffusion Model Generative Adversarial Network Various Generative Modeling Methods Various Generative Modeling Methods Large Language Models Flow Matching

General Anomaly Detection Biology, including protein design and DNA sequence General, including Image Classification and NLP Tasks General, including CV, NLP, Multimodal Tasks Various Tasks in Computer Vision Recommendation General, including image, video and audio Generation Biology, including protein, molecular, gene-expression tasks Biomedical Image Synthesis De Novo Drug Design Molecular Design Biology and Chemistry, e.g., molecular, protein, genomic tasks Various Tasks in Biology and Life Science

tematically categorize and review the use of FM across three primary biological domains: biological sequence modeling, molecule generation and design, and protein generation. We further explore several other emerging applications beyond this scope.

• Comprehensive Benchmark and Dataset Survey. We compile and review widely used biological datasets, benchmarks, and software tools adopted in FM research. • Trend, Challenges, and Emerging Directions. We con- textualize the evolution of FM through bibliometric trends and identify key methodological innovations. We further analyze domain-specific modeling challenges which may motivate new FM research directions.

• Bridging Modeling and Biology Communities. By mapping methodological advances in FM to diverse biological challenges, we offer a cross-disciplinary bridge that connects the machine learning community developing FM algorithms with the biological sciences community seeking powerful generative tools.

applications in biological domains, this survey aims to fill a critical gap in the literature.

D. Outline of the survey

To provide a comprehensive understanding of FM in the context of biology and life sciences, this survey is organized into several key sections. We begin by introducing the funda- mental concepts and methodologies underlying FM in Section III, establishing a foundation for its application in biological contexts. Next, in Section IV, we delve into specific areas of application, starting with biology sequence generation, followed by molecule generation and design in Section V, and then peptide and protein generation in Section VI, each highlighting recent advancements and representative studies. In section VII, we also discuss other emerging applications of FM in biology. Finally, we conclude by outlining future research directions and potential challenges, aiming to inspire further exploration and innovation in this rapidly evolving field. Figure 3 presents the overall structure of this survey, with each section divided into various subtopics for a more detailed exploration.

C. Connection to existing survey

II. BACKGROUND

Existing related surveys can be broadly categorized into three groups. The first category focuses exclusively on generative modeling methodologies. These surveys either provide com- prehensive overviews of specific classes of generative models [115, 118, 121] or examine their applications within particular domains, such as computer vision [119], recommendation systems [120], and anomaly detection [116]. The second category surveys the use of generative models in biology prior to the advent of FM. For example, [124] reviews generative models for molecular design, [123] focuses on de novo drug design, and [117] provides a broad overview of machine learning methods in both predictive and generative biological modeling. Table I presents a comparison of existing surveys on generative modeling, highlighting their covered model classes and application domains. To the best of our knowledge, this work presents the first comprehensive survey dedicated to FM and its applications in biology and life sciences. By bridging recent developments in generative modeling with their emerging

Generative modeling seeks to learn a probability distribution pdata(x) from a dataset of examples {xi}N i=1, such that we can generate new samples ˆx ∼ pθ(x) that resemble real data. These models underpin advances in biology tasks ranging from molecular generation to protein design and cellular imaging [123, 124, 126, 127, 128], with AlphaFold [11, 12, 129] standing out as one of the most prominent and transformative examples, recognized with the Nobel Prize in 2024. AlphaFold leverages deep generative principles to predict protein 3D structures directly from amino acid sequences, a task that had challenged the field for decades [13, 114, 130]. By effectively modeling the conditional distribution over protein conformations, AlphaFold not only revolutionized protein structure prediction but also highlighted the broader potential of generative models to capture complex, structured biological phenomena at scale. In biology domains, data is often high- dimensional, multimodal, and governed by physical or bio- chemical constraints [131, 132, 133, 134], requiring generative

PREPRINT. JULY 2025.

5

TABLE II NOTATION USED IN GENERATIVE MODELING PARADIGMS.

Symbol

Description

x z pdata(x) pθ(x) fθ uθ(x, t) pt(x) ϵ LFM LDM

Data sample Latent variable True data distribution Model distribution Invertible function (flow) Velocity field in FM Intermediate distribution at time t Noise in diffusion model Flow Matching loss Diffusion model loss

models to strike a careful balance between validity, diversity, and interpretability. In this section, we provide a brief overview of the major paradigms in generative modeling, with the goal of establishing a conceptual and mathematical foundation for understanding more recent developments such as FM. For clarity and consistency, all symbols used throughout this paper are summarized in Table II.

A. Variational Autoencoder (VAE)

Variational Autoencoders (VAEs) [135, 136, 137, 138, 139] are a class of latent-variable generative models that aim to model the data distribution pdata(x) through a learned probabilistic decoder pθ(x|z), where z is a latent variable drawn from a prior p(z), typically a standard Gaussian. Since the true posterior p(z|x) is often intractable, VAEs introduce an approximate posterior qϕ(z|x), known as the encoder, and optimize the model using variational inference. The training objective is to maximize a variational lower bound, known as the evidence lower bound (ELBO), on the marginal log- likelihood of the data:

log pθ(x) ≥ Eqϕ(z|x) [log pθ(x|z)] − KL (qϕ(z|x)∥p(z))

term encourages accurate reconstruction of the The first input data from the latent variable z, while the second term regularizes the approximate posterior to stay close to the prior distribution. During training, the reparameterization trick is used to allow gradients to backpropagate through the sampling process, typically by expressing z ∼ qϕ(z|x) as z = µ(x)+σ(x)⊙ϵ, where ϵ ∼ N (0, I). However, VAEs often suffer from over-regularization and produce blurred outputs, especially in high-dimensional domains such as images and molecular graphs [140, 141, 142].

B. Generative Adversarial Network (GAN)

Generative Adversarial Networks (GANs) [18] are a class of implicit generative models that learn to generate realistic data by playing a two-player minimax game between two neural networks: a generator Gθ and a discriminator Dϕ. The generator maps noise samples z ∼ p(z), typically drawn from a simple prior such as a Gaussian, into synthetic data samples Gθ(z). The discriminator attempts to distinguish between real samples x ∼ pdata and generated samples Gθ(z). The original GAN objective is formulated as:

min Gθ

max Dϕ

Ex∼pdata[log Dϕ(x)] + Ez∼p(z)[log(1 − Dϕ(Gθ(z)))]

GANs are known to suffer from several practical challenges, including training instability, sensitivity to hyperparameters, and mode collapse Numerous variants have been proposed to improve training dynamics and sample diversity, such as Wasserstein GANs [143], Least-Squares GANs [144], and conditional GANs [145]. In biological applications, GANs have been used for generating realistic cell images [146], synthesizing gene expression profiles [20, 147], and augmenting scarce datasets [148]. Despite their limitations, their ability to capture complex data distributions without explicit density estimation makes them a compelling choice for modeling high- dimensional biological data [149].

C. Flow-Based Model

Flow-based models (also known as normalizing flows) [150, 151] are a family of generative models that construct complex data distributions by applying a sequence of invertible transformations to a simple base distribution, typically a standard Gaussian distribution. Given a base variable z ∼ pz(z), a flow model learns an invertible mapping x = fθ(z) such that the model distribution pθ(x) can be computed exactly via the change-of-variables formula:

log pθ(x) = log pz(f −1

θ

(x)) + log

(cid:12) (cid:12) (cid:12) (cid:12)

det

(x)

(cid:18) ∂f −1 θ ∂x

(cid:19)(cid:12) (cid:12) (cid:12) (cid:12)

The goal is to train the parameters θ to maximize the log-likelihood of the observed data under this model. The invertibility of fθ allows for exact and tractable likelihood computation, efficient sampling, and deterministic inference. To ensure both tractability and expressivity, flow models are often constructed as a composition of multiple simple bijective transformations:

fθ = fK ◦ fK−1 ◦ · · · ◦ f1

(1)

Each component fk is designed to allow efficient computation of the Jacobian determinant and its inverse. Representative architectures include NICE [152], RealNVP [153], Glow [154], and Masked Autoregressive Flows (MAF) [155], which utilize affine coupling layers or autoregressive transforms to maintain invertibility.

However, the invertible constraint on fθ along with the need to compute the determinant of the Jacobian ∂fθ(x) imposes significant constraints on model expressiveness and design flexibility. Continuous normalizing flow (CNF) [156] address these limitations by replacing the discrete sequence of transformations (Eq. (1)) with a continuous-time dynamic system dx dt = f (x(t), t). This formulation leads to a more efficient computation of the log-density change:

∂x

∂ log p(x(t)) ∂t

= −Tr

(cid:18) df

(cid:19)

dx(t)

Notably, the vector field f is not required to be invertible.

CNFs serve as a foundational building block for FM. While CNFs allow for more expressive modeling, their training via maximum likelihood still demands computationally expensive ODE solvers. A core motivation behind flow matching is to simplify the training of ODE-based generative models, without sacrificing the benefits of continuous-time formulations.

PREPRINT. JULY 2025.

6

D. Diffusion Models (DM)

III. FLOW-MATCHING BASICS

Diffusion models [25, 157, 158, 159, 160] are a family of likelihood-based generative models that generate data by reversing a gradual noising process. They define a forward process that incrementally transforms data into noise, and parameterize a neural network to fit the groundtruth reverse process, recovering data from noise step by step.

a) Forward Process: The forward process defines a sequence of latent variables {xt}T t=0, which are the gradually corrupted version of the clean data x0 ∼ pdata. A typical forward process is formulated as a set of Gaussian distributions conditioned on the previous step:

q(xt|xt−1) = N (xt; (cid:112)1 − βtxt−1, βtI)

where {βt} is called noise schedule. Usually, the distribution of the corrupted data at any time t has a closed form:

q(xt|x0) = N (xt;

√

¯αtx0, (1 − ¯αt)I),

t (cid:89)

¯αt =

(1 − βs)

s=1

b) Training: Similar to many likelihood-based models, negative log-likelihood is a canonical choice of the loss function [25, 157, 161]. Beyond that, cross-entropy or square error are also widely used [25, 162]. Based on that, neural networks (NNs) are used to parameterize various components of the diffusion process, such as to predict the data [163], predict the noise [25], and predict the score [160]. The following unweighted regression loss for predicting the noise is a popular example:

LDM = Ex0,t,ϵ

√

(cid:104)

∥ϵ − ϵθ(xt, t)∥2(cid:105) √

xt =

¯αtx0 +

1 − ¯αtϵ, ϵ ∼ N (0, I)

c) Generation: Equipped with the NN-parameterized component, the reverse process of the diffusion process is used for generation. For example, the reverse process with the NN-predicted noise ϵθ can denoise the Gaussian noise xT ∼ N (0, I) gradually: (cid:18)

(cid:19)

xt−1 =

√

1 1 − βt

βt√

xt −

1 − ¯αt

ϵθ(xt, t)

+ noise

A well-known limitation of diffusion models is their slow sampling process, which often requires hundreds of itera- tive steps. To address this inefficiency, several acceleration techniques have been proposed, including the adoption of tailored numerical solvers [164], model distillation [163], and continuous-time formulations [160, 161]. Notably, Probability flow ODE [159] and DDIM [160] demonstrate that there exists a deterministic ODE whose solution shares the same marginal distributions as the reverse-time stochastic differential equation (SDE) used in diffusion models. This observation is conceptually aligned with the idea behind flow matching (FM), although both probability flow ODE and DDIM remain trained using the standard loss functions of diffusion models, such as the evidence lower bound (ELBO).

In this section, we provide background knowledge on flow- matching (FM) models, including general FM and discrete FM.

A. General Flow-Matching

Flow-matching is a continuous-time generative framework that generalizes diffusion models by regressing a vector field that transports one distribution into another [54]. In general, FM aims to construct a velocity field uθ(x, t) to transport a source p0 to a target p1 via the continuity equation:

∂pt ∂t

+ ∇ · (ptuθ(x, t)) = 0.

An FM can be trained by minimizing the squared loss between the neural velocity field uθ(x, t) and a reference velocity field u∗ t (x, t) as follows

LFM = Et∼[0,1],xt∼pt(x)∥u∗(xt, t) − uθ(xt, t)∥2.

(2)

Promising as it might be, directly optimizing the objective in Eq. (2) is impractical: the optimal velocity field u∗(x, t) en- codes a highly complex joint transformation between two high- dimensional distributions [165]. To overcome this challenge, conditional FM variants have been introduced to enable more tractable training (Paragraph III-A0a). Concurrently, rectified FM methods propose improved noise couplings along the straight-line probability path (Paragraph III-A0b). Finally, non- Euclidean FM extensions generalize the framework from flat Euclidean space to curved manifolds, accommodating data with intrinsic geometric structure (Paragraph III-A0c).

a) Conditional FM [52, 53, 54, 55]: To resolve the intractable u∗(x, t), conditional FM introduces a condition- ing variable z, e.g., class label, and define a conditional path p(x|t, z) such that the induced global path p(x|t) = (cid:82) z p(x|t, z)p(z)dz transforms p0 to pdata and the corresponding conditional velocity field has analytical form. A conditional FM can be trained by minimizing the quadratic loss between the neural velocity field uθ(x, t) and the conditional velocity field u∗

t (x, t, z) as follows

Et∼[0,1],xt∼pt(x|z),z∼pz ∥u∗(xt, t, z) − uθ(xt, t)∥2.

(3)

The training procedure involves sampling a conditioning variable z, e.g., via linear interpolation [52, 166] or Gaussian path [54], and random time t, constructing xt along the prescribed path, and minimizing the corresponding loss. Once the model is trained, the sampling/generation process is done by solving the learned ODE dx/dt = uθ(x, t) using an ODE solver from t = 0 (noise) to t = 1 (data). The key theoretical foundation of conditional FM is that the gradient of the FM objective in Eq. (2) is equivalent to gradient of the CFM objective in Eq. (3). Building upon the conditioning variable z, one can define velocity field in analytical forms with tractable training.

PREPRINT. JULY 2025.

7

TABLE III COMPREHENSIVE COMPARISON OF MAJOR GENERATIVE MODELING PARADIGMS.

Model Type

Training Objective

VAE GAN Diffusion Flow-Based Flow Matching

Likelihood Adversarial loss Likelihood Likelihood Velocity matching

Number of Function Evaluations

Low Low SDE solver-dependent Low ODE solver-dependent

Structured Data Support

Moderate (via extensions) Weak (limited geometry) Strong (SE(3), graph diffusion) Moderate (design-dependent) Strong (geometry-aware, equivariant)

b) Rectified FM [49, 55, 56, 57, 166]: Infinite probability path exist between source and target distributions that can be leveraged by conditional FM, rectified FM prefers the linear transport trajectory that best connect two distributions. [166] proposes to train a velocity field carrying each sample x0 to its paired target x1 along nearly-straight lines via

E(x0,x1)∼π

(cid:90) 1

0

∥uθ(xt, t) − (x1 − x0)∥2dt

where pi is a coupling of p0 and p1. It is shown that the optimal transport (OT) coupling provides a straight coupling for p0 and p1, simplifying the flow and reducing curliness [55, 57]. c) Non-Euclidean FM [50, 58, 60, 61, 62]: Non- Euclidean flows extend continuous flows to curved data spaces. For example, [62] introduce Riemannian Continuous Normalizing Flows, defining the generative flow by an ODE on the manifold to model flexible densities on spheres, tori, hyperbolic spaces, etc.. [61] propose Neural Manifold ODEs, integrating dynamics chart-by-chart (e.g. via local coordinate charts) so that the learned velocity field stays tangent to the manifold. More recently, [58] propose Riemannian FM by using geodesic distances as a “premetric” they derive a closed- form target vector field pushing a base distribution to the data without any stochastic diffusion or divergence term. On simple manifolds (e.g. spheres or hyperbolic space where geodesics are known) Riemannian FM is completely simulation-free, and even on general geometries it only requires solving a single ODE without calculating expensive score or density estimates. [60] introduce Fisher FM, treating categorical distributions as points on the probability simplex with the Fisher–Rao metric and transporting them along spherical geodesics. In general, Riemannian flows replace straight-line interpolations with intrinsic geodesics and explicitly account for the manifold’s metric (e.g. via the Riemannian divergence in the change-of- density). These works tackle the challenges of defining tangent vector fields and volume corrections on curved spaces via chart-based integration, metric-adjusted log-density formulas, or flow-matching losses that avoid divergence estimates. Overall, they enable scalable generative modeling on curved domains (spheres, Lie groups, statistical manifolds, etc.), respecting curvature in ways standard Euclidean FM cannot.

B. Discrete Flow-Matching

Discrete FM has emerged as a powerful paradigm for generative modeling over discrete data domains, such as sequences, graphs, and categorical structures, covering a wide range of biological objects [4, 162]. By extending the principles

of continuous FM to discrete spaces, DFM enables the design of efficient, non-autoregressive generative models. This section delves into two principal frameworks: Continuous-Time Markov Chain (CTMC)-based methods (Paragraph III-B0a) and simplex-based methods (Paragraph III-B0b).

a) Continuous-Time Markov Chain (CTMC): CTMC- based approaches model the generative process as a continuous- time stochastic evolution over discrete states, leveraging the mathematical framework of continuous-time Markov chains to define and learn probability flows. [63] utilizes CTMCs to model flows over discrete state spaces. This approach allows for the integration of discrete and continuous data, facilitating applications like protein co-design by enabling multimodal generative modeling. Fisher Flow [60] adopts a geometric perspective by considering categorical distributions as points on a statistical manifold endowed with the Fisher-Rao metric. This approach leads to optimal gradient flows that minimize the forward Kullback-Leibler divergence, improving the quality of generated discrete data. [65] expanded the design space of discrete generative models by allowing arbitrary discrete probability paths within the CTMC framework. This holistic approach enables the use of diverse corruption processes, providing greater flexibility in modeling complex discrete data distributions. DeFog [64] is a discrete FM framework tailored for graph generation. By employing a CTMC-based approach, DeFoG achieves efficient training and sampling, outperforming existing diffusion models in generating realistic graph.

b) Simplex-based discrete FM: Simplex-based methods operate within the probability simplex, modeling flows over continuous relaxations of discrete distributions. These ap- proaches often employ differentiable approximations to handle the challenges posed by discrete data. SimplexFlow [167] combines continuous and categorical flow matching for 3D de novo molecule generation, where intermediate states are guaranteed to reside on the simplex. Dirichlet FM [51] utilizes mixtures of Dirichlet distributions to define probability paths over the simplex, addressing discontinuities in training targets and enables efficient. α-flow [59] unifies various continuous- state discrete FM models under the lens of information geometry. By operating on different α-representations of probabilities, this framework optimizes the generalized kinetic energy, enhancing performance in tasks such as image and protein sequence generation. STGFlow [67] employs a Gumbel- Softmax interpolant with a time-dependent temperature for controllable biological sequence generation, which includes a classifier-based guidance mechanism that enhances the quality and controllability of generated sequences.

PREPRINT. JULY 2025.

8

IV. SEQUENCE MODELING

FM has emerged as a powerful framework for biological sequence generation, offering deterministic and controllable modeling of discrete structures such as DNA, RNA, and whole- genome data. In this section, we survey different FM models designed for biological sequence generation, including DNA sequence, RNA sequence, whole-genome modeling, and anti- body design. By leveraging continuous transformations, flow matching enables efficient generation of sequences conditioned on various biological constraints and properties.

A. DNA Sequence Generation

Early deep generative models, e.g. GANs or autoregres- sive models, struggled to satisfy the complex constraints of functional genomics sequences. FM models provide natural solutions to bridge this gap by mapping discrete nucleotide sequences into continuous probabilistic spaces for training [51]. Instead of simulating a stochastic diffusion [51], FM models directly train a continuous vector field that transports a simple base distribution, e.g., uniform distribution over nucleotides, into the empirical DNA data distribution.

Fisher-Flow [60] introduces a geometry-based flow matching approach, which treats discrete DNA sequences as points on a statistical manifold endowed with the Fisher–Rao metric. By allowing for continuous reparameterization of discrete data, probability mass is transported along optimal geometric paths on the positive orthant of a hypersphere, achieving state-of- the-art performance on DNA promoter and enhancer sequence generation benchmarks compared to earlier diffusion-based and flow-based models.

Besides categorical distribution, Dirichlet distribution is adopted to handle discrete sequences. Dirichlet Flow [51] utilizes mixtures of Dirichlet distributions to define probability paths on the simplex, addressing discontinuities and pathologies in naive linear flow matching. Dirichlet Flow enables one-step DNA sequence generation and achieves superior distributional metrics and target-specific design performance compared to prior models on complex DNA design tasks.

In addition, STGFlow [67] proposes straight-through guid- ance, combining Gumbel-Softmax flows with classifier-based guidance to steer the generation process toward desired sequence properties, facilitating controllable de novo DNA sequence generation.

B. RNA Sequence Generation

Flow matching has recently been applied to RNA sequence and structure design. Rather than focusing solely on se- quence generation, existing FM methods prioritize structural fidelity, enabling advanced applications in inverse folding, protein-conditioned design, and ensemble backbone sampling. RNACG [68] introduces a versatile flow-matching frame- work for conditional RNA generation that supports tasks ranging from 3D inverse folding to translation efficiency prediction. RNAFlow [69] couples an RNA inverse-folding module with a pretrained structure predictor to co-generate RNA sequences and their folded structures in the context

of bound proteins. RiboGen [70] develops the first deep network to jointly synthesize RNA sequences and all-atom 3D conformations via equivariant multi-flow architectures. Most recently, RNAbpFlow [168] presents a SE(3)-equivariant flow- matching model that conditions on both sequence and base-pair information to sample diverse RNA backbone ensembles.

C. Whole-Genome Modeling

At the whole-genome level, flow matching has been applied to model single-cell genomics data. GENOT [71] employs entropic Gromov-Wasserstein flow matching to learn mappings between cellular states in single-cell transcriptomics, facilitating studies of cell development and drug response. cellFlow [72] proposes a generative flow-based model for single-cell count data that operates directly in raw transcription count space, pre- serving the discrete nature of the data. CFGen [73] introduces a flow-based conditional generative model capable of generating multi-modal and multi-attribute single-cell data, addressing tasks such as rare cell type augmentation and batch correction.

D. Antibody Sequence Generation

FM has also been utilized for antibody sequence generation. IgFlow [74] proposes a SE(3)-equivariant FM model for de novo antibody variable region generation (heavy/light chains and CDR loops). IgFlow supports unconditional antibody sequence-structure generation and conditional CDR loop in- painting, producing structures comparable to those from a diffusion-based model while achieving higher self-consistency in conditional designs; it also offers efficiency benefits like faster inference and better sample efficiency than the diffusion counterpart. dyAb [75] proposes a flexible antibody design FM, which integrates coarse-grained antigen–antibody interface alignment with fine-grained flow matching on both sequences and structures. By explicitly modeling antigen conformational changes (via AlphaFold2 predictions) before binding, dyAb significantly improves the design of high-affinity antibodies in cases where target antigens undergo dynamic structural shifts. These advancements demonstrate the versatility of flow matching in modeling complex biological sequences and structures, providing a unified framework for deterministic and controllable generation across various biological domains.

V. MOLECULE GENERATION

Molecule generation is a fundamental task in biological mod- eling, playing a crucial role in drug discovery, material design, and understanding molecular interactions [169, 170, 171]. The ability to generate novel molecules with desired properties has significant implications for both theoretical and applied research in life sciences [172, 173]. Traditional approaches, such as rule-based simulations and heuristic algorithms, often face challenges in scalability and diversity [174, 175]. In contrast, generative models, including flow matching, offer a data-driven approach to efficiently explore the vast chemical space [26, 176, 177].

In this section, we review recent advancements in molecule generation using flow matching techniques. We focus on

PREPRINT. JULY 2025.

9

methods that leverage continuous probability flow trajectories to generate novel molecular structures and properties, highlighting how flow matching has enhanced molecule generation.

A. 2D Molecule Generation

Fig. 4. 2D graph representations of example molecules generated from the GEOM-Drugs [178] (left two) and QM9 [179] (right two) datasets. Each molecule is visualized as a 2D graph, where atoms are nodes and chemical bonds are edges, capturing both structural and topological properties.

Although real-world molecules are inherently three- dimensional objects, researchers often simplify the problem by using 2D graph-based molecular modeling when the 3D structure is not the primary focus [180, 181, 182]. This approach offers several advantages, including increased compu- tational efficiency and reduced information requirements during inference.

Flow matching on graph data remains relatively unexplored, as the concept of flow matching itself is still under development. Nevertheless, existing studies often use 2D molecule generation as a preliminary test case to evaluate newly proposed flow matching variants. For instance, Eijkelboom et al. [77] combine flow matching with variational inference to introduce Varia- tional Flow Matching for graph generation and CatFlow for handling categorical data. Additionally, GGFlow [76] presents a discrete flow matching generative model that integrates optimal transport for molecular graphs. This model features an edge- augmented graph transformer, enabling direct communication among chemical bonds, thereby improving the representation of molecular structures. DeFoG [78] introduces a discrete formulation of flow matching tailored to the graph domain, explicitly decoupling the training and sampling phases to overcome inefficiencies in traditional diffusion-based models. By leveraging permutation-invariant graph matching objectives and exploring a broader sampling design space, DeFoG achieves strong empirical results on molecular graph generation with significantly fewer refinement steps.

B. 3D Molecule Generation

Fig. 5. 3D graph representations of example molecules generated from the GEOM-Drugs [178] (left two) and QM9 [179] (right two) datasets. Atoms are shown as nodes positioned in 3D Euclidean space, and bonds are represented as edges connecting them. These visualizations capture spatial geometry and stereochemistry important for molecular property prediction.

Generating accurate 3D molecular structures is a critical task in drug discovery and structural biology [183]. Unlike 2D graph- based approaches, which primarily capture atomic connectivity, 3D molecular representations inherently encode spatial infor- mation, including bond angles, torsions, and stereochemistry. This spatial fidelity is essential for modeling interactions such as molecular docking, binding affinity, and conformational stability. While 2D representations cannot distinguish between stereoisomers or capture geometric nuances, 3D methods accurately model spatial conformation, enabling a more precise understanding of molecular properties [169, 184, 185].

a) SE(3)-equivariant: To ensure physically meaningful and symmetry-consistent outputs, recent advancements have incorporated SE(3)-equivariant neural architectures into flow matching models. These models leverage the inherent sym- metries of molecular systems, modeling graph generation as a continuous normalizing flow over node and edge features. For instance, Megalodon [80] introduces scalable transformer models with basic equivariant layers, trained using a hybrid denoising objective to generate 3D molecules efficiently, achieving state-of-the-art results in both structure generation and energy benchmarks. EquiFM [45] further improves the generation of 3D molecules by combining hybrid probability transport with optimal transport regularization, significantly speeding up sampling while maintaining stability. EquiFlow [79] addresses the challenge of conformation prediction using conditional flow matching and an ODE solver for fast and accurate inference. By leveraging equivariant modeling, these methods improve the generation of valid and physically consistent molecular conformations, advancing the field of 3D molecule generation. Equivariant Variational Flow Matching [81] frames flow matching as a variational inference problem and enables both end-to-end conditional generation and post- hoc controlled sampling without retraining. The model further provides a principled equivariant formulation of VFM, ensuring invariance to rotations, translations, and atom permutations, which are essential for molecular applications.

b) Efficiency: Generating high-quality 3D molecular structures efficiently is a major challenge in drug discovery and structural biology. While generative models have shown promise in modeling complex molecular structures, many existing approaches suffer from slow sampling speeds and computational inefficiency. Flow matching-based methods leverage optimal transport and equivariant architectures to achieve faster and more reliable generation. For instance, GOAT [82] formulates a geometric optimal transport objective to map multi-modal molecular features efficiently, using an equivariant representation space to achieve a double speedup compared to previous methods. MolFlow [83] introduces scale optimal transport, significantly reducing sampling steps while maintaining high chemical validity. SemlaFlow [84] combines latent attention with equivariant flow matching, achieving an order-of-magnitude speedup with as few as 20 sampling steps. A recent work introduces SO(3)-Averaged Flow Matching with Reflow [85], targeting both training and inference efficiency for 3D molecular conformer generation. The proposed SO(3)-averaged training objective leads to faster convergence and improved generalization compared to Kabsch-

PREPRINT. JULY 2025.

10

aligned or optimal transport baselines. ET-Flow [86] leverages equivariant flow matching to generate low-energy molecular conformations efficiently, bypassing the need for complex geometric calculations.

c) Guided Generation: Guided and conditional enables the creation of structures that align with specific biological properties or conditions. In the context of flow matching, guided generation incorporates domain-specific knowledge to steer the generative process, while conditional generation aims to produce diverse outputs based on given inputs or contexts. These approaches are especially valuable in applications where accurate constraints are available. Recent advancements in flow matching have introduced several methods to enhance guided and conditional generation. FlowDPO [87] addresses the challenge of 3D structure prediction by combining flow matching with Direct Preference Optimization (DPO), min- imizing hallucinations while producing high-fidelity atomic structures. In conditional generation, Extended Flow Matching (EFM) [88] generalizes the continuity equation, enabling more flexible modeling by incorporating inductive biases. For mixed- type molecular data, FlowMol [89] extends flow matching to handle both continuous and categorical variables, achieving robust performance in 3D de novo molecule generation. 3D energy-based flow matching [90] further enhances conditional generation by explicitly incorporating energy signals into both training and inference, improving structural plausibility and convergence. Together, these advances highlight the growing adaptability of flow-based approaches in generating biologically meaningful 3D molecular structures under domain constraints. Additionally, OC-Flow [91] leverages optimal control theory to guide flow matching without retraining, showing superior efficiency on complex geometric data, including protein design.

C. Conditional Molecule Design and Applications

Recent advancements in flow matching for property-driven molecule design focus on not only generating the molecules themselves, but also predicting potential functionalities of the generated molecules. In scenarios requiring precise ge- ometric control, GeoRCG [93] enhances molecule generation by integrating geometric representation conditions, achieving significant quality improvements on challenging benchmarks. Additionally, conditional generation with improved struc- tural plausibility has been addressed by integrating distorted molecules into training datasets, as demonstrated in Improving Structural Plausibility in 3D Molecule Generation [94]. This method leverages property-conditioned training to selectively generate high-quality conformations. Stiefel Flow Matching [95] tackles the problem of structure elucidation under moment constraints by embedding molecular point clouds within the Stiefel manifold, allowing for efficient and accurate generation of 3D structures with precise physical properties. Finally, IDFlow [186] adopts an energy-based perspective on flow matching for molecular docking, where the generative process learns a deep mapping function to transform random molecular conformations into physically plausible protein-ligand binding structures.

Structure-Based Drug Design (SBDD) is a key task in AI- assisted drug discovery, aiming to design small-molecule drugs

that can bind to a given protein pocket structure. The main chal- lenges in this domain lie in modeling the target protein structure, capturing protein–ligand interactions, enabling multimodal generation, and ensuring the chemical validity of generated molecules. In recent years, generative models have shown great potential in addressing these challenges, with Flow Matching (FM) models demonstrating unique advantages in multimodal modeling and generation efficiency. MolFORM [187] applies multimodal FM to the SBDD setting and employs DPO to optimize molecular binding affinity. FlexSBDD [92] further introduces protein pocket flexibility into the model, making it more reflective of real-world binding scenarios. In addition, MolCRAFT [188] adopts a Bayesian Flow Network (BFN) to model multimodal distributions in continuous parameter space, where BFN similarly defines a flow distribution. Moreover, [189] reveals the equivalence between BFN, diffusion models, and stochastic differential equations (SDEs). PocketXMol [190] provides a unified generative model for handling a variety of protein–ligand tasks.

VI. PROTEIN GENERATION

A. Unconditional Generation

a) Backbone Generation: Protein backbone generation aims to rapidly synthesize physically realizable 3D scaffolds that are diverse, designable, and functionally conditionable, while adhering to SE(3)-equivariance, local bond constraints, and global topological consistency. Recent efforts approach this challenge from two directions: enhancing the flow match- ing framework and improving protein feature representation learning. From the flow matching perspective, FrameFlow [191] accelerates diffusion by reframing it as deterministic SE(3) flow matching, cutting sampling steps five-fold and doubling designability over FrameDiff. FoldFlow-SFM [192] further extends this by introducing stochastic flows on SE(3) manifolds using Riemannian optimal transport, enabling the rapid generation of long backbones (up to 300 residues) with high novelty and diversity. Complementarily, recent work also advances architectural designs for protein representation learning. Yang et al. [193] combine global Invariant Point Attention (IPA) with local neighborhood aggregation to extract meaningful features, and further use ESMFold and AlphaFold3 to filter the invalid generated backbones. Wagner, Simon, et al. [194] proposes Clifford frame attention (CFA), an extension of IPA by exploiting projective geometric algebra and higher- order message passing to capture residue-frame interactions, yielding highly designable proteins with richer fold topologies. FoldFlow-2 [195] augments SE(3) flows with PLM embeddings and a multi-modal fusion trunk, enabling sequence-conditioned generation with reinforced reward alignment and state-of- the-art diversity, novelty, and designability on million-scale synthetic–real datasets. Proteina [196] scales unconditional FM to a 400 M-parameter non-equivariant transformer trained on 21 M synthetic backbones, using hierarchical CATH conditioning to transport isotropic noise to native-like Cα traces. ProtComposer [197] augments a Multiflow [63] backbone with SE(3)-invariant cross-attention to user-sketched 3-D ellipsoid tokens, steering the FM vector field toward compositional spatial layouts while preserving unconditional diversity.

PREPRINT. JULY 2025.

11

TABLE IV COMPARISON OF MAJOR PROTEIN MODELING TASKS. WE HIGHLIGHT THE DISTINCTIONS IN INPUT, OUTPUT, OBJECTIVE, AND REPRESENTATIVE METHODS.

Task

Input

Output

Objective

Protein Structure Prediction Protein Design Protein Backbone Generation

Amino acid sequence Target structure or functional constraint Partial structure, constraints, or motifs

Full 3D structure (backbone + side chains) Amino acid sequence (or full structure) Backbone atomic coordinates (N, Cα, C)

Predict natural folded conformation Design a sequence that folds into a desired structure or achieves a function Generate realistic backbone conformations as design templates

b) Co-design Generation: Recent work reframes se- quence–structure co-design as learning a unified vector field that jointly models discrete amino acid identities and continuous 3D coordinates, bypassing the traditional two-stage pipeline that separately samples a backbone before fitting a compatible sequence. This co-generative setting is especially challenging due to the need to reconcile fundamentally different data manifolds, enforce SE(3) symmetry, and ensure bidirectional invertibility, all while scaling to the vast combinatorial space of long proteins. CoFlow [198] proposes a joint discrete flow that models residue identities and inter-residue distances as CTMC states, augmented with a multimodal masked language module that allows structural flows and sequence tokens to condition each other. Discrete Flow Models (DFM) [63] formalize flow matching on arbitrary discrete spaces by interpreting score- based guidance as CTMC generator reversal. Instantiated as MultiFlow, this framework enables sequence-only, structure- only, or joint generation within a single architecture-agnostic model, achieving state-of-the-art perplexity and TM-scores while being orders of magnitude faster than diffusion-based baselines. Finally, APM [199] introduces a Seq&BB module that jointly learns continuous SE(3) flows for backbone frames and discrete token flows for sequences, leveraging protein language models, Invariant Point Attention, and Transformer encoders to capture residue-level and pairwise interactions. APM supports precise interchain modeling and de novo design of protein complexes with specified binding properties.

B. Conditional Generation

a) Motif-scaffolding Generation: Motif-Scaffolding Gen- eration. Conditional SE(3) flow-matching models embed fixed functional motifs into de-novo backbones by learning equiv- ariant vector fields that respect both local motif geometry and global fold constraints, overcoming the diversity and fidelity limits of earlier diffusion approaches. FrameFlow- Motif [200] augments FrameFlow [191] with motif amorti- zation and inference-time motif guidance, enabling scaffold generation around functional motifs with special-designed data augmentation and estimated conditional scores. EVA [201] casts scaffolding as geometric inverse design, steering a pretrained flow along motif-aligned probability paths to accelerate convergence and boost structural fidelity.

b) Pocket & binder Design: Conditional pocket and binder design tackles the dual challenge of sculpting a protein interface that both accommodates a specific ligand conforma- tion and retains global fold stability, all while respecting SE(3) symmetry and the rich geometric-chemical priors that govern non-covalent recognition. Flow-matching models address these hurdles by learning equivariant vector fields that map an easy base distribution to the manifold of ligand-compatible

protein–ligand complexes in a single, differentiable pass, avoiding the slow guidance loops and hand-crafted potentials of earlier diffusion or docking pipelines. AtomFlow [202] unifies protein and ligand atoms into “biotokens” and applies atomic-resolution SE(3) flow matching to co-generate ligand conformations and binding backbones directly from a 2-D molecular graph. FlowSite [203] introduces a self-conditioned harmonic flow objective that first aligns apo proteins to a harmonic potential and then co-generates discrete residue types and 3-D ligand poses, supporting multi-ligand docking and outperforming prior generative and physics-based baselines on pocket-level benchmarks. PocketFlow [204] incorporates pro- tein–ligand interaction priors (e.g., hydrogen-bond geometry) directly into the flow, then applies multi-granularity guidance to produce high-affinity pockets that significantly improve Vina scores and generalize across small molecules, peptides, and RNA ligands. To efficiently recover all-atom structures from coarse-grained simulations, FlowBack [205] utilizes flow matching to map coarse-grained representations to all-atom configurations, achieving high fidelity in protein and DNA structure reconstruction.

C. Structure Prediction

a) Conformer Prediction: Accurately sampling the con- formational ensembles underlying protein function remains challenging due to the cost of exhaustive molecular dynamics. Recent work leverages sequence-conditioned, SE(3)-equivariant flow matching to efficiently generate diverse, physically consis- tent states aligned with experimental observables. AlphaFold Meets Flow Matching [206] repurposes single-state predictors (AlphaFold, ESMFold) as generative engines by fine-tuning them under a harmonic flow-matching objective, yielding AlphaFlow/ESMFlow ensembles that surpass MSA-subsampled AlphaFold on the precision-diversity trade-off and reach equilibrium observables faster than replicate MD trajectories. P2DFlow [207] augments SE(3) flow matching with a latent “ensemble” dimension and a physics-motivated prior, enabling it to reproduce crystallographic B-factor fluctuations and ATLAS MD distributions more faithfully than earlier baselines.

b) Side-chain Packing: Predicting rotameric states for each residue requires joint compliance with steric constraints, energetic preferences, and SE(3)-equivariance. Recent work has explored constrained side-chain prediction through flow matching. FlowPacker [208] formulates side-chain placement as torsional flow matching, coupling the learned vector field to EquiformerV2 [209], an SE(3)-equivariant graph attention backbone. PepFlow [210] generalizes this approach to full- atom peptides using a multi-modal flow that captures joint distributions over backbone frames, side-chain torsions, and residue identities. Partial sampling from this flow achieves

PREPRINT. JULY 2025.

12

state-of-the-art results in fixed-backbone packing and receptor- bound refinement, while maintaining full differentiability for downstream design applications.

c) Docking Prediction: Recent work reframes protein- ligand docking as a flow-matching (FM) generative problem, replacing diffusion with a simulation-free objective that learns a bijective map from unbound receptors (apo) to bound complexes (holo). FlowSite [203] introduces a self-conditioned FM objective that harmonically couples translational, rotational and torsional degrees of freedom. By leveraging GAT and TFN layers for ligand–protein interaction modeling, it further extends to jointly generate contact residues and ligand coordi- nates, substantially improving sample quality, simplicity, and generality in pocket-level docking. Meanwhile, FlowDock [211] learns a geometric flow mapping unbound to bound structures, while predicting per-complex confidence and binding affinity estimates. It achieves a 51% blind docking success rate on the PoseBusters benchmark, outperforming single-sequence AlphaFold3 without MSA inputs, and ranks in the top-5 for affinity prediction in CASP16 across 140 complexes.

D. Peptide and Antibody Generation

Recent work [207, 210, 212, 213, 214] formulates peptide design as conditional flow matching over multiple geometric and categorical manifolds, explicitly modeling residue type, spatial position, orientation, and angles in a unified generative framework. PepFlow [210] introduces the first multi-modal flow matching framework for protein structure design, jointly modeling residue positions via Euclidean CFM, orientations via Spherical CFM, angles via Toric CFM, and types via Simplex CFM. This unified approach achieves excellent performance on sequence recovery and side-chain packing in receptor- conditioned design tasks. D-Flow [207] extends this paradigm to D-peptides by augmenting limited training data through a chirality-aware mirror transformation and incorporating a lightweight structural adapter into a pretrained protein language model. PPFlow [212] formulates peptide torsion generation as flow matching on a (3n − 3)-torus with n being the number of amino acids, while modeling global transitions and residue types via Euclidean flows and employing SO(3)-CFM for rota- tions. This formulation enables effective conditional sampling for diverse tasks such as peptide optimization and docking. Finally, NLFlow [213] pioneers non-linear conditional vector fields by employing polynomial interpolation over the position manifold, enabling faster convergence toward binding pockets and effectively addressing temporal inconsistencies across modalities. This approach leads to improvements in structural stability and binding affinity compared to prior linear flow models. Collectively, these studies underscore the importance of manifold-specific flows, conditioning strategies, and geometric priors for scalable, high-fidelity peptide generation. In contrast to these geometry-intensive approaches, ProtFlow [214] treats peptides as amino acid sequences and bypasses non-Euclidean representations by embedding each residue using a pretrained protein language model (PLM). In the embedding space of PLMs, ProtFlow trains a reflow-enabled sequence flow model that supports both single-step generation and multi-chain co- design. Collectively, these studies highlight the critical role of

manifold-specific flows, conditioning strategies, and geometric priors in enabling scalable and high-fidelity peptide generation. The study of antibody structure design with flow matching is emerging as well. For instance, FlowAB [215] utilizes energy-guided SE(3) flow matching to improve antibody structure refinement, integrating physical priors to enhance CDR accuracy with minimal computational overhead.

VII. OTHER BIO APPLICATIONS

A. Dynamic Cell Trajectory Prediction

Dynamic Cell Trajectory. Generative trajectory models seek to reconstruct the continuously branching, stochastic evolution of cells from high-dimensional, sparsely sampled single-cell readouts, which is an endeavor hampered by severe noise, irregular time points, and the risk that straight Euclidean interpolants stray outside the biological manifold. CellFlow [96] tackles this by framing morphology evolution under per- turbations as an image-level flow-matching problem on cellular masks, enabling realistic, perturbation-conditioned movies of shape change that outperform diffusion and GAN baselines in both faithfulness and diversity. GENOT-L [71] introduces an entropic Gromov-Wasserstein flow that couples gene-expression geometry across time points, producing probabilistic lineage trajectories that capture heterogeneity and branching better than optimal-transport predecessors while remaining simulation-free. Metric Flow Matching [97] instead learns geodesic vector fields under a data-induced Riemannian metric, yielding smoother in- terpolations that respect the manifold’s curvature and achieving state-of-the-art accuracy on single-cell trajectory benchmarks with fewer artifacts than Euclidean flows. Diversified Flow Matching [98] extends this line of work by ensuring translation identifiability across diverse conditional distributions, a key challenge in modeling heterogeneous cellular states. Unlike prior GAN-based solutions, this work formulates the problem within an ODE-based flow matching framework, offering stable training and explicit transport trajectories. Collectively, these works highlight the importance of geometry-aware objectives and probabilistic conditioning for faithful dynamic cell-state generation.

B. Bio-image Generation and Enhancement

Leveraging continuous probability flow to efficiently model biological structures, flow matching has shown great potential for bio-image generation and enhancement, enabling faster and more accurate modeling of complex biological data. One notable application is FlowSDF [99], which introduces image-guided conditional flow matching for medical image segmentation. By modeling signed distance functions (SDF) instead of binary masks, FlowSDF achieves smoother and more accurate segmentation. This method also generates uncertainty maps, enhancing robustness in prediction tasks. For medical image synthesis, an optimal transport flow matching approach [100] addresses the challenge of balancing generation speed and image quality. By creating a more direct mapping between dis- tributions, this method reduces inference time while maintaining high-quality outputs, and supports diverse imaging modalities,

PREPRINT. JULY 2025.

13

TABLE V DATASETS AND SOFTWARE IN BIOLOGY AND LIFE SCIENCE TO TEST FLOW MATCHING METHODS (PART I)

Task

Dataset

Scale / Number of Samples

Links

Used By

DNA Sequence Generation

Promoter DNA Sequence Enhancer DNA Sequence

100, 000 104,665 (fly brain); 88,870 (human melanoma)

Paper1; Paper2 Paper3 Code1; Code2; Code3; Paper1 Paper2; Code1; Code2;

[51] [60] [67] [51] [60]

Single-cell Trajectory

Molecule Generation

RNA Sequence Generation

Rfam Database [216] Muscle/PC3/HEK 5’ UTR libraries [217] RNAsolo [218]

Over 20M sequences 41,446 18,808 RNA 3D structures

Pancreas single-cell data [219] Drug perturbation single-cell data [220] Multi-modal single-cell analysis [221] PBMC [222] Dentate gyrus dataset [223] Human Lung cells Atlas [224] Tabula Muris [226] Embryoid Body (EB) [227] CITE-seq (Cite) [228] Multiome (Multi) [228]

36,351 cells 650K single-cell transcriptomes 120K single cells (human bone marrow) 30K cells 18,213 cells 584,944. [72] uses a subset of 32,272 [225] 245,389 cells 5 marginals 4 marginals 4 marginals

Paper; Homepage; Huggingface; Paper; Code Paper; Homepage

Paper; Download Link Paper; Download Instruction Paper; Homepage; Dataset List Paper; Download Link Paper; scVelo Documentation Paper; Homepage; Dataset List Paper; Homepage; Code Paper; Code Paper; Homepage Paper Homepage

[68] [68] [69] [70]

[71] [71] [71] [73] [72] [73] [72] [73] [72] [73] [73] [97] [97] [97]

Quantum Machine (QM) [179]

Various sizes. QM9: 133,885

Paper; Homepage; Paper With Code; Kaggle

ZINC [229]

Guacamol [230] MOSES [231]

GEOM-Drugs [178]

PoseBusters benchmark [232] GEOM-QM9 [178] SAbDab [233]

Molecular Binder Generation

Binding MOAD [235] CrossDocked [236]

Molecular Docking

PDBBind [237] PPDBench [238]

Various sizes. ZINC250K: 249,456

Paper; Paper With Code; Huggingface; Kaggle

1,591,378 1,936,963

430,000

308 curated protein–ligand complexes 133,885 9,680

41K complexes 22.5M protein-molecule pairs

33,653 biomolecular complexes 133 protein-peptide complexes

Paper; Code; Paper With Code Paper; Code; Paper With Code

Paper; Code; Paper With Code

Paper; code; Paper with code Paper; Code; Paper With Code Paper; Homepage;

Paper; Homepage Paper; Code

Paper; Homepage Paper; Homepage

Protein Sequence Design

UniRef [239] Protein Data Bank (PDB) [240] Open Metagenomic Corpus (OMG) [243] SAbDab [233] OAS-paired antibody sequences [244] RAbD Benchmark [245] UniProt [246] UniProtKB/SwissProt [247]

Various sizes. UniRef50: 70,198,728 Over 200K. 18,684 for curated version [241] 3.3B in total. OMG_prot50: 207M 9,680 1.86M pairs 60 Over 60 million sequences 18364 sequence entries, 5,986,949 amino acids

Paper; Homepage; Huggingface Paper; Homepage; Wikipedia Paper ; Code; HuggingFace; Genomic LM Paper; Homepage; Paper; Homepage Paper; Manual; Paper; Homepage Paper; Homepage

Protein Backbone Generation

Protein Data Bank (PDB) [240] SCOPe [248] Huguet et al. [195]

Over 200K. 18,684 for curated version [241] 108,069 160K structures

Paper; Homepage; Wikipedia Paper; Homepage Paper; Code

[90] [191] [192] [195] [202] [200] [194] [202] [195]

De Novo Protein Generation

PepBDB [249] PepMerge [210]

Protein Ensemble Dynamics

Protein Data Bank (PDB) [240] ATLAS [251]

Protein Docking or Side-chain Packing

CASP [252]

13,299 peptide-protein complex 8,365

Over 200K proteins 1390 protein chains [241]

Paper; Homepage; PepBDB-ML Paper; Code

[212] [250] [210] [213]

Paper; Homepage; Wikipedia Paper; Homepage

Various sizes

Paper; Homepage

Peptide Binder Design

PDBBind [237] PepNN (peptide binding sites) [254] BioLip2 [255] Binder discrimination dataset [74]

33,653 biomolecular complexes Varoius sizes ranging from 251 to 2,517 385,160 protein chains; 781,684 interactions 4,883 antibody–antigen complexes

Peptide Design

PepBDB [249] PepMerge [210]

13,299 peptide-protein complex 8,365

Paper; Homepage Paper; Code; Wikipedia Paper; Homepage; Web Code Paper;

Paper; Homepage; PepBDB-ML Paper; Code

including 2D and 3D. In MR image reconstruction, Multi- Modal Straight Flow Matching (MMSFlow) [101] significantly reduces the number of inference steps by forming a linear path between undersampled and reconstructed images. Leveraging multi-modal information with low- and high-frequency fusion layers, MMSFlow achieves state-of-the-art performance in fastMRI and Brats-2020 benchmarks.

C. Cellular Microenvironments from Spatial Transcriptomics

Flow matching has also emerged as a powerful framework for modeling spatial transcriptomics (ST) data, which captures gene expression levels across spatial locations within a tissue. The core task in ST involves reconstructing or generating spatially- resolved gene expression maps that reflect underlying cellular microenvironments and tissue organization. One such method is STFlow [102] which introduces a scalable flow matching framework for generating spatial transcriptomics data from

whole-slide histology images. It models the joint distribution of gene expression across all spatial spots in a slide, thereby explicitly capturing cell-cell interactions and tissue organization. Complementarily, Wasserstein Flow Matching (WFM) [103] generalizes flow-based generative modeling to families of distributions. It introduces a principled way to model both 2D and 3D spatial structures of cellular microenvironments, and leverages the geometry of Wasserstein space to better match distributional characteristics across biological contexts. Together, these methods highlight the utility of flow matching in capturing the spatially-aware, high-dimensional distributions characteristic of modern transcriptomics datasets.

D. Neural Activities

Flow matching has recently shown promise in modeling and aligning neural activity, particularly for time series and brain- computer interface (BCI) applications, where neural signals are

[53] [64] [66] [76] [77] [78] [45] [79] [80] [45] [81] [83] [82] [91] [89] [93] [94] [95] [53] [64] [76] [77] [78] [45] [88] [81] [94] [64] [78] [45] [64] [78] [45] [66] [80] [45] [81] [82] [83] [85] [89] [86] [93] [94] [95] [74] [66] [79] [85] [86] [87] [215] [234]

[92] [203] [204] [92] [204]

[90] [203] [204] [204]

[59] [67] [63] [205] [242] [67] [74] [75] [74] [75] [214] [214]

[206] [206] [207]

[208] [211]

[69] [193] [253] [67] [67] [74]

[212] [250] [210] [213]

PREPRINT. JULY 2025.

14

TABLE VI DATASETS AND SOFTWARE IN BIOLOGY AND LIFE SCIENCE TO TEST FLOW MATCHING METHODS (PART II)

Task

Dataset

Scale / Number of Samples

Links

Used By

Cell Morphology Profiling

Medical Image Segmentation

BBBC021 [256] RxRx1 [257] JUMP Cell Painting [258]

MoNuSeg [259] GlaS [260] CAMUS [261] MSD Brain MRI [262]

39,600 images 125,510 images 1.6 billion profiles

Paper; Homepage; Paper; Homepage; Code; Paper With Code Paper; Code; AWS

30 train + 14 test images 85 train + 80 test images 450 patients; 1,600 images 750 scans (T1-weighted)

MRI Reconstruction

fastMRI [263] BraTS-2020 (Brain Tumor Segmentation) [264]

Knee: 1,398 scans; Brain: 7,002 scans 494 subjects , 240×240

Spatial Transcriptomics

HEST-1k [265] STImage-1K4M [266]

Single-cell Omics

Neural Time Series

seqFISH [267] scRNA-seq [268]

Mouse brain LFP [269]

Neural Population Dynamics

CO-C (Monkey C) [270] CO-M (Monkey M) [271] RT-M (Monkey M) [272]

1,229 profiles 1,149 slides (4 293 195 spots)

351 genes; 29 cells per niche 32 PCs per meta-cell

50 marginals (50–500 ms)

5 sessions; 957 units 9 sessions; 1,728 units 1 session; 130 units

Paper; Homepage; Paper; Homepage; Paper; Homepage Paper; AWS; PapersWithCode

Homepage; Paper; Code Homepage; Paper; Kaggle

Paper; Code Paper; Code; HuggingFace

Paper; Code Paper; Code

Paper; Code

DREAM; Paper pmd-1; Paper NLB-RTT; Paper

[96] [96] [96]

[99] [99] [100] [100]

[101] [101]

[102] [102]

[103] [103]

[104]

[105] [105] [105]

often stochastic and nonstationary. Stream-level Flow Match- ing with Gaussian Processes [104] extends conditional flow matching by introducing streams, which arelatent stochastic paths modeled with Gaussian processes. This reduces variance in vector field estimation, enabling more accurate modeling of correlated time series such as neural recordings. Flow- Based Distribution Alignment [105] tackles inter-day neural signal shifts in BCIs through source-free domain adaptation. By learning stable latent dynamics via flow matching and ensuring stability through Lyapunov analysis, it enables reliable few- trial neural adaptation across days. These approaches highlight the versatility of flow matching for neural data, supporting both high-fidelity generation and robust adaptation with limited supervision.

VIII. EVALUATION TASKS AND DATASETS

In this section, we summarize evaluation tasks and datasets used for assessing flow matching methods in biology and life sciences. As listed in Table V and Table VI, these tasks span a wide spectrum of domains, including genomics, transcriptomics, molecular chemistry, and structural biology. For each dataset, we also report its data scale or number of samples. Flow matching has been applied to a diverse set of generation and modeling problems, such as biological sequence generation, cell trajectory inference, molecule design, and protein structure modeling.

Sequence-Level Generation. Flow matching models have been evaluated on tasks like DNA [51, 60, 67], RNA [216, 217, 218], and protein [239, 240, 243] sequence generation. These datasets range from promoter and enhancer sequences to large-scale protein and metagenomic corpora, covering both canonical and noncoding regions of the genome.

Single-Cell Modeling and Trajectory Inference. Flow match- ing has been used to model temporal or conditional transitions in high-dimensional single-cell gene expression data, including developmental trajectories [219], perturbation responses [220], and modality prediction [221]. Datasets such as PBMC [222],

dentate gyrus [223], and Tabula Muris [226] provide diverse experimental contexts for evaluating these tasks.

Molecular Generation and Conformation Modeling. Datasets such as QM9 [179], ZINC [229], GEOM-Drugs [178], and MOSES [231] provide chemically diverse molecular structures, enabling evaluation of molecular validity, novelty, and 3D geometry. Flow matching models are tested on their ability to generate, edit, or align molecular graphs and conformers.

Protein and Complex Design. Structural datasets like SCOPe [248], ATLAS [251], and curated PDB subsets support eval- uation of flow-based models on protein backbone generation, folding, and structural refinement. Complementary datasets such as Binding MOAD [235], CrossDocked [236], BioLip2 [255], and PepBDB [249] enable studies on molecular docking, peptide-protein interactions, and binder generation.

Notably, many datasets are reused across different tasks due to their structural richness and biological relevance. For instance, the Protein Data Bank (PDB) [240] is used in tasks ranging from protein sequence design and backbone gener- ation to modeling conformational dynamics and performing docking. Similarly, SAbDab [233] supports antibody sequence generation, structural modeling, and binder discrimination.

Despite the growing adoption of flow matching in biology, the field still lacks unified benchmarks for many tasks. This is likely due to the inherent heterogeneity of biological problems, ranging from sequence to structure, from single-cell to population scale, which makes standardized evaluation more challenging. This stands in contrast to fields like computer vision or NLP, where well-defined benchmarks are more prevalent [273, 274, 275, 276]. Continued efforts in dataset curation and task formulation are needed to support consistent and reproducible assessment of generative models in the life sciences.

IX. FUTRUE DIRECTION

A. Flow Matching for Discrete Sequence Generation

Flow Matching has recently emerged as a promising gen- erative modeling paradigm, offering a compelling balance

PREPRINT. JULY 2025.

15

between generation quality and training stability. While its success in continuous domains like image and molecule generation has been widely documented, applying FM to discrete sequence generation—especially in domains such as natural language, genomics, and code—remains a vibrant and largely underexplored frontier.

One of the most intriguing directions lies in understanding the representational advantages of discrete Flow Matching compared to traditional paradigms such as Masked Language Modeling (MLM). Unlike MLM, which relies on partial observation and token masking, FM provides a direct mapping from a base distribution to the target sequence via a continuous probability flow. This raises the question: Can discrete FM yield more semantically coherent representations and facilitate better downstream performance in tasks such as classification? Recent advances, such as Fisher Flow[60] and Dirichlet FM[51], demonstrate that geometry-aware formulations over the probability simplex can encode meaningful geometric constraints and structure-aware trajectories, enabling more faithful modeling of discrete data distributions.

Another fundamental question concerns the generation capabilities of discrete FM relative to autoregressive (AR) models. While AR models remain the gold standard in natural language generation due to their strong likelihood modeling and contextual fluency, they suffer from slow sampling and exposure bias. In contrast, discrete FM supports parallel generation through ODE integration or sampling over learned Markov trajectories, offering substantial efficiency gains. However, its generation quality still lags behind state-of-the-art AR transformers in language generation [60], prompting future research into architectural refinements and better training objectives.

Furthermore, the integration of FM with Transformer ar- chitectures remains an open challenge. Existing Transformer- based FM models either operate in latent embedding space or use discrete-continuous relaxations (e.g., Gumbel-Softmax) to approximate gradient flows. Yet, the Transformer’s causal attention structure may be suboptimal for non-autoregressive FM-based sequence generation, especially in domains where left-to-right order is arbitrary or non-existent (e.g., protein sequences, biological pathways). This invites research into order-agnostic architectures or the use of permutation-invariant encoders to better align with FM-based modeling.

Finally, flow matching may offer unique advantages in non- language sequence modeling tasks, such as biomolecular design and genome modeling, where biological constraints (e.g., base- pairing, structural motifs) must be enforced. Unlike language, these sequences often lack natural generation order and exhibit rich multi-modal dependencies. FM’s ability to incorporate conditioning, geometry-aware constraints, and structure-guided generation (e.g., via SE(3)-equivariant or manifold-aware flows) makes it a particularly attractive candidate. Future work may focus on developing discrete FM formulations that are not only domain-adaptive, but also biologically interpretable and sample-efficient.

B. Small Molecule Generation and Modeling

Small molecule generation is a core task in cheminformatics and drug discovery, where FM has recently shown promising capabilities in both unconditional and conditional generation settings. By modeling continuous probability flows between simple priors and molecular distributions, FM offers an appealing alternative to diffusion models, with improved sample efficiency and the potential to integrate domain knowledge. However, due to the scarcity of molecular structure data and the complexity of structural constraints, several key challenges remain before FM can fully realize its potential for small molecule generation.

One fundamental limitation lies in the data scarcity and structural heterogeneity of small molecule datasets. Unlike macromolecules such as proteins, which benefit from large- scale structural repositories (e.g., PDB), small molecule datasets are often limited in size and diversity, especially for annotated 3D conformers. As a result, FM models trained on these datasets may struggle to generalize across different chemical scaffolds, limiting their utility in low-resource or out-of- distribution scenarios. Addressing this issue may require more effective data augmentation strategies (e.g., using force field simulations or generative conformer expansion), transfer learning pipelines, or semi-supervised flow matching objectives that make better use of unlabeled data.

To improve the physical plausibility and functional rel- evance of generated small molecules, a key direction lies in incorporating domain-specific inductive priors into both the training and sampling stages of flow matching. Small molecules are governed by well-defined chemical and physical constraints—such as bond lengths and angles, valence rules, charge distributions, and conformational energetics—which can be explicitly modeled to constrain the learned probability flow. Embedding such priors into the vector field design or generation trajectories (e.g., via energy-guided loss functions or structure-aware conditioning) can substantially improve the realism and synthesizability of generated compounds.

At the same time, enhancing the conditional generation capabilities of FM is essential for tasks that demand goal- directed molecular design, such as generating molecules with desired pharmacological properties, satisfying functional group templates, or fitting into predefined binding pockets. Conditional flow matching offers a natural framework for structure- and property-guided generation, enabling fine-grained control over outputs via learned trajectories that satisfy specific constraints. Future work may explore more expressive condi- tioning schemes, multi-property guidance, or interaction-aware control mechanisms, paving the way for FM-based models to support precision molecular design in high-stakes domains such as drug discovery and materials engineering.

A further challenge lies in modeling molecular interactions and dynamic processes. Molecular docking and binding affinity prediction remain critical tasks in early-stage drug design, requiring models to account for conformational flexibility in small molecules and the adaptive nature of protein binding pockets, particularly with respect to side-chain rearrangements. Even more challenging tasks, such as enzyme design, involve

PREPRINT. JULY 2025.

16

not just molecular recognition but also modeling of specific reaction mechanisms. Thus, leveraging the FM framework to capture inter-molecular interactions and reaction dynamics represents a crucial and promising direction for future research.

C. Protein

In the field of protein modeling, Flow Matching (FM) has emerged as an efficient approach for sequence and structure modeling, demonstrating complementary advantages to traditional methods. Proteins, as highly complex biological macromolecules, exhibit a unique combination of discrete primary sequences and continuous three-dimensional structures, which poses distinct challenges for the design and training of FM-based models.

One important future direction is to establish effective matching mechanisms across different protein modalities. For example, in mapping from amino acid sequences to 3D structures, FM could serve as a bridge between discrete and continuous spaces, enhancing the model’s expressiveness in structure prediction and generation tasks. Furthermore, in applications such as protein-protein docking and complex assembly modeling, FM offers a promising framework for capturing transformation paths in high-dimensional, complex spaces.

In addition, modeling protein dynamics—such as con- formational changes or ligand-induced fit—remains a core challenge in structural biology. Future work may explore integrating FM with physical simulations (e.g., molecular dynamics) or diffusion-based processes, enabling the learning of natural transition paths between protein states and improving interpretability of their functional mechanisms.

X. CONCLUSION

Flow matching has become a compelling alternative to diffusion-based generative modeling, offering advantages in stability, efficiency, and control. In this survey, we provide a structured overview of its growing use in biology and life sciences, covering a diverse range of tasks from sequence generation and molecular design to protein modeling. We also compile a comprehensive list of datasets used for evaluation, including their scale and cross-task applicability. Despite promising progress, we also summarize the challenges that the field faces. We hope this survey could clarify current trends and motivate future research at the intersection of generative modeling and the life sciences.

REFERENCES

[1] Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le, “Flow matching for generative modeling,” in The Eleventh International Conference on Learning Representations, 2023. [Online]. Available: https: //openreview.net/forum?id=PqvMRDCJT9t

[2] Y. Jin, Z. Sun, N. Li, K. Xu, K. Xu, H. Jiang, N. Zhuang, Q. Huang, Y. Song, Y. Mu, and Z. Lin, “Pyramidal flow matching for efficient video generative modeling,” in The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28,

2025. OpenReview.net, 2025. [Online]. Available: https://openreview.net/forum?id=66NzcRQuOq

the 18th Conference of

[3] V. T. Hu, D. Wu, Y. M. Asano, P. Mettes, B. Fernando, B. Ommer, and C. Snoek, “Flow matching for conditional text generation in a few sampling steps,” in Proceedings of the European Chapter of the Association for Computational Linguistics, EACL 2024 - Volume 2: Short Papers, St. Julian’s, Malta, March 17-22, 2024, Y. Graham and M. Purver, Eds. Association for Computational Linguistics, 2024, pp. 380–392. [Online]. Available: https://aclanthology.org/2024.eacl-short.33

[4] I. Gat, T. Remez, N. Shaul, F. Kreuk, R. T. Chen, G. Synnaeve, Y. Adi, and Y. Lipman, “Discrete flow matching,” Advances in Neural Information Processing Systems, vol. 37, pp. 133 345–133 385, 2024.

[5] G. M. Church and W. Gilbert, “Genomic sequencing.” the National Academy of Sciences,

Proceedings of vol. 81, no. 7, pp. 1991–1995, 1984.

[6] J. C. Venter, M. D. Adams, E. W. Myers, P. W. Li, R. J. Mural, G. G. Sutton, H. O. Smith, M. Yandell, C. A. Evans, R. A. Holt et al., “The sequence of the human genome,” science, vol. 291, no. 5507, pp. 1304–1351, 2001.

[7] C. S. Pareek, R. Smoczynski, and A. Tretyn, “Sequenc- ing technologies and genome sequencing,” Journal of applied genetics, vol. 52, pp. 413–435, 2011.

[8] S. Luo, J. Guan, J. Ma, and J. Peng, “A 3d generative model for structure-based drug design,” Advances in Neural Information Processing Systems, vol. 34, pp. 6229–6239, 2021.

[9] A. V. Sadybekov and V. Katritch, “Computational approaches streamlining drug discovery,” Nature, vol. 616, no. 7958, pp. 673–685, 2023.

[10] S. Mathur and C. Hoskins, “Drug development: Lessons from nature,” Biomedical reports, vol. 6, no. 6, pp. 612– 614, 2017.

[11] J. Abramson, J. Adler, J. Dunger, R. Evans, T. Green, A. Pritzel, O. Ronneberger, L. Willmore, A. J. Ballard, J. Bambrick et al., “Accurate structure prediction of biomolecular interactions with alphafold 3,” Nature, vol. 630, no. 8016, pp. 493–500, 2024.

[12] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Fig- urnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko et al., “Highly accurate protein structure prediction with alphafold,” nature, vol. 596, no. 7873, pp. 583–589, 2021.

[13] M. Baek, I. Anishchenko, I. R. Humphreys, Q. Cong, D. Baker, and F. DiMaio, “Efficient and accurate pre- diction of protein structure using rosettafold2,” BioRxiv, pp. 2023–05, 2023.

[14] R. A. Robb, Biomedical imaging, visualization, and

analysis.

John Wiley & Sons, Inc., 1999.

[15] C. M. Tempany and B. J. McNeil, “Advances in biomedical imaging,” Jama, vol. 285, no. 5, pp. 562–567, 2001.

[16] A. Webb,

# Introduction to biomedical imaging.

John

Wiley & Sons, 2022.

PREPRINT. JULY 2025.

17

[17] R. M. Rangayyan, Biomedical image analysis. CRC

no. 1, pp. 1–34, 2020.

press, 2004.

[18] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial networks,” Communications of the ACM, vol. 63, no. 11, pp. 139–144, 2020.

[19] L. Lan, L. You, Z. Zhang, Z. Fan, W. Zhao, N. Zeng, Y. Chen, and X. Zhou, “Generative adversarial networks and its applications in biomedical informatics,” Frontiers in public health, vol. 8, p. 164, 2020.

[20] M. Lee, “Recent advances in generative adversarial networks for gene expression data: a comprehensive review,” Mathematics, vol. 11, no. 14, p. 3055, 2023.

[21] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, “Masked autoencoders are scalable vision learners,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 16 000–16 009. [22] O. Kraus, K. Kenyon-Dean, S. Saberian, M. Fallah, P. McLean, J. Leung, V. Sharma, A. Khan, J. Bal- akrishnan, S. Celik et al., “Masked autoencoders are scalable learners of cellular morphology,” arXiv preprint arXiv:2309.16064, 2023.

[23] M. Yuan, A. Shen, K. Fu, J. Guan, Y. Ma, Q. Qiao, and M. Wang, “Proteinmae: masked autoencoder for protein surface self-supervised learning,” Bioinformatics, vol. 39, no. 12, p. btad724, 2023.

[24] H.-Y. S. Chien, H. Goh, C. M. Sandino, and J. Y. Cheng, “Maeeg: Masked auto-encoder for eeg representation learning,” arXiv preprint arXiv:2211.02625, 2022. [25] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” Advances in neural information processing systems, vol. 33, pp. 6840–6851, 2020. [26] Z. Guo, J. Liu, Y. Wang, M. Chen, D. Wang, D. Xu, and J. Cheng, “Diffusion models in bioinformatics and computational biology,” Nature reviews bioengineering, vol. 2, no. 2, pp. 136–154, 2024.

[27] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang, B. Cui, and M.-H. Yang, “Diffusion models: A comprehensive survey of methods and applications,” ACM Computing Surveys, vol. 56, no. 4, pp. 1–39, 2023. [28] J. R. Faeder, M. L. Blinov, B. Goldstein, and W. S. Hlavacek, “Rule-based modeling of biochemical net- works,” Complexity, vol. 10, no. 4, pp. 22–41, 2005.

[29] M. Hwang, M. Garbey, S. A. Berceli, and R. Tran-Son- Tay, “Rule-based simulation of multi-cellular biological systems—a review of modeling techniques,” Cellular and molecular bioengineering, vol. 2, pp. 285–294, 2009.

[30] J. R. Faeder, M. L. Blinov, and W. S. Hlavacek, “Rule- based modeling of biochemical systems with bionetgen,” Systems biology, pp. 113–167, 2009.

[31] L. A. Chylek, L. A. Harris, J. R. Faeder, and W. S. Hlavacek, “Modeling for (physical) biologists: an intro- duction to the rule-based approach,” Physical biology, vol. 12, no. 4, p. 045007, 2015.

[32] J. Willard, X. Jia, S. Xu, M. Steinbach, and V. Kumar, “Integrating physics-based modeling with machine learn- ing: A survey,” arXiv preprint arXiv:2003.04919, vol. 1,

[33] J. Newman, Physics of the life sciences.

Springer

Science & Business Media, 2008.

[34] K. Franklin, P. Muir, T. Scott, and P. Yates, Introduction to biological physics for the health and life sciences. John Wiley & Sons, 2019.

[35] K. Baverstock, “Life as physics and chemistry: A system view of biology,” Progress in Biophysics and Molecular Biology, vol. 111, no. 2-3, pp. 108–115, 2013.

[36] B. Yelmen and F. Jay, “An overview of deep generative models in functional and evolutionary genomics,” Annual Review of Biomedical Data Science, vol. 6, no. 1, pp. 173–189, 2023.

[37] D. M. Anstine and O. Isayev, “Generative models as an emerging paradigm in the chemical sciences,” Journal of the American Chemical Society, vol. 145, no. 16, pp. 8736–8750, 2023.

[38] C. Bilodeau, W. Jin, T. Jaakkola, R. Barzilay, and K. F. Jensen, “Generative models for molecular discovery: Recent advances and challenges,” Wiley Interdisciplinary Reviews: Computational Molecular Science, vol. 12, no. 5, p. e1608, 2022.

[39] D. Xue, Y. Gong, Z. Yang, G. Chuai, S. Qu, A. Shen, J. Yu, and Q. Liu, “Advances and challenges in deep generative models for de novo molecule generation,” Wi- ley Interdisciplinary Reviews: Computational Molecular Science, vol. 9, no. 3, p. e1395, 2019.

[40] D. Fu and J. He, “DPPIN: A biological repository of dynamic protein-protein interaction network data,” in IEEE International Conference on Big Data, Big Data 2022, Osaka, Japan, December 17-20, 2022, S. Tsumoto, Y. Ohsawa, L. Chen, D. V. den Poel, X. Hu, Y. Motomura, T. Takagi, L. Wu, Y. Xie, A. Abe, and V. Raghavan, Eds. IEEE, 2022, pp. 5269–5277. [Online]. Available: https://doi.org/10.1109/BigData55660.2022.10020904

[41] L. Zheng, B. Jing, Z. Li, Z. Zeng, T. Wei, M. Ai, X. He, L. Liu, D. Fu, J. You, H. Tong, and J. He, “Pyg-ssl: A graph self-supervised learning toolkit,” CoRR, vol. abs/2412.21151, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2412.21151

[42] D. Fu, Y. Zhu, Z. Liu, L. Zheng, X. Lin, Z. Li, L. Fang, K. Tieu, O. Bhardwaj, K. Weldemariam, H. Tong, H. F. Hamann, and J. He, “Climatebench-m: A multi-modal climate data benchmark with a simple generative method,” CoRR, vol. abs/2504.07394, 2025. [Online]. Available: https://doi.org/10.48550/arXiv.2504.07394 [43] L. Zheng, B. Jing, Z. Li, H. Tong, and J. He, “Heterogeneous contrastive learning for foundation the 30th models and beyond,” in Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2024, Barcelona, Spain, August 25-29, 2024, R. Baeza-Yates and F. Bonchi, Eds. ACM, 2024, pp. 6666–6676. [Online]. Available: https://doi.org/10.1145/3637528.3671454

[44] D. Fu, L. Fang, Z. Li, H. Tong, V.

I. Torvik, and J. He, “Parametric graph representations in the era of foundation models: A survey and position,” CoRR, vol. abs/2410.12126, 2024. [Online]. Available:

PREPRINT. JULY 2025.

18

https://doi.org/10.48550/arXiv.2410.12126

in Neural

[45] Y. Song, J. Gong, M. Xu, Z. Cao, Y. Lan, S. Ermon, H. Zhou, and W. Ma, “Equivariant flow matching with hybrid probability transport for 3d molecule generation,” in Advances Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., 2023. [Online]. Available: http://papers.nips.cc/paper_files/paper/2023/hash/ 01d64478381c33e29ed611f1719f5a37-Abstract-Conference. html

[46] L. Klein, A. Krämer, and F. Noé, “Equivariant flow matching,” Advances in Neural Information Processing Systems, vol. 36, pp. 59 886–59 910, 2023.

[47] A. J. Bose, T. Akhound-Sadegh, G. Huguet, K. Fatras, J. Rector-Brooks, C. Liu, A. C. Nica, M. Korablyov, M. M. Bronstein, and A. Tong, “Se(3)-stochastic flow matching for protein backbone generation,” in The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [Online]. Available: https://openreview.net/forum?id=kJFIH23hXb

Systems

[48] C. Cheng, J. Li, J. Peng, and G. Liu, “Categorical flow matching on statistical manifolds,” in Advances in Neural 38: Information Processing Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, [Online]. Available: and C. Zhang, Eds., 2024. http://papers.nips.cc/paper_files/paper/2024/hash/ 62a58f2130894e44e8a272c563a2c6f1-Abstract-Conference. html

[49] N. Kornilov, P. Mokrov, A. Gasnikov, and A. Korotin, “Optimal flow matching: Learning straight trajectories in just one step,” Advances in Neural Information Processing Systems, vol. 37, pp. 104 180–104 204, 2024. [50] R. T. Chen and Y. Lipman, “Flow matching on general

geometries,” arXiv preprint arXiv:2302.03660, 2023.

[51] H. Stark, B. Jing, C. Wang, G. Corso, B. Berger, R. Barzilay, and T. Jaakkola, “Dirichlet flow matching with applications to dna sequence design,” arXiv preprint arXiv:2402.05841, 2024.

[52] M. S. Albergo and E. Vanden-Eijnden, “Building normal- izing flows with stochastic interpolants,” arXiv preprint arXiv:2209.15571, 2022.

[53] F. Eijkelboom, G. Bartosh, C. Andersson Naesseth, M. Welling, and J.-W. van de Meent, “Variational flow matching for graph generation,” Advances in Neural Information Processing Systems, vol. 37, pp. 11 735– 11 764, 2024.

[54] Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le, “Flow matching for generative modeling,” arXiv preprint arXiv:2210.02747, 2022.

[55] A. Tong, K. Fatras, N. Malkin, G. Huguet, Y. Zhang, J. Rector-Brooks, G. Wolf, and Y. Bengio, “Im-

proving and generalizing flow-based generative mod- els with minibatch optimal transport,” arXiv preprint arXiv:2302.00482, 2023.

[56] S. Lee, Z. Lin, and G. Fanti, “Improving the training of rectified flows,” Advances in Neural Information Processing Systems, vol. 37, pp. 63 082–63 109, 2024. [57] X. Liu, C. Gong, and qiang liu, “Flow straight and fast: Learning to generate and transfer data with rectified flow,” in The Eleventh International Conference on Learning Representations, 2023. [Online]. Available: https://openreview.net/forum?id=XVjTT1nw5z

[58] R. T. Chen and Y. Lipman, “Riemannian flow matching on general geometries,” arXiv e-prints, pp. arXiv–2302, 2023.

[59] C. Cheng, J. Li, J. Fan, and G. Liu, “α-flow: A unified framework for continuous-state discrete flow matching models,” arXiv preprint arXiv:2504.10283, 2025. [60] O. Davis, S. Kessler, M. Petrache, I. Ceylan, M. Bron- stein, and J. Bose, “Fisher flow matching for generative modeling over discrete data,” Advances in Neural Infor- mation Processing Systems, vol. 37, pp. 139 054–139 084, 2024.

[61] A. Lou, D. Lim, I. Katsman, L. Huang, Q. Jiang, S. N. Lim, and C. M. De Sa, “Neural manifold ordinary differential equations,” Advances in Neural Information Processing Systems, vol. 33, pp. 17 548–17 558, 2020. [62] E. Mathieu and M. Nickel, “Riemannian continuous normalizing flows,” Advances in Neural Information Processing Systems, vol. 33, pp. 2503–2515, 2020. [63] A. Campbell, J. Yim, R. Barzilay, T. Rainforth, and T. Jaakkola, “Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design,” arXiv preprint arXiv:2402.04997, 2024. [64] Y. Qin, M. Madeira, D. Thanou, and P. Frossard, “Defog: Discrete flow matching for graph generation,” arXiv preprint arXiv:2410.04263, 2024.

[65] N. Shaul, I. Gat, M. Havasi, D. Severo, A. Sriram, P. Holderrieth, B. Karrer, Y. Lipman, and R. T. Chen, “Flow matching with general discrete paths: A kinetic- optimal perspective,” arXiv preprint arXiv:2412.03487, 2024.

[66] I. Dunn and D. R. Koes, “Mixed continuous and categorical flow matching for 3d de novo molecule generation,” ArXiv, pp. arXiv–2404, 2024.

[67] S. Tang, Y. Zhang, A. Tong, and P. Chatterjee, “Gumbel- softmax flow matching with straight-through guidance for controllable biological sequence generation,” arXiv preprint arXiv:2503.17361, 2025.

[68] L. Gao and Z. J. Lu, “Rnacg: A universal rna sequence conditional generation model based on flow-matching,” arXiv preprint arXiv:2407.19838, 2024.

[69] D. Nori and W. Jin, “Rnaflow: Rna structure & sequence design via inverse folding-based flow matching,” arXiv preprint arXiv:2405.18768, 2024.

[70] D. Rubin, A. d. S. Costa, M. Ponnapati, and J. Jacobson, “Ribogen: Rna sequence and structure co-generation with equivariant multiflow,” arXiv preprint arXiv:2503.02058, 2025.

PREPRINT. JULY 2025.

19

[71] D. Klein, T. Uscidda, F. Theis, and M. Cuturi, “Genot: Entropic (gromov) wasserstein flow matching with ap- plications to single-cell genomics,” Advances in Neural Information Processing Systems, vol. 37, pp. 103 897– 103 944, 2024.

[72] A. Palma, T. Richter, H. Zhang, A. Dittadi, and F. J. Theis, “cellflow: a generative flow-based model for single-cell count data,” in ICLR 2024 Workshop on Machine Learning for Genomics Explorations.

[73] A. Palma, T. Richter, H. Zhang, M. Lubetzki, A. Tong, A. Dittadi, and F. J. Theis, “Multi-modal and multi- attribute generation of single cells with cfgen,” in The Thirteenth International Conference on Learning Representations.

[74] S. Nagaraj, A. Shanehsazzadeh, H. Park, J. King, and S. Levine, “Igflow: Flow matching for de novo antibody design,” Advances in Neural Information Processing Systems, 2024.

[75] C. Tan, Y. Zhang, Z. Gao, Y. Huang, H. Lin, L. Wu, F. Wu, M. Blanchette, and S. Z. Li, “dyab: Flow matching for flexible antibody design with alphafold- driven pre-binding antigen,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 1, 2025, pp. 782–790.

[76] X. Hou, T. Zhu, M. Ren, D. Bu, X. Gao, C. Zhang, and S. Sun, “Improving molecular graph generation with flow matching and optimal transport,” CoRR, vol. abs/2411.05676, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2411.05676

[77] F. Eijkelboom, G. Bartosh, C. A. Naesseth, M. Welling, and J. van de Meent, “Variational flow in matching for graph generation,” in Advances Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang, Eds., 2024. [Online]. Available: http://papers.nips.cc/paper_files/paper/2024/hash/ 15b780350b302a1bf9a3bd273f5c15a4-Abstract-Conference. html

[78] Y. Qin, M. Madeira, D. Thanou, and P. Frossard, “Defog: Discrete flow matching for graph generation,” CoRR, vol. abs/2410.04263, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2410.04263

[79] Q. Tian, Y. Xu, Y. Yang, Z. Wang, Z. Liu, P. Yan, and X. Li, “Equiflow: Equivariant conditional flow matching with optimal transport for 3d molecular conformation prediction,” CoRR, vol. abs/2412.11082, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2412.11082 [80] D. Reidenbach, F. Nikitin, O. Isayev, and S. G. Paliwal, “Applications of modular co-design for de novo 3d molecule generation,” in NeurIPS 2024 Workshop on AI for New Drug Modalities.

[81] F. Eijkelboom, H. Zimmermann, S. Vadgama, E. Bekkers, M. Welling, C. A. Naesseth, and J.-W. van de Meent, “Controlled generation with equivariant variational flow matching,” in Forty-second International Conference on

Machine Learning, ICML 2025. OpenReview.net, 2025. [82] H. Hong, W. Lin, and K. C. Tan, “Accelerating 3d molecule generation via jointly geometric optimal trans- port,” in The Thirteenth International Conference on Learning Representations.

[83] R. Irwin, A. Tibo, J. P. Janet, and S. Olsson, “Efficient 3d molecular generation with flow matching and scale optimal transport,” CoRR, vol. abs/2406.07266, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2406. 07266

[84] ——, “Semlaflow–efficient 3d molecular generation with latent attention and equivariant flow matching,” in The 28th International Conference on Artificial Intelligence and Statistics, 2025.

[85] Z. Cao, M. Geiger, A. D. S. Costa, D. Reidenbach, K. Kreis, T. Geffner, F. Pellegrini, G. Zhou, and E. Ku- cukbenli, “Efficient molecular conformer generation with so (3) averaged flow-matching and reflow.”

[86] M. Hassan, N. Shenoy, J. Lee, H. Stärk, S. Thaler, and D. Beaini, “Et-flow: Equivariant flow-matching for molecular conformer generation,” in Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, Vancouver, BC, Canada, 2024, NeurIPS December 10 - 15, 2024, A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang, Eds., 2024. [Online]. Available: http://papers.nips.cc/paper_files/paper/2024/hash/ e8bd617e7dd0394ceadf37b4a7773179-Abstract-Conference. html

in Advances

[87] R. Jiao, X. Kong, W. Huang, and Y. Liu, “3d structure prediction of atomic systems with flow-based in direct preference optimization,” Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, Vancouver, BC, Canada, 2024, NeurIPS December 10 - 15, 2024, A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang, Eds., 2024. [Online]. Available: http://papers.nips.cc/paper_files/paper/2024/hash/ c6fdc94aeb2cb3a426d510d970045dab-Abstract-Conference. html

[88] N. Isobe, M. Koyama, K. Hayashi, and K. Fukumizu, “Extended flow matching: a method of conditional generation with generalized continuity equation,” CoRR, vol. abs/2402.18839, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2402.18839

[89] I. Dunn and D. R. Koes, “Mixed continuous and categorical flow matching for 3d de novo molecule generation,” CoRR, vol. abs/2404.19739, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2404.19739 [90] W. Zhou, C. I. Sprague, V. Viliuga, M. Tadiello, A. Elof- sson, and H. Azizpour, “Energy-based flow matching for generating 3d molecular structure,” in Forty-second International Conference on Machine Learning, ICML 2025. OpenReview.net, 2025.

[91] L. Wang, C. Cheng, Y. Liao, Y. Qu, and G. Liu, “Training free guided flow matching with optimal

PREPRINT. JULY 2025.

20

control,” CoRR, vol. abs/2410.18070, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2410.18070 [92] Z. Zhang, M. Wang, and Q. Liu, “Flexsbdd: Structure- based drug design with flexible protein modeling,” in Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang, Eds., 2024. [Online]. Available: http://papers.nips.cc/paper_files/paper/2024/hash/ 60fb8cf8000f0386063fb24ead366330-Abstract-Conference. html

[93] Z. Li, C. Zhou, X. Wang, X. Peng, and M. Zhang, “Geometric representation condition improves equivariant molecule generation,” CoRR, vol. abs/2410.03655, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2410. 03655

[94] L. Vost, V. Chenthamarakshan, P. Das, and C. M. Deane, “Improving structural plausibility in 3d molecule gen- eration via property-conditioned training with distorted molecules,” bioRxiv, pp. 2024–09, 2024.

[95] A. H. Cheng, A. Lo, K. L. K. Lee, S. Miret, and A. Aspuru-Guzik, “Stiefel flow matching for moment-constrained structure elucidation,” CoRR, vol. abs/2412.12540, 2024. [Online]. Available: https: //doi.org/10.48550/arXiv.2412.12540

[96] Y. Zhang, Y. Su, C. Wang, T. Li, Z. Wefers, J. Nirschl, J. Burgess, D. Ding, A. Lozano, E. Lundberg et al., “Cellflow: Simulating cellular morphology changes via flow matching,” arXiv preprint arXiv:2502.09775, 2025. [97] K. Kapusniak, P. Potaptchik, T. Reu, L. Zhang, A. Tong, M. Bronstein, J. Bose, and F. Di Giovanni, “Metric flow matching for smooth interpolations on the data manifold,” Advances in Neural Information Processing Systems, vol. 37, pp. 135 011–135 042, 2024.

[98] S. Shrestha and X. Fu, “Diversified flow matching with translation identifiability,” in Forty-second International Conference on Machine Learning, ICML 2025. Open- Review.net, 2025.

[99] L. Bogensperger, D. Narnhofer, A. Falk, K. Schindler, and T. Pock, “Flowsdf: Flow matching for medical image segmentation using distance transforms,” CoRR, vol. abs/2405.18087, 2024. [Online]. Available: https: //doi.org/10.48550/arXiv.2405.18087

[100] M. Yazdani, Y. Medghalchi, P. Ashrafian, I. Haci- haliloglu, and D. Shahriari, “Flow matching for medical image synthesis: Bridging the gap between speed and quality,” CoRR, vol. abs/2503.00266, 2025. [Online]. Available: https://doi.org/10.48550/arXiv.2503.00266

[101] D. Zhang, Q. Han, Y. Xiong, and H. Du, “Mutli- modal straight flow matching for accelerated MR imaging,” Comput. Biol. Medicine, vol. 178, p. 108668, 2024. [Online]. Available: https://doi.org/10.1016/j. compbiomed.2024.108668

[102] T. Huang, T. Liu, M. Babadi, W. Jin, and R. Ying, “Scalable generation of spatial transcriptomics from histology images via whole-slide flow matching,” arXiv

preprint arXiv:2506.05361, 2025.

[103] D. Haviv, A.-A. Pooladian, D. Pe’er, and B. Amos, “Wasserstein flow matching: Generative modeling preprint over arXiv:2411.00698, 2024.

distributions,”

families

arXiv

of

[104] G. Wei and L. Ma, “Stream-level flow matching with gaussian processes,” in Forty-second International Conference on Machine Learning, 2025. [Online]. Available: https://openreview.net/forum?id=qg9p1I5lmp [105] P. Wang, Y. Qi, Y. Wang, and G. Pan, “Flow matching few-trial neural adaptation with stable latent for dynamics,” in Forty-second International Conference on Machine Learning, 2025. [Online]. Available: https://openreview.net/forum?id=nKJEAQ6JCY [106] M. Ruth, B. Hannon, M. Ruth, and B. Hannon, Modeling

dynamic biological systems. Springer, 1997.

[107] G. M. Edelman and J. A. Gally, “Degeneracy and complexity in biological systems,” Proceedings of the national academy of sciences, vol. 98, no. 24, pp. 13 763– 13 768, 2001.

[108] J. W. Haefner, Modeling biological systems:: principles and applications. Springer Science & Business Media, 2005.

[109] A. Rhie, S. A. McCarthy, O. Fedrigo, J. Damas, G. Formenti, S. Koren, M. Uliano-Silva, W. Chow, A. Fungtammasan, J. Kim et al., “Towards complete and error-free genome assemblies of all vertebrate species,” Nature, vol. 592, no. 7856, pp. 737–746, 2021. [110] J. L.-M. et al, “One thousand plant transcriptomes and the phylogenomics of green plants,” Nature, vol. 574, no. 7780, pp. 679–685, 2019.

[111] D. Kim, J.-Y. Lee, J.-S. Yang, J. W. Kim, V. N. Kim, and H. Chang, “The architecture of sars-cov-2 transcriptome,” Cell, vol. 181, no. 4, pp. 914–921, 2020.

[112] U. Sahin, K. Karikó, and Ö. Türeci, “mrna-based therapeutics—developing a new class of drugs,” Nature reviews Drug discovery, vol. 13, no. 10, pp. 759–780, 2014.

[113] D. Baker and A. Sali, “Protein structure prediction and structural genomics,” Science, vol. 294, no. 5540, pp. 93–96, 2001.

[114] M. Baek, F. DiMaio, I. Anishchenko, J. Dauparas, S. Ovchinnikov, G. R. Lee, J. Wang, Q. Cong, L. N. Kinch, R. D. Schaeffer et al., “Accurate prediction of protein structures and interactions using a three-track neural network,” Science, vol. 373, no. 6557, pp. 871– 876, 2021.

[115] A. Jabbar, X. Li, and B. Omar, “A survey on generative adversarial networks: Variants, applications, and training,” ACM Computing Surveys (CSUR), vol. 54, no. 8, pp. 1–49, 2021.

[116] X. Xia, X. Pan, N. Li, X. He, L. Ma, X. Zhang, and N. Ding, “Gan-based anomaly detection: A review,” Neurocomputing, vol. 493, pp. 497–535, 2022. [117] J. G. Greener, S. M. Kandathil, L. Moffat, and D. T. Jones, “A guide to machine learning for biologists,” Nature reviews Molecular cell biology, vol. 23, no. 1, pp. 40–55, 2022.

PREPRINT. JULY 2025.

21

[118] P. Li, Y. Pei, and J. Li, “A comprehensive survey on design and application of autoencoder in deep learning,” Applied Soft Computing, vol. 138, p. 110176, 2023.

[119] F.-A. Croitoru, V. Hondru, R. T. Ionescu, and M. Shah, “Diffusion models in vision: A survey,” IEEE Trans- actions on Pattern Analysis and Machine Intelligence, vol. 45, no. 9, pp. 10 850–10 869, 2023.

[120] S. Liang, Z. Pan, W. Liu, J. Yin, and M. De Rijke, “A survey on variational autoencoders in recommender systems,” ACM Computing Surveys, vol. 56, no. 10, pp. 1–40, 2024.

[121] H. Cao, C. Tan, Z. Gao, Y. Xu, G. Chen, P.-A. Heng, and S. Z. Li, “A survey on generative diffusion models,” IEEE Transactions on Knowledge and Data Engineering, 2024.

[122] M. M. Saad, R. O’Reilly, and M. H. Rehmani, “A survey on training challenges in generative adversarial networks for biomedical image analysis,” Artificial Intelligence Review, vol. 57, no. 2, p. 19, 2024.

[123] X. Tang, H. Dai, E. Knight, F. Wu, Y. Li, T. Li, and M. Gerstein, “A survey of generative ai for de novo drug design: new frontiers in molecule and protein generation,” Briefings in Bioinformatics, vol. 25, no. 4, p. bbae338, 2024.

[124] Y. Du, A. R. Jamasb, J. Guo, T. Fu, C. Harris, Y. Wang, C. Duan, P. Liò, P. Schwaller, and T. L. Blundell, “Machine learning-aided generative molecular design,” Nature Machine Intelligence, vol. 6, no. 6, pp. 589–604, 2024.

[125] Q. Zhang, K. Ding, T. Lv, X. Wang, Q. Yin, Y. Zhang, J. Yu, Y. Wang, X. Li, Z. Xiang et al., “Scientific large language models: A survey on biological & chemical domains,” ACM Computing Surveys, vol. 57, no. 6, pp. 1–38, 2025.

[126] M. Mock, C. J. Langmead, P. Grandsard, S. Edavettal, and A. Russell, “Recent advances in generative biology for biotherapeutic discovery,” Trends in Pharmacological Sciences, vol. 45, no. 3, pp. 255–267, 2024.

[127] D. B. Kell, S. Samanta, and N. Swainston, “Deep learning and generative methods in cheminformatics and chemical biology: navigating small molecule space intelligently,” Biochemical Journal, vol. 477, no. 23, pp. 4559–4580, 2020.

[132] M. Shimoyama, J. De Pons, G. T. Hayman, S. J. Laulederkind, W. Liu, R. Nigam, V. Petri, J. R. Smith, M. Tutaj, S.-J. Wang et al., “The rat genome database 2015: genomic, phenotypic and environmental variations and disease,” Nucleic acids research, vol. 43, no. D1, pp. D743–D750, 2015.

[133] M. AlQuraishi, “Proteinnet: a standardized data set for machine learning of protein structure,” BMC bioinfor- matics, vol. 20, pp. 1–10, 2019.

[134] S. Kim, P. A. Thiessen, E. E. Bolton, J. Chen, G. Fu, A. Gindulyte, L. Han, J. He, S. He, B. A. Shoemaker et al., “Pubchem substance and compound databases,” Nucleic acids research, vol. 44, no. D1, pp. D1202– D1213, 2016.

[135] D. P. Kingma, M. Welling et al., “Auto-encoding

variational bayes,” 2013.

[136] ——, “An introduction to variational autoencoders,” Foundations and Trends® in Machine Learning, vol. 12, no. 4, pp. 307–392, 2019.

[137] L. Girin, S. Leglaive, X. Bie, J. Diard, T. Hueber, and X. Alameda-Pineda, “Dynamical variational au- toencoders: A comprehensive review,” arXiv preprint arXiv:2008.12595, 2020.

[138] Y. Pu, Z. Gan, R. Henao, X. Yuan, C. Li, A. Stevens, and L. Carin, “Variational autoencoder for deep learning of images, labels and captions,” Advances in neural information processing systems, vol. 29, 2016. [139] M. J. Kusner, B. Paige, and J. M. Hernández-Lobato, “Grammar variational autoencoder,” in International conference on machine learning. PMLR, 2017, pp. 1945–1954.

[140] G. Bredell, K. Flouris, K. Chaitanya, E. Erdil, and E. Konukoglu, “Explicitly minimizing the blur error of variational autoencoders,” arXiv preprint arXiv:2304.05939, 2023.

[141] Y. Takida, W.-H. Liao, C.-H. Lai, T. Uesaka, S. Taka- hashi, and Y. Mitsufuji, “Preventing oversmoothing in vae via generalized variance parameterization,” Neuro- computing, vol. 509, pp. 137–156, 2022.

[142] B. Dai, Z. Wang, and D. Wipf, “The usual suspects? reassessing blame for vae posterior collapse,” in Interna- tional conference on machine learning. PMLR, 2020, pp. 2313–2322.

[128] M. Liu, C. Li, R. Chen, D. Cao, and X. Zeng, “Geometric deep learning for drug discovery,” Expert Systems with Applications, vol. 240, p. 122498, 2024.

[143] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein GAN,” CoRR, vol. abs/1701.07875, 2017. [Online]. Available: http://arxiv.org/abs/1701.07875

[129] Z. Yang, X. Zeng, Y. Zhao, and R. Chen, “Alphafold2 and its applications in the fields of biology and medicine,” Signal Transduction and Targeted Therapy, vol. 8, no. 1, p. 115, 2023.

[130] V. Mariani, M. Biasini, A. Barbato, and T. Schwede, “lddt: a local superposition-free score for comparing protein structures and models using distance difference tests,” Bioinformatics, vol. 29, no. 21, pp. 2722–2728, 2013.

[131] H. Berman, K. Henrick, and H. Nakamura, “Announcing the worldwide protein data bank,” Nature structural & molecular biology, vol. 10, no. 12, pp. 980–980, 2003.

[144] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. Paul Smolley, “Least squares generative adversarial networks,” in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2794–2802. [145] M. Mirza and S. Osindero, “Conditional generative adversarial nets,” arXiv preprint arXiv:1411.1784, 2014. [146] S. M. Bafti, C. S. Ang, G. Marcelli, M. M. Hossain, S. Maxamhud, and A. D. Tsaousis, “Biogan: An unpaired gan-based image to image translation model for micro- biological images,” arXiv preprint arXiv:2306.06217, 2023.

[147] P. Chaudhari, H. Agrawal, and K. Kotecha, “Data

PREPRINT. JULY 2025.

22

augmentation using mg-gan for improved cancer classifi- cation on gene expression data,” Soft Computing, vol. 24, pp. 11 381–11 391, 2020.

[148] H. Yang, Z. Xiang, X. Li, and W. Zhang, “An improved gan-based data augmentation model for addressing data scarcity in srms,” Measurement Science and Technology, vol. 36, no. 2, p. 026129, 2025.

[149] A. Osokin, A. Chessel, R. E. Carazo Salas, and F. Vaggi, “Gans for biological image synthesis,” in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2233–2242.

[150] D. Rezende and S. Mohamed, “Variational inference with normalizing flows,” in International conference on machine learning. PMLR, 2015, pp. 1530–1538. [151] I. Kobyzev, S. J. Prince, and M. A. Brubaker, “Nor- malizing flows: An introduction and review of current methods,” IEEE transactions on pattern analysis and machine intelligence, vol. 43, no. 11, pp. 3964–3979, 2020.

[152] L. Dinh, D. Krueger, and Y. Bengio, “NICE: non- linear independent components estimation,” in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2015. [Online]. Available: http://arxiv.org/abs/ 1410.8516

[153] L. Dinh, J. Sohl-Dickstein, and S. Bengio, “Density estimation using real NVP,” in 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. [Online]. Available: https://openreview.net/forum?id=HkpbnH9lx [154] D. P. Kingma and P. Dhariwal, “Glow: Generative flow with invertible 1x1 convolutions,” in Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, [Online]. Avail- Eds., 2018, pp. 10 236–10 245. able: https://proceedings.neurips.cc/paper/2018/hash/ d139db6a236200b21cc7f752979132d0-Abstract.html

[155] G. Papamakarios, I. Murray, and T. Pavlakou, “Masked autoregressive flow for density estimation,” in Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett, Eds., 2017, pp. 2338–2347. [Online]. Avail- able: https://proceedings.neurips.cc/paper/2017/hash/ 6c1da886822c67822bcf3679d04369fa-Abstract.html

[156] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duve- naud, “Neural ordinary differential equations,” Advances in neural information processing systems, vol. 31, 2018. [157] J. Sohl-Dickstein, E. A. Weiss, N. Maheswaranathan, and S. Ganguli, “Deep unsupervised learning using nonequilibrium thermodynamics,” in Proceedings of

the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, ser. JMLR Workshop and Conference Proceedings, F. R. Bach and D. M. Blei, Eds., vol. 37. JMLR.org, 2015, pp. 2256–2265. [Online]. Available: http://proceedings.mlr.press/v37/sohl-dickstein15.html

in Neural

“Generative modeling [158] Y. Song and S. Ermon, the data distribution,” by estimating gradients of Information Processing in Advances Systems on Neural Annual Conference 32: Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, H. M. Wallach, H. Larochelle, A. Beygelzimer, and R. Garnett, F. d’Alché-Buc, E. B. Fox, [Online]. Avail- Eds., 2019, pp. 11 895–11 907. able: https://proceedings.neurips.cc/paper/2019/hash/ 3001ef257407d5a371a96dcd947c7d93-Abstract.html

9th

International Conference

[159] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, “Score-based generative modeling through stochastic differential equations,” on Learning in Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [Online]. Available: https://openreview.net/forum?id=PxTIG12RRHS [160] J. Song, C. Meng, and S. Ermon, “Denoising diffusion implicit models,” in 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [Online]. Available: https://openreview.net/forum?id= St1giarCHLP

[161] A. Campbell, J. Benton, V. D. Bortoli, T. Rainforth, G. Deligiannidis, and A. Doucet, “A continuous time framework for discrete denoising models,” in Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., 2022. [Online]. Available: http://papers.nips.cc/paper_files/paper/2022/hash/ b5b528767aa35f5b1a60fe0aaeca0563-Abstract-Conference. html

[162] J. Austin, D. D.

van

discrete

Johnson,

“Structured

state-spaces,”

den Berg, in

J. Ho, D. Tarlow, denoising and R. diffusion models in Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021, pp. 17 981–17 993. [Online]. Avail- able: https://proceedings.neurips.cc/paper/2021/hash/ 958c530554f78bcd8e97125b70e6973d-Abstract.html

[163] T. Salimans and J. Ho, “Progressive distillation for fast sampling of diffusion models,” in The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [Online]. Available: https://openreview.net/forum?

PREPRINT. JULY 2025.

23

id=TIdIXIpzhoI

[164] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu, “Dpm-solver: A fast ODE solver for diffusion probabilistic model sampling in around 10 steps,” in Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., 2022. [Online]. Available: http://papers.nips.cc/paper_files/paper/2022/hash/ 260a14acce2a89dad36adc8eefe7c59e-Abstract-Conference. html

[165] Y. Lipman, M. Havasi, P. Holderrieth, N. Shaul, M. Le, B. Karrer, R. T. Chen, D. Lopez-Paz, H. Ben-Hamu, and I. Gat, “Flow matching guide and code,” arXiv preprint arXiv:2412.06264, 2024.

[166] X. Liu, C. Gong, and Q. Liu, “Flow straight and fast: Learning to generate and transfer data with rectified flow,” arXiv preprint arXiv:2209.03003, 2022. [167] I. Dunn and D. R. Koes, “Exploring discrete flow matching for 3d de novo molecule generation,” ArXiv, pp. arXiv–2411, 2024.

graph

“Constrained

[168] S. Tarafder and D. Bhattacharya, “Rnabpflow: Base pair- augmented se (3)-flow matching for conditional rna 3d structure generation,” bioRxiv, pp. 2025–01, 2025. [169] E. Hoogeboom, V. G. Satorras, C. Vignac, and M. Welling, “Equivariant diffusion for molecule gen- eration in 3d,” in International conference on machine learning. PMLR, 2022, pp. 8867–8887. [170] Q. Liu, M. Allamanis, M. Brockschmidt,

and variational A. L. Gaunt, for molecule design,” in Advances autoencoders in Neural 31: Information Processing Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds., 2018, pp. 7806–7815. [Online]. Avail- https://proceedings.neurips.cc/paper/2018/hash/ able: b8a03c5c15fcfa8dae0b03351eb1742f-Abstract.html [171] C. Vignac, I. Krawczuk, A. Siraudin, B. Wang, V. Cevher, and P. Frossard, “Digress: Discrete denoising diffusion for graph generation,” in The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [Online]. Available: https://openreview.net/forum? id=UaAD-Nu86WX

Systems

in Neural

[172] S. Luo, J. Guan, J. Ma, and J. Peng, “A 3d generative model for structure-based drug design,” in Advances Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, 2021, Eds., [Online]. Avail- 6229–6239. https://proceedings.neurips.cc/paper/2021/hash/ able: 314450613369e0ee72d0da7f6fee773c-Abstract.html

pp.

[173] X. Peng, S. Luo, J. Guan, Q. Xie, J. Peng, and J. Ma, “Pocket2mol: Efficient molecular sampling based on 3d protein pockets,” in International conference on machine learning. PMLR, 2022, pp. 17 644–17 655.

[174] F. Noé, A. Tkatchenko, K.-R. Müller, and C. Clementi, “Machine learning for molecular simulation,” Annual review of physical chemistry, vol. 71, no. 1, pp. 361– 390, 2020.

[175] S. A. Hollingsworth and R. O. Dror, “Molecular dy- namics simulation for all,” Neuron, vol. 99, no. 6, pp. 1129–1143, 2018.

[176] W. P. Walters and R. Barzilay, “Applications of deep learning in molecule generation and molecular property prediction,” Accounts of chemical research, vol. 54, no. 2, pp. 263–270, 2020.

[177] Y. Du, A. R. Jamasb, J. Guo, T. Fu, C. Harris, Y. Wang, C. Duan, P. Liò, P. Schwaller, and T. L. Blundell, “Machine learning-aided generative molecular design,” Nat. Mac. Intell., vol. 6, no. 6, pp. 589– 604, 2024. [Online]. Available: https://doi.org/10.1038/ s42256-024-00843-5

[178] S. Axelrod and R. Gomez-Bombarelli, “Geom, energy- annotated molecular conformations for property predic- tion and molecular generation,” Scientific Data, vol. 9, no. 1, p. 185, 2022.

[179] R. Ramakrishnan, P. O. Dral, M. Rupp, and O. A. Von Lilienfeld, “Quantum chemistry structures and properties of 134 kilo molecules,” Scientific data, vol. 1, no. 1, pp. 1–7, 2014.

[180] Z. Guo, K. Guo, B. Nan, Y. Tian, R. G.

Iyer, Y. Ma, O. Wiest, X. Zhang, W. Wang, C. Zhang, and N. V. Chawla, “Graph-based molecular representation the Thirty-Second learning,” International Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China. ijcai.org, 2023, pp. 6638–6646. [Online]. Available: https://doi.org/10.24963/ijcai.2023/744

in Proceedings of

[181] N. De Cao and T. Kipf, “Molgan: An implicit generative model for small molecular graphs,” arXiv preprint arXiv:1805.11973, 2018.

[182] Y. Li, L. Zhang, and Z. Liu, “Multi-objective de novo drug design with conditional graph generative model,” Journal of cheminformatics, vol. 10, pp. 1–24, 2018.

[183] B. Baillif, J. Cole, P. McCabe, and A. Bender, “Deep generative models for 3d molecular structure,” Current Opinion in Structural Biology, vol. 80, p. 102566, 2023. [184] X. Peng, J. Guan, Q. Liu, and J. Ma, “Moldiff: Addressing the atom-bond inconsistency problem in 3d molecule diffusion generation,” in International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, ser. Proceedings of Machine Learning Research, A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, Eds., vol. 202. PMLR, 2023, pp. 27 611–27 629. [Online]. Available: https://proceedings.mlr.press/v202/peng23b. html

[185] L. Huang, H. Zhang, T. Xu, and K. Wong, “MDM: molecular diffusion model for 3d molecule generation,”

PREPRINT. JULY 2025.

24

in Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, B. Williams, Y. Chen, and J. Neville, Eds. AAAI Press, 2023, pp. 5105–5112. [Online]. Available: https://doi.org/10.1609/aaai.v37i4. 25639

[186] W. Zhou, C. I. Sprague, and H. Azizpour, “Energy-based

flow matching for molecular docking,” 2025.

[187] J. Huang and D. Zhang, “Molform: Multi-modal flow matching for structure-based drug design,” arXiv preprint arXiv:2507.05503, 2025.

[188] Y. Qu, K. Qiu, Y. Song, J. Gong, J. Han, M. Zheng, H. Zhou, and W.-Y. Ma, “Molcraft: Structure-based drug design in continuous parameter space,” in Forty-first International Conference on Machine Learning. [189] K. Xue, Y. Zhou, S. Nie, X. Min, X. Zhang, J. Zhou, and C. Li, “Unifying bayesian flow networks and diffusion models through stochastic differential equations,” in International Conference on Machine Learning. PMLR, 2024, pp. 55 656–55 681.

[190] X. Peng, R. Guo, Y. Xu, J. Guan, Y. Jia, Y. Huang, M. Zhang, J. Peng, J. Sun, C. Han et al., “Decipher fundamental atomic interactions to unify generative molecular docking and design,” bioRxiv, pp. 2024–10, 2024.

[191] J. Yim, A. Campbell, A. Y. Foong, M. Gastegger, J. Jiménez-Luna, S. Lewis, V. G. Satorras, B. S. Veeling, R. Barzilay, T. Jaakkola et al., “Fast protein backbone generation with se (3) flow matching,” arXiv preprint arXiv:2310.05297, 2023.

[192] A. J. Bose, T. Akhound-Sadegh, G. Huguet, K. Fatras, J. Rector-Brooks, C.-H. Liu, A. C. Nica, M. Ko- rablyov, M. Bronstein, and A. Tong, “Se (3)-stochastic flow matching for protein backbone generation,” arXiv preprint arXiv:2310.02391, 2023.

[193] J. Yan, Z. Cui, W. Yan, Y. Chen, M. Pu, S. Li, and S. Ye, “Robust and reliable de novo protein design: A flow-matching-based protein generative model achieves remarkably high success rates,” bioRxiv, pp. 2025–04, 2025.

[194] S. Wagner, L. Seute, V. Viliuga, N. Wolf, F. Gräter, and J. Stühmer, “Generating highly designable proteins with geometric algebra flow matching,” arXiv preprint arXiv:2411.05238, 2024.

[195] G. Huguet, J. Vuckovic, K. Fatras, E. Thibodeau- Laufer, P. Lemos, R. Islam, C.-H. Liu, J. Rector-Brooks, T. Akhound-Sadegh, M. Bronstein et al., “Sequence- augmented se (3)-flow matching for conditional protein backbone generation,” arXiv preprint arXiv:2405.20313, 2024.

[196] T. Geffner, K. Didi, Z. Zhang, D. Reidenbach, Z. Cao, J. Yim, M. Geiger, C. Dallago, E. Kucukbenli, A. Vahdat et al., “Proteina: Scaling flow-based protein structure generative models,” arXiv preprint arXiv:2503.00710, 2025.

[197] H. Stark, B. Jing, T. Geffner, J. Yim, T. Jaakkola, A. Vahdat, and K. Kreis, “Protcomposer: Compositional protein structure generation with 3d ellipsoids,” arXiv preprint arXiv:2503.05025, 2025.

[198] S. Yang, L. Ju, P. Cheng, J. Zhou, Y. Cai, and D. Feng, “Co-design protein sequence and structure in discrete space via generative flow,” Bioinformatics, vol. 41, no. 5, p. btaf248, 2025.

[199] R. Chen, D. Xue, X. Zhou, Z. Zheng, X. Zeng, and Q. Gu, “An all-atom generative model for designing protein complexes,” arXiv preprint arXiv:2504.13075, 2025.

[200] J. Yim, A. Campbell, E. Mathieu, A. Y. Foong, M. Gastegger, J. Jiménez-Luna, S. Lewis, V. G. Satorras, B. S. Veeling, F. Noé et al., “Improved motif-scaffolding with se (3) flow matching,” ArXiv, pp. arXiv–2401, 2024. [201] Y. Huang, Y. Liu, L. Wu, H. Lin, C. Tan, O. Zhang, Z. Gao, S. Li, Z. Liu, Y. Liu et al., “Eva: Geometric inverse design for fast protein motif-scaffolding with coupled flow,” in The Thirteenth International Confer- ence on Learning Representations.

[202] J. Liu, S. Li, C. Shi, Z. Yang, and J. Tang, “Design of ligand-binding proteins with atomic flow matching,” arXiv preprint arXiv:2409.12080, 2024.

[203] H. Stark, B. Jing, R. Barzilay, and T. Jaakkola, “Har- monic self-conditioned flow matching for joint multi- ligand docking and binding site design,” in Forty-first International Conference on Machine Learning, 2024. [204] Z. Zhang, M. Zitnik, and Q. Liu, “Generalized protein pocket generation with prior-informed flow matching,” arXiv preprint arXiv:2409.19520, 2024.

[205] M. S.

Jones, S. Khanna, and A. L. Ferguson, “Flowback: A generalized flow-matching approach for biomolecular backmapping,” J. Chem. Inf. Model., vol. 65, no. 2, pp. 672–692, 2025. [Online]. Available: https://doi.org/10.1021/acs.jcim.4c02046

[206] B. Jing, B. Berger, and T. Jaakkola, “Alphafold meets flow matching for generating protein ensembles,” arXiv preprint arXiv:2402.04845, 2024.

[207] Y. Jin, Q. Huang, Z. Song, M. Zheng, D. Teng, and Q. Shi, “P2dflow: A protein ensemble generative model with se (3) flow matching,” Journal of Chemical Theory and Computation, vol. 21, no. 6, pp. 3288–3296, 2025. [208] J. S. Lee and P. M. Kim, “Flowpacker: Protein side-chain packing with torsional flow matching,” Bioinformatics, p. btaf010, 2025.

[209] Y.-L. Liao, B. Wood, A. Das, and T. Smidt, “Equiformerv2: Improved equivariant transformer for scaling to higher-degree representations,” arXiv preprint arXiv:2306.12059, 2023.

[210] J. Li, C. Cheng, Z. Wu, R. Guo, S. Luo, Z. Ren, J. Peng, and J. Ma, “Full-atom peptide design based on multi- modal flow matching,” arXiv preprint arXiv:2406.00735, 2024.

[211] A. Morehead and J. Cheng, “Flowdock: Geometric flow matching for generative protein-ligand docking and affinity prediction,” ArXiv, pp. arXiv–2412, 2025. [212] H. Lin, O. Zhang, H. Zhao, D. Jiang, L. Wu, Z. Liu,

PREPRINT. JULY 2025.

25

Y. Huang, and S. Z. Li, “Ppflow: Target-aware peptide design with torsional flow matching,” bioRxiv, pp. 2024– 03, 2024.

[213] D. Huang and S. Tu, “Non-linear flow matching for full- atom peptide design,” arXiv preprint arXiv:2502.15855, 2025.

[214] Z. Kong, Y. Zhu, Y. Xu, H. Zhou, M. Yin, J. Wu, H. Xu, C.-Y. Hsieh, T. Hou, and J. Wu, “Protflow: Fast protein sequence design via flow matching on compressed protein language model embeddings,” arXiv preprint arXiv:2504.10983, 2025.

on Bioinformatics

[215] J. Zhang, Z. Liu, S. Bai, H. Cao, Y. Li, and L. Zhang, “Efficient antibody structure refinement using energy- guided SE(3) flow matching,” in IEEE International and Biomedicine, Conference BIBM 2024, Lisbon, Portugal, December 3-6, 2024, M. Cannataro, H. J. Zheng, L. Gao, J. Cheng, J. L. de Miranda, E. Zumpano, X. Hu, Y. Cho, and T. Park, IEEE, 2024, pp. 146–153. [Online]. Available: Eds. https://doi.org/10.1109/BIBM62325.2024.10821730 [216] S. Griffiths-Jones, A. Bateman, M. Marshall, A. Khanna, and S. R. Eddy, “Rfam: an rna family database,” Nucleic Acids Research, vol. 31, no. 1, pp. 439–441, 2003. [Online]. Available: https://doi.org/10.1093/nar/gkg006 [217] Y. Chu, D. Yu, Y. Li, K. Huang, Y. Shen, L. Cong, J. Zhang, and M. Wang, “A 5’ utr language model for decoding untranslated regions of mrna and function predictions,” Nature Machine Intelligence, vol. 6, no. 4, pp. 449–460, 2024. [Online]. Available: https://doi.org/10.1038/s42256-024-00823-9

[218] B. Adamczyk, M. Antczak, and M. Szachniuk, “Rnasolo: a repository of cleaned pdb-derived rna 3d structures,” Bioinformatics, vol. 38, no. 14, pp. 3668–3670, 2022.

[219] A. Bastidas-Ponce, S. Tritschler, L. Dony, K. Scheibner, M. Tarquis-Medina, C. Salinno, S. Schirge, I. Burtscher, A. Böttcher, F. J. Theis et al., “Comprehensive single cell mrna profiling reveals a detailed roadmap for pancreatic endocrinogenesis,” Development, vol. 146, no. 12, p. dev173849, 2019.

[220] S. R. Srivatsan, J. L. McFaline-Figueroa, V. Ramani, L. Saunders, J. Cao, J. Packer, H. A. Pliner, D. L. Jackson, R. M. Daza, L. Christiansen et al., “Massively multiplex chemical transcriptomics at single-cell resolu- tion,” Science, vol. 367, no. 6473, pp. 45–51, 2020.

[221] M. D. Luecken, D. B. Burkhardt, R. Cannoodt, C. Lance, A. Agrawal, H. Aliee, A. T. Chen, L. Deconinck, A. M. Detweiler, A. A. Granados et al., “A sandbox for prediction and integration of dna, rna, and proteins in single cells,” in Thirty-fifth conference on neural information processing systems datasets and benchmarks track (Round 2), 2021.

[222] C. Derbois, M.-A. Palomares, J.-F. Deleuze, E. Cabannes, and E. Bonnet, “Single cell transcriptome sequencing of stimulated and frozen human peripheral blood mononu- clear cells,” Scientific Data, vol. 10, no. 1, p. 433, 2023. [223] G. La Manno, R. Soldatov, A. Zeisel, E. Braun, H. Hochgerner, V. Petukhov, K. Lidschreiber, M. E. Kastriti, P. Lönnerberg, A. Furlan et al., “Rna velocity

of single cells,” Nature, vol. 560, no. 7719, pp. 494–498, 2018.

[224] L. Sikkema, C. Ramírez-Suástegui, D. C. Strobl, T. E. Gillett, L. Zappia, E. Madissoon, N. S. Markov, L.-E. Zaragosi, Y. Ji, M. Ansari et al., “An integrated cell atlas of the lung in health and disease,” Nature medicine, vol. 29, no. 6, pp. 1563–1577, 2023.

[225] F. A. Vieira Braga, G. Kar, M. Berg, O. A. Carpaij, K. Polanski, L. M. Simon, S. Brouwer, T. Gomes, L. Hesse, J. Jiang et al., “A cellular census of human lungs identifies novel cell states in health and in asthma,” Nature medicine, vol. 25, no. 7, pp. 1153–1163, 2019. [226] N. Schaum, J. Karkanias, N. F. Neff, A. P. May, S. R. Quake, T. Wyss-Coray, S. Darmanis, J. Batson, O. Botvinnik, M. B. Chen et al., “Single-cell transcrip- tomics of 20 mouse organs creates a tabula muris: The tabula muris consortium,” Nature, vol. 562, no. 7727, p. 367, 2018.

[227] K. R. Moon, D. van Dijk, Z. Wang, S. Gigante, D. B. Burkhardt, W. S. Chen, K. Yim, A. van den Elzen, M. J. Hirn, R. R. Coifman, N. B. Ivanova, G. Wolf, and S. Kr- ishnaswamy, “Visualizing structure and transitions in high-dimensional biological data,” Nature Biotechnology, vol. 37, no. 12, pp. 1482–1492, 2019.

[228] C. Lance, M. D. Luecken, D. B. Burkhardt, R. Cannoodt, P. Rautenstrauch, A. Laddach, A. Ubingazhibov, Z.-J. Cao, K. Deng, S. Khan, Q. Liu, N. Russkikh, G. Ryazant- sev, U. Ohler, A. O. Pisco, J. Bloom, S. Krishnaswamy, and F. J. Theis, “Multimodal single cell data integration challenge: Results and lessons learned,” bioRxiv, 2022. [229] J. J. Irwin, T. Sterling, M. M. Mysinger, E. S. Bolstad, and R. G. Coleman, “Zinc: a free tool to discover chemistry for biology,” Journal of chemical information and modeling, vol. 52, no. 7, pp. 1757–1768, 2012.

[230] N. Brown, M. Fiscato, M. H. Segler, and A. C. Vaucher, “Guacamol: benchmarking models for de novo molecular design,” Journal of chemical information and modeling, vol. 59, no. 3, pp. 1096–1108, 2019.

[231] D. Polykovskiy, A. Zhebrak, B. Sanchez-Lengeling, S. Golovanov, O. Tatanov, S. Belyaev, R. Kurbanov, A. Artamonov, V. Aladinskiy, M. Veselov et al., “Molec- ular sets (moses): a benchmarking platform for molecular generation models,” Frontiers in pharmacology, vol. 11, p. 565644, 2020.

[232] M. Buttenschoen, G. M. Morris, and C. M. Deane, “Posebusters: Ai-based docking methods to generate physically valid poses or generalise to novel sequences,” Chemical Science, vol. 15, pp. 3130– 3139, 2024. [Online]. Available: https://doi.org/10.1039/ D3SC04185A

fail

[233] J. Dunbar, K. Krawczyk, J. Leem, T. Baker, A. Fuchs, G. Georges, J. Shi, and C. M. Deane, “Sabdab: the structural antibody database,” Nucleic Acids Research, vol. 42, no. D1, pp. D1140–D1146, 2014. [Online]. Available: https://doi.org/10.1093/nar/gkt1043

[234] J. Wu, X. Kong, N. Sun, J. Wei, S. Shan, F. Feng, F. Wu, J. Peng, L. Zhang, Y. Liu et al., “Flowdesign: Improved design of antibody cdrs through flow matching and better

PREPRINT. JULY 2025.

26

prior distributions,” Cell Systems, 2025.

[235] L. Hu, M. L. Benson, R. D. Smith, M. G. Lerner, and H. A. Carlson, “Binding moad (mother of all databases),” Proteins: Structure, Function, and Bioinfor- matics, vol. 60, no. 3, pp. 333–340, 2005.

[236] P. G. Francoeur, T. Masuda, J. Sunseri, A. Jia, R. B. Iovanisci, I. Snyder, and D. R. Koes, “Three-dimensional convolutional neural networks and a cross-docked data set for structure-based drug design,” Journal of chemical information and modeling, vol. 60, no. 9, pp. 4200–4215, 2020.

[237] R. Wang, X. Fang, Y. Lu, and S. Wang, “The pdbbind database: Collection of binding affinities for protein- ligand complexes with known three-dimensional struc- tures,” Journal of medicinal chemistry, vol. 47, no. 12, pp. 2977–2980, 2004.

[238] P. Agrawal, H. Singh, H. K. Srivastava, S. Singh, G. Kishore, and G. P. Raghava, “Benchmarking of different molecular docking methods for protein-peptide docking,” BMC bioinformatics, vol. 19, pp. 105–124, 2019.

[239] B. E. Suzek, Y. Wang, H. Huang, P. B. McGarvey, C. H. Wu, and U. Consortium, “Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches,” Bioinformatics, vol. 31, no. 6, pp. 926–932, 2015.

[240] H. M. Berman, J. Westbrook, Z. Feng, G. Gilliland, T. N. Bhat, H. Weissig, I. N. Shindyalov, and P. E. Bourne, “The protein data bank,” Nucleic acids research, vol. 28, no. 1, pp. 235–242, 2000.

[241] J. Yim, B. L. Trippe, V. De Bortoli, E. Mathieu, A. Doucet, R. Barzilay, and T. Jaakkola, “Se (3) diffusion model with application to protein backbone generation,” arXiv preprint arXiv:2302.02277, 2023.

[242] M. S. Jones, S. Khanna, and A. L. Ferguson, “Flowback: A generalized flow-matching approach for biomolecular backmapping,” Journal of Chemical Information and Modeling, 2025.

[243] A. Cornman, J. West-Roberts, A. P. Camargo, S. Roux, M. Beracochea, M. Mirdita, S. Ovchinnikov, and Y. Hwang, “The omg dataset: An open metagenomic corpus for mixed-modality genomic language modeling,” bioRxiv, pp. 2024–08, 2024.

[244] T. H. Olsen, F. Boyles, and C. M. Deane, “Observed antibody space: A diverse database of cleaned, annotated, and translated unpaired and paired antibody sequences,” Protein Science, vol. 31, no. 1, pp. 141–146, 2022. [Online]. Available: https://doi.org/10.1002/pro.4205

[245] J. Adolf-Bryfogle, O. Kalyuzhniy, M. Kubitz, B. D. Weitzner, X. Hu, Y. Adachi, W. R. Schief, and R. L. J. Dunbrack, “RosettaAntibodyDesign (RAbD): A general framework for computational antibody design,” PLOS Computational Biology, vol. 14, no. 4, p. [Online]. Available: e1006112, 2018. https://doi.org/10.1371/journal.pcbi.1006112

[246] T. UniProt Consortium, “Uniprot: the universal protein knowledgebase,” Nucleic acids research, vol. 46, no. 5, pp. 2699–2699, 2018.

[247] A. Bairoch and R. Apweiler, “The swiss-prot protein sequence database and its supplement trembl in 2000,” Nucleic acids research, vol. 28, no. 1, pp. 45–48, 2000. [248] J.-M. Chandonia, L. Guan, S. Lin, C. Yu, N. K. Fox, and S. E. Brenner, “Scope: improvements to the structural classification of proteins–extended database to facilitate variant interpretation and machine learning,” Nucleic acids research, vol. 50, no. D1, pp. D553–D559, 2022. [249] Z. Wen, J. He, H. Tao, and S.-Y. Huang, “Pepbdb: a comprehensive structural database of biological peptide– protein interactions,” Bioinformatics, vol. 35, no. 1, pp. 175–177, 2019.

[250] F. Wu, T. Xu, S. Jin, X. Tang, Z. Xu, J. Zou, and B. Hie, “D-flow: Multi-modality flow matching for d-peptide design,” arXiv preprint arXiv:2411.10618, 2024. [251] Y. Vander Meersche, G. Cretin, A. Gheeraert, J.-C. Gelly, and T. Galochkina, “Atlas: protein flexibility description from atomistic molecular dynamics simulations,” Nucleic acids research, vol. 52, no. D1, pp. D384–D392, 2024. [252] A. Kryshtafovych, T. Schwede, M. Topf, K. Fidelis, and J. Moult, “Critical assessment of methods of protein structure prediction (casp)—round xv,” Proteins: Structure, Function, and Bioinformatics, vol. 91, no. 12, pp. 1539–1549, 2023.

[253] A. Morehead, J. Liu, P. Neupane, N. Giri, and J. Cheng, “Protein-ligand structure and affinity prediction in casp16 using a geometric deep learning ensemble and flow matching,” Proteins: Structure, Function, and Bioinfor- matics, 2025.

[254] O. Abdin, S. Nim, H. Wen, and P. M. Kim, “Pepnn: a deep attention model for the identification of peptide binding sites,” Communications biology, vol. 5, no. 1, p. 503, 2022.

[255] C. Zhang, X. Zhang, L. Freddolino, and Y. Zhang, “Biolip2: an updated structure database for biologically relevant ligand–protein interactions,” Nucleic acids re- search, vol. 52, no. D1, pp. D404–D412, 2024. [256] V. Ljosa, K. L. Sokolnicki, and A. E. Carpenter, “Annotated high-throughput microscopy image sets for validation,” Nature Methods, vol. 9, no. 7, p. 637, 2012. [257] J. Taylor, B. Earnshaw, B. Mabey, M. Victors, and J. Yosinski, “RxRx1: An Image Set for Cellular Morpho- logical Variation Across Many Experimental Batches,” in ICLR AI for Social Good Workshop, 2019.

[258] S. N. Chandrasekaran, J. Ackerman, E. Alix, D. M. Ando, J. Arevalo, and et al., “JUMP Cell Painting dataset: morphological impact of 136,000 chemical and genetic perturbations,” bioRxiv, 2023, preprint.

[259] N. Kumar, R. Verma, D. Anand, Y. Zhou, O. F. On- der, E. Tsougenis, H. Chen, P. Heng, J. Li, Z. Hu, Y. Wang, N. A. Koohbanani, M. Jahanifar, N. Z. Tajeddin, A. Gooya, N. Rajpoot, X. Ren, S. Zhou, Q. Wang, D. Shen, C. Yang, C. Weng, W. Yu, C. Yeh, S. Yang, S. Xu, P. Yeung, P. Sun, A. Mahbod, G. Schaefer, I. Ellinger, R. Ecker, O. Smedby, C. Wang, B. Chidester, T. Ton, M. Tran, J. Ma, M. N. Do, . . . , and A. Sethi, “A multi-organ nucleus segmentation challenge,” IEEE Transactions on Medical Imaging, vol. 39, no. 5, pp.

PREPRINT. JULY 2025.

27

D. Li, “STimage-1K4M: A Histopathology Image–Gene Expression Dataset for Spatial Transcriptomics,” arXiv preprint arXiv:2406.06393, 2024.

[267] T. Lohoff, S. Ghazanfar, A. Missarova, N. Koulena, E. Pierson, and et al., “Integration of spatial and single- cell transcriptomic data elucidates mouse organogenesis,” Nature Biotechnology, vol. 40, no. 1, pp. 74–85, 2022. [268] E. Stephenson, G. Reynolds, R. A. Botting, F. J. Calero- Nieto, M. D. Morgan, and et al., “Single-cell multi-omics analysis of the immune response in covid-19,” Nature Medicine, vol. 27, no. 5, pp. 904–916, 2021.

[269] N. A. Steinmetz, P. Zatka-Haas, M. Carandini, and K. D. Harris, “Distributed coding of choice, action and engagement across the mouse brain,” Nature, vol. 576, no. 7786, pp. 266–273, 2019.

[270] R. D. Flint, Z. A. Wright, M. R. Scheid, and M. W. Slutzky, “Long-term stability of neural prosthetic control signals from silicon cortical arrays in rhesus macaque motor cortex,” Journal of Neural Engineering, vol. 9, no. 5, p. 056009, 2012.

[271] M. M. Churchland, J. P. Cunningham, M. T. Kaufman, S. I. Ryu, and K. V. Shenoy, “Neural population dynamics during reaching,” Nature, vol. 487, pp. 51–56, 2012.

[272] E. J. Cornblath, E. Heravi, J. P. Cunningham, and D. Sus- sillo, “An empirical evaluation of neural population dynamics models for motor cortex,” in Neural Latents Benchmark Workshop at NeurIPS, 2021.

[273] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang et al., “A survey on evaluation of large language models,” ACM transactions on intelligent systems and technology, vol. 15, no. 3, pp. 1–45, 2024.

[274] J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, W. Lu, Y. Chen, W. Zeng, and P. S. Yu, “Generalizing to unseen domains: A survey on domain generalization,” IEEE transactions on knowledge and data engineering, vol. 35, no. 8, pp. 8052–8072, 2022.

[275] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in 2009 IEEE conference on computer vision and pattern recognition.

Ieee, 2009, pp. 248–255.

[276] V. P. Dwivedi, C. K. Joshi, A. T. Luu, T. Laurent, Y. Bengio, and X. Bresson, “Benchmarking graph neural networks,” Journal of Machine Learning Research, vol. 24, no. 43, pp. 1–48, 2023.

1380–1391, May 2020.

[260] K. Sirinukunwattana, J. P. W. Pluim, H. Chen, X. Qi, P. Heng, Y. B. Guo, L. Y. Wang, B. J. Matuszewski, E. Bruni, U. Sanchez, A. Böhm, O. Ronneberger, B. B. Cheikh, D. Racoceanu, P. Kainz, M. Pfeiffer, M. Urschler, D. R. J. Snead, and N. M. Rajpoot, “Gland segmentation in colon histology images: The glas challenge contest,” Medical Image Analysis, vol. 35, pp. 489–502, Jan. 2017.

[261] S. Leclerc, E. Smistad, J. Pedrosa, A. Ostvik, F. Cerve- nansky, F. Espinosa, T. Espeland, E. A. R. Berg, P.-M. Jodoin, T. Grenier, C. Lartizien, J. D’hooge, L. Lovs- takken, and O. Bernard, “Deep learning for segmentation using an open large-scale dataset in 2d echocardiography,” IEEE Transactions on Medical Imaging, vol. 38, no. 9, pp. 2198–2210, 2019.

[262] M. Antonelli, A. Reinke, S. Bakas, K. Farahani, A. Kopp- Schneider, B. A. Landman, G. Litjens, B. Menze, O. Ronneberger, R. M. Summers, B. van Ginneken, M. Bilello, P. Bilic, P. F. Christ, R. K. G. Do, M. J. Gollub, S. H. Heckers, H. Huisman, W. R. Jarnagin, M. K. McHugo, S. Napel, J. S. G. Pernicka, K. Rhode et al., “The medical segmentation decathlon,” Nature Communications, vol. 13, no. 1, p. 4128, 2022. [263] F. Knoll, J. Zbontar, A. Sriram, M. J. Muckley, M. Bruno, A. Defazio, M. Parente, K. J. Geras, J. Katsnelson, H. Chandarana, Z. Zhang, M. Drozdzal, A. Romero, M. Rabbat, P. Vincent, J. Pinkerton, D. Wang, N. Yakubova, E. Owens, C. L. Zitnick, M. P. Recht, D. K. Sodickson, and Y. W. Lui, “fastmri: A publicly available raw kspace and dicom dataset of knee images for accelerated mr image reconstruction using machine learning,” Radiology: Artificial Intelligence, vol. 2, no. 1, p. e190007, 2020.

[264] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, Y. Burren, N. Porz, J. Slot- boom, R. Wiest, L. Lanczi, E. Gerstner, M.-A. Weber, T. Arbel, B. B. Avants, N. Ayache, P. Buendia, D. L. Collins, N. Cordier, J. J. Corso, A. Criminisi, T. Das, H. Delingette, C. Demiralp, C. R. Durst, M. Dojat, S. Doyle, J. Festa, F. Forbes, E. Geremia, B. Glocker, P. Golland, X. Guo, A. Hamamci, K. M. Iftekharuddin, R. Jena, N. M. John, E. Konukoglu, D. Lashkari, J. A. Mariz, R. Meier, S. Pereira, D. Precup, S. J. Price, T. R. Raviv, S. M. S. Reza, M. Ryan, D. Sarikaya, L. Schwartz, H.-C. Shin, J. Shotton, C. A. Silva, N. Sousa, N. K. Subbanna, G. Szekely, T. J. Taylor, O. M. Thomas, N. J. Tustison, G. Unal, F. Vasseur, M. Wintermark, D. H. Ye, L. Zhao, B. Zhao, D. Zikic, M. Prastawa, M. Reyes, and K. Van Leemput, “The multimodal brain tumor image segmentation benchmark (brats),” IEEE Transactions on Medical Imaging, vol. 34, no. 10, pp. 1993–2024, 2015. [265] G. Jaume, P. Doucet, A. H. Song, M. Y. Lu, C. Almagro- Pérez, S. J. Wagner, A. J. Vaidya, R. J. Chen, D. F. K. Williamson, A. Kim, and F. Mahmood, “HEST-1k: A Dataset for Spatial Transcriptomics and Histology Image Analysis,” arXiv preprint arXiv:2406.16192, 2024. [266] J. Chen, M. Zhou, W. Wu, J. Zhang, Y. Li, and