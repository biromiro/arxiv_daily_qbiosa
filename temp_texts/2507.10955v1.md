# Abstract

Peptide de novo sequencing is a method used to reconstruct amino acid sequences from tandem mass spectrometry data without relying on ex- isting protein sequence databases. Traditional deep learning approaches, such as Casanovo, mainly utilize autoregressive decoders and predict amino acids sequentially. Subsequently, they en- counter cascading errors and fail to leverage high- confidence regions effectively. To address these issues, this paper investigates using diffusion decoders adapted for the discrete data domain. These decoders provide a different approach, al- lowing sequence generation to start from any pep- tide segment, thereby enhancing prediction accu- racy. We experiment with three different diffusion decoder designs, knapsack beam search, and vari- ous loss functions. We find knapsack beam search did not improve performance metrics and simply replacing the transformer decoder with a diffusion decoder lowered performance. Although peptide precision and recall were still 0, the best diffusion decoder design with the DINOISER loss function obtained a statistically significant improvement in amino acid recall by 0.373 compared to the baseline autoregressive decoder-based Casanovo model. These findings highlight the potential of diffusion decoders to not only enhance model sen- sitivity but also drive significant advancements in peptide de novo sequencing.

1.

# Introduction

Peptide de novo sequencing is the task of reconstructing the amino acid sequence of peptides directly from tandem mass spectrometry (MS/MS) data, without relying on existing pro- tein sequence databases (Tran et al., 2017). It is particularly valuable for identifying novel peptides or sequences from organisms with unsequenced genomes (Mao et al., 2023). Obtaining high accuracy is challenging due to incomplete

1University of Waterloo, Waterloo, Canada. Correspondence

to: Chi-en Amy Tai <amy.tai@uwaterloo.ca>.

Figure 1. Overview of the peptide de novo sequencing process, illustrating the flow from inputting an observed spectrum to generating the corresponding peptide sequence, copied from Casanovo (Yilmaz et al., 2022).

fragmentation of precursor peptides and noise present in MS/MS spectra (seen as black peaks in Figure 1 (Tran et al., 2017). Deep learning offers innovative solutions to these challenges by efficiently modeling intricate relationships within spectral data (Tran et al., 2017). Consequently, recent breakthroughs in deep learning have surpassed earlier meth- ods, which relied on heuristic search and dynamic program- ming, delivering enhanced accuracy and efficiency (Yilmaz et al., 2022).

However, existing deep learning models for peptide sequenc- ing predominantly use autoregressive decoders inspired by natural language processing frameworks (Yilmaz et al., 2022; Mao et al., 2023) with an example seen in Figure 2. Autoregressive decoders sequentially generate sequences of amino acids where each prediction is dependent on its predecessor (Yilmaz et al., 2022). In the context of pep- tide de novo sequencing, this behaviour is problematic as early errors could cascade through the entire sequence and it does not take advantage of performance gains from de- coding high confidence parts first (often the middle part of the peptide) (Tran et al., 2017). Using a diffusion decoder, especially one adapted for the discrete data domain, is a promising alternative as diffusion decoders can start decod- ing from any part of the peptide sequence, allowing them to capitalize on high confidence parts to enhance overall prediction accuracy (Lou et al., 2023).

1













Diffusion Decoding for Peptide De Novo Sequencing

tween low binning resolution, which reduced sequencing accuracy, and increased model complexity, which caused longer inference times (Yilmaz et al., 2022) as it needed to discretize the mass-to-charge (m/z) axis of the mass spectra during sequencing (Yilmaz et al., 2022).

Casanovo instead eliminated the need for discretization and used both the precursor mass and charge, along with indi- vidual spectrum peaks, as inputs to its model (Yilmaz et al., 2022). Casanovo was designed using transformers with the self-attention mechanism to translate observed spectrum peaks into amino acid sequences and the model was trained using supervised learning where database searches supplied the ground truth sequences for prediction (Yilmaz et al., 2022). Instead of relying on dynamic programming, a delta mass filter was applied to process the peptide sequences (Yil- maz et al., 2022). Notably the knapsack dynamic program- ming algorithm was tested for post-processing but the sim- ple m/z filter yielded better performance (Yilmaz et al., 2022). Beam search was subsequently implemented in the publicly available code to obtain the optimal predic- tion (Yilmaz et al., 2022). Based on experiments using a cross-species evaluation benchmark, Casanovo achieved bet- ter performance with lower inference time and complexity compared to DeepNovo (Yilmaz et al., 2022). However, some Casanovo predictions could be eliminated by the delta mass filter, resulting in gaps in plausible predictions for significant portions of the spectra (Yilmaz et al., 2022).

GraphNovo is a two-stage graph-based deep learning model with the goal to tackle the problem of missing fragmentation for peptide de novo sequencing (Mao et al., 2023). A graph is constructed from the spectral data and the first stage fo- cuses on identifying an optimal path using the graph (Mao et al., 2023). This optimal path is mapped to a peptide sequence containing mass tags for missing fragmentation regions (Mao et al., 2023). In the second stage, these mass tags are processed and replaced with predicted amino acids to produce the final peptide sequence (Mao et al., 2023). Both stages use a transformer structure as the encoder and decoder and its design subsequently required high computa- tional power; training needed four A100 graphic processing units (GPUs) (Mao et al., 2023). Since GraphNovo relied on transformer decoders, a limitation also exists for pre- dicting long sequences in that early errors could cascade through the entire sequence. In addition, GraphNovo does not incorporate all the provided information such as isotope peaks which could provide performance gains. Similar to DeepNovo, GraphNovo also used the knapsack algorithm for filtering and the beam search technique for final peptide generation (Mao et al., 2023).

InstaNovo is a transformer model recently introduced for de novo sequencing and proposes an iterative refinement step using InstaNovo+, its diffusion-powered model (Eloff

Figure 2. Sample deep learning model architecture, copied from Casanovo (Yilmaz et al., 2022).

This paper addresses the challenge of using diffusion de- coding for peptide de novo sequencing and explores how to better integrate diffusion in the deep learning model archi- tecture. The objectives of the project include incorporating different diffusion decoder designs into peptide sequencing workflows to replace traditional autoregressive decoders, exploring the compatibility of diffusion decoding with knap- sack beam search, and evaluating the effectiveness of dif- ferent loss functions compared to traditional cross-entropy. This work highlights how deep learning is vital in over- coming challenges in noisy MS/MS data and how diffusion decoding is a promising direction for more accurate peptide sequencing processes.

2. Related Work

2.1. Deep Learning Models for Peptide De Novo

Sequencing

DeepNovo was the first deep neural network model pro- posed for peptide de novo sequencing (Tran et al., 2017). DeepNovo combined convolutional neural networks and long short-tappearingerm memory recurrent neural networks with local dynamic programming to predict peptide se- quences from MS/MS data (Tran et al., 2017). Dynamic programming using the knapsack approach was applied to exclude amino acids whose masses exceeded the suffix mass (Tran et al., 2017). The final heuristic for exploring candidate sequences, however, was beam search, which eval- uated a predetermined set of the most promising sequences at each iteration to determine the optimal prediction (Tran et al., 2017). Compared to state-of-the-art methods at the time, DeepNovo achieved higher accuracies at the amino acid and peptide level, but it was noted that the model per- formance was heavily dependent on the dataset (Tran et al., 2017). Moreover, DeepNovo had to strike a balance be-

2

Diffusion Decoding for Peptide De Novo Sequencing

et al., 2025). Given predicted sequences from InstaNovo, In- staNovo+ refines and improves the sequence by treating it as a corrupted sequence and iteratively removing noise (Eloff et al., 2025). Key elements of InstaNovo are its use of knapsack beam search decoding and multi-scale sinusoidal embeddings to improve performance, but at high compu- tational cost (Eloff et al., 2025). In addition, InstaNovo+ is seen as a secondary step after InstaNovo is run and re- quires multiple rounds of refinement for prediction, making it computationally expensive and time-consuming with the performance of InstaNovo+ limited by the performance of InstaNovo (Eloff et al., 2025). Furthermore, InstaNovo+ is trained using average KL-divergence loss of the model and they do not explore training with other loss functions which could be more appropriate for discrete diffusion mod- elling (Eloff et al., 2025).

2.2. Discrete Diffusion Modelling Techniques

Diffusion models are widely used in generative research be- cause they simulate data generation via a stochastic process, transforming random noise into structured data through iter- ative denoising steps to produce high-quality outputs (Ho et al., 2020). During training, the forward step adds con- trolled noise to structured data (Ho et al., 2020). Then, to create coherent outputs, noise is removed step-by-step (Ho et al., 2020). Initially, diffusion models were implemented for continuous data domains such as images and were sub- sequently less effective for discrete data domains like text.

Diffusion-LM was introduced to tackle the challenge of text generation in the discrete domain by utilizing continuous diffusion models (Li et al., 2022). By transforming text sequences into continuous embeddings, Diffusion-LM in- crementally refines a sequence of Gaussian noise vectors into meaningful word representations (Li et al., 2022). The model achieves this through a ”rounding” method, where it selects the word with the highest likelihood and adjusts the embeddings to correspond to actual token embeddings at each step of the diffusion process (Li et al., 2022). How- ever, this approach relies heavily on continuous diffusion processes and has significant computational demands during both training and inference due to the rounding step.

Score Entropy Discrete Diffusion models (SEDD) shifted away from converting discrete data into a continuous space for modelling and instead, operated entirely in the discrete token space using probability vectors and transition matri- ces. SEDD is thus controllable, allowing for prompting from arbitrary positions (Lou et al., 2023). In text-based experi- ments, SEDD outperformed earlier discrete and continuous diffusion language models on both standard (left-to-right) generation and infilling tasks, surpassing strong autoregres- sive models such as GPT-2 (Lou et al., 2023). Notably, SEDD is computationally intensive and needs 8 A100 80

GB GPUs for training.

DINOISER is a less computationally demanding method that also tries to tackle the problem of diffusion modelling for discrete data (Ye et al., 2023). Unlike SEDD, DI- NOISER builds on Diffusion-LM and focuses on manip- ulating noises (Ye et al., 2023). DINOISER uses noise scale clipping to avoid training on small noise scales for better generalization and was shown to achieve state-of-the- art results compared to other models like DiffusionLM for machine translation, text simplification, and paraphrasing tasks (Ye et al., 2023). On the other hand, DINOISER would still struggle with discreteness in embedding spaces as it concentrates on manipulating noise scales whereas SEDD directly addresses this challenge through its loss function modelling.

3. Methodology

3.1. Tools and Frameworks

As seen in Figure 3, we used Casanovo (Yilmaz et al., 2022) as our development framework as it has relatively lower computational demands but fairly high performance for the goal of peptide de novo sequencing compared to other state- of-the-art techniques like GraphNovo (Mao et al., 2023) and InstaNovo (Eloff et al., 2025). We replaced the autoregres- sive decoder with diffusion decoders and assessed discrete diffusion-specific losses inspired from score entropy (Lou et al., 2023) and DINOISER (Ye et al., 2023). Although SEDD models have shown better performance compared to DINOISER, they are computationally more expensive and more difficult to work with. However, DINOISER is computationally less expensive and still has relatively high performance for discrete diffusion modelling. For training and evaluation, we used two NVIDIA GeForce RTX 4090 GPUs with 25 GB of memory each. Notably, GitHub Copi- lot (GitHub, 2025) was employed to help with the diffusion decoder code design, write code comments, and assist in debugging the code. The complete code is also attached as a zip folder with this report for reproducibility.

3.2. Dataset

For training and evaluation, we leveraged the same com- bined tryptic and non-tryptic dataset used in the latest ver- sion of Casanovo (Casanovo v4.2) (Melendez et al., 2024). Tryptic refers to the use of the standard trypsin digestive enzyme for MS/MS (Melendez et al., 2024). While trypsin is the standard digestive enzyme, other alternative enzymes have demonstrated improved peptide detection, making the combined dataset more representative for general peptide de novo sequencing (Melendez et al., 2024). We obtained the data as annotated MGF files (a format for MS/MS data that is human readable (Levitsky et al., 2020)) from the public

3

Diffusion Decoding for Peptide De Novo Sequencing

Figure 3. Overview of our methodology, with Part 1 exploring the replacement with different diffusion decoders in Casanovo, Part 2 trying knapsack with beam search, and Part 3 evaluating two of the diffusion model designs with different loss functions.

Zenodo link on Casanovo’s website (Melendez & Noble, 2024). The combined dataset was created by obtaining data from MassIVE-KB v.2018-06-15 (tryptic) and MassIVE- KB v2.0.15 (non-tryptic) (Wang et al., 2018). We employ the same respective train-validation-test split of 277,045, 27,437, and 200,000 PSMs (Melendez et al., 2024) for all of our experiments.

3.3. Part 1: Replacement with Diffusion Decoder

In the first part, we analyzed the merit of replacing the tradi- tional autoregressive decoder with diffusion decoders. We utilized the original Casanovo with the transformer decoder (with 28.4 million parameters in the decoder) as the baseline for comparison. Three different diffusion decoder designs were integrated into the Casanovo framework and evaluated: Casanovo-DS (with 9.7 million parameters in the decoder), Casanovo-DM1 (with 29.5 million parameters in the de- coder), and Casanovo-DM2 (with 29.6 million parameters in the decoder). Their architectures are shown below in Figure 4, Figure 5, and Figure 6 respectively. We used the cross-entropy loss function for training along with the same training settings from Casanovo.

3.4. Part 2: Knapsack Beam Search Comparison

For this part, we assessed the benefit of using the knapsack method with the beam search algorithm. Inspired by its use in InstaNovo (Eloff et al., 2025), we also incorporated it directly into the decoding process alongside beam search. The knapsack technique was used to iteratively build candi- date sequences while considering constraints like precursor mass tolerance. Constraints were updated and reapplied after new candidates were generated and candidates were iteratively filtered if they exceeded the mass constraint. No- ticeably, knapsack beam search was time demanding (Eloff et al., 2025) and as such, we only compared incorporating knapsack beam search for Casanovo-DS as it is smaller than Casanovo-DM1 and Casanovo-DM2. As a point of compar- ison, we also evaluated how the model’s performance for

Figure 4. Casanovo-DS.

Casanovo changed when knapsack beam search was imple- mented as well. Using the performance recorded in Part 1 with beam search, we calculated the shift in performance for both models. All model hyperparameters remained consis- tent between Part 1 and Part 2 to ensure a fair comparison.

3.5. Part 3: Loss Function Evaluation

In this section, we examined incorporating different loss functions for our two most promising diffusion decoder designs, Casanovo-DM1 and Casanovo-DM2. Inspired by score entropy, we considered a weighted entropy loss func- tion that combined the standard cross-entropy loss with an average entropy term that penalized the model if it is too uncertain, promoting more confident predictions. We also

4

Diffusion Decoding for Peptide De Novo Sequencing

Figure 5. Casanovo-DM1.

Figure 6. Casanovo-DM2.

experimented with a discrete diffusion-specific loss from DINOISER that adds noise to the predicted logits. We compared the performance of using a weighted entropy loss function and the DINOISER loss function to values obtained from Part 1 with the cross-entropy loss function. Similar to Part 2, we kept the same model hyperparameters as before to ensure consistency in the comparison.

4. Experiments and Results

We compared models using peptide precision, peptide cov- erage, amino acid precision, and amino acid recall as these are the four most common evaluation metrics for peptide de novo sequencing and they provide a comprehensive view of the model performance for peptide sequencing at both the peptide and amino acid levels (Tran et al., 2017; Yilmaz et al., 2022; Mao et al., 2023; Eloff et al., 2025). Peptide precision measures the fraction of predicted peptides that exactly match the true sequences over the total number of predicted spectra. Peptide coverage denotes the num- ber of predicted spectra over the total number of available spectra. Amino acid precision assesses how many of the predicted amino acids are correct, calculated as the ratio of matched amino acids to total predicted amino acids.

Lastly, amino acid recall evaluates the model’s sensitiv- ity by measuring the proportion of true amino acids that are correctly predicted, calculated as the ratio of matched amino acids to the total number of actual amino acids in the true sequences. In all experiments, we used the same train-validation-test split described in Section 3.2 along with the default Casanovo model hyperparameters. To compute statistical significance for the best model improvement, we used scipy.stats wilcoxon (SciPy, 2025) for the Wilcoxon signed-rank test (Rainio et al., 2024).

4.1. Part 1

In this part, we evaluated the performance of Casanovo (with a transformer decoder) along with three other variants of Casanovo that used different decoder designs (seen in Figure 4, Figure 5, and Figure 6). As seen in Table 1, simply replacing the transformer decoder with different decoder designs led to worse performance across all four metrics. A qualitative examination of the outputs showed interesting findings as Casanovo-DM1 and Casanovo-DM2 generally produced more accurate sequences than Casanovo-DS (see left of Figure 7) but tended to fail more than Casanovo-DS (see right part of Figure 7) which led to an overall lower

5

Diffusion Decoding for Peptide De Novo Sequencing

quantitative metric score. Interestingly, all three diffusion models sometimes produced sequences of more similar size to the ground truth than Casanovo in failure cases (see mid- dle part of Figure 7).

4.2. Part 2

In Part 2, we evaluated the merit of replacing beam search with knapsack beam search. Given the time cost of knapsack beam search, we experimented with only the Casanovo and Casanovo-DS models. The quantitative results in Table 2 demonstrate that using knapsack beam search resulted in worse metric performance across all four metrics for both Casanovo and Casanovo-DS. As seen in Figure 8, knapsack beam search greatly changed the predicted sequence for these two models where Casanovo-DS with Knapsack gen- erally produced qualitatively better sequences, but had more errors than Casanovo-DS (using only beam search). On the other hand, Casanovo with Knapsack generally led to worse performance but in failure cases, the model sometimes pro- duced sequences with more similar size to the ground truth than Casanovo (with only beam search). In terms of time cost, using knapsack beam search increased the training time for both models from roughly 1.5 hours to 35 hours.

4.3. Part 3

Part 3 evaluated different loss functions for training different diffusion decoder designs. Despite their lower performance in Part 1, we chose to experiment with Casanovo-DM1 and Casanovo-DM2 instead of Casanovo-DS as they had more promising sequencing results qualitatively. Unfortunately, the peptide precision and peptide coverage remained at 0 across all three loss functions. However, the amino acid pre- cision and recall metrics were improved by using different loss functions as exhibited in Table 4. Interestingly, the op- timal loss function for both Casanovo-DM1 and Casanovo- DM2 was DINOISER with higher amino acid precision and recall metrics compared to using cross-entropy or weighted entropy loss functions. Although using weighted entropy was better than using cross-entropy for Casanovo-DM1, the performance for Casanovo-DM2 was similar for both loss functions. Overall, Casanovo-DM2 with the DINOISER loss function achieved the highest amino acid precision and recall. Although the amino acid precision for the best setup (0.070) was still lower than Casanovo’s at 0.080, the amino acid recall with the best configuration (0.454) far surpassed that of Casanovo at 0.081 with a p-value < 0.001, indicating statistical significance.

A qualitative examination of the predicted sequences does raise concerns as most of the outputs are much longer com- pared to the ground truth sequence (example in Figure 9). Although the amino acid sequence more closely matches the ground truth at the start of the sequence, the remainder

of the sequence appears to be noise. Despite the noise, the amino acid recall still remained high as this metric is based on the number of amino acids in the true sequence rather than the predicted sequence. Since the predicted sequence typically matches the true sequence for the true sequence’s length, it leads to a higher recall.

5. Discussion and Conclusion

This paper studies the problem of diffusion decoding in pep- tide de novo sequencing. We investigated using different diffusion decoders as an alternative to conventional autore- gressive decoders, compared the merit of using knapsack beam search against beam search, and assessed the per- formance of various loss functions relative to the standard cross-entropy loss. In our work, we showed that simply replacing the transformer decoder with a diffusion decoder was inadequate for obtaining high performance and switch- ing from beam search to knapsack beam search incured a high training time cost (from 1.5 hours to 35 hours) with worse performance. However, by incorporating an appro- priate loss function for discrete diffusion (DINOISER), we were able to increase amino acid recall by 0.373 with sta- tistical significance compared to Casanovo (with an autore- gressive transformer decoder).

The main advantage of our approach is the ability to de- code from any part of the peptide sequence whereas current state-of-the-art methods can only decode sequentially from one end of the sequence. Additionally, our method is less computationally demanding and can be trained in a single step, unlike the two-step resource-intensive training process used in InstaNovo, which constrains the performance of the diffusion model to rely on the training of a separate transformer-based model. However, our approach does have limitations, including much lower peptide precision and coverage. Our work is also limited by difficulties in acquir- ing the necessary GPUs to train other published methods, such as GraphNovo and InstaNovo, making it challenging to fairly compare our approach to theirs. Running their pre- trained models on our test set is not a fair comparison, as their models were trained on larger and different datasets and previous studies have demonstrated that models trained on higher-quality data can yield better results, not necessar- ily due to superior architecture, but because of the quality of the data used during training (Tran et al., 2017). An- other limitation is that our final best model, obtained using Casanovo-DM2 with a DINOISER loss function, predicted sequences that were much longer than the true sequence and contained noise at the end, likely exceeding the desired mass capacity.

Future work could address this challenge by finding bet- ter ways to truncate the leftover noise through potentially leveraging knapsack filtering when developing the sequence.

6

Diffusion Decoding for Peptide De Novo Sequencing

Table 1. Model performance comparison for replacing the transformer decoder (in Casanovo) with three different diffusion decoder designs (in Casanovo-DS, Casanovo-DM1, Casanovo-DM2) where AA stands for Amino Acid.

Model Casanovo Casanovo-DS Casanovo-DM1 Casanovo-DM2

Peptide Precision 0.037 0.000 0.000 0.000

Peptide Coverage AA Precision AA Recall

0.690 0.000 0.000 0.000

0.080 0.042 0.031 0.037

0.081 0.041 0.018 0.027

Figure 7. Comparison of sample predicted sequences from models in Part 1 highlighting the impact of using three different decoder designs (Casanovo-DS, Casanovo-DM1, and Casanovo-DM2) versus an autoregressive transformer decoder (Casanovo).

Table 2. Model performance comparison after switching to knapsack beam search for Casanovo and Casanovo-DS with the change in performance from beam search indicated in brackets (negative value indicates that using beam search resulted in a higher metric value than using knapsack beam search), where AA refers to Amino Acid.

Model Casanovo Casanovo-DS

Peptide Precision 0.000 (-0.037) 0.000 (0.000)

Peptide Coverage AA Precision 0.043 (-0.037) 0.018 (-0.024)

0.000 (-0.690) 0.000 (0.000)

AA Recall 0.035 (-0.046) 0.000 (-0.041)

Figure 8. Comparison of sample predicted sequences from models in Part 2 showcasing the impact of knapsack beam search decoding.

Table 3. Amino Acid (AA) performance of Casanovo-DM1 and Casanovo-DM2 for different loss functions with best metrics obtained by Casanovo-DM2 with DINOISER.

Loss Function

Cross-Entropy Weighted Entropy DINOISER

Casanovo-DM1

Casanovo-DM2

AA Precision AA Recall AA Precision AA Recall

0.031 0.066 0.070

0.018 0.423 0.451

0.037 0.037 0.070

0.027 0.026 0.454

Table 4. Amino Acid (AA) performance of Casanovo-DM1 and Casanovo-DM2 for different loss functions with best metrics obtained by Casanovo-DM2 with DINOISER.

Model Casanovo (Cross-Entropy) Casanovo-DS (Cross-Entropy) Casanovo-DM1 (Cross-Entropy) Casanovo-DM2 (Cross-Entropy) Casanovo-DM1 (Weighted Entropy) Casanovo-DM2 (Weighted Entropy) Casanovo-DM1 (DINOISER) Casanovo-DM2 (DINOISER)

AA Precision AA Recall

0.080 0.042 0.031 0.037 0.066 0.037 0.070 0.070

0.081 0.041 0.018 0.027 0.0423 0.026 0.451 0.454

7

Diffusion Decoding for Peptide De Novo Sequencing

Figure 9. Sample predicted sequences produced by Casanovo-DM1 and Casanovo-DM2 using the three loss functions where Weighted refers to the weighted entropy loss function.

Although our work showed decreased performance when using the knapsack algorithm during beam search, there may be better ways to incorporate this algorithm to address this challenge. Furthermore, we used the initial Casanovo hyper- parameters but hyperparameter tuning should be explored for improved performance for these diffusion decoder-based models. For generalization, future work should also include training and inferencing with other data sources such as those in GraphNovo (Mao et al., 2023) and InstaNovo (Eloff et al., 2025). Even though it may be computationally ex- pensive to train GraphNovo or InstaNovo, developing and benchmarking models using the same data would allow for a fair comparison of model performance.

References

Eloff, K., Kalogeropoulos, K., Mabona, A., Morell, O., Catzel, R., Rivera-de Torre, E., Berg Jespersen, J., Williams, W., van Beljouw, S. P., Skwark, M. J., et al. Instanovo enables diffusion-powered de novo peptide se- quencing in large-scale proteomics experiments. Nature Machine Intelligence, pp. 1–15, 2025.

GitHub. Github copilot, 2025. URL https://github.

com/features/copilot.

Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba- bilistic models. Advances in neural information process- ing systems, 33:6840–6851, 2020.

Levitsky, L., Goloborodko, A., and Gorshkov, M. mgf - read and write ms/ms data in mascot generic format, 2020. URL https://pyteomics.readthedocs. io/en/latest/api/mgf.html.

Li, X., Thickstun, J., Gulrajani, I., Liang, P. S., and Hashimoto, T. B. Diffusion-lm improves controllable text generation. Advances in neural information process- ing systems, 35:4328–4343, 2022.

Lou, A., Meng, C., and Ermon, S. Discrete diffusion model- ing by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023.

8

Mao, Z., Zhang, R., Xin, L., and Li, M. Mitigating the missing-fragmentation problem in de novo peptide se- quencing with a two-stage graph-based deep learning model. Nature Machine Intelligence, 5(11):1250–1260, 2023.

Melendez, C. and Noble, W. Data for ’accounting for digestion enzyme bias in casanovo’ (melendez et al., 2024), 2024. URL https://zenodo.org/ records/12587317.

Melendez, C., Sanders, J., Yilmaz, M., Bittremieux, W., Fondrie, W. E., Oh, S., and Noble, W. S. Accounting for digestion enzyme bias in casanovo. Journal of Proteome Research, 23(10):4761–4769, 2024.

Rainio, O., Teuho, J., and Kl´en, R. Evaluation metrics and statistical tests for machine learning. Scientific Reports, 14(1):6086, 2024.

SciPy. Statistical functions (scipy.stats), 2025. URL

https://docs.scipy.org/doc/scipy/ reference/stats.html.

Tran, N. H., Zhang, X., Xin, L., Shan, B., and Li, M. De novo peptide sequencing by deep learning. Proceedings of the National Academy of Sciences, 114(31):8247–8252, 2017.

Wang, M., Wang, J., Carver, J., Pullman, B. S., Cha, S. W., and Bandeira, N. Assembling the community-scale dis- coverable human proteome. Cell systems, 7(4):412–421, 2018.

Ye, J., Zheng, Z., Bao, Y., Qian, L., and Wang, M. Dinoiser: Diffused conditional sequence learning by manipulating noises. arXiv preprint arXiv:2302.10025, 2023.

Yilmaz, M., Fondrie, W., Bittremieux, W., Oh, S., and No- ble, W. S. De novo mass spectrometry peptide sequencing with a transformer model. In International Conference on Machine Learning, pp. 25514–25522. PMLR, 2022.