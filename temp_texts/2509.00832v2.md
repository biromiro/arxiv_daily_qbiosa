# Abstract

Crystalline structure prediction remains an open challenge in materials design. Despite recent advances in computational materials science, accurately predicting the three-dimensional crystal structures of organic materials—an essential first step for designing materials with targeted properties—remains elusive. In this work, we address the problem of molecular assembly, where a set S of identical rigid molecules is packed to form a crystalline structure. Existing state-of-the-art models typically rely on computationally expensive, iterative flow-matching approaches. We propose a novel loss function that correctly captures key geometric molecular properties while maintaining permutation invariance over S. We achieve this via a differentiable linear assignment scheme based on the Sinkhorn algorithm. Remarkably, we show that even a simple regression using our method SinkFast significantly outperforms more complex flow-matching approaches on the COD-Cluster17 benchmark, a curated subset of the Crystallography Open Database (COD). Our code is available at https://github.com/EmmanuelJhno/SinkFast

1

# Introduction

Generative modeling and deep learning have enabled rapid progress in the understanding and design of materials, molecules, and drugs. On the one hand, for material property prediction, advances in graph neural networks and transformers have significantly improved the understanding of molecular structures [Joshi et al., 2024, Lin et al., 2023, Choudhary and DeCost, 2021], linking their three-dimensional (3D) geometry to physical and chemical properties. Particular attention has been paid to SE(3)-equivariant representations, which present higher expressivity by preserving geometric symmetries [Schütt et al., 2021]. These methods have been adapted to crystalline structures, with their inherent challenges of infinite periodicity and rich symmetry patterns [Yan et al., 2024a]. The state-of-the-art performance of Yan et al. [2022, 2024a], Ito et al. [2025] reflect the need for physically grounded methods. On the other hand, for material design, generative models such as diffusion models [Song et al., 2021] and flow matching methods [Liu et al., 2023] have greatly enhanced the capacity to generate valid and diverse molecular and material structures [Watson et al., 2023]. This work aims to combine these two aspects for the task of molecular assembly prediction, where a finite set of identical rigid molecules is packed into a crystalline structure.

∗Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France. †Univ. Grenoble Alpes, CNRS, Grenoble INP, LJK, 38000 Grenoble, France.

1













1.1 Related Works

A fundamental step in designing a material with specific properties is to know its crystallization pattern. As represented in Figure 1-center, a crystal is conventionally described by a unit cell, the smallest volume that contains all the structural and symmetry information necessary to generate the whole crystal by translation. This three-dimensional infinitely periodic shape largely determines the physical and chemical properties of the resulting material. This shape can be predicted either by direct regression [Liang et al., 2020, Cao et al., 2024] or by flow matching/diffusion methods that allow for probabilistic answers [Merchant et al., 2023, Xie et al., 2022, Luo et al., 2025, Pakornchote et al., 2023, Jiao et al., 2024].

Most of the previous methods model atoms in the unit cell individually. While such an approach works well for simple crystals of atomic point clouds [Miller et al., 2024] from Jain et al. [2013], the performance degrades on more complex molecular materials with symmetries other than translations. These contain internal point-group symmetries within the unit cell. An asymmetric unit (ASU) is defined as an elementary pattern of the unit cell, irreducible under the symmetry group transformations. A unit cell can be composed of multiple ASUs and an example is shown in Figure 1-left. As this basic structure maintains a fixed internal structure, generating the crystal by directly predicting the ASU position, orientation, and symmetry operations in the world frame significantly reduces the dimensionality of the task, compared to moving each atom individually. In this setting, the goal of the molecular assembly prediction problem can be formulated as follows: given an elementary structure – an ASU – predict its local crystalline structure, or in other words, how it packs in space.

Historically, the problem of computational material design has been extensively studied through the lens of Crystal Structure Prediction (CSP) challenge. This was first tackled through iterative process involving expensive spatial optimization [Martínez et al., 2009] and energy assessment of predicted structures with first-principles calculations based on the density functional theory (DFT) [Pickard and Needs, 2011, Kresse and Furthmüller, 1996]. However, these methods are slow, scale poorly with the number of atoms in the unit cell and thus may not be adapted to infinite materials. More recently, generative models have emerged as promising candidates for this task, especially for simple inorganic crystals [Levy et al., 2025, Nam et al., 2025]. However, they have not yet been widely adapted for complex organic materials. Nonetheless, very recent deep learning-based approaches study the molecular assembly prediction task by atom-wise [Liu et al., 2024a] and rigid-body [Guo et al., 2025] flow matching. However, some assumptions made by these approaches may not be fully realistic for material generation.

1.2 Contributions

In this work we show how to integrate domain-specific physics knowledge in the training scheme of material generative models. Our contributions can be summarised as follows:

1. Physics grounded loss. We introduce a new loss for rigid-body flow matching, grounded in physical

principles, which leads to the improved prediction of crystalline structures.

2. Permutation-invariant loss. We design a permutation-invariant soft matching objective that ensures

invariance to global geometric transformations and to the order of repeated units.

3. An order of magnitude faster than flow matching. We show on the crystalline prediction benchmark that direct regression with this objective is more accurate than flow matching methods, while being easier to train and an order of magnitude faster at inference.

2 Problem setting

2.1 Problem formulation

An organic crystal is a solid material in which molecules are arranged in a highly ordered pattern repeating in the three spatial dimensions (3D). The asymmetric units ASU that constitute it are molecules and are

2

Figure 1: A crystalline material at three different scales. From left to right: (a) The asymmetric subunit (ASU). (b) The unit cell with mirror images of the ASU. (c) The unit cell is repeated periodically in all three directions. Illustrations correspond to the COD-4316210 crystal structure from Crystallographic Open Database [Gražulis et al., 2009].

identical objects in 3D. The unit cell is then defined by a finite number of symmetry operators applied to the ASU. The pinnacle of crystalline structure prediction is to compute the infinite 3D structure of a material given its substituent chemical compounds. To solve this very challenging task, one can take a number of approximations and hypotheses. Molecular assembly subproblem is a simplification of the original problem, of identical rigid molecules is packed together to form a pattern that can be then replicated where a finite set i to map each molecule i in in space into a crystal. Our goal is thus to predict rigid spatial transformation final. We propose an efficient and model-agnostic way to guide any machine learning model with

initial to

S

T

S physical knowledge of the task.

S

Dataset A molecule assembly dataset is COD-Cluster17, introduced in Liu et al. [2024a]. This dataset contains 111k assemblies and is a simplified, sanitized version of the 507k crystals from the real world Crystallography Open Database (COD) Gražulis et al. [2009]. Firstly, the authors of COD-Cluster17 extracted from the COD only those crystals that contain exactly one organic molecule per ASU. We will thus refer to this ASU as the central molecule. Then, the dataset is built by computing for each filtered crystal the ground-truth supercell of an arbitrary asymmetric unit, which is the aggregation of 27 unit cells into a parallelepiped centered on the unit cell of the asymmetric unit of interest. An example of a supercell is given in Figure 1-right. The authors of COD-Cluster17 then extracted the central molecule’s 17 nearest neighbors (including itself) using a cutoff in Euclidean space within this supercell. This procedure outputs the final positions set consisting of each atom Cartesian coordinates. Then, a random rigid-body transformation is applied to the atomic positions of each molecule, which results in the initial positions set. The task for the COD-Cluster17 benchmark is then a point cloud packing matching task of predicting all atoms final positions, provided the known correspondence with the initial positions However, this exact mapping enforcing specific index correspondences between the assembly atoms or molecules is very unrealistic as the mapping is arbitrary and molecules are geometrically identical.

Indeed, thanks to the procedure of the COD-Cluster17 dataset construction, we can assume that the molecules in each sample are rigid and geometrically indistinguishable: (1) This dataset is built by selecting crystals from COD such that its asymmetric unit contains only one molecule. This filtering of unsuited structures also applies to (2) ones with disordered atoms (cases where some atoms do not occupy unique and uniquely attributed positions) and (3) polymeric crystals. (4) Finally, while being available in 2 distinct versions – with and without molecular inversion – previous methods focus exclusively on the version without inversions, for simplicity. We argue in Supplementary Materials D.1 that our method can also be efficiently adapted to the inversion dataset.

One of our contributions is thus to propose a more reasonable task of packing molecules without enforcing

3

index correspondences, preserving the invariance to permutations of the set of 17 molecules.

Rigid body description The position of a rigid molecule in 3D can not be described by a single position vector as for an atom. Instead, we represent the position of the rigid molecule as a rigid spatial transformation SO3, operator where s is its scalar part and ⃗q is its vector part. See Supplementary Materials B for details.

R3 and a 3D rigid rotation quaternion q = [s, ⃗q]

= (⃗r, q) composed of a 3D translation ⃗r

∈

∈

T

2.2 Metrics

Packing matching. Current crystal structure prediction methods [Guo et al., 2025, Liu et al., 2024a] typically use the Packing Matching (PM) score as an assessment metric. It is defined for atoms positions ⃗x as follows,

PM2

atom =

1 N 2

N

N

i=1 (cid:88)

j=1 (cid:16) (cid:88)

⃗x pred i ∥

−

⃗x pred j

⃗x gt i −

∥−∥

2

,

⃗x gt j ∥ (cid:17)

(1)

where N is the number of atoms in the assembly, ⃗xpred j ) is the position vector of atom i (resp. j) in the predicted (resp. ground-truth) assembly. PM quantifies how well the pairwise distances between the atoms are predicted, and is invariant to global rotations and translations.

(resp. ⃗xgt

i

Root mean square deviation. RMSD is another metric common in chemistry, structural biology, physics, and materials science. RMSD performs direct comparisons of atom positions, which requires representing them in a common frame:

RMSD2

atom =

1 N

⃗x pred i ∥

⃗x gt i ∥

2.

−

N i (cid:88) ∈

(2)

√2RMSDatom showing the correlation Note that Supplementary Materials C proves the relation PMatom between both metrics even though PM compares relative positions, whereas RMSD relies on absolute ones. This relation shows that PM score is a good proxy for the RMSD assessment. In particular: "as long as reported PM is greater than 2 times square root of 2 angstroms, RMSD is greater than 2 angstroms.".

≤

Also commonly used, the PMcenter metric, is defined as follows,

PM2

center =

1 M 2

M

M

i=1 (cid:88)

j=1 (cid:16) (cid:88)

⃗c pred i ||

−

⃗c pred j

⃗c gt i −

||−||

⃗c gt j

2

,

||

(cid:17)

where M is the number of molecules in the assembly, ⃗c pred j ) is the position of ith molecule’s (resp. i jth) center of mass in the predicted (resp. ground-truth) assembly. It evaluates the quality of the molecule positions regardless of their orientations.

(resp. ⃗c gt

2.3 SE(3) flow matching

In the Euclidean space Conditional Flow Matching [Liu et al., 2023, Lipman et al., 2023, Albergo and Vanden-Eijnden, 2023] is a simple scalable method to train generative models. The basic principle is to of interpolating paths between any source distribution P0 and the target choose a family X = distribution P1. The paths should be differentiable and have their marginal laws at both ends t = 0 and t = 1 (X1) = P1, respectively. The flow matching (X0) = P0 and match the source and target distributions: procedure consists in training a neural network u to match the conditional velocity field vX induced by these paths:

[0,1]} ∈

(Xt)t

L

L

{

vX(t, x) = E

˙Xt (cid:104)

|

Xt = x

.

(cid:105)

4

(3)

In practice, this family path is created with linear interpolations (LERP) between samples X0, X1 from

P0, P1:

At inference time, samples are generated by solving the forward ODE induced by the velocity field, by

Xt = (1

−

t)X0 + tX1 = LERP(X0, X1, t).

(4)

Euler discretization for example.

In SO(3) While this framework was originally designed for Rd, there exists an extension to SO(3) [Chen and Lipman, 2024]. Indeed, by representing rotations with unit quaternions, there is a natural equivalent to linear interpolation, called Spherical Linear Interpolation (SLERP) [Shoemake, 1985]. This creates differentiable interpolation paths (qt) between source and target quaternions q0, q1:

qt = SLERP(q0, q1; t) = q0(q1

0q1)t.

(5)

Combining LERP and SLERP, it is possible to linearly interpolate between two rigid-body transformations t = (LERP(⃗r0, ⃗r1, t), SLERP(q0, q1; t)). T0 = (⃗r0, q0) and T

T1 = (⃗r1, q1) as

3 Methods

We will now refer to the global reference frame as an arbitrarily chosen coordinate system used to represent . In contrast, we define local frames as those the positions and orientations of the M molecules in the set attached to the center of mass of each individual molecule, which rigidly move with them. These local frames are initialized using the principal components of each molecule’s inertia tensor. While this initialization may not be unique, the specific choice does not affect the relative transformations between local frames, which are the quantities actually used in the model computations.

S

initial of M equivalent initial molecules to a set

-Permutation invariance The molecular assembly task as defined in this paper aims at matching a S set final of M final ones. As these molecules are identical, S positioning one at a given place or another is strictly equivalent physically. A proper metric thus should reflect this

-permutation invariant property, which we present formally below.

S

We represent molecules i from the predicted assembly and j from the ground truth assembly by their L of any Lij is the cost of assigning molecule i from the ground truth

rigid-body positions metric C assembly with the molecule j in the predicted assembly. This cost matrix is computed as follows:

j gt in the global reference frame. We consider the cost matrix

such as PMatom or PMcenter, such that

i pred and

L

T

T

C

S

i, j

∀{

} ∈ (cid:74)

1, M

2,

(cid:75)

Lij =

C

L

i pred,

j gt

T

T

(cid:16)

(cid:17)

.

(6)

the ground truth assembly, which minimizes the metric

The goal is then to find a complete assignment of molecules in the predicted assembly with molecules in -permutations. This minimizer is denoted L ∗. Formally it is defined by the linear sum assignment problem. Let P be a boolean pairing matrix in which L Pij = 1 if and only if molecule i from ground truth assembly is mapped with molecule j in the predicted assembly:

over all

S

∗ := min

P

L

C Lij.Pij

ij (cid:88)

} In practice we use scipy’s linear sum assignment method to compute this exact minimizer

∈ {

·

·

with Pij

0, 1

s.t. P

1 = P ⊤

1 = 1

(7)

∗ of

L

.

L

5

3.1 Physically grounded losses

We now consider rigid body predicted (resp. ground-truth) positions in the global reference frame as gt = (⃗r gt, q gt)). The loss currently used in the literature decouples R3 and

pred = (⃗r pred, q pred) (resp.

T SO(3) spaces as:

T

L SO(3)( T and then one can combine them with a tuned hyperparameter α as

⃗r pred ∥

gt) =

pred,

⃗r gt

R3 (

−

L

T

T

∥

2

ML =

L

R3 + α

L

LSO(3).

pred,

T

gt) =

q pred

∥

q gt

2,

∥

−

(8)

(9)

The α parameter has to be adjusted to the task one is trying to solve. It has to balance the weight of unbounded distance in R3 to the bounded distance in SO(3). As different samples in the dataset may have very different geometries, with inter-molecular distances spanning orders of magnitudes, having a single parameter is suboptimal. Finally, as the space of rigid transformations SE(3) is not a direct product of R3 and SO(3), this loss has no physical or geometrical meaning.

Rigid RMSD loss Popov and Grudinin [2014] introduced a more suitable rigid-body transformation loss that is strictly equivalent to the RMSD2 = (⃗r, q), with quaternion q = (s, ⃗q) composed of a scalar s and a vector part ⃗q, as follows.

atom metric in eq. 2. It is defined for a rigid transformation

T

RMSD2(

, I) =

T

4 N

⃗q ⊤I⃗q + ⃗r 2,

(10)

where I is an inertia tensor of the rigid body computed in its center-of-mass local frame. One can notice that in this frame the two RMSD2 contributions, rotation and translation, are additive. The inertia tensor naturally provides a weight between the rotation and the translation contributions. However, the cross-terms appear in the equation if we change the reference frame as detailed in Supplementary Materials B. Thus, gt, of the same rigid body with the inertia tensor I, we can given two spatial transformations T naturally define the physically-grounded RMSD loss without additional hyperparameters as

pred and

T

We will use this RMSD loss as default during training and test to compare absolute positions in the predicted assembly of molecules with the ground truth.

RMSD(

L

pred,

T

gt) = RMSD2( T

T

gt

◦ T

1 pred, I). −

(11)

S

consisting of M molecules, i, j

j,pred. We want to compare these transformations to the corresponding ground-truth ones

pred of these molecules resulting in individual global spatial transformations (or positions)

Geometric loss. Regarding the task of molecular assembly prediction, we aim to define a loss that better reflects the relative packing of molecules and not memorizing their absolute positions. Let us consider two rigid molecules in an assembly M . Let us assume we have predicted a packing and j,gt gt. We can define the assembly transformation-invariant PM metric for these molecules from the packing similar to the one in eq. 1. However, this computation requires to first compute positions of all corresponding atoms. Instead, we propose a more elegant rigid-body RMSD-based solution presented in Figure 2. Concretely, we compute the RMSD2 metric between the ith molecules in the superposed local frames of the jth molecules, as follows,

T i,gt and

i,pred

∈

S

S

T

T

T

1 − j,gt ◦ T T We can then extend the above expression to the comparison of M

i,gt) = RMSD2(

− j,pred ◦ T T

1 − j,gt ◦ T

RMSD(

i,pred,

i,gt

L

T

1

1

◦ T

−

j,pred, I).

− i,pred ◦ T 1 molecules to a reference one. Geom as

(12)

Without loss of generality, we can assume it is the M th molecule and define the geometric loss follows:

L

Geom (

L

pred, T

gt) =

T

1

M

1

−

M

1

−

i=1 (cid:88)

RMSD(

L

1

− M,pred ◦ T T

i,pred,

1 − M,gt ◦ T T

i,gt).

(13)

In practice in COD-Cluster17 dataset, as detailed in section 2.1, the way it is constructed defines a

reference molecule around which we extract the 16 nearest neighbors.

6

Figure 2: A schematic illustration of our geometric loss alignment and similarity measure computation. A: Predicted and target couples of molecules with local frames before alignment. B: The reference molecule from the predicted packing is aligned on the one from the target packing. C: Predicted and target molecules after aligning both reference molecules on each other. The similarity measure is then computed as the RMSD2 between the non-reference molecules.

3.2 Differentiable optimal assignment

As metrics are computed with invariance to -permutations, it is essential to also train models with permutation invariant losses. However, the linear sum assignment problem 7 is not differentiable and results in training instabilities, as our preliminary experiments demonstrated. We thus use during training the differentiable version of it provided by the Sinkhorn algorithm with the boolean pairing P matrix being RMSD or relaxed as

, 0 < Pij < 1. The problem is then defined for any training loss

, like

ML,

i, j

S

L

L

L

} Geom, such that:

∀{

L

P,

∗train = min P ⟨

L with Pij

C [0, 1]

∈

L

F + reg ⟩ s.t. P

·

Ω(P )

·

1 = P ⊤

1 = 1 and P

·

0

≥

(14)

with Ω(P ) =

Pijlog(Pij)

ij (cid:88)

An implementation of this algorithm as defined in Cuturi [2013] can be found in Python Optimal Transport library [Flamary et al., 2021]. This approach provides a feedback to the model with multiple possible assignments weighted by P , which behaves like a probability map.

4 Results

4.1 Experimental setup

Dataset We evaluate the performance of our approach on the COD-Cluster17 benchmark introduced in Liu et al. [2024a]. The dataset and the task are detailed in section 2.1. It contains 111k assemblies and is a simplified, sanitized version of the 507k crystals from the real world Crystallography Open Database. Previous methods also benchmark on a subset of 5k assemblies. This benchmark comes with a splitting strategy into 80% for train, 10% for validation and 10% for test. The validation set is used for best method selection throughout the training epochs and the final performances presented in this section are obtained on

7

ABReferenceMoleculeReferenceMoleculeTargetPredictionTarget vs Prediction before alignmentAlignment of both reference moleculesReferenceMoleculeCRMSDTarget vs Prediction after alignmentthe test set. Following previous works, we compare our approach on 3 seeds and report below the average performance.

Model We use AssembleFlow Atom level SE(3)-equivariant model described in Supplementary Materials D.2. It is composed of a first PaiNN embedding layer to encode each molecular structure individually followed by N layers of atom-to-molecules attention message passing. Each molecule’s transformation prediction is then obtained by aggregating the resulting atomic embeddings per molecule and passed through a projection head. Standard hyperparameters and models parameters are provided in Supplementary Materials D.2 but in particular we keep the hyperparameter α to 10 and use 50 time steps of flow matching.

All models were trained on a single NVidia H100 GPU system, with 80GB memory and 67 TFlops. We 4 adapted by a

trained them for 500 epochs, with a batch size of 8, Adam optimizer with a learning rate 10− Cosine Annealing scheduler.

Training methods We define direct regression as the task of predicting the target positions directly from the initial positions. In this setting, the model is trained exclusively on the initial positions as input. In contrast, in the flow matching setting as detailed in Algorithm 1, the model is trained on various interpolated positions, which helps guide the optimization process. Direct regression is equivalent to a one-step flow matching.

Algorithm 1 Flow matching inference

: atom initial positions, T timesteps = 50, j 1 }

{T

: molecule final transformation)

) # defined in Supp. Mat. D.2

{

=

{T

0}

, δt =

⃗Xi {

1 T timesteps

⃗Xi ai : atom types, 0} } { j : molecule initial transformation, 0 } {T ⃗Xi j , t} 0 } { [1, ..., T timesteps] do pred, q j (⃗r j = ⃗r j ⃗r j t pred− t 1 − j t ) T

(cid:111) t = q j , q j

t + δt

pred)

(cid:110) t + δt Move( ⃗Xi,

ai

d

⃗Xi = AtomModel( t} { { pred, q j dt SLERP(q j

, t,

}

t , t)

def Inference(

1:

j = t } 2: for all t

{T

3:

∈ j pred} {T ⃗r j t = ⃗r j 4: ⃗Xi 5: t ← 6: end for j return t

T

4.2 Main results

In Table 1 we present our models performance against 4 other state-of-the-art models on the COD-Cluster17 dataset. First, while original state-of-the-art results are presented in PMcenter and PMatom, we show under PM∗center and PM∗atom the importance of optimal assignment to get the best metric performance over the set of -permutations of the predicted assembly as detailed in section 3.2. Indeed the metric decreases greatly under this optimal assignment, indicating the inability of models to memorize positions for each molecule as they are equivalent. Then, we show that our method outperforms all other baselines on both 5k subset and the full dataset by a significant margin. Notably our method is the only deep learning method that outperforms PackMol on both datasets.

S

4.3 Ablation studies

Training with introduced losses Table 2 lists experiments conducted when training with physically grounded losses, both in flow matching as in AssembleFlow [Guo et al., 2025] and in our direct regression RMSD performs model with permutation invariant loss. We can draw 2 main conclusions: (1) training with

L

8

Table 1: Our best model against state-of-the-art models on COD-Cluster17. The best results are marked in bold.

Flow Matching PMcenter PMatom PM∗center PM∗atom

Packing Matching in Å

↓

Dataset: COD-Cluster17-5K

PackMol [Martínez et al., 2009] GNN-MD [Liu et al., 2024b] CrystalFlow-LERP [Liu et al., 2024a] AssembleFlow [Guo et al., 2025]

✓ ✓

SinkFast - SinkFast -

∗ML (ours) ∗RMSD (ours)

L L

PackMol [Martínez et al., 2009] GNN-MD [Liu et al., 2024b] CrystalFlow-LERP [Liu et al., 2024a] AssembleFlow [Guo et al., 2025]

✓ ✓

SinkFast - SinkFast -

∗ML (ours) ∗RMSD (ours)

L L

6.05 ± 13.80 13.26 6.13

±

±

0.05

0.07

0.09

0.10

7.10 ± 13.67 13.59 7.27

±

±

0.05

0.06

0.09

0.04

- - - 3.86

±

5.80 5.85

0.03 6.96 0.05 6.98

0.03 3.60 3.77 0.05

5.79

0.012

0.13

± 0.04 5.54 5.67 0.12

±

0.04

0.08

±

±

±

6.09 ± 14.51 13.28 6.21

±

7.15 0.01 0.82 22.30 ± 13.61 0.01 7.37

0.01

±

±

±

0.01

12.04

0.00

0.01

- - - 3.51

±

5.80 5.80

0.00 7.00 0.00 7.00

3.47 0.01 0.01 3.41

0.05 0.04 5.51 5.54 0.04

0.03

0.02

0.01

±

±

±

±

±

- - -

- - - 5.60

±

±

±

±

±

±

±

±

±

±

±

±

Dataset: COD-Cluster17-All

ML while having no parameter to tune. And (2) both previous on par with training with the standard absolute losses fail to perform on the geometric relative packing loss metric ∗Geom while training with it is the solution. In reverse, training with this relative loss yields poor results on absolute ones. This mainly shows the limitations of the absolute packing matching task of interest here.

L

L

Moreover, these results along with ablation in Supplementary Materials E show the great performance of training with the -permutation invariant losses, while not being useful in flow matching. This reveals a redundancy between the two and that the main interest of flow matching on this task thus lies in it being an optimal transport approximator. While the added value of flow matching is yet to be proven, adapting it better to the domain specificities and tasks, its usage out of the box may not come handy.

S

Execution time Table 3 lists the execution time for the different methods. In particular, we are interested in the expense of our -permutation invariant loss and how it compares to the cost of using a flow matching scheme. While it increases the training execution time by 20% compared to direct regression without the permutation invariance, it saves by a factor 42 the overall training time compared to flow matching. And as it is only used during training, the gain at inference is a factor 50 (number of time steps) compared to flow matching.

S

5 Conclusion

In this paper we have focused on a simpler subtask of complex organic crystal structure prediction named molecule assembly. We have introduced new meaningful metrics on a benchmark and proven its utmost importance to accurately compare methods. We have also proposed new rigid-body losses grounded in physical principles that greatly improve performance on this packing matching task and provide simpler information to material science physicists. Our main contribution lies in the definition of a very powerful, differentiable, fast permutation invariant loss adapted to crystal structure prediction and agnostic to model definition. This method pushes significantly the state-of-the-art on this benchmark while reducing by a factor 50 the execution time. It invites to take a step back from large generative models and expensive methods, to instead focus on better problem definitions and principled, physics-inspired solutions.

9

Table 2: Ablation study of using physically grounded losses during training on COD-Cluster17 in 2 different training scheme.

Loss

Flow Matching

✓ ✓ ✓

✓ ✓ ✓

ML

RMSD

Geom

∗ML ∗RMSD ∗Geom

ML

RMSD

Geom

∗ML ∗RMSD ∗Geom

L L L

L L L

L L L

L L L

Test Loss in Å

Packing matching in Å

∗RMSD

L

↓ ∗Geom

L

PM∗center PM∗atom PMcenter PMatom

↓

Dataset: COD-Cluster17-5K

9.56 9.41 9.22

±

±

±

0.07

0.19

0.06

13.35 13.37 10.45

8.69 8.73 9.32

0.06 12.16 0.07 12.05 0.06 8.78

±

±

±

±

±

±

±

0.55 0.12 3.60 3.77 0.15 5.55

0.05

0.30

0.24

3.86 3.89 3.90

0.13

0.16

0.08

5.79 5.80 5.85

±

±

±

0.12

0.16

0.04

6.28 6.26 6.14

±

±

±

0.15

0.20

0.02

7.39 7.35 7.20

0.16

0.21

0.05

±

±

±

±

±

±

0.04 5.54 5.67 0.12 6.54

0.15

±

±

0.04 5.80 0.08 5.85 6.92 0.07

0.03 6.96 0.05 6.98 7.46 0.07

0.03

0.05

0.02

±

±

±

± Dataset: COD-Cluster17-All

±

±

9.26 9.08 9.33

±

±

±

0.18

0.12

0.10

12.02 12.17 10.77

±

±

±

8.65 8.70 9.35

±

±

±

0.02 12.10 0.03 12.16 0.00 8.71

±

0.13 0.10 3.47 0.08 3.41 5.43 0.03

±

±

±

±

±

0.30

0.33

3.51 3.51 3.78

0.05

0.04

0.09

5.60 5.60 5.76

±

±

±

0.03

0.03

0.05

5.96 5.97 6.09

±

±

±

0.02

0.05

0.07

7.15 7.18 7.21

0.03

0.05

0.05

±

±

±

±

±

±

0.04 5.51 0.04 5.54 6.52 0.10

0.02 5.80 0.01 5.80 6.84 0.05

0.00 7.00 0.00 7.00 7.45 0.06

0.01

0.01

0.02

±

±

±

±

±

±

±

±

±

±

±

±

±

±

±

Table 3: Execution times for our method reported on COD-Cluster17-5k. Results are presented as average over 10 epochs at training and over 10 batches at inference. AssembleFlow is trained with 50 timesteps.

Method

Training time (s) per epoch

Test time (s) per batch

AssembleFlow SinkFast - without permutation-invariant loss SinkFast - with permutation-invariant loss

2678.5 53.6 64.1

0.89 0.02 0.02

10

Limitations While this work pushes the frontier of materials discovery on a specific benchmark, its usefulness to other benchmarks is yet to be assessed. Our work has been designed to tackle a weakness in the problem definition of common molecular assembly tasks and highlights the need for a revised dataset definition. With a real-life application in mind, the absence of periodic boundary conditions is a fundamental limitation of the COD-Cluster17 dataset and thus to this method. Indeed, a predicted molecule position should be correct up to any unit cell translation. However, as no periodicity information is available, prediction has to match absolute target positions, which hinders the generalisation capability of any model. A second major limitation in COD-Cluster17 is the absence of space group information for each training sample. The same rigid molecule can crystallize in different configurations according to specific symmetry groups inside a unit cell that then replicates infinitely in space. This conditions global structure prediction – and thus also the subtask of molecules assembly – and can give different targets for the same common data. As a result, the generalisation capability of any model is greatly hindered.

The reason why our loss does not add any value during flow matching is of great interest and should be investigated further. The visualizations reported in Supplementary Materials G also show the large performance gap remaining to be closed in discovering plausible and stable crystal structures. We believe new methods should make use of all the geometrical properties of materials science to design powerful yet efficient algorithms that can reliably perform on tasks always closer to real-life data.

Acknowledgements

EJ, RM and JM were supported by the ERC grant number 101087696 (APHELEIA project). This work was granted access to the HPC resources of IDRIS under the allocation 2025-AD011014006R2 made by GENCI. We thank Kliment Olechnovič for his help with molecular visualization, Roman Klypa for his support with equivariant models and Romain Séailles for his help with the optimization techniques. We are particularly grateful to Saulius Grazulis from Vilnius University for guiding us through the COD database and sharing with us his valuable expertise.

11

References

Chaitanya K. Joshi, Cristian Bodnar, Simon V. Mathis, Taco Cohen, and Pietro Liò. On the expressive

power of geometric graph neural networks, 2024. URL https://arxiv.org/abs/2301.09308.

Yuchao Lin, Keqiang Yan, Youzhi Luo, Yi Liu, Xiaoning Qian, and Shuiwang Ji. Efficient approximations of complete interatomic potentials for crystal property prediction, 2023. URL https://arxiv.org/abs/ 2306.10045.

Kamal Choudhary and Brian DeCost. Atomistic line graph neural network for improved materials ISSN 2057-3960. doi:

property predictions. npj Computational Materials, 7(1), November 2021. 10.1038/s41524-021-00650-1. URL http://dx.doi.org/10.1038/s41524-021-00650-1.

Kristof Schütt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 9377–9388. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/schutt21a.html.

Keqiang Yan, Cong Fu, Xiaofeng Qian, Xiaoning Qian, and Shuiwang Ji. Complete and efficient graph transformers for crystal material property prediction, 2024a. URL https://arxiv.org/abs/2403.11857.

Keqiang Yan, Yi Liu, Yuchao Lin, and Shuiwang Ji. Periodic graph transformers for crystal material property

prediction, 2022. URL https://arxiv.org/abs/2209.11807.

Yusei Ito, Tatsunori Taniai, Ryo Igarashi, Yoshitaka Ushiku, and Kanta Ono. Rethinking the role of frames for se (3)-invariant crystal structure modeling. In The Thirteenth International Conference on Learning Representations, 2025.

Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=St1giarCHLP.

Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=XVjTT1nw5z.

Joseph L. Watson, David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, Andrew J. Borst, Robert J. Ragotte, Lukas F. Milles, Basile I. M. Wicky, Nikita Hanikel, Samuel J. Pellock, Alexis Courbet, William Sheffler, Jue Wang, Preetham Venkatesh, Isaac Sappington, Susana Vázquez Torres, Anna Lauko, Valentin De Bortoli, Emile Mathieu, Sergey Ovchinnikov, Regina Barzilay, Tommi S. Jaakkola, Frank DiMaio, Minkyung Baek, and David Baker. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976):1089–1100, 2023.

Haotong Liang, Valentin Stanev, A. Gilad Kusne, and Ichiro Takeuchi. Cryspnet: Crystal structure predictions via neural networks. Physical Review Materials, 4(12), December 2020. ISSN 2475-9953. doi: 10.1103/physrevmaterials.4.123802. URL http://dx.doi.org/10.1103/PhysRevMaterials.4.123802.

Zhendong Cao, Xiaoshan Luo, Jian Lv, and Lei Wang. Space group informed transformer for crystalline

materials generation, 2024. URL https://arxiv.org/abs/2403.15734.

Amil Merchant, Simon Batzner, Samuel S Schoenholz, Muratahan Aykol, Gowoon Cheon, and Ekin Dogus

Cubuk. Scaling deep learning for materials discovery. Nature, 624(7990):80–85, 2023.

Tian Xie, Xiang Fu, Octavian-Eugen Ganea, Regina Barzilay, and Tommi Jaakkola. Crystal diffusion variational autoencoder for periodic material generation, 2022. URL https://arxiv.org/abs/2110.06197.

12

Xiaoshan Luo, Zhenyu Wang, Qingchang Wang, Jian Lv, Lei Wang, Yanchao Wang, and Yanming Ma. Crystalflow: A flow-based generative model for crystalline materials, 2025. URL https://arxiv.org/ abs/2412.11693.

Teerachote Pakornchote, Natthaphon Choomphon-anomakhun, Sorrjit Arrerut, Chayanon Atthapak, Sakarn Khamkaeo, Thiparat Chotibut, and Thiti Bovornratanaraks. Diffusion probabilistic models enhance variational autoencoder for crystal structure generative modeling, 2023. URL https://arxiv.org/abs/ 2308.02165.

Rui Jiao, Wenbing Huang, Peijia Lin, Jiaqi Han, Pin Chen, Yutong Lu, and Yang Liu. Crystal structure

prediction by joint equivariant diffusion, 2024. URL https://arxiv.org/abs/2309.04475.

Benjamin Kurt Miller, Ricky T. Q. Chen, Anuroop Sriram, and Brandon M Wood. FlowMM: Generating materials with riemannian flow matching. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=W4pB7VbzZI.

A Jain, SP Ong, G Hautier, W Chen, WD Richards, S Dacek, S Cholia, D Gunter, D Skinner, G Ceder, et al. The materials project: a materials genome approach to accelerating materials innovation, apl mater. 1 (2013) 011002, 2013.

L. Martínez, R. Andrade, E. G. Birgin, and J. M. Martínez. Packmol: A package for building initial configurations for molecular dynamics simulations. Journal of Computational Chemistry, 30(13):2157–2164, 2009. ISSN 1096-987X. doi: 10.1002/jcc.21224. URL http://dx.doi.org/10.1002/jcc.21224.

Chris J Pickard and R J Needs. Ab initiorandom structure searching. Journal of Physics: Condensed ISSN 1361-648X. doi: 10.1088/0953-8984/23/5/053201. URL

Matter, 23(5):053201, January 2011. http://dx.doi.org/10.1088/0953-8984/23/5/053201.

Georg Kresse and Jürgen Furthmüller. Efficient iterative schemes for ab initio total-energy calculations using

a plane-wave basis set. Physical review B, 54(16):11169, 1996.

Daniel Levy, Siba Smarak Panigrahi, Sekou-Oumar Kaba, Qiang Zhu, Kin Long Kelvin Lee, Mikhail Galkin, Santiago Miret, and Siamak Ravanbakhsh. Symmcd: Symmetry-preserving crystal generation with diffusion models. arXiv preprint arXiv:2502.03638, 2025.

Juno Nam, Sulin Liu, Gavin Winter, KyuJung Jun, Soojung Yang, and Rafael Gómez-Bombarelli. Flow matching for accelerated simulation of atomic transport in materials, 2025. URL https://arxiv.org/ abs/2410.01464.

Shengchao Liu, Divin Yan, Hongyu Guo, and Anima Anandkumar. An equivariant flow matching framework for learning molecular crystallization. In ICML 2024 Workshop on Geometry-grounded Representation Learning and Generative Modeling, 2024a.

Hongyu Guo, Yoshua Bengio, and Shengchao Liu. Assembleflow: Rigid flow matching with inertial frames for molecular assembly. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=jckKNzYYA6.

Saulius Gražulis, Daniel Chateigner, Robert T. Downs, A. F. T. Yokochi, Miguel Quirós, Luca Lutterotti, Elena Manakova, Justas Butkus, Peter Moeck, and Armel Le Bail. Crystallography Open Database – an open-access collection of crystal structures. Journal of Applied Crystallography, 42(4):726–729, Aug 2009. doi: 10.1107/S0021889809016690. URL https://doi.org/10.1107/S0021889809016690.

Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t.

13

Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=li7qeBbCR1t.

Ricky T. Q. Chen and Yaron Lipman. Flow matching on general geometries. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=g7ohDlTITL.

Ken Shoemake. Animating rotation with quaternion curves. In Proceedings of the 12th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH ’85, page 245–254, New York, NY, USA, ISBN 0897911660. doi: 10.1145/325334.325242. URL 1985. Association for Computing Machinery. https://doi.org/10.1145/325334.325242.

P Popov and S Grudinin. Rapid determination of rmsds corresponding to macromolecular rigid body motions.

Journal of Computational Chemistry, 35(12):950–956, 2014.

Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transportation distances, 2013. URL

https://arxiv.org/abs/1306.0895.

Rémi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurélie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, Léo Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. Pot: Python optimal transport. Journal of Machine Learning Research, 22(78):1–8, 2021. URL http://jmlr.org/papers/v22/20-451.html.

Shengchao Liu, Weitao Du, Hannan Xu, Yanjing Li, Zhuoxinran Li, Vignesh Bhethanabotla, Divin Yan, Christian Borgs, Anima Anandkumar, Hongyu Guo, and Jennifer Chayes. A multi-grained symmetric differential equation model for learning protein-ligand binding dynamics, 2024b. URL https://arxiv. org/abs/2401.15122.

Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum

chemistry structures and properties of 134 kilo molecules. Scientific data, 1(1):1–7, 2014.

Kamal Choudhary, Kevin F Garrity, Andrew CE Reid, Brian DeCost, Adam J Biacchi, Angela R Hight Walker, Zachary Trautt, Jason Hattrick-Simpers, A Gilad Kusne, Andrea Centrone, et al. The joint automated repository for various integrated simulations (jarvis) for data-driven materials design. npj computational materials, 6(1):173, 2020.

Daniel S Levine, Muhammed Shuaibi, Evan Walter Clark Spotte-Smith, Michael G Taylor, Muhammad R Hasyim, Kyle Michel, Ilyes Batatia, Gábor Csányi, Misko Dzamba, Peter Eastman, et al. The open molecules 2025 (omol25) dataset, evaluations, and models. arXiv preprint arXiv:2505.08762, 2025.

Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv

preprint arXiv:1609.02907, 2016.

Ladislav Rampášek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for a general, powerful, scalable graph transformer. Advances in Neural Information Processing Systems, 35:14501–14515, 2022.

Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? Advances in neural information processing systems, 34:28877–28888, 2021.

Romain Menegaux, Emmanuel Jehanno, Margot Selosse, and Julien Mairal. Self-attention in colors: Another take on encoding graph structure in transformers, 2023. URL https://arxiv.org/abs/2304.10933.

Tian Xie and Jeffrey C Grossman. Crystal graph convolutional neural networks for an accurate and

interpretable prediction of material properties. Physical review letters, 120(14):145301, 2018.

14

Chi Chen, Weike Ye, Yunxing Zuo, Chen Zheng, and Shyue Ping Ong. Graph networks as a universal machine

learning framework for molecules and crystals. Chemistry of Materials, 31(9):3564–3572, 2019.

Steph-Yves Louis, Yong Zhao, Alireza Nasiri, Xiran Wang, Yuqi Song, Fei Liu, and Jianjun Hu. Graph convolutional neural networks with global attention for improved materials property prediction. Physical Chemistry Chemical Physics, 22(32):18141–18148, 2020.

Alexandre Duval, Simon V Mathis, Chaitanya K Joshi, Victor Schmidt, Santiago Miret, Fragkiskos D Malliaros, Taco Cohen, Pietro Liò, Yoshua Bengio, and Michael Bronstein. A hitchhiker’s guide to geometric gnns for 3d atomic systems. arXiv preprint arXiv:2312.07511, 2023.

Kristof Schütt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert Müller. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. Advances in neural information processing systems, 30, 2017.

Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J Bekkers, and Max Welling. Geometric and physical quantities improve e (3) equivariant message passing. arXiv preprint arXiv:2110.02905, 2021.

Yi Liu, Limei Wang, Meng Liu, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spherical message passing for

3d graph networks, 2022. URL https://arxiv.org/abs/2102.05013.

Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess E Smidt, and Boris Kozinsky. E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. Nature communications, 13(1):2453, 2022.

Yi-Lun Liao and Tess Smidt. Equiformer: Equivariant graph attention transformer for 3d atomistic graphs.

arXiv preprint arXiv:2206.11990, 2022.

Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.

End-to-end object detection with transformers, 2020. URL https://arxiv.org/abs/2005.12872.

Yihong Xu, Aljosa Osep, Yutong Ban, Radu Horaud, Laura Leal-Taixé, and Xavier Alameda-Pineda. How to train your deep multi-object tracker. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6787–6796, 2020.

Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot at- In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Ad- tention. vances in Neural Information Processing Systems, volume 33, pages 11525–11538. Curran As- sociates, URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 8511df98c02ab60aea1b2356c013bc0f-Paper.pdf.

Inc., 2020.

Avinash Kori, Francesco Locatello, Fabio De Sousa Ribeiro, Francesca Toni, and Ben Glocker. In B. Kim, Y. Yue, S. Chaudhuri, K. Fragkiadaki, Grounded object-centric M. Khan, and Y. Sun, editors, International Conference on Representation Learning, volume 2024, pages 42559–42603, 2024. URL https://proceedings.iclr.cc/paper_files/paper/2024/file/ ba4caa85ecdcafbf9102ab8ec384182d-Paper-Conference.pdf.

learning.

Yue Wang and Justin M. Solomon. Deep closest point: Learning representations for point cloud registration,

2019. URL https://arxiv.org/abs/1905.03304.

G. Dias Pais, Pedro Miraldo, Srikumar Ramalingam, Jacinto C. Nascimento, Venu Madhav Govindu, and Rama Chellappa. 3dregnet: A deep neural network for 3d point registration. pages 7193–7203, 2019.

Sungheon Park, Minsik Lee, and Nojun Kwak. Procrustean regression networks: Learning 3d structure of

non-rigid objects from 2d annotations, 2020. URL https://arxiv.org/abs/2007.10961.

15

Simon Axelrod and Rafael Gomez-Bombarelli. Geom, energy-annotated molecular conformations for property

prediction and molecular generation. Scientific Data, 9(1):185, 2022.

Justin S Smith, Olexandr Isayev, and Adrian E Roitberg. Ani-1: an extensible neural network potential with

dft accuracy at force field computational cost. Chemical science, 8(4):3192–3203, 2017.

Walter Kohn and Lu Jeu Sham. Self-consistent equations including exchange and correlation effects. Physical

review, 140(4A):A1133, 1965.

François Cornet, Grigory Bartosh, Mikkel Schmidt, and Christian Andersson Naesseth. Equivariant neural diffusion for molecule generation. Advances in Neural Information Processing Systems, 37:49429–49460, 2024.

Yuxuan Song, Jingjing Gong, Minkai Xu, Ziyao Cao, Yanyan Lan, Stefano Ermon, Hao Zhou, and Wei-Ying Ma. Equivariant flow matching with hybrid probability transport for 3d molecule generation. Advances in Neural Information Processing Systems, 36:549–568, 2023.

Hai-Chen Wang, Silvana Botti, and Miguel AL Marques. Predicting stable crystalline compounds using

chemical similarity. npj Computational Materials, 7(1):12, 2021.

Colin W Glass, Artem R Oganov, and Nikolaus Hansen. Uspex—evolutionary crystal structure prediction.

Computer physics communications, 175(11-12):713–720, 2006.

Jonathan Schmidt, Noah Hoffmann, Hai-Chen Wang, Pedro Borlido, Pedro JMA Carriço, Tiago FT Cerqueira, Silvana Botti, and Miguel AL Marques. Large-scale machine-learning-assisted exploration of the whole materials space. arXiv preprint arXiv:2210.00579, 2022.

Luis M Antunes, Keith T Butler, and Ricardo Grau-Crespo. Crystal structure generation with autoregressive

large language modeling. Nature Communications, 15(1):1–16, 2024.

Keqiang Yan, Xiner Li, Hongyi Ling, Kenna Ashen, Carl Edwards, Raymundo Arróyave, Marinka Zitnik, Heng Ji, Xiaofeng Qian, Xiaoning Qian, et al. Invariant tokenization of crystalline materials for language model enabled generation. Advances in Neural Information Processing Systems, 37:125050–125072, 2024b.

John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. nature, 596(7873):583–589, 2021.

Jason Yim, Brian L Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi Jaakkola. Se (3) diffusion model with application to protein backbone generation. arXiv preprint arXiv:2302.02277, 2023a.

Jason Yim, Andrew Campbell, Andrew YK Foong, Michael Gastegger, José Jiménez-Luna, Sarah Lewis, Victor Garcia Satorras, Bastiaan S Veeling, Regina Barzilay, Tommi Jaakkola, et al. Fast protein backbone generation with se (3) flow matching. arXiv preprint arXiv:2310.05297, 2023b.

Guillaume Pagès, Elvira Kinzina, and Sergei Grudinin. Analytical symmetry detection in protein assemblies.

i. cyclic symmetries. Journal of Structural Biology, 203(2):142–148, 2018.

Guillaume Pagès and Sergei Grudinin. Analytical symmetry detection in protein assemblies. ii. dihedral and

cubic symmetries. Journal of structural biology, 203(3):185–194, 2018.

Greg Landrum, Paolo Tosco, Brian Kelley, Ricardo Rodriguez, David Cosgrove, Riccardo Vianello, sriniker, Peter Gedeck, Gareth Jones, Eisuke Kawashima, NadineSchneider, Dan Nealschneider, Andrew Dalke, tadhurst cdd, Matt Swain, Brian Cole, Samo Turk, Aleksandr Savelev, Alain Vaucher, Maciej Wójcikowski, Ichiru Take, Hussein Faara, Vincent F. Scalfani, Rachel Walker, Daniel Probst, Kazuya Ujihara, Niels Maeder, Jeremy Monat, Juuso Lehtivarjo, and guillaume godin. rdkit/rdkit: 2025_03_5 (q1 2025) release, July 2025. URL https://doi.org/10.5281/zenodo.16439048.

16

Schrödinger, LLC. The PyMOL molecular graphics system, version 1.8. November 2015.

17

Supplementary Information

We release a version of the code available at https://github.com/EmmanuelJhno/SinkFast

A Related Works

A.1 Physics informed GNN for property prediction

Datasets. The fast-moving field of materials science has seen significant advances in recent years, largely driven by the release of large-scale open-source datasets. Many of the works discussed here rely on the QM9 database [Ramakrishnan et al., 2014], the Materials Project [Jain et al., 2013] and JARVIS [Choudhary et al., 2020]. With the recent release of even larger datasets such as OMol25 [Levine et al., 2025], the domain of materials property prediction and small molecule generation continues to push the boundaries of materials discovery. OMol25 includes over 100 million DFT calculations for larger molecular structures, providing an unprecedented wealth of properties to be predicted.

GNN models. Graph Neural Networks (GNNs) with message passing [Kipf and Welling, 2016, Rampášek et al., 2022] and transformer-based architectures [Ying et al., 2021, Menegaux et al., 2023] have been widely applied to molecular property prediction. Initially adapted from 2D molecular representations, GNNs have been extended to crystalline materials. Notable models include CGCNN [Xie and Grossman, 2018], MEGNet [Chen et al., 2019], and GATGNN [Louis et al., 2020], which pioneered the application of GNNs to materials property prediction.

Geometry informed GNN models. To better capture the geometric and physical properties of materials, geometry-aware GNNs have been developed [Duval et al., 2023]. Physically grounded models such as ALIGNN [Choudhary and DeCost, 2021], Matformer [Yan et al., 2022], PotNet [Lin et al., 2023] and ComFormer [Yan et al., 2024a] achieve state-of-the-art results on the Materials Project dataset, demonstrating the importance of incorporating materials science knowledge into predictive models. Concurrently, SE(3)-equivariant methods, known for their expressivity, have emerged with models such as SchNet [Schütt et al., 2017], PaiNN [Schütt et al., 2021], SEGNN [Brandstetter et al., 2021], SphereNet [Liu et al., 2022], NequIP [Batzner et al., 2022] and Equiformer [Liao and Smidt, 2022].

A.2 Object-centric learning

Permutation invariance in object detection. In computer vision, permutation-invariant loss functions have been used and developed in multiple object detection and segmentation [Carion et al., 2020] and multi-object tracking [Xu et al., 2020]. Locatello et al. [2020] and Kori et al. [2024] learn a binding scheme for assigning objects to slots in object property prediction and unsupervised instance discovery.

Point cloud rigid alignment distances. In the point cloud registration domain, Wang and Solomon [2019] have studied rigid alignment of point clouds as well as prediction to target assignment. However, they decorrelate R3 and SO(3) in the loss and reassign predictions to target only when correspondence is unknown. Pais et al. [2019] study the registration of 3D scans and learn the rigid alignment using different distances. Park et al. [2020] use Procrustes-alignment of 3D shapes to learn a regression problem of predicting 3D positions of a deformable object from 2D frame observations.

A.3 Generative models in materials science

Single molecule conformation prediction. Generating the 3D stable configuration of a single molecule is essential for materials discovery. Datasets such as GEOM-Drugs [Axelrod and Gomez-Bombarelli, 2022] and OMol25 [Levine et al., 2025] are tailored for this task. The OMol25 dataset includes evaluations based on

18

linear sum assignment for assessing optimal conformers, guided by machine learning interatomic potentials [Smith et al., 2017] and Density Functional Theory (DFT) [Kohn and Sham, 1965]. Generative approaches include flow matching models and SE(3)-equivariant generative models such as those by Cornet et al. [2024] and Song et al. [2023].

Crystal Structure Prediction (CSP). Historically, CSP has relied on computationally expensive iterative methods based on DFT [Kohn and Sham, 1965], including techniques by Wang et al. [2021], Glass et al. [2006], Pickard and Needs [2011], where atoms are iteratively replaced by chemically similar ones and validated with DFT calculations. Recently, machine learning has accelerated this process [Schmidt et al., 2022, Merchant et al., 2023].

Generative models for atomic point clouds. For simple crystals from the Materials Project [Jain et al., 2013], heir 3D infinitely periodic structures can now be directly predicted [Liang et al., 2020, Cao et al., 2024]. These methods are further enhanced by diffusion models [Merchant et al., 2023, Xie et al., 2022, Pakornchote et al., 2023, Jiao et al., 2024, Levy et al., 2025] and flow-matching approaches [Luo et al., 2025, Miller et al., 2024]. Inspired by their success in other domains, Large Language Models have been adapteed to CSP, as seen in CrystalLLM [Antunes et al., 2024] and models that integrate SE(3) equivariance and periodic boundary conditions [Yan et al., 2024b].

Rigid-body generative models for organic molecular CSP. Rigid-body generative models are extensively explored in protein design and backbone generation, as in AlphaFold2 [Jumper et al., 2021], FrameDiff [Yim et al., 2023a], and FrameFlow [Yim et al., 2023b]. Closer to molecular crystals, studies now focus on assembly prediction. For example, Liu et al. [2024a] propose atom-wise equivariant flow matching, while Guo et al. [2025] introduce a rigid body flow matching model for molecular cluster packing prediction.

B RMSD and Rigid Motions

B.1 Notations

3 matrices and 3-vectors. Therefore, for linear Throughout this section we will be generally dealing with 3 algebra operations we will stick to the following notation. Bold upper case letters (i.e., A) will denote matrices, normal weight lower case letters (i.e., c) will denote scalars, and we will also use an arrow notation for 3-vectors, such as ⃗v. Most of the information reported here can be found in the original papers that deal with rigid-body measures for rigid molecules by Popov and Grudinin [2014], Pagès et al. [2018], Pagès and Grudinin [2018].

×

B.2 Quaternion arithmetic

It is very convenient to express three-dimensional rotations using quaternion arithmetic. Thus, we will give a brief summary of it here. We consider a quaternion Q as a combination of a scalar s with a 3-component vector ⃗q = , Q = [s, ⃗q]. Quaternion algebra defines multiplication, division, inversion and norm, } among other operations. The product of two quaternions Q1 = [s1, ⃗q1] and Q2 = [s2, ⃗q2] is a quaternion and can be expressed through a combination of scalar and vector products:

qx, qy, qz

{

Q1 ·

Q2 ≡

[s1, ⃗q1]

·

[s2, ⃗q2] = [s1s2 −

⃗q2), s1⃗q2 + s2⃗q1 + (⃗q1 ×

⃗q2)] .

(B.1)

The squared norm of a quaternion Q is given as with its norm equal to 1. An inverse quaternion Q− treated as a quaternion with a zero scalar component, ⃗v rotate vector ⃗v to a new position ⃗v′ as follows

Q

|

≡

· 1 is given as Q−

⃗q, and a unit quaternion ˆQ is a quaternion 2. A vector ⃗v can be Q | | [0, ⃗v]. Then, a unit quaternion ˆQ can be used to

1 = [s,

⃗q]/

−

(⃗q1 · 2 = s2 + ⃗q |

19

[0, ⃗v′] = ˆQ [0, ⃗v] ˆQ−

1 =

0, (s2

⃗q2)⃗v + 2s(⃗q

⃗v) + 2(⃗q

⃗v)⃗q

= [0, ⃗v + 2⃗q

(⃗q

⃗v + s⃗v)] .

(B.2)

× Equivalently, the same rotation can be represented with a rotation matrix R, such that ⃗v′ = R⃗v, where R (cid:2) can be expressed through the components of the quaternion ˆQ as

×

×

−

(cid:3)

·

s2 + q2

q2 x − y − 2qxqy + 2sqz 2sqy 2qxqz

−

q2 z

s2

2sqz 2qxqy − x + q2 q2 y − − 2qyqz + 2sqx

q2 z

2qxqz + 2sqy 2sqx 2qyqz − y + q2 q2 q2 z x −

−

s2

R =





.



(B.3)



A unit quaternion ˆQ corresponding to a rotation by an angle α around a unit axis ⃗u is given as ˆQ = 2 ], and its inverse is ˆQ− [cos α ⃗u sin α 2 ]. Finally, N sequential rotations around different unit axes defined by unit quaternions

1 = [cos α 2 , ˆQi

2 , ⃗u sin α

−

N result in a new vector ⃗v′ according to } [0, ⃗v′] = ˆQN ˆQN

1... ˆQ2 ˆQ1 [0, ⃗v] ˆQ− 1

2 ... ˆQ−

1 N .

ˆQ−

ˆQ−

{

N

1

1

1

1

−

−

(B.4)

B.3 Root mean square deviation

The root mean square deviation (RMSD) is one of the most widely used similarity criteria in chemistry, structural biology, bioinformatics, and material science. We will stick to this measure here, as it is very powerful, easy to understand and also because it can be computed very efficiently. For our particular needs we will use the definition of RMSD between two ordered sets of points, where each point has an equal contribution to the overall RMSD loss. More precisely, given a set of N points A = with associated weights w =

N and B =

⃗bi {

⃗ai {

}

}

N

N , the RMSD between them is defined as }

wi

{

RMSD(A, B)2 =

1 W

wi

N i (cid:88)1 ≤ ≤

−

⃗ai (cid:12) (cid:12) (cid:12)

(cid:12) (cid:12) (cid:12)

2

,

⃗bi

(B.5)

i wi. Here,

where W = N are statistical weights that may emphasize the importance of a certain part } of the molecular structure, for example in case of a protein, the backbone or Cα atoms. These weights can also be equal to atomic masses (in this case W equals to the total mass of the molecule) or may be set to unity (in this case W = N ). In this work, we set the weights to unity, thus

wi

(cid:80)

{

RMSD(A, B)2 =

1 N

2

,

⃗bi

(B.6)

⃗ai

−

i (cid:88)1 ≤ ≤

N (cid:12) (cid:12) (cid:12)

(cid:12) (cid:12) (cid:12)

since it makes the following equations simpler to read and to use in practice. However, we should keep in mind that the weights can be easily added to all the corresponding equations.

B.4 Rigid body motion described with quaternions

Let R be a rotation matrix and ⃗t a translation vector applied to a molecule with N atoms at positions N are given as ⃗a′i = R⃗ai + ⃗t. Then, A = xi, yi, zi the weighted RMSD between A and A′ will be given as

T , such that the new positions A′ = }

N with ⃗ai =

⃗a′i} {

⃗ai {

{

}

We can rewrite the previous expression using quaternion representation of vectors ⃗ai and ⃗t as

RMSD2(A, A′) =

1 W

wi

⃗ai

R⃗ai

−

−

⃗t

2

.

i (cid:88)

(cid:12) (cid:12)

(cid:12) (cid:12)

RMSD2 =

1 W

ˆQ[0, ⃗ai] ˆQ−

1

−

−

0, ⃗t

2

.

(cid:2)

(cid:3)(cid:12) (cid:12) (cid:12)

wi

[0, ⃗ai]

i (cid:88)

(cid:12) (cid:12) (cid:12)

20

(B.7)

(B.8)

(B.10)

(B.11)

(B.12)

Here, the unit quaternion ˆQ corresponds to the rotation matrix R. Since the norm of a quaternion does not change if we multiply it by a unit quaternion, we may right-multiply the kernel of the previous expression by ˆQ to obtain

RMSD2 =

1 W

wi

[0, ⃗ai] ˆQ

ˆQ[0, ai]

−

−

2

.

(B.9)

i (cid:88)

(cid:12) (cid:12) (cid:12)

[0, ⃗t] ˆQ (cid:12) (cid:12) (cid:12)

Using the scalar–vector representation of a quaternion, ˆQ = [s, ⃗q], we rewrite the previous RMSD expression as

Performing scalar and vector products in Eq. (B.10), we obtain

RMSD2 =

1 W

wi

⃗q − (cid:2)

i (cid:88)

⃗t,

·

−

s⃗t + (2⃗ai

⃗t)

−

×

2

.

⃗q

(cid:3)

RMSD2 =

wi

[qxtx + qyty + qztz]2 (cid:16)

1 W

−

−

i (cid:88) stx + qy(2zi

+ [

+ [

sty + qz(2xi

+ [

stz + qx(2yi

−

tz)

tx)

ty)

−

−

−

−

−

−

qz(2yi

qx(2zi

qy(2xi

−

−

−

ty)]2 tz)]2 tx)]2

.

(cid:17)

Grouping terms in Eq. (B.11) that depend on atomic positions together, we obtain

RMSD2 = t2

x + t2

y + t2

z +

4 W

wi

x(y2 q2

i + z2

i ) + q2

y(x2

i + z2

i ) + q2

z (x2

i + y2 i )

{

2qyqzziyi

}

− +

+

+

i (cid:88) 2qxqzxizi

2qxqyxiyi 4 W

−

qxqztz + qxqyty

− q2 z tx

−

(cid:8)

qyqztz + qxqytx

(cid:8)

qyqzty + qxqztx

4 W

4 W

q2 xty

q2 xtz

−

−

(cid:8)

q2 ytx + sqzty

q2 z ty + sqxtz

q2 ytz + sqytx

−

−

−

−

−

−

sqytz

wixi

i (cid:9) (cid:88)

sqztx

wiyi

i (cid:9) (cid:88)

sqxty

wizi.

i (cid:9) (cid:88)

Introducing the inertia tensor I, the rotation matrix R, the center of mass vector ⃗c, and the 3 matrix E3, we may simplify the previous expression to

×

3 identity

RMSD2 = ⃗t2 +

4 W

⃗qT I⃗q + 2⃗tT (R

E3) ⃗c,

−

(B.13)

where ⃗c = 1 wizi quaternion ˆQ according to Eq. (B.3), and the inertia tensor I is given as

T , rotation matrix R corresponds to the rotation with the unit }

wixi,

wiyi,

W {

(cid:80)

(cid:80)

(cid:80)

wi(y2

i + z2 i ) wixiyi wixizi

− wi(x2 (cid:80)

wixiyi i + z2 i ) wiyizi

− (cid:80)

I =



− (cid:80) −

(cid:80) (cid:80)



wixizi wiyizi i + y2

− − (cid:80) wi(x2 (cid:80)

i )  

.

(B.14)

(cid:80) The RMSD expression (B.13) consists of three parts, the pure translational contribution ⃗t2, the pure rotational contribution 4 E3) ⃗c. In this equation, only two variables depend on the atomic positions N , the inertia tensor I, and the center of mass vector ⃗c. These depend only on the reference structure of a rigid molecule, and can be precomputed. Moreover, it is practical to choose a

W ⃗qT I⃗q, and the cross term 2⃗tT (R

⃗ai {

(cid:80)

−

}

21

reference frame centred on the molecular center of mass. In this frame, the cross term vanishes and the above RMSD equation simplifies to

RMSD2 = ⃗t2 +

⃗qT I⃗q.

(B.15)

4 W

However, we must bring reader’s attention that the inertia tensor must be specifically computed in the chosen reference frame.

B.5

Illustration of the proposed physically-grounded losses

We illustrate in Figure B.1 how the different proposed losses evolve when the prediction is similar to the ground-truth up to a certain rigid-body transformation, either rotation, translation or permutation. In each of these cases, the predicted structure is correct chemically and physically and the loss should thus be 0. This figure helps us illustrate 3 main motivations. First, the difference between the parameter dependent RMSD. Second, the geometric loss is invariant to SE(3) transformations L of the global picture but is not invariant to the index permutation of the arbitrarily chosen ordering of identical molecules. Third, this invariance to index permutation is enabled through the use of the linear sum assignment problem as detailed in section 3.

ML and the physically meaningful

L

Figure B.1: Illustration how the proposed physically-grounded losses evolve under some transformations on a toy example. The numbers are arbitrary and not physically related.

22

<latexit sha1_base64="Vp8ZIA/zOIq06X+gu5ch7aIs/Tk=">AAAC53ichZHLbtNAFIbH5lbMLS1LNiMiJMQislMrzQapUpFg0UrlkraiDtF4cpKOOhdr5hgRWX4GdogtL8Set2HsmkVQCkea0a//fJ7fcyYvpHAYx7+C8MbNW7fvbN2N7t1/8PBRb3vnxJnScphwI409y5kDKTRMUKCEs8ICU7mE0/zyoOmffgbrhNEfcFXAVLGlFgvBGXpr1vuZKYYXnMnqsJ5VGcIXrI4O65q+pEmUZdGG9ruj969aYHgN8BqMaoG4BTYyTcan6sX/czro31kdFEezXj8epMNkPE5pI/biYdyK0Wh3lyaDuK0+6ep4th0E2dzwUoFGLplzFbMouIQ6ykoHBeOXbAmVEtyaZn5r9rmXmilw06p9iZo+886cLoz1SyNt3bWDmHJupXJPNndxf/cac1PvvMTFeFoJXZQIml8FLUpJ0dDmt+hcWOAoV14wboW/AuUXzDKO/vHXUsAJjf4Av8PSMumaof2ZDL1enAwHyWiQvk37++NufFvkCXlKnpOE7JF98oYckwnhQRp8DHgwD0X4NfwWfr9Cw6D75jFZq/DHb9XK6KY=</latexit>LML=1LRMSD=2LGeom=0L⇤ML=1L⇤RMSD=2L⇤Geom=012zxy12zxy12zxy21zxyGround TruthPredictionRotation = 180°Translation = <latexit sha1_base64="NSHdYzU+W/NxG5AbRsZ4PeHH2DY=">AAACLHicdVDLTgIxFG19O75Al24aiYluyAwQYOlj41ITARMhpFPuDA2dzqTtmBCCv+JWN27c+xVujGHrd9gBXWD0Jm1OzrnP4yeCa+O673hhcWl5ZXVt3dnY3NreyeV3mzpOFYMGi0WsbnyqQXAJDcONgJtEAY18AS1/cJ7prTtQmsfy2gwT6EQ0lDzgjBpLdXN7bZ+HoTgizr0zg8dON1dwi5WSV69XSAZqbsmdgmq1XCZe0Z1G4eTUfZnkX+VlN49xuxezNAJpmKBaj6gynAkYO+1UQ0LZgIYwijhTcbbEHH1roaQR6M5oes6YHFqmR4JY2ScNmbJzjWik9TDybWZETV//1jLyL+02NUG9M+IySQ1INhsUpIKYmGRrkR5XwIwYWkCZ4vYEwvpUUWasg3NTQHNpbAP7Q6io0JlpP86Q/0GzVPSqxcqVde8MzWIN7aMDdIQ8VEMn6AJdogZiaIge0CN6ws/4DX/gySx1AX/X7KG5wJ9fvgSqHA==</latexit>✓◆111Permutation : 1→22→1<latexit sha1_base64="gDXREmmlfsNcqdsjNZT12S0dOtk=">AAAC53ichVFNaxsxENVuvxL3y0mPuYiYQunBrJ3F8SUQSCA9JJCkdRKadY1WHjsi+lik2VCz7G/orfTaP9R7/020tlvq4CYDEo83T/M0M2kmhcMo+h2Ejx4/efpsZbX2/MXLV6/ra+tnzuSWQ48baexFyhxIoaGHAiVcZBaYSiWcp9d7Vf78BqwTRn/CSQZ9xcZajARn6KlB/VeiGF5xJovDclAkCF+xODosS7pDo1qS1JakT48+7t8rOACj/hEs1VQeX4r3D/s8IJp5/RUN6o2oGbdb3W5MK7AdtaMp6HS2tmirGU2jQeZxPFgLgmRoeK5AI5fMuYJZFFxCWUtyBxnj12wMhRLcmmp+C/Slh5opcP1iuomSvvXMkI6M9UcjnbILhZhybqJSr6x6cXdzFbksd5njqNsvhM5yBM1nRqNcUjS0+hYdCgsc5cQDxq3wLVB+xSzj6Je/4AJOaPQF/A1jy6SrhvZnMvT/4KzdbHWa8Unc2O3Ox7dCNsgmeUdaZJvskg/kmPQID+Lgc8CDYSjCb+H38MdMGgbzN2/IQoQ/bwHJJOig</latexit>LML=0LRMSD=0LGeom=0L⇤ML=0L⇤RMSD=0L⇤Geom=0<latexit sha1_base64="l8N9Fng6c5PXDt6q0iScnSNY8gQ=">AAAC53ichVFLa9tAEF6pr1R9Oemxl6WmUHowsi0cXwKBFJpDAunDSWjkmtV67CzZh9gdhRqh39Bb6LV/qPf+m65k9+DiNgO7fHzz7Xw7M1kuhcM4/hWEd+7eu/9g62H06PGTp89a2zunzhSWw4gbaex5xhxIoWGEAiWc5xaYyiScZVcHdf7sGqwTRn/CRQ5jxeZazARn6KlJ62eqGF5yJsujalKmCF+xPD6qKrpH+1GaRhvSH44/vv2v4B0Y1QjiRrBRU3t8Kd/c7nOLaOm1EsXRpNWOO0mvOxwmtAa7cS9uwGDQ79NuJ26iTVZxMtkOgnRqeKFAI5fMuZJZFFxCFaWFg5zxKzaHUgluTT2/NfrCQ80UuHHZbKKirzwzpTNj/dFIG3atEFPOLVTmlXUv7u9cTW7KXRQ4G45LofMCQfOl0ayQFA2tv0WnwgJHufCAcSt8C5RfMss4+uWvuYATGn0Bf8PcMunqof2ZDP03OO11uoNO8j5p7w9X49siL8hL8pp0yS7ZJ4fkhIwID5Lgc8CDaSjCb+FN+H0pDYPVm+dkLcIfvwHituis</latexit>LML=3LRMSD=3LGeom=0L⇤ML=3L⇤RMSD=3L⇤Geom=0<latexit sha1_base64="guswpYWXvbCN9TXE6gCSKcPmm2o=">AAAC53ichZHLahsxFIY101vi3pxmmY2IKZQuzNiZ2rMpBFpIFgkkbZ2EZlyjkY8dEV0GSVNqhnmG7EK2faHu8zaRxm7BxWkPSPz85xv9o6Ms58zYKLoNwgcPHz1+srbeePrs+YuXzY1XJ0YVmsKAKq70WUYMcCZhYJnlcJZrICLjcJpdfvD90++gDVPyi53lMBRkKtmEUWKdNWr+SgWxF5Tw8qAalamFH7Y8PKgq/B6/a6RpY0X70+HnjzXQvwfYAyVqIKmBlYzP+Fa+9VT0z5z/QPOsP9Co2YracbeTJDH2oh91o1r0ejs7uNOO6mqhRR2NNoIgHStaCJCWcmJMSbRllEPVSAsDOaGXZAqlYFQrP78l+9xJSQSYYVm/RIVfO2eMJ0q7JS2u3aWDiDBmJjJH+ruYv3veXNU7L+wkGZZM5oUFSedBk4Jjq7D/LTxmGqjlMycI1cxdAdMLogm17vGXUsAwad0BboepJtz4of2eDL5fnHTbnV47Po5bu8lifGtoC22jN6iD+mgX7aMjNEA0iIOvAQ3GIQuvwuvwZo6GweKbTbRU4c879jnotA==</latexit>LML=5LRMSD=7LGeom=8L⇤ML=0L⇤RMSD=0L⇤Geom=0C Metrics

Theorem C.1. PM2

atom ≤

2RMSD2

atom

Proof. Let us first define two metrics PMatom and RMSDatom as

PM2

atom =

1 n2

atom

i

(

⃗xi,pred ||

−

⃗xj,pred

⃗xi,gt

||−||

⃗xj,gt

)2,

||

−

natom

natom (cid:88) j (cid:88) ∈ ∈ 1 natom

atom =

RMSD2

natom (cid:88) ∈ i ⃗xi,pred, ¯xgt = 1

i

natom

⃗xi,pred ||

−

⃗xi,gt

2. ||

i ⃗xi,gt, and use

·

(cid:80)

We also define ¯xpred = 1 write down the following expression, (cid:80)

natom

as the scalar product. Let us

(C.1)

(C.2)

PM2

atom −

2RMSD2

atom =

1 n2

atom

i

j natom (cid:88) (cid:88) ∈ ∈

natom (cid:16)

4⃗xi,pred

⃗xi,gt

·

−

2(⃗xi,pred

·

⃗xj,pred + ⃗xi,gt

⃗xj,gt

·

= 4(xpred

xgt)

·

−

2¯xpred

¯xpred

2¯xgt

¯xgt

·

−

−

·

2 n2

atom

+

⃗xi,pred ||

−

⃗xj,pred

⃗xi,gt

||||

−

⃗xj,gt

)

||

⃗xi,pred ||

−

⃗xj,pred

⃗xi,gt

||||

⃗xj,gt

||

−

i

j natom (cid:88) (cid:88) ∈ ∈

natom

(cid:17) (C.3)

By Cauchy-Schwarz enquality (or maximizing the cosine of an angle between two vectors), we obtain

2 n2

atom

2 n2

atom

⃗xi,pred ||

−

⃗xj,pred

⃗xi,gt

||||

−

⃗xj,gt

||≥

i

j natom (cid:88) (cid:88) ∈ ∈

natom

(⃗xi,pred

⃗xj,pred)

(⃗xi,gt

·

−

⃗xj,gt),

−

i

j natom (cid:88) (cid:88) ∈ ∈

natom

which gives

thus,

PM2

atom −

2RMSD2

atom ≤ −

2(¯⃗xpred

¯⃗xgt)2,

−

PM2

atom −

2RMSD2

atom ≤

0.

Theorem C.2. PM metric is SE3-invariant.

Proof. Let us again consider

(C.4)

(C.5)

(C.6)

PM2(xpred, xgt) =

1 n2

atom

i

j natom (cid:88) (cid:88) ∈ ∈

natom

(

⃗xi,pred ||

−

⃗xj,pred

⃗xi,gt

||−||

⃗xj,gt

)2.

||

−

(C.7)

This quantity is invariant up to any rigid transformation

= (

R

T

, ⃗t) of one of its inputs. Indeed,

23

PM2

atom(

T ◦

xpred, ⃗xgt) =

=

=

=

1 n2

atom

1 n2

atom

1 n2

atom

1 n2

atom

(

(

(

i

j natom (cid:88) (cid:88) ∈ ∈

natom

i

j natom (cid:88) (cid:88) ∈ ∈

natom

i

j natom (cid:88) (cid:88) ∈ ∈

natom

⃗xi,pred

− T ◦

⃗xj,pred

⃗xi,gt

||−||

⃗xj,gt

)2

||

−

||T ◦

⃗xi,pred

||R

⃗t

−

− R

⃗xj,pred + ⃗t

⃗xi,gt

⃗xj,gt

)2

||

−

||−||

(⃗xi,pred

⃗xj,pred)

⃗xi,gt

||−||

⃗xj,gt

)2

||

−

−

||R

(C.8)

(

⃗xi,pred ||

−

⃗xj,pred

⃗xi,gt

||−||

⃗xj,gt

)2

||

−

i

j natom (cid:88) natom (cid:88) ∈ ∈ atom(xpred, xgt)

= PM2

Theorem C.3. Geometric loss is SE3-invariant.

Proof. We consider:

Geom (

L

pred, T

gt) =

T

1

M

1

−

M

1

−

i=1 (cid:88)

RMSD(

L

1

− M,pred ◦ T T

i,pred,

1 − M,gt ◦ T T

i,gt).

(C.9)

This quantity is invariant up to any transformation

noise of one of its inputs: T

Geom (

L

noise

T

◦ T

pred,

gt) =

T

=

=

=

1

M

1

−

M

1

−

i=1 (cid:88)

RMSD

L

1

M

1

−

M

1

−

i=1 (cid:88)

RMSD

L

1

M

1

−

RMSD

M

1

− Geom (

L

L

i=1 (cid:88) pred, T

gt)

T

(cid:16)

(cid:16)

(cid:16)

(

noise

T

◦ T

M,pred)−

1

noise

◦ T

◦ T

i,pred,

1 − M,gt ◦ T

T

i,gt

(cid:17) i,pred,

1

− M,pred ◦ T

1 − noise ◦ T

T

noise

◦ T

1 − M,gt ◦ T

T

i,gt

1

− M,pred ◦ T

T

i,pred,

1 − M,gt ◦ T T

i,gt

(cid:17)

(cid:17)

D Method and implementation

D.1 Extension to the inversion dataset

We argue that our method can also be applied to the inversion version of the dataset. Indeed this version, defined in Liu et al. [2024a], presents half of the 17 molecules in each assembly as the left-handed and right- handed geometries of a chiral or achiral molecule. The latter molecules can interconvert during crystallization and thus, our permutation-invariant approach can be applied on this dataset. In the case of chiral molecules which can not interconvert during crystallization, the invariance to permutation can be adapted to the 2 subsets of left-handed and right handed geometries individually.

24

Algorithm D.1 Atom-level model.

def AtomModel(

: atoms, t : time,

: positions, Nlayer = 5, Nconv = 5, c = 128)

[c] [Natom, c] [Nmol, c] [Nmol, 3] Atom to Molecules edges

⃗Pt ⃗Xt

i)) j))

[Edges, 3, 3] [Edges, c] [Edges, c] [Edges, c]

ai

{

}

⃗Pt i}

{

}

ij)

{ { { {

⃗Pt i} {

) + Linear(SiLU(t))

} i − i × ij ×

1: t = Linear(SiLU(Linear(time_embed(t)))) ht , = PaiNN( ai 2: i} { st ht 3: = ScatterMeanper mol( ) i} i} { ⃗Xt ⃗Pt 4: = ScatterMeanper mol( ) i} i} { ⃗Xt ⃗Pt et 5: = RadialGraph( , ) ij} i} i} { { /et i, j 6: for all ij = 1 : { ij = ⃗Pt ⃗Xt ⃗Xt ⃗Pt 7: ∆t j/ j∥ i − ∥ ij = ⃗Pt ⃗Xt ⃗Pt ⃗Xt χt 8: j/ j∥ i × ∥ Λt ij = ∆t χt 9: ij Baset ij, Λt ij, χt ij = concat(∆t 10: 11: Et i = MLP(GaussianFourierEmbed(Baset 12: Et j = MLP(GaussianFourierEmbed(Baset = MLP(concat(Et zt 13: ij} { t t i = 0 14: i = 0 and S R [1, ..., Nlayer]: for all l 15: [1, ..., Nconv]: for all f 16: t ˜h = Transformerf 17: i} { ht ht = i} i} { t ˜h = FFNf ( i} { ht ht = i} i} { if l < Nconv : ht i} { end if end for st ht i} i} { { t t i + ScatterMeanper mol i ← R

ht , i} { ˜h + LayerNorm( { ) ˜h + LayerNorm( {

18: 19: 20: 21: 22: 23: 24: 25:

= ScatterMeanper mol(

= SiLU( {

ht i} {

st j} )

ht i}

i, Et

conv(

{ t i}

∈ ∈

t i}

j))

26:

R

{

{

{

)

)

,

ij · ij ·

zt ij}

)

) + Linear(SiLU(t))

[Nmol, 3]

Baset ij

·

(cid:1)(cid:1) (cid:17)

[Nmol, 4]

27:

S

t i ← S

t i + ScatterMeanper mol

(cid:80)

(i) MLP(concat(ht

(cid:16)

i + st

j, zt

ij))

j

∈N

Baset ij

·

(cid:17)

28: end for 29: return

t i ,

{S

(i) Proj

(cid:16) Linear

j

∈N

(cid:0)

(cid:0)

(cid:80) t i} R

MLP(concat(ht

i + st

j, zt

ij))

25

D.2 AssembleFlow atom-level model

We use the Atom-level implemented in AssembleFlow and which can be described in Algorithm D.1.

D.3

Implementation details

D.3.1 Hyperparameters and number of parameters

Table D.1 lists the hyperparameters used during training along with the number of parameters for the model and the memory usage.

Table D.1: Hyperparameters used in the model.

Parameters

{500} {8}

{LM: {alpha: 10}} } }

{RMSD: {Geometric:

∅ ∅

{None: {’Exact’:

{’Differentiable’:

} }

∅ ∅

{reg=5.10−

2.median_score}}

{Adam} 4} {10− {0} {’CosineAnnealingLR’}

{5} {128} {3} {20} {’mean’} {3.25}

{128} {128} {10} {50} {1, 50} {’mean’} {20} {8} {5} {5} {3.25}

Model part

Training

Optimizer

Molecular Encoder (PaiNN)

Backbone (AssembleFlow Atom)

Function

Epochs Batch size

Loss

Assignment

Name Learning rate Weight decay Scheduler

cutoff embedding dim number of interactions number of rbf scatter gamma

emb_dim hidden dim cutoff cluster cutoff number of timesteps scatter number of Gaussians number of heads number of layers number of convolutions gamma

Total number of parameters: 4 292 718

Total memory usage:

38.9 GB

26

D.3.2 Licenses and versions

The common environment packages are released with the code through a conda environment. We also report in Table D.2 the versions and licenses of the main packages used.

Table D.2: Versions and licenses.

Package

Version

License

COD-Cluster17

git commit

AssembleFlow Model git commit

POT

RMSD

0.9.5

-

MIT

MIT

MIT

CeCILL

E Ablation studies

E.1 Differential assignment with direct regression

In Table E.1, we list the experiments of training or not with differential assignment in direct regression with the AssembleFlow atom-level model. We want to draw the attention to the PM∗ methods and the great added value of using our assignment method regardless of the loss being used.

Table E.1: Ablation study of using differentiable assignment (Diff. assign.) COD-Cluster17 with direct regression.

losses during training on

Loss

Diff. assign.

Test Loss in Å ↓

∗RMSD

L

∗Geom

L

Packing matching in Å

↓

PM∗center PM∗atom PMcenter PMatom

Dataset: COD-Cluster17-5K

ML

RMSD

Geom

L L L

∗ML ∗RMSD ∗Geom

L L L

ML

RMSD

Geom

L L L

∗ML ∗RMSD ∗Geom

L L L

✓ ✓ ✓

✓ ✓ ✓

±

9.64 9.64 ± 10.10

±

8.69 8.73 9.32

±

±

±

0.21

11.43 11.24 0.03 0.14 10.05 12.16 12.05 8.78

0.07

0.06

0.06

±

±

±

±

±

0.08

0.15

0.11

0.12

0.15

0.05

5.62 5.57 8.44

3.60 3.77 5.55

±

±

±

±

±

0.31

0.19

0.43

0.04

0.12

0.15

6.68 6.67 8.37

5.54 5.67 6.54

±

±

±

±

±

0.24

0.07

0.26

0.04

0.08

0.07

±

± Dataset: COD-Cluster17-All

±

6.97 6.93 9.05

5.80 5.85 6.92

0.23

0.12

0.37

±

±

±

7.62 7.61 8.74

0.18

0.02

0.22

±

±

±

0.03

0.05

0.07

±

±

±

6.96 6.98 7.46

±

±

±

0.03

0.05

0.02

11.67 11.58 11.90

8.65 8.70 9.35

±

±

±

±

±

±

0.07 11.33 0.04 11.20 0.08 11.38 12.10 12.16 8.71

0.03

0.00

0.02

0.05 12.94 0.12 12.98 0.09 13.62 3.47 3.41 5.43

0.10

0.08

±

±

±

0.16 10.47 0.13 10.44 0.07 10.52 5.51 5.54 6.52

0.04

0.10

0.04

±

±

±

0.03 13.03 0.01 13.07 0.01 13.66 5.80 5.80 6.84

0.01

0.05

0.02

±

±

±

0.15 10.47 0.12 10.43 0.06 10.49 7.00 7.00 7.45

0.00

0.06

0.00

0.03

±

±

±

±

±

±

±

±

±

±

±

±

±

±

0.02

0.01

0.01

±

±

±

0.01

0.01

0.02

±

±

±

±

27

E.2 Differential assignment with flow matching

Table E.2 lists the experiments of switching on and off the expensive flow matching framework (table 3) along with using the differential assignment. The added value of flow matching when using the differential assignment loss is not very clear in the current framework. As it does not always significantly help the method, we suspect a need to further adapt the assignment method to the iterative flow matching scheme. However, we would like to point out two things. Firstly, it greatly improves the performance of the relative geometric method on the absolute metrics while decreasing it on the relative metric. Secondly, it enable to reach the overall best performance in the PM∗center metric.

Table E.2: Ablation study of using flow matching in addition to differentiable assignment losses during training on COD-Cluster17.

Test Loss in Å

↓

Packing matching in Å

↓

Flow

Matching L

∗RMSD

∗Geom

L

PM∗center PM∗atom PMcenter PMatom

8.69 8.73 9.32

9.31 9.53 9.09

8.65 8.70 9.35

9.37 9.51 9.28

✓ ✓ ✓

✓ ✓ ✓

Dataset: COD-Cluster17-5K

±

±

0.06 12.16 0.07 12.05 8.78 0.06 ± 0.25 13.54 0.54 13.71 0.09 10.48

±

±

±

±

±

±

±

±

±

0.12

0.15

0.05

0.50

0.40

0.18

3.60 3.77 5.55

3.48 3.43 3.72

0.04

0.12

0.15

0.19

0.20

0.11

±

±

±

±

±

±

5.54 5.67 6.54

5.60 5.56 5.73

0.04

0.08

0.07

0.14

0.14

0.04

±

±

±

±

±

±

Dataset: COD-Cluster17-All

±

±

0.02 12.10 0.03 12.16 8.71 0.00 ± 0.09 13.69 0.38 13.42 0.09 10.72

±

±

±

±

±

±

±

±

±

0.10

0.08

0.03

0.21

0.22

0.13

3.47 3.41 5.43

3.42 3.29 3.89

0.04

0.04

0.10

0.12

0.04

0.23

±

±

±

±

±

±

5.51 5.54 6.52

5.63 5.53 5.88

±

±

±

±

±

±

0.02

0.01

0.05

0.07

0.04

0.12

5.80 5.85 6.92

6.12 6.12 6.04

5.80 5.80 6.84

6.15 6.01 6.27

0.03

0.05

0.07

0.23

0.19

0.10

±

±

±

±

±

±

0.00

0.00

0.06

0.12

0.06

0.17

±

±

±

±

±

±

6.96 6.98 7.46

7.29 7.28 7.19

7.00 7.00 7.45

7.36 7.23 7.40

0.03

0.05

0.02

0.21

0.17

0.05

±

±

±

±

±

±

0.01

0.01

0.02

0.09

0.07

0.12

±

±

±

±

±

±

Loss

∗ML ∗RMSD ∗Geom

∗ML ∗RMSD ∗Geom

∗ML ∗RMSD ∗Geom

∗ML ∗RMSD ∗Geom

L L L

L L L

L L L

L L L

E.3 Using linear sum assignment during training against differentiable assign-

ment

We report in Table E.3 the experiment of using the linear sum assignment (exact) during training against the differential assignment (relaxed ). On the one hand, using the exact solver during training enables backpropagation for each molecule in the assembly along the path leading to its assigned target, while killing the other gradients corresponding to other paths to unassigned targets. On the other hand, the relaxed differential version preserves the gradients to all possible paths with probability attached to each, which enables a more diverse learning. While being suboptimal compared to the differential assignment, the added value of using the latter is very small as shown in Table E.3. We report here the performance obtained without tuning the regularization parameter of the Sinkhorn algorithm and exploring its influence on the overall performance. Nonetheless we argue that this hyperparameter should should play an important role with better-performing methods in the future. Indeed we believe that if the method learned nearly perfectly to match a molecule to its target position, this relaxed method would diversify the search space and act as a data augmentation method, the amount of which would be set by the regularization parameter.

28

Table E.3: Ablation study of using differential or exact assignment losses during training on COD-Cluster17 with direct regression.

Loss

Assignment type

Test Loss in Å

↓

∗RMSD

L

∗Geom

L

Packing matching in Å

↓

PM∗center PM∗atom PMcenter PMatom

Dataset: COD-Cluster17-5K

∗ML ∗RMSD ∗Geom

L L L

∗ML ∗RMSD ∗Geom

L L L

∗ML ∗RMSD ∗Geom

L L L

∗ML ∗RMSD ∗Geom

L L L

Exact Exact Exact

Diff. Diff. Diff.

Exact Exact Exact

Diff. Diff. Diff.

8.70 8.72 9.32

8.69 8.73 9.32

±

±

0.06 12.24 0.07 12.19 8.80 0.05 ± 0.06 12.16 0.07 12.05 8.78 0.06

±

±

±

±

±

±

±

0.14

0.05

0.08

0.12

0.15

0.05

3.64 3.65 5.51

3.60 3.77 5.55

±

±

±

±

±

0.12

0.05

0.25

0.04

0.12

0.15

5.56 5.61 6.53

5.54 5.67 6.54

0.08

0.03

0.14

0.04

0.08

0.07

±

±

±

±

±

±

5.81 5.81 6.90

5.80 5.85 6.92

0.04

0.02

0.14

0.03

0.05

0.07

±

±

±

±

±

±

6.96 6.96 7.45

6.96 6.98 7.46

0.04

0.04

0.06

0.03

0.05

0.02

±

±

±

±

±

±

± Dataset: COD-Cluster17-All

±

±

8.65 8.70 9.35

8.65 8.70 9.35

±

±

±

±

±

±

±

±

0.02 12.18 0.03 12.14 8.71 0.03 ± 0.02 12.10 0.03 12.16 8.71 0.00

±

±

0.02

0.09

0.03

0.10

0.08

0.03

±

3.37 3.44 5.40

3.47 3.41 5.43

0.03

0.09

0.07

0.04

0.04

0.10

±

±

±

±

±

±

5.47 5.56 6.51

5.51 5.54 6.52

0.02

0.03

0.05

0.02

0.01

0.05

±

±

±

±

±

±

5.78 5.80 6.84

5.80 5.80 6.84

0.01

0.01

0.05

0.00

0.00

0.06

±

±

±

±

±

±

6.99 7.00 7.46

7.00 7.00 7.45

0.01

0.01

0.03

0.01

0.01

0.02

±

±

±

±

±

±

E.4 Angular VS translational prediction from the

ML loss

L

We report in Table E.4 the results of an experiment predicting only translations. It is conducted by removing ML loss, setting the hyperparameter α = 0. We observe that the performance the angular prediction from the is not significantly hindered by not predicting the angular part in both SinkFast and AssembleFlow. Thus we can conclude that the angular part is not well predicted at all by both models and this should stand as a major future line of work in the field.

L

Table E.4: Ablation study of predicting rotations or not during training with the with direct regression.

L

ML loss on COD-Cluster17

Loss

Flow matching

α

Test Loss in Å

↓

∗RMSD

L

∗Geom

L

Packing matching in Å

↓

PM∗center PM∗atom PMcenter PMatom

Dataset: COD-Cluster17-5K

✓ ✓

✓ ✓

ML

ML

∗ML ∗ML

L L

L L

ML

ML

∗ML ∗ML

L L

L L

6.28 6.31

5.80 5.82

5.96 5.97

5.80 5.79

0.15

0.09

0.03

0.04

±

±

±

±

0.02

0.04

0.00

0.01

±

±

±

±

7.39 7.42

6.96 6.97

7.15 7.17

7.00 6.99

0.16

0.10

0.03

0.05

±

±

±

±

0.03

0.04

0.01

0.02

±

±

±

±

10 9.56 9.65 0

10 8.69 8.86 0

0.07 13.35 0.33 13.31 0.06 12.16 0.08 12.15

0.30

0.27

0.12

0.21

±

±

±

±

3.86 3.87

3.60 3.64

±

±

±

±

0.13

0.11

0.04

0.09

5.79 5.85

5.54 5.61

0.12

0.09

0.04

0.04

±

±

±

±

Dataset: COD-Cluster17-All

±

±

±

±

±

±

±

±

10 9.26 9.40 0

10 8.65 8.79 0

0.18 12.02 0.22 12.18 0.02 12.10 0.03 12.18

0.05

0.07

0.04

0.08

±

±

±

±

5.60 5.64

5.51 5.54

0.03

0.04

0.02

0.01

±

±

±

±

0.30

0.21

0.10

0.06

±

±

±

±

3.51 3.49

3.47 3.41

29

F Additional experiments

F.1 Comparison to inorganic-based methods

Inorganic crystal structure prediction is a fast-moving domain in which many state of the art models compete and innovate. We here want to compare the performance of current organic state of the art to the inorganic one. Thus, we conduct experiments on the COD-Cluster17-5k dataset by retraining both CDVAE [Xie et al., 2022] and DiffCSP [Jiao et al., 2024] models. In both cases, the models are trained to predict the target set of atomic positions from a noise distribution, where the same atoms are randomly positioned in space. Both methods operate in fractional coordinates and require a lattice definition. However, since the COD-Cluster17 dataset provides only point clouds without explicit lattice parameters or periodic boundary conditions, we define a pseudo lattice as the bounding box that encompasses all sets of molecules. Atom positions are then expressed in fractional coordinates relative to this pseudo lattice.

This setup introduces a stringent constraint that is not optimal for symmetry-based algorithms like CDVAE and DiffCSP, as we do not supply accurate information about atomic density or minimal symmetry groups. Despite this, both methods were able to produce high-quality predictions in certain cases. Notably, their performance did not show a strong correlation with the number of atoms per ASU.

At inference, we sample from the learned distribution of atomic positions rather than using initial positions provided by COD-Cluster17. As shown in Table F.1, both CDVAE and DiffCSP underperform significantly compared to rigid-body-based AssembleFlow and SinkFast methods, indicating that these point cloud models are not well suited to this task out-of-the-box. In Tables F.2 and F.3 we explore whether these methods perform particularly well on small graphs, but this tendency is actually also shared by both AssembleFlow and SinkFast.

Table F.1: Performance in Å( inorganic crystal structure prediction models CDVAE and DiffCSP on COD_Cluster17 - 5k test set.

) of our proposed SinkFast and AssembleFlow rigid-body methods against ↓

Method

CDVAE DiffCSP AssembleFlow SinkFast

0.52

PM∗center 10.50 23.50 3.76 3.60

0.00

2.44

0.04

±

±

±

±

0.89

PM∗atom 14.81 30.61 5.73 5.54

0.02

0.04

±

±

±

2.53

±

) of our proposed SinkFast and AssembleFlow rigid-body methods against Table F.2: Performance in Å( ↓ inorganic crystal structure prediction models CDVAE and DiffCSP on COD_Cluster17 - 5k test set filtered on natom

16 corresponding to the 20 smallest graphs.

≤

Method

CDVAE DiffCSP AssembleFlow SinkFast

0.07

PM∗center 8.17 19.74 2.58 2.60

0.42

0.19

0.04

±

±

±

±

0.91

PM∗atom 12.34 25.89 3.49 3.48

0.48

0.19

0.11

±

±

±

±

We present in Table F.4 for each model the best predictions based on minimal Packing Matching (PM) score, and in Tables F.5 and F.6 the 5th and 10th percentiles, respectively. However, due to CDVAE’s long training and very slow inference time, we compute its performance on 120 test samples. To ensure a fair comparison, we evaluate all models on this shared subset, which we refer to as the CDVAE subset. We observe from these experiments that CDVAE and DiffCSP can perform extremely well on very few structures. However, their effectiveness quickly decreases across the dataset. This suggests that while these models have potential, they require further adaptation to be competitive on this task. In our view, adapting such

30

) of our proposed SinkFast and AssembleFlow rigid-body methods against Table F.3: Performance in Å( ↓ inorganic crystal structure prediction models CDVAE and DiffCSP on COD_Cluster17 - 5k test set filtered on natom

50 corresponding to half of the dataset.

≤

Method

CDVAE DiffCSP AssembleFlow SinkFast

0.82

PM∗center 10.37 22.93 3.26 3.35

0.06

2.66

0.11

±

±

±

±

1.10

PM∗atom 14.63 29.98 4.96 4.95

0.03

0.06

±

±

±

2.91

±

models meaningfully goes beyond a quick out-of-the-box comparison. Nonetheless, they represent promising directions and could enrich the set of baselines on COD-Cluster17 in future dedicated studies or reviews.

Table F.4: Single best structure performance in Å( ) of our proposed SinkFast and AssembleFlow rigid-body ↓ methods against inorganic crystal structure prediction models CDVAE and DiffCSP on COD_Cluster17 - 5k test set : filtered on the CDVAE subset.

Method

CDVAE DiffCSP AssembleFlow SinkFast

PM∗center 1.19 0.99 2.04 2.06

PM∗atom 2.57 4.61 3.03 2.73

) of our proposed SinkFast and AssembleFlow rigid-body methods Table F.5: 5th percentile performance in Å( ↓ against inorganic crystal structure prediction models CDVAE and DiffCSP on COD_Cluster17 - 5k test set : filtered on the CDVAE subset.

Method

CDVAE DiffCSP AssembleFlow SinkFast

PM∗center 1.91 6.61 2.67 2.66

PM∗atom 3.21 11.08 3.86 3.83

Table F.6: 1st quantile performance in Å( ) of our proposed SinkFast and AssembleFlow rigid-body methods ↓ against inorganic crystal structure prediction models CDVAE and DiffCSP on COD_Cluster17 - 5k test set : filtered on the CDVAE subset.

Method

CDVAE DiffCSP AssembleFlow SinkFast

PM∗center 2.55 9.61 2.77 2.84

PM∗atom 4.67 15.19 4.43 4.23

31

F.2 Dependence to the correctness of the conformation

To evaluate our model’s dependency on the correctness of molecular conformations, we conducted the following experiment. For each molecule in the COD-Cluster17-5k test set, we extracted the corresponding SMILES representation and generated five stable conformations using RDKit [Landrum et al., 2025], using EmbedMolecule followed by UFFOptimizeMolecule functions. Each generated conformation is then passed through our model to predict the packed molecular positions.

To assess the quality of RDKit-generated conformtations, we computed the Packing Matching (PM) between each RDKit sampled molecule conformation and its corresponding COD-Cluster17 conformation. On average, PM was 3.27 Å with a standard deviation of 2.19 Å and a median of 3.11 Å. Due to RDKit failures on 170 of the 500 test set structures caused by issues such as improper valences or atom count mismatches–typically to experimentally invisible hydrogens–our analysis focuses on a subset of 330 molecules, referred to as the RDKit subset.

The results are presented in Tables F.7, F.8 and F.9 under the RDKit column. First, we compare performance on RDKit-generated versus crystallographic conformations for both SinkFast and AssembleFlow. In terms of center-of-mass alignment (PMcenter), the methods perform comparably across the two types of input. However, the performance are slightly hindered in the atom-to-atom comparison. This shows that conformations are not well represented in our model. Second, comparing Table 2 to Table 3 we observe that both methods perform much better on crystallographic structures from which we can generate 5 different conformations that are close to the crystallographic ones. This confirms the importance of initial conformational accuracy.

Our conclusion is that while the models get a sense of how important initial conformations are, the learned representations are independent to the molecular conformations. We therefore agree that future models should be trained end-to-end, jointly learning conformation and crystal structure prediction. This represents a promising direction for advancing research in this very complex domain. We believe our study helps to identify key challenges and can serve as a foundation for future work in organic crystal structure prediction.

Table F.7: Performance in Å( and RDKit generated conformations on COD-Cluster17-5k test set : filtered on the RDKit subset.

) of our proposed SinkFast and AssembleFlow methods on both crystallographic ↓

Method

RDKit

AssembleFlow AssembleFlow SinkFast SinkFast

✓

✓

0.01

PM∗center 3.54 3.58 3.59 3.55

0.13

0.00

0.13

±

±

±

±

0.00

PM∗atom 5.44 5.59 5.41 5.53

0.08

0.08

0.15

±

±

±

±

Table F.8: Performance in Å( ) of our proposed SinkFast and AssembleFlow methods on both crystallographic ↓ and RDKit generated conformations on COD-Cluster17-5k test set : filtered on the RDKit subset with the lowest packing matching distance to original ones.

0.03

PM∗atom 4.92 4.90 4.88 4.81

0.03

0.11

0.12

±

±

±

±

0.01

PM∗center 3.27 3.27 3.28 3.18

0.13

0.03

0.11

±

±

±

±

Method

RDKit

AssembleFlow AssembleFlow SinkFast SinkFast

✓

✓

32

) of our proposed SinkFast and AssembleFlow methods on both crystallographic Table F.9: Performance in Å( ↓ and RDKit generated conformations on COD-Cluster17-5k test set : filtered on the RDKit subset with the highest packing matching distance to original ones.

Method

RDKit

AssembleFlow AssembleFlow SinkFast SinkFast

✓

✓

0.03

PM∗center 3.80 3.89 3.88 3.92

0.01

0.11

0.12

±

±

±

±

0.08

PM∗atom 5.95 6.27 5.92 6.25

0.00

0.09

0.14

±

±

±

±

G Visualizations

Figure G.1 shows the packing of three assemblies randomly picked from the test set. We visualize all atoms as van der Waals (vdW) spheres. We took the standard vdW radii for chemical elements, colored using JMol colors and ray-traced the scenes with PyMol. The image does not demonstrate common patterns, only certain packing similarities. One can conclude on the generally poor reconstruction obtained from the two compared algorithms. Indeed, the method and the problem formulation do not allow to generalize well enough to be applied and used at large scale.

33

Figure G.1: Visualization of our SinkFast- ∗ML prediction against ground truth and AssembleFlow method on 3 examples randomly picked from the test set. Scores of each prediction are reported with PM∗atom, ∗ML errors with different values of L the α parameter. Atoms are colored using the JMol color convention and shown using PyMol molecular visualization system [Schrödinger, LLC, 2015].

∗rot the rotational error and 3

∗tran the translational error,

∗RMSD,

L

L

L

L

34