aims-PAX: Parallel Active eXploration for the automated construction of Machine Learning Force Fields

aims-PAX: Parallel Active eXploration for the automated construction of Machine Learning Force Fields

Tobias Henkes,1 Shubham Sharma,2 Alexandre Tkatchenko,1 Mariana Rossi,2 and Igor Poltavskyi1 1)Department of Physics and Materials Science, University of Luxembourg, L-1511 Luxembourg, Luxembourg 2)Max Planck Institute for the Structure and Dynamics of Matter, 22761 Hamburg, Germany

(*Electronic mail: igor.poltavskyi@uni.lu)

(Dated: August 19, 2025)

Recent advances in machine learning force fields (MLFF) have significantly extended the capabilities of atomistic simulations. This progress highlights the critical need for reliable reference datasets, accurate MLFFs, and, crucially, efficient active learning strategies to enable robust modeling of complex chemical and materials systems. Here, we introduce AIMS-PAX, an automated, multi-trajectory active learning framework that streamlines the development of MLFFs. Designed for both experts and newcomers, AIMS-PAX offers a modular, high-performance workflow that cou- ples flexible sampling with scalable training across CPU and GPU architectures. Built on the widely adopted ab initio code FHI-AIMS, the framework seamlessly integrates with state-of-the-art ML models and supports pretraining using general-purpose (or "foundational") models for rapid deployment in diverse systems. We demonstrate the capabilities of AIMS-PAX on two challenging systems: a highly flexible peptide and bulk CsPbI3 perovskite. Across these cases, AIMS-PAX achieves a reduction of up to two orders of magnitude in the number of required reference calculations and enables over 20x speedup in AL cycle time through optimized resource utilization. This positions AIMS-PAX as a powerful and versatile platform for next-generation ML-driven atomistic simulations in both academic and industrial settings.

I.

INTRODUCTION:

The success of Machine Learning Force Fields (MLFFs)1 has deeply transformed the field of molecular simulations. They are now the preferred method for simulating the dy- namics of large systems, such as extensive perovskite unit cells2 or solvated proteins3, with quantum-chemical accu- racy. While general-purpose (GP) (sometimes called “founda- tional”) models4–11 trained on large datasets12–17 are becom- ing more widespread, there remains a strong demand for high- quality data to fine-tune these models or to build new custom models for challenging applications.18–20

The process of collecting representative high-quality datasets can be labor-intensive, requiring considerable man- ual effort and computational resources. To address these challenges, a common approach is to employ active learning (AL).21,22 In AL, an uncertainty measure of a model’s pre- diction is used to select data points for labeling and inclusion in the training dataset. This approach enriches the training dataset with data points that represent a challenge for the cur- rent state of the model. In essence, the model autonomously determines which data to prioritize for training and which to disregard. Therefore, this procedure reduces human interven- tion and decreases the computational cost of model training by requiring only a small number of costly and slow refer- ence ab initio calculations to reach an acceptable accuracy. In addition, it also improves the robustness of the MLFF by auto-detecting and correcting its possible failures during the training procedure.

AL has been successfully applied to a plethora of applica- tions. For example, Young et al.23 used active learning to iter- atively improve a MLFF that was able to accurately simulate solvents and selected chemical reactions. In a study by Stark

et al.24, an AL workflow leveraging clustering algorithms was used to model reactive hydrogen dynamics on copper surfaces. Furthermore, Mohanty et al.25 showed how AL was neces- sary to augment a dataset for efficiently training MLFFs for polymer dynamics and Kang et al.26 highlighted how AL was crucial to model strongly anharmonic materials. Numerous other successful AL applications can be found in the litera- ture.27–41 While AL is always beneficial in the data collec- tion process, the automation degree of the procedure varies broadly. Often, AL is done manually or by users who de- velop tailored scripts for their specific problems. This situa- tion results in the need for expert knowledge, such as selecting starting geometries, setting uncertainty thresholds, or decid- ing when to stop sampling. Additionally, employing collec- tions of custom scripts instead of a defined workflow makes the process less accessible to new practitioners and less re- producible by other researchers. In recent years, the commu- nity has started addressing these challenges by offering var- ious automated software solutions. For example, in the DFT codes such as the VIENNA AB INITIO SIMULATION PACKAGE (VASP)42–44, CASTEP45,46 and the AMSTERDAM MODEL- ING SUITE (AMS)47 different automated AL workflows are implemented. Next to AL methods directly integrated into quantum chemistry codes, there also exist separate software packages offering AL or automated simulation functionalities such as FLARE48, CATFLOW49, ALEBREW50, PSIFLOW51, ALMOMD52, apax53 or PAL54. Although such tools have helped establish MLFFs and AL as a standard tool in molecu- lar simulations, there is a potential for improvements that we address in this work, in particular with respect to the efficiency of configurational space exploration, hardware utilization and support for multisystem sampling.

We present AIMS-PAX, short for ab initio molecular simulation-Parallel Active eXploration, a flexible, fully au-

5 2 0 2

g u A 8 1

] h p - m e h c . s c i s y h p [

1 v 8 8 8 2 1 . 8 0 5 2 : v i X r a













aims-PAX: Parallel Active eXploration for the automated construction of Machine Learning Force Fields

2

tomated, open-source software package for performing AL using a parallelized algorithm that enables efficient resource management. The current implementation is coupled to the FHI-AIMS55 program for DFT calculations and the MACE56,57 architecture as an MLFF model. However, the algorithm itself is agnostic to both the MLFF architecture and the DFT code.

We showcase AIMS-PAX performance by applying it to a large, flexible peptide. In the case of this peptide, AIMS-PAX automatically produces accurate and stable MLFFs and pro- vides a high-quality dataset with a minimal number of ab ini- tio calculations. For the latter, we compare AIMS-PAX per- formance to a more traditional manual data collection strategy used in the TEA challenge.58,59 Fully automated, AIMS-PAX reduced the number of required DFT calculations by two or- ders of magnitude while achieving essentially the same re- sults.

The structure of the article is the following. First, in the Methods section we provide a technical description of the AIMS-PAX workflow, as well as a thorough report of com- putational methods and their settings used. In the Results sec- tion, we report the application of AIMS-PAX to the peptide Ac-F-A5-K. In addition, the computational advantages of the parallelized AL algorithm are shown on the example of bulk CsPbI3 pervoskite. We conclude the article with a short sum- mary and outlook for the future of AIMS-PAX.

II. METHODS

In this section, we describe the proposed AIMS-PAX AL workflow by first explaining how initial datasets and initial models can be generated efficiently through an integrated pro- cedure that leverages GP-MLFFs. Afterwards, we explain the core of the proposed method, which consists of the paral- lelized AL algorithm. We describe the implementation details, multi-trajectory sampling, and the employed uncertainty mea- sure, along with its adaptive threshold. We close the section with a comprehensive account of the computational methods used in the various studies.

A.

Initial Dataset and Model Generation

A starting point for an AL procedure involves generating an initial ensemble of MLFFs, or a single MLFF, that simul- taneously predicts the potential energy surface (PES) and as- sociated uncertainties, capable of producing stable molecular dynamics within a limited region of the PES. We want to em- phasize that this part of the workflow is not active in the sense that the model does not choose which points to include in the training. At this stage, the model is not yet sufficiently reli- able to guide this selection process. Thus, data is generated using a sampling strategy, such as molecular dynamics (MD). The described initial dataset generation (IDG) can be estab-

lished using one of two approaches:

1. Short ab initio simulations can be run to generate

molecular configurations along with their respective en- ergies, forces etc.

2. A GP-MLFF can be used to produce physically plausi- ble system geometries. These geometries are then re- computed using a reference ab initio method.

The second approach is generally preferable, as it helps decorrelate the geometries, making the IDG significantly more computationally efficient—by at least an order of mag- nitude. Also, we coupled the IDG with PARSL60, similar to what is done in PSIFLOW51. This enables users to per- form DFT calculations on sampled geometries across multiple nodes in parallel. Importantly, the GP model does not need to provide accurate energies or forces; it acts solely as a geome- try generator in combination with MD simulations.

Both initialization strategies are implemented in AIMS- PAX, see Fig. 1b. Currently, the implementation includes the MACE-MP061 model, with additional models to be incorpo- rated in the future, such as SO3LR4.

Once the dataset reaches a user-defined threshold in size, it is split into several equally sized subsets. These subsets are randomly selected from the full dataset, with one subset being assigned to each MLFF ensemble member. This ensures that each model is trained and validated on slightly different data. In addition to varying the training data, we introduce further diversity between ensemble members by using different ran- dom seeds for initializing model weights. Each MLFF is then trained on its assigned subset for a user-specified number of epochs.

Optionally, this procedure of generating data and training can be repeated multiple times. More precisely, after training, new structures are sampled and subsequently labeled, which is followed by more training steps. More details on this ap- proach can be found in Section A 3 a.

During these cycles, the MLFFs are trained without reini- tializing their weights. Instead, the existing weights are reused at every training step, and models are trained on their entire datasets to prevent catastrophic forgetting62–66. This contin- ual learning (CL) approach67–69 enables models to improve iteratively without retraining from scratch each time, reducing the number of required training steps. Crucially, at this stage, models are not trained to full convergence; instead, training is deliberately limited to a small number of epochs (typically five or fewer). The described early-stopping strategy avoids overfitting the models on the initial datasets and hinders them from getting stuck in local minima. This would make updat- ing the models with new data during the AL significantly more difficult without reinitializing their weights.

The IDG is repeated until a user-defined stopping criterion is met. Possible stopping criteria include a maximum number of training epochs, a predefined training set size, or a target performance (e.g., force mean absolute error, MAE) on the validation set. The latter can be aligned with the overall AL workflow termination condition. For instance, the user may specify a target force MAE that should be achieved on the validation dataset by the end of the AL process. A scaling factor can be applied to this target MAE to define the stop- ping criterion for MLFF ensemble pretraining. At this stage,

aims-PAX: Parallel Active eXploration for the automated construction of Machine Learning Force Fields

the goal is not to develop highly accurate MLFFs or exhaus- tive datasets but to obtain a robust MLFF ensemble capable of generating stable dynamics within an initial region of the PES, from which the main AL workflow can begin sampling the broader PES landscape.

B. Parallelized Active Learning

The AL phase involves sampling the configurational space of the target system using a pre-trained ensemble of MLFFs, which are employed for both sampling and uncertainty quan- tification. The latter is used together with a threshold that de- termines when a sampled structure is supposed to be labeled via a DFT calculation.

In the case of AIMS-PAX, each time the threshold is crossed and the DFT calculation has been performed, the new data is added to the training (or validation) set of all MLFFs. These are then updated in a CL scheme using a user-specified, ideally low, number of epochs similarly to the one employed in the IDG. For more details on the exact training strategy we refer to Section A 3 a.

While the algorithm proposed herein is, in principle, agnos- tic to the choice of uncertainty quantification method, we em- ploy the query by committee (QBC)70–72 approach due to its conceptual simplicity and widespread adoption. The integra- tion of alternative uncertainty estimation techniques into our framework is straightforward and will be explored in future work.

In the QBC approach, an ensemble of independently trained ML models is used to produce a distribution of predicted outputs during inference. As described previously, diversity among ensemble members arises from differences in initial weight initialization seeds and distinct initial training datasets. The variance within the ensemble predictions serves as a way to quantify a model’s uncertainty. Specifically, we quantify uncertainty based on the variance of atomic force predictions, using the maximum per-atom force variance across the sys- tem, as defined in Eq. 127,

(cid:118) (cid:117) (cid:117) (cid:116)

δn = max

i

1 3M

M ∑ j=1

∑ k∈x,y,z

(cid:0)Fni jk − ¯Fnik

(cid:1)2,

(1)

where δn denotes the uncertainty associated with geometry n. The maximum is computed over all atoms i in the system. The ensemble consists of M models indexed by j, and the summation over k spans the three spatial components x, y, and z. The term Fni jk represents the k-th Cartesian component of the force on atom i in system n predicted by model j, while ¯Fnik denotes the ensemble-averaged force component on atom i in direction k.

For setting the uncertainty threshold, we adopt an approach analogous to the one implemented in VASP42–44, where a scaled moving average of the uncertainties is used in place of a fixed threshold. Specifically, the threshold at iteration t, denoted by δt , is computed using Eq. 2,

3

(2)

δt =

1 + cx N

N ∑ n=1

δn.

Here, N represents the number of past uncertainty values in- cluded in the moving average, for which we follow definitions introduced in the VASP code and use a default window size of 400. The scaling factor cx allows the threshold to be ad- justed: values cx < 0 tighten the threshold, while cx > 0 relax it. In our implementation, the default value is cx = 0. We also include the option to freeze the threshold after a user-specified training set size.

The primary advantage of this adaptive thresholding ap- proach is that it eliminates the need for a fixed, user-defined uncertainty cutoff, which can vary between systems.73 Since the moving average naturally decreases over time, some con- figurations will always exceed the threshold. As a result, the sampling frequency depends on the value of cx: if set too high, very few points may be sampled; if too low, the method may oversample. Based on our experience and also reported for the MLFF training in the VASP code, values of cx ∈ [−0.1, 0.1] serve as practical starting points.

To improve the efficiency and robustness of the active sam- pling, we adopt a multi-trajectory approach that has also been successfully applied in similar frameworks.40,49,54 Herein multiple ML-driven simulations are executed in parallel, see Fig. 1c. These trajectories may differ in their sampling strate- gies, utilizing various thermostats, barostats, external condi- tions, or simulation schemes ranging from classical MD to en- hanced sampling techniques such as metadynamics. Although only a subset of these options is currently implemented, addi- tional methods are under development and will be integrated in future updates. We point out that the uncertainty threshold as defined in Eq. 2 is shared across all of these trajectories.

A key advantage of multi-trajectory sampling is its abil- ity to decouple the generation of new configurations from the evaluation of high-uncertainty states. While MLFFs generate new candidate geometries, DFT calculations are performed in parallel on selected high-uncertainty configurations to en- rich the reference dataset. These calculations are done us- ing FHI-AIMS55 compiled as a library and interfaced through the ATOMIC SIMULATION INTERFACE74. The latter allows to run an instance of FHI-AIMS continuously, meaning that the DFT code does not have to be reinitialized before every calculation. This can remove significant overhead, which is especially valuable when handling smaller systems.

As mentioned earlier the training is done using a CL scheme, similar to the one used during pre-training, which allows MLFFs to be incrementally updated during sampling. this strategy Together with the parallel DFT calculations, also optimizes utilization of available computational resources (CPUs and GPUs), thereby enhancing the overall efficiency and throughput of the AL workflow.

Similarly to the IDG, PARSL60 can also be used here, al- lowing to distribute DFT calculations across multiple nodes in parallel. Additionally, the number of DFT workers can be adapted dynamically up to a user-defined maximum. Through the flexible allocation of resources, we ensure that no worker

aims-PAX: Parallel Active eXploration for the automated construction of Machine Learning Force Fields

4

is idle or overloaded. This is particularly useful in AL as the demand for new data can change during the procedure. For example, there can be long sequences where an MLFF trajec- tory is certain about all encountered geometries or intervals where all trajectories lead to high uncertainty regions.

As with the IDG, the AL workflow proceeds until a user- defined stopping criterion is met. This may be based on the total training set size, performance on the validation set (e.g., force MAE), number of training epochs, or total number of MD steps. Once the stopping criterion is met, either the en- tire ensemble or only the best-performing ML model, selected based on validation error, is further trained to converge on the whole training set.

C. Computational Details

All DFT calculations were performed using FHI AIMS75 version 241114 compiled as a library and called through the python ASI package ASI4PY74 version 1.3.18 connected with ASE76 version 3.23.0. For MACE56,57 we used MACE- with PYTORCH77 version 2.3.1. TORCH version 0.3.9. For AIMS-PAX with PARSL, we used PARSL version 2024.12.16 and FHI AIMS75 compiled as an executable. In this implementation, the ASE76 calculator is used to perform DFT calculations.

N-acetylphenylalanyl-pentaalanyl-lysine (Ac-F-A5-K)

During AL, we employed the Perdew-Burke-Ernzerhof (PBE) functional78 with non-local many-body dispersion (nl- MBD)79 using the LIGHT species defaults for numerical set- tings and basis sets. Relativistic corrections were applied using the atomic ZORA approximation.80 The total energy, eigenvalue, density, and force convergence criteria were set to 10−6 eV, 10−4 eV, 10−5 e/Å3, and 10−4 eV/Å, respectively. For recomputing the dataset to a higher level of theory, the PBE081 functional with the nl-MBD dispersion correction and the INTERMEDIATE species defaults for basis sets and numer- ical settings was used, keeping the other settings fixed.

The serial version of AIMS-PAX was used for both IDG and AL. The former was performed by sampling 8 points for each member of an ensemble of 4 models with a stopping criterion of a maximum of 50 epochs. The structures were sampled using the small MACE-MP0 GP model61 by running MD in the NVT ensemble with the Langevin thermostat82 at 500 K with a timestep of 1 fs and a friction coefficient of 0.001 fs−1. In order to decorrelate the data points, structures were picked every 20th MD step. Their energies and forces where then computed using DFT.

The AL workflow was run until a training set size of 500 structures was reached with 1:1 ratio for the validation set. During the AL, when new data was added to the training set, the models were trained for a total of 10 epochs on the updated dataset. The training was split into two steps, each involving 5 epochs. More precisely, this means that after 5 epochs are trained, the other running trajectories are propagated first be- fore finishing with 5 more epochs of training. For more de- tails on this see Section A 3 a. During AL, the structures were

sampled using the same MD settings as in the IDG. The uncer- tainty threshold parameter cx (see Eq. 2) was set to the default value of 0.

The test sets for Ac-F-A5-K were created by running MD with the small MACE-OFF6 potential in the NVT ensemble at 300, 500, and 700 K using the Langevin82 thermostat with a friction coefficient of 0.001 fs−1 and a time step of 1 fs for 1 ns. Every 100th geometry was selected and from the remain- ing points 1000 were selected by farthest point sampling83 us- ing the mean, invariant atomic MACE-OFF descriptors. The chosen geometries were then recomputed using FHI AIMS with the same functional and settings used in the AL.

The MD simulations for assessing the stability of models were performed in the NVT ensemble at 300, 500, and 700 K using the Langevin thermostat with a friction coefficient of 0.001 fs−1 and a time step of 1 fs for 1 ns. Throughout the simulation, the bond lengths were monitored, and if any of them exceeded 2 Å, the simulation was stopped.

Bulk Perovskite (CsPbI3 2x2x2)

During the AL,

the PBE functional with the pairwise Tkatchenko-Scheffler dispersion correction84 was used, em- ploying the INTERMEDIATE species defaults for numerical set- tings and basis sets. Relativistic corrections were applied using the atomic ZORA approximation.80 The total energy, eigenvalue, density, and force convergence criteria were set to 10−6 eV, 10−5 eV, 10−5 e/Å3, and 10−4 eV/Å, respectively. A Gaussian smearing of 0.05 eV was applied to the orbital occu- pations. The calculations employed a parallel KS method with load balancing and local indexing enabled. A maximum of 300 self-consistency iterations was allowed. The charge mix- ing parameter was set to 0.02. The k grid was set to 1 × 1 × 1. The lattice vectors were [17.23958, 0, 0], [0, 17.23958, 0], and [0, 0, 25.00256], all in Ångstrom.

The parallel version of AIMS-PAX employing PARSL was used for both IDG and AL. The former was performed by sampling 10 points for each member of an ensemble of 4 models with a stopping criterion of 50 epochs for the initial training. The structures were sampled using the small MACE- MP0 GP model61 by running NPT MD with the Nosé-Hoover thermostat85 at 300 K and the Parinello-Rahman barostat86 at 1 bar. The timestep was set to 1 fs and otherwise default ASE parameters were used. In order to decorrelate the sampled data points, structures were picked every 20th MD step. Their energies, forces and stress where then computed using DFT. The models were converged on the initial dataset before con- tinuing with AL by training them until no improvements w.r.t. the validation set was achieved for 50 epochs

The AL workflow was run until a training set size of 100 structures was reached with 7:3 ratio for the validation set. The maximum epochs per trajectory are 10, and the interme- diate epochs are 10. The structures were sampled using the same MD settings as described in the IDG above. The uncer- tainty threshold parameter cx (see Eq. 2) was set to 0.2 and the uncertainty itself was checked every 10th MD step.

Settings for MACE

aims-PAX: Parallel Active eXploration for the automated construction of Machine Learning Force Fields

5

Figure 1: Overview of the AIMS-PAX workflow. (a) Required input files: The first file (control.in) follows FHI-aims conventions75 and contain the DFT settings and the system’s geometry, initial geometries can either be inside a folder (geometry/) or, in case of a single geometry, in a file (geometry.in), the third file (mace.yaml) contains MACE56,57 model parameters and the fourth (aims_PAX.yaml) is an AIMS-PAX-specific file containing the AL settings. For the AL workflow, folders containing the initial datasets (data/) and models (models/) are required. (b) Initial dataset generation (IDG): Geometries are sampled using either DFT or a GP model, with DFT providing labels in both cases. Sampling continues until a (* user specified) criterion is met. (c) Parallelized active learning: The AL workflow requires input files, existing data, and models, which can be provided by the IDG procedure. Sampling occurs over multiple trajectories, triggering DFT calculations when an uncertainty threshold is exceeded. GPU-based ML tasks (orange) and CPU-based DFT tasks (blue) can run in parallel. AL is continued until a (* user specified) stopping condition is met. (d) Output: Models and collected data produced during AL (and IDG).

The MACE architectures used during the AIMS-PAX runs are summarized in Table I. For the training during AL with AIMS-PAX the follwing settings were used. The AMSGrad optimizer87 and a learning rate of 0.01 were utilized through- out. For the the IDG, the learning rate was decreased by 0.8 using the Reduce On Plateau scheduler with a patience of 5 and γ = 0.9993. No learning rate scheduler was used dur- ing the AL. An exponential moving average of 0.99 for the model parameters and a gradient clipping of 10 were used. For the loss function a weighted mean square loss of ener- gies and forces with weights 1 and 1000, respectively, was utilized. A batch size of 5 was used throughout. After the AL runs themselves, the best performing models of the respective ensembles were trained on the final training set until there was no improvement w.r.t. the validation set for 50 epochs.

For training the large model for Ac-F-A5-K (third column in Table I) from scratch on the recomputed dataset, the same settings as described above for AIMS-PAX were used except the following changes. The energy and force weights of the

loss function were set to 44 and 1000, respectively. The model was trained for 1000 epochs and after the 750 epochs the en- ergy and force weights were swapped and the learning rate set to 0.001. The learning rate was decreased by 0.8 using the Reduce On Plateau scheduler with a patience of 256 and γ = 0.9993.

Hardware

The benchmarks, the active learning and training were per- formed using a NVIDIA Ampere 40 GB HBM GPU and AMD EPYC Rome 7452 CPU. Recomputing the data on a higher level of theory and the DFT calculations through PARSL were done using AMD EPYC Rome 7H12. The train- ing of MACE models from scratch and MD runs for Ac-F-A5- K were performed using a NVIDIA A100 80GB GPU.

aims-PAX: Parallel Active eXploration for the automated construction of Machine Learning Force Fields

6

Parameter Channels Max degree Lmax Cutoff [Å] Radial Bessel functions Message-passing layers Correlation order Radial MLP layers Neurons per MLP layer Activation function Output Layer Irreps

System Ac-F-A5-K (small) Ac-F-A5-K (large) CsPbI3

32 1 6 8 2 3 3 32 SiLU "128x0e"

256 2 6 8 2 3 3 64 SiLU "16x0e"

64 1 6 10 2 3 3 16 SiLU "16x0e"

Table I: Architectural parameters of the MACE models used for the systems studied in this work.

III. RESULTS

To demonstrate the performance of AIMS-PAX, we use it to generate reference datasets and train MLFFs for a challeng- ing system: a N-acetylphenylalanyl-pentaalanyl-lysine (Ac- F-A5-K) peptide. This system was selected because of its complexity and relevance to typical MLFF applications in biochemistry. Our results demonstrate that the proposed AL framework reduces the number of required reference evalu- ations by up to three orders of magnitude and substantially minimizing the necessary human effort.

A. The phase-space of a peptide: Ac-F-A5-K

We begin by applying our workflow to generate a robust MLFF for the Ac-F-A5-K peptide, a system comprising 100 atoms. This peptide exhibits multiple local minima explored during MD simulations under ambient conditions, which typi- cally require numerous costly reference calculations when us- ing conventional, non-AL methods. To assess the reliability of uncertainty estimates within our AL workflow, we trained an ensemble of models for the Ac-F-A5-K peptide using the AIMS-PAX framework, based on three parallel MD trajecto- ries, as detailed in Section II C. During the AL process, we also perform DFT reference calculations every 50 MD steps independently from the uncertainty selection criterion. Ad- ditionally, the actual prediction error and model uncertainty were evaluated at these points. Using this data we analyze the behavior of the uncertainty measure throughout the AL pro- cedure without a bias towards high uncertainty states.

Figure 2 (a) shows the evolution of prediction error, uncer- tainty threshold, and training set size over the course of an AL run for each trajectory.

All three trajectories display consistent behavior: model uncertainty, the uncertainty threshold, and prediction error all decrease systematically as AL progresses. Notably, the tem- poral profiles of uncertainty and error follow similar trends, indicating a positive correlation between these quantities. To quantify this observation, we plot the uncertainty against the maximum atomic force error in Figure 2 (b), along with a linear regression fit, and compute the Pearson correlation co- efficient. Across all trajectories, we observe a clear positive

correlation between uncertainty and error, with only a limited number of outliers. This positive correlation is crucial, con- firming that ensemble uncertainty can serve as an effective proxy for prediction error. Consequently, the AL algorithm selectively targets challenging configurations for high-fidelity DFT calculations while avoiding redundant sampling of triv- ial structures. Importantly, a perfect agreement between un- certainty and error is not required for practical applications; some errors may be missed in the early stages but captured at later AL steps as more diverse geometries are encountered.

Despite the widespread use of the QBC strategy for MLFFs, concerns have been raised regarding its reliability.88 Also, we have chosen a relatively small ensemble size of 4 and it has been reported that small ensembles result in biased estima- tors of uncertainties and other properties89. However, it is not unusual, to use only a few ensemble members for AL in MLFFs.24,26,27,32,33 This is often done to reduce the compu- tational expense of an AL procedure as increasing the num- ber of ensemble members means more ML models have to be trained and evaluated. We opted for a small ensemble because, as shown above, the uncertainty measure is already a good ap- proximation for the real error with four members. Thus, there is no need to incur greater computational cost by using more MLFF models in the ensemble.

To further probe the reliability of our approach, we ana- lyze the evolution of the Pearson correlation coefficient be- tween uncertainty and error throughout the AL process, see Figure 2 (c). In the initial 3k MD steps, the correlation ex- ceeds 0.5 for all three trajectories. However, the correlation declines from 3k to 9k MD steps, even turning negative for the third trajectory (green). This degradation in uncertainty qual- ity may be attributed to increasing overlap among the training sets of individual ensemble members as the AL progresses. As the models are exposed to similar data, they tend to converge on the same underlying potential energy surface, thereby re- ducing ensemble diversity. Nonetheless, the use of multiple trajectories helps alleviate this issue. A significant correlation for even a single trajectory can drive effective data acquisition, ensuring the continued efficacy of the overall AL scheme.

Another important aspect of the proposed AL workflow is the influence of multiple concurrent trajectories on the sam- pling process. To evaluate how model accuracy depends on the number of parallel trajectories used during AL, we con-

aims-PAX: Parallel Active eXploration for the automated construction of Machine Learning Force Fields

7

ducted multiple AIMS-PAX runs with varying numbers of concurrent MD simulations. We performed three indepen- dent AL runs with different random seeds per setup for sta- tistical reliability. For subsequent tests, we selected the best- performing model (based on validation set accuracy) from each of these three independent runs, resulting in three mod- els per setup. The test sets were generated by performing 1 ns of NVT MD at 300, 500, and 700 K using the MACE- OFF (small) potential6. Representative structures were se- lected from these trajectories using farthest point sampling (FPS) based on ML-derived descriptors. Reference energies and forces were then computed at the chosen level of theory. Additional computational details are provided in Section II C. The average MAEs for different numbers of concurrent tra- jectories during the AL procedure are reported along with their variances in Table II. At 300 K, all models achieved MAEs between 16 and 18 meV/Å, independent of the number of trajectories used. Similarly, for the 500 and 700 K test sets, MAEs ranged from 27 to 30 and 37 to 40 meV/Å, respec- tively—again showing negligible dependence on the number of parallel trajectories. The comparable final model accura- cies at given temperatures across all setups indicate that the AL-generated datasets are of similar quality. Notably, the to- tal number of MD steps required to gather the training data remained approximately constant across all settings. For in- stance, a run with a single trajectory required an average of ∼68k MD steps to collect 1,000 structures (500 for training and 500 for validation). In contrast, setups using 8 and 32 tra- jectories converged after only ∼9k and ∼2k MD steps per tra- jectory, respectively. These findings and the above-mentioned improvement in the uncertainty measure robustness for the multi-trajectory approach suggest that increasing the number of trajectories improves sampling efficiency without compro- mising data quality.

To investigate whether the use of CL during AL influences the performance of the resulting MLFFs, we retrained new models from scratch using the datasets acquired throughout the AL process. These models were evaluated using the same protocol described above for multiple trajectories. No deteri- oration in performance was observed for the models trained with CL compared to those trained from scratch. Detailed results are presented in the Appendix (Section A 1). These re- sults confirm that the continuous learning paradigm offers a more computationally efficient alternative to repeated retrain- ing without compromising the accuracy or robustness of the final MLFF models.

An essential requirement for MLFFs is the stability of the resulting MD simulations.90. We performed four 1 ns-long NVT MD runs with each of the three models at 300, 500, and 700 K. This resulted in a total of 12 MD runs per number of trajectories used in the AL procedure and temperature. We define a simulation as stable if no covalent bond in the system exceeds 2 Å, a condition that is not expected to be violated at the temperatures considered. For more details, see Sec- tion II C.

The results of all MD stability tests are summarized in Ta- ble III. At 300 K, all MD trajectories remained stable, regard- less of the number of trajectories used during the AL process.

At 500 K, the temperature used during AL, nearly all MD runs were also stable, with only one exception: a single unsta- ble simulation was observed for the model trained using eight parallel AL trajectories. In contrast, at the elevated tempera- ture of 700 K, instability was observed in at least one MD run for every MLFF model tested. Specifically, for models trained with 8 and 32 trajectories, 7 out of 12 MD simulations were unstable. For the model obtained from a single-trajectory AL run, 3 simulations were unstable. Finally, for models trained with 4 and 16 trajectories, 1 and 2 simulations were unstable, respectively. These results align with the sampling strategy used in AL: since the training data were collected at 500 K, it is expected that MD simulations at or below this temperature (e.g., 300 K and 500 K) remain stable, as the MLFF is un- likely to encounter configurations outside its training domain. At 700 K, however, the MD trajectories explore more diverse and potentially unseen regions of configuration space, which can lead to instability due to extrapolation beyond the model’s training domain.

Overall, no clear trend emerges linking the stability of the MLFFs to the number of trajectories used during AL. This suggests that, for the current AL setup, model robustness in MD simulations is not significantly affected by the number of concurrent sampling trajectories. This behavior may be at- tributed to all trajectories using the same sampling protocol, potentially limiting exploration diversity. Future work will explore diverse sampling strategies across trajectories during AL to improve coverage of the potential energy surface and enhance model robustness under elevated temperatures and extreme simulation conditions.

Finally, the most reliable validation of an MLFF model lies in evaluating its performance in realistic application scenarios. Here, we assess the model’s ability to repro- duce the Ramachandran plots from molecular dynamics sim- ulations conducted under ambient conditions. The proce- dure follows that of the TEA 2023 Challenge benchmark59. We take the Ramachandran plots produced by the MACE model trained on the complete dataset in Ref. 59 as a refer- ence. We recomputed the structures sampled by the AIMS- PAX workflow, using three parallel AL trajectories, at the same level of theory employed in the TEA 2023 Challenge (PBE0+MBDNL/intermediate). A new MACE model was then trained using the same architecture and hyperparame- ters as the reference study. For further details, see Sec- tion II C. This recomputation was necessary because, during the AL phase, we employed a smaller MACE model trained on PBE+MBDNL/light to reduce computational costs. Such sampling-by-proxy strategies are commonly used in MLFF development1, and we demonstrate here how AIMS-PAX can efficiently generate high-quality, diverse datasets with mini- mal DFT overhead.

The retrained model was used to perform 12 independent 1 ns NVT MD simulations at 300 K, each initialized from a different starting geometry, following the protocol of the TEA 2023 Challenge. The resulting trajectories were analyzed to extract dihedral angle distributions, which were then clustered following the methodology from Ref. 59. The Ramachandran plots obtained from our model and the TEA reference model

aims-PAX: Parallel Active eXploration for the automated construction of Machine Learning Force Fields

8

are shown in Fig. 2 (d). Both this work’s and the reference MACE models yield nearly identical cluster structures and populations for dihedral angles A and B, which correspond to the peptide backbone. Specifically, for angle A, a single dom- inant cluster is located at approximately (−π/4, π/2) (blue), with a relative population of 100%. For angle B, two clusters appear in both models: one at (−π/4, π/2) (blue) and an- other at (−π/2, π) (green), with relative populations of 84% and 16%, respectively.

Minor differences are observed only in the dihedral angle C, which pertains to the peptide tail. Both models identify three clusters at similar angular positions, but relative popu- lations differ slightly. For the blue cluster at (−π/2, π/2), our model predicts a population of 64%, compared to 60% in the reference model. The orange cluster at (±π, 0) appears with a population of 7% in our model and 3% in the reference. The green cluster at (±π, π/2) is equally represented in both cases, with a population of 33%. The observed differences between MD results can likely be attributed to limited sam- pling statistics, as capturing slow conformational changes at the peptide tails may require simulations significantly longer than 12 ns.

A crucial advantage of the proposed AL workflow is that our model was trained on only 500 reference structures, re- quiring a total of just 1,000 DFT calculations—including those performed during the AL process and the subsequent re- computation at a higher level of theory (excluding validation structures). In comparison, the reference model in Ref. 59 was trained on 4,000 structures generated from 100,000 DFT calculations, a process that also involved several months of manual effort. These results highlight the efficiency and scal- ability of the AIMS-PAX framework for the automated gen- In particular, we eration of high-quality training datasets. demonstrate a reduced number of DFT evaluations by up to three orders of magnitude, achieved with minimal human in- tervention, while obtaining a final MACE model that delivers comparable predictive performance. Future developments, in- cluding the implementation of more reliable uncertainty quan- tification methods and diverse sampling techniques, are ex- pected to strengthen further the advantages of the proposed automated AL workflow over traditional dataset generation and MLFF training approaches.

MAE Forces (meV/Å)

# Traj.

300 K

500 K

700 K

1

4

8

16

32

17.93 ± 2.28

27.07 ± 0.25

37.73 ± 0.84

16.60 ± 0.44

28.50 ± 0.61

37.47 ± 1.11

16.43 ± 0.67

28.73 ± 0.99

37.93 ± 1.36

17.20 ± 0.82

29.67 ± 0.95

39.70 ± 1.77

16.83 ± 0.38

29.77 ± 1.30

38.73 ± 1.36

Table II: Mean absolute test errors of models acquired using AIMS-PAX with various number of trajectories used for sampling. Average and standard deviation over the best models from three separate AIMS-PAX runs.

# Traj.

300 K

500 K

700 K

# Stable MD Runs

1

4

8

16

32

12

12

12

12

12

12

12

11

12

12

9

11

7

10

7

Table III: Number of stable MD simulations performed with models acquired using AIMS-PAX with various number of trajectories used for sampling at multiple temperatures. Three models were obtained from separate AIMS-PAX runs and four simulations were run for each. Stability was defined as no bonded atoms separated by more than 2 Å.

B. Computational Benefits of Parallelized Active Learning

In order to investigate the efficiency of the proposed parallel AL algorithm we choose to run AIMS-PAX in parallel and se- rial mode for the small peptide Ac-Ac-A3-NHMe (42 atoms) and the perovskite CsPbI3 (160 atoms in the unit cell). More detail regarding the exact settings for DFT and AIMS-PAX are described in Section II C.

As the number of trajectories using in AIMS-PAX is inte- gral part of the parallel algorithm, we ran the procedure with 4, 8, 16 and 32 trajectories for Ac-A3-NHMe. We used the com- pletely serial version (the MLFF waits for DFT calculations which themselves are processed serially) and the CPU/GPU parallel version (the MLFF does not wait for DFT calculations but the latter wait for each other). The latter works through an MPI-based implementation, using ASI74. The whole work- flow has 1 GPU card as well as 1 CPU node with 128 cores available irrespective of how many trajectories are being used. The results of this study are shown in the appendix in Sec- tion A 2.

Through the implementation using PARSL we can easily distribute DFT calculations across multiple nodes dynami- cally. Therefore, the AL becomes CPU/GPU parallel, i.e., DFT calculations can run while the MLFF is being used, and CPU/CPU parallel, i.e., multiple DFT calculations can run in parallel. To investigate the advantage of this approach, we fix the number of trajectories to 32 and consider 1, 2, 4, 8, 16 and 32 CPU available nodes. The test is performed for the perovskite CsPbI3. We emphasize the number of available nodes here, as AIMS-PAX automatically scales up or down the number of workers (with one node per worker in our case) up to a user-defined maximum.

One DFT calculation, using the hardware and settings de- scribed in Section II for this perovskite takes around 20 user- minutes. Therefore, contrary to smaller systems where the bottleneck of the AL run can be the MLFF computation time, in the AL procedure of the perovskite the bottleneck will al- ways be due to the DFT calculations. This is why we chose this system to investigate the scaling of AIMS-PAX to more workers. The results of the scaling test are shown in Fig. 3.

aims-PAX: Parallel Active eXploration for the automated construction of Machine Learning Force Fields

9

Figure 2: Results from applying AIMS-PAX to the peptide Ac-F-A5-K: (a) Model uncertainty, actual maximum force error, uncertainty threshold and training set size as a function of MD steps throughout the AL procedure. (b) Actual maximum force error vs. model uncertainty with Pearson correlation coefficient over the whole AL workflow. A linear fit is shown as a guide to the eye. (c) Pearson correlation coefficient and training set size over multiple segments of the AL workflow for n = 3 trajectories that were used for sampling. (d) Strucutre of Ac-F-A5-K including highlighting of relevant dihedral angles A,B and C. Ramachandran plot for said angles acquired with a model used in the TEA challenge58,59 (left) and ours, acquired using AIMS-PAX (right). Relative populations of highlighted clustered are given in bold font (black) and the blue number in the bottom left corner of each plot indicates the percentage of configurations from the MD trajectories assigned to a cluster.58,59

aims-PAX: Parallel Active eXploration for the automated construction of Machine Learning Force Fields

10

AIMS-PAX reduces the number of required DFT calculations by two orders of magnitude compared to traditional sampling approaches, providing robust and accurate MLFF suitable for running long MD simulations. Finally, using the example of bulk CsPbI3 perovskite, we demonstrate the advantage of par- allel multi-trajectory sampling, reducing the AL time by an order of magnitude for systems requiring demanding DFT cal- culations. The presented AIMS-PAX software package can accomplish all these out of the box with minimal human ef- fort.

We are convinced that the model- and ab initio method- agnostic nature of AIMS-PAX will be the basis of fruitful open-source collaboration. We envision multiple technical and practical avenues to explore. Optimizing the algorithm, e.g. by parallelizing across multiple GPU devices, will im- prove the efficiency and scalability of AIMS-PAX even fur- ther. Also, we plan to add support for automated fine-tuning of GP models as well as sampling across multiple chemical species. The latter will enable the automated creation of di- verse, information-rich datasets and, together with the fine- tuning capabilities, pave the way to expand the chemical space of pre-trained MLFFs. Also, while we showcased AIMS-PAX with selected examples in this study, a central focus of current work is its application to complex and extended systems.

Thus, AIMS-PAX stands as a powerful, adaptable frame- work, enabling new frontiers in MLFF development and their ability to address complex chemical challenges.

ACKNOWLEDGMENTS

T.H. thanks Sander Vandenhaute for his time and valuable suggestions concerning the PSIFLOW and PARSL packages. He also thanks Iryna Knysh for her support in finding bugs inside the code.

We acknowledge financial support from the Luxembourg National Research (FNR) under the AFR project 17932705. The simulations were performed on the HPC facilities of the University of Luxembourg, (see hpc.uni.lu), the Luxembourg national supercomputer MeluXina, the computing resources at the Max Planck Institute for the Stucture and Dynamics of Matter in Hamburg and at the MPCDF. The authors grate- fully acknowledge the LuxProvide teams for their expert sup- port. S.S. acknowledges support from the UFAST Interna- tional Max Planck Research School.

DATA AVAILABILITY STATEMENT

The data and models used for the results in this study can

be found on: 10.5281/zenodo.16893060

The code of AIMS PAX is available under the GITHUB

repository: github.com/tohenkes/aims-PAX.

Figure 3: Wall-clock runtime in hours as a function of the number of available CPU nodes for AIMS-PAX using PARSL applied to the pervoskite CsPbI3.

Running AIMS-PAX with only 1 available node takes about 57 hrs of wall-clock runtime. By going to two nodes, the time spent is reduced to roughly 29 h, i.e. by a factor of nearly 2. Doubling the number of nodes to 4 and then 8, halves the run time to 14 h and 7 h, respectively.

The deviation from ideal scaling observed when using 16 and 32 nodes, as indicated by the decreasing slope in Fig. 3, stem from the fixed number of sampling trajectories, which is 32. The likelihood that all 32 (or even 16) trajectories require halting simultaneously, and thus trigger concurrent DFT eval- uations, is low. As a result, the computational resources on all nodes are not fully utilized at all times. Overall, the use of the parallel AIMS-PAX implementation can reduce MLFF creation time by a large factor for systems requiring compu- tationally demanding reference labeling, while efficiently uti- lizing both GPU and CPU resources.

IV. CONCLUSION

In this paper, we introduced AIMS-PAX, a flexible, fully automated, open-source software package for performing AL using a parallelized algorithm that ensures efficient resource management. The key advantages of the proposed workflow include: minimal human intervention; the use of state-of-the- art GP-MLFF models for initial dataset generation and pre- training; and a parallel workload manager that effectively uti- lizes all available computational resources. The AL process is distributed across multiple sampling runs. Reference DFT calculations can proceed independently whenever a member of the MLFF ensemble exhibits uncertainty regarding a newly encountered geometry.

The performance of AIMS-PAX is demonstrated on two di- verse and challenging systems: Ac-F-A5-K, a highly flexible peptide; and the bulk perovskite CsPbI3. For the Ac-F-A5-K

aims-PAX: Parallel Active eXploration for the automated construction of Machine Learning Force Fields

11

Appendix A: Appendixes

1. Training models from scratch

In order to assess the difference between continuously training the models during AL and training models from scratch afterwards, we trained 3 models with different seeds on the dataset of Ac-F-A5-K, acquired through the AIMS- PAX run with 4 trajectories as described in Section II C. All settings for training, testing, MD simulations and the model architecture were kept the same. The results for the accuracy and stability are shown in Table IV. Comparing with results in Tables II and III, there is no meaningful difference between training from scratch and using continual learning (CL) observable for accuracy and stability. Given that CL is computational more efficient, it is the mode of action used in AIMS-PAX.

Temperature MAE Forces (meV/Å) # Stable MD Runs

300 K 500 K 700 K

17.53 ± 0.90 28.03 ± 1.03 37.77 ± 1.25

12 12 10

Table IV: Mean absolute test errors (mean ± standard deviation) and number of stable MD simulations at various temperatures for models trained from scratch on data acquired via an AIMS-PAX run for Ac-F-A5-K. MAE values are averaged over three models with different seeds. Each model was used to generate 4 MD simulations, for a total of 12 per temperature. Stability was defined as no bonded atoms separated by more than 2 Å.

2. Speedup of CPU/GPU parallel aims-PAX version

The results regarding the speedup using the CPU/GPU parallel AIMS-PAX version are summarized in Fig. 4. We found that using 4 or more trajectories results in a speedup of 15 %. The largest speed-up can be observed with 8 trajectories with 18 %. For 16 and 32 the speedup is slightly lower than for 8 trajectories at 16 % for both runs. The speed-up observed when enabling more trajectories is caused by DFT calculations that run concurrently with the sampling of new points and training. Furthermore, a slight speed-up is observed between 4 and 8 trajectories, likely due to the reduced probability of no trajectory being propagated at a given time. Because trajectories are stopped when the uncertainty threshold is exceeded, it is more likely that all trajectories will stop if there are fewer of them, thus halting all AL progress.

Computational details

The same DFT settings used for Ac-F-A5-K were used for Ac-A3-NHMe during AL. The only exception is that no dis- persion correction was applied. For generating the initial data

Figure 4: Speedup of the parallel version of AIMS-PAX over the serial version applied to Ac-Ala3-NHMe running on a single CPU node with 128 cores and 1 GPU card.

set using AIMS-PAX the same settings as for Ac-F-A5-K were used.

For the AL workflow with AIMS-PAX, the same settings were used as for Ac-F-A5-K, except that the parallel AIMS- PAX version was used and the procedure stopped when the training set size of 200 was reached. Also, the same MACE architecture as described in Table I under Ac-F-A5-K (small) was used.

3. Technical Details of aims-PAX

Here we provide some technical details of the inner work- ings of AIMS-PAX for interested readers. We focus on the training during AL, explain how data as well as failed SCF convergence are handled and highlight slight differences be- tween the order of operations in the serial and parallel AL algorithm.

a. Training during Initial Dataset Generation and Active Learning

During the initial dataset generation,

the user decides how many points are selected for each ensemble member during each sampling step. For example, the user specifies that 5 structures per member are to be selected. AIMS-PAX then runs the sampling algorithm, and once 5 points were picked from the trajectory for each member (and labels are computed), the models are trained for a user specified number of epochs, namely intermediate_epochs. This process running sampling, picking is repeated multiple times i.e. points, labeling and training. The rationale behind this is, that if the user wants to have a specific accuracy of the models before running AL, AIMS-PAX makes sure that not too many

aims-PAX: Parallel Active eXploration for the automated construction of Machine Learning Force Fields

12

structures are sampled and DFT calculations are performed.

During the active learning procedure each trajectory is as- sociated with a state. Technically speaking, a loop is per- formed over all trajectories and depending on their state, dif- ferent actions are performed. At the beginning of AL, all tra- jectories have the state running, which means the sampling algorithm is performed. Once a point is picked for labeling, the state of this specific trajectory is set to waiting until the DFT calculation is done and the results were received. This then changes the state to training and the user specified number of training epochs are performed, these are called intermediate_epochs_al. Afterwards, AIMS-PAX con- tinues the loop over the trajectories. Only once a maximum, user-specified number of epochs, max_epochs_worker, is reached, the trajectory’s state swtiches back to running. This is done to enable other trajectories to continue sampling, po- tentially triggering new DFT calculations, which can then run while the models are trained. In addition, this means that the trajectories are always propagated with continuously updated model parameters.

b. Resetting the Optimizer during Active Learning

While training during AL, the weights are not reset when a new point is added. We have seen that repeatedly us- ing the updated weights can result in the model being stuck in a minimum. We found it advantageous in this case to reset the optimizer state if the model is not improved af- ter max_epochs_worker epochs (see Section A 3 a). This deletes the history of the adaptive optimizer (e.g. Adam or AMSGrad), resulting in a larger learning rate which helps the model to leave the local minimum.

c. Handling of new data points during Active Learning

Once a new point is selected and labeled during AL, it is either added in the training or validation set. To which dataset the new point contributes depends on a user specified ratio, that is kept consistent, e.g. 0.5, which means points are added to both sets alternately. In contrast to the IDG, both datasets are shared across models (except for the initial starting points that are present before the AL).

d. Handling of Failed SCF Convergence

During the IDG, if a DFT computation does not converge the geometry is discarded from the dataset. The procedure then just continues until any stopping criterion is met. This is especially relevant when using the GP model. However, we have not noticed any instances where SCF convergence could not be achieved for a geometry generated by the GP model for our systems. In the case of AL, points where the SCF cycles do not con- verge are also discarded. On the trajectory where this is the

case, a checkpoint geometry is loaded. This checkpoint is up- dated each time a selected structure is successfully labeled us- ing DFT and the data is added to the training set. This ensures, that if the checkpoint is loaded, the MD continues from a ge- ometry that is known to the MLFF.

e. Operational Differences: Serial vs. Parallel Version

While the overall AL workflow of AIMS-PAX is the same for its serial and parallel versions, there are slight differences that we want to point out. In the case of the serial procedure, the sampling and training of the ML models is halted if DFT calculations are performed. Afterwards, the model parame- ters are updated on the new data. Practically this results in all trajectories being propagated with the new information. For the parallel version, other trajectories can be propagated dur- ing the DFT calculation, meaning that sampling is done with- out the information of the current DFT calculation. While it can mean that potentially redundant points are sampled, the computational benefit, and thus possibility of scaling up the workflow, outweighs this inefficiency.

REFERENCES

1O. T. Unke, S. Chmiela, H. E. Sauceda, M. Gastegger, I. Poltavsky, K. T. Schütt, A. Tkatchenko, and K.-R. Müller, “Machine learning force fields,” Chemical Reviews 121, 10142–10186 (2021), pMID: 33705118, https://doi.org/10.1021/acs.chemrev.0c01111. 2W. J. Baldwin, X. Liang, J. Klarbring, M. Dubajic, D. Dell’Angelo, C. Sutton, C. Caddeo, S. D. Stranks, A. Mattoni, A. Walsh, and G. Csányi, “Dynamic local structure in caesium lead iodide: Spa- tial correlation and transient domains,” Small 20, 2303565 (2024), https://onlinelibrary.wiley.com/doi/pdf/10.1002/smll.202303565. 3O. T. Unke, M. Stöhr, S. Ganscha, T. Unterthiner, H. Maennel, S. Kashubin, D. Ahlin, M. Gastegger, L. M. Sandonas, J. T. Berry- and K.-R. Müller, “Biomolecular dynamics man, A. Tkatchenko, with machine-learned quantum-mechanical force fields trained on di- verse chemical fragments,” Science Advances 10, eadn4397 (2024), https://www.science.org/doi/pdf/10.1126/sciadv.adn4397. 4A. Kabylda, J. T. Frank, S. S. Dou, A. Khabibrakhmanov, L. M. Sandonas, O. T. Unke, S. Chmiela, K.-R. Müller, and A. Tkatchenko, “Molecular simulations with a pretrained neural network and universal pairwise force fields,” (2025), 10.26434/chemrxiv-2024-bdfr0-v2. 5J. S. Smith, O. Isayev, and A. E. Roitberg, “Ani-1: an extensible neu- ral network potential with dft accuracy at force field computational cost,” Chemical Science 8, 3192–3203 (2017). 6D. P. Kovács, J. H. Moore, N. J. Browning, I. Batatia, J. T. Horton, Y. Pu, V. Kapil, W. C. Witt, I.-B. Magd˘au, D. J. Cole, and G. Csányi, “Mace-off: Short-range transferable machine learning force fields for or- ganic molecules,” Journal of the American Chemical Society (2025), 10.1021/jacs.4c07099. 7C. Chen, W. Ye, Y. Zuo, C. Zheng, and S. P. Ong, “Graph networks as a universal machine learning framework for molecules and crystals,” Chem- istry of Materials 31, 3564–3572 (2019). 8B. Deng, P. Zhong, K. Jun, J. Riebesell, K. Han, C. J. Bartel, and G. Ceder, “Chgnet as a pretrained universal neural network potential for charge- informed atomistic modelling,” Nature Machine Intelligence 5, 1031–1041 (2023). 9K. Choudhary and B. DeCost, “Atomistic line graph neural network for improved materials property predictions,” npj Computational Materials 7 (2021), 10.1038/s41524-021-00650-1.

aims-PAX: Parallel Active eXploration for the automated construction of Machine Learning Force Fields

13

10A. Merchant, S. Batzner, S. S. Schoenholz, M. Aykol, G. Cheon, and E. D. Cubuk, “Scaling deep learning for materials discovery,” Nature 624, 80–85 (2023).

11J. Gasteiger, F. Becker, and S. Günnemann, “Gemnet: Universal direc-

tional graph neural networks for molecules,” (2021).

12J. Hoja, L. Medrano Sandonas, B. G. Ernst, A. Vazquez-Mayagoitia, R. A. DiStasio, and A. Tkatchenko, “Qm7-x, a comprehensive dataset of quantum-mechanical properties spanning the chemical space of small organic molecules,” Scientific Data 8 (2021), 10.1038/s41597-021-00812- 2.

13P. Eastman, P. K. Behara, D. L. Dotson, R. Galvelis, J. E. Herr, J. T. Hor- ton, Y. Mao, J. D. Chodera, B. P. Pritchard, Y. Wang, G. De Fabritiis, and T. E. Markland, “Spice, a dataset of drug-like molecules and pep- tides for training machine learning potentials,” Scientific Data 10 (2023), 10.1038/s41597-022-01882-6.

14B. Deng, P. Zhong, K. Jun, J. Riebesell, K. Han, C. J. Bartel,

and G. Ceder, “Chgnet: Pretrained universal neural network potential for charge-informed atomistic modeling,” (2023).

15L. Barroso-Luque, M. Shuaibi, X. Fu, B. M. Wood, M. Dzamba, M. Gao, A. Rizvi, C. L. Zitnick, and Z. W. Ulissi, “Open materials 2024 (omat24) inorganic materials dataset and models,” (2024).

16D. S. Levine, M. Shuaibi, E. W. C. Spotte-Smith, M. G. Taylor, M. R. Hasyim, K. Michel, I. Batatia, G. Csányi, M. Dzamba, P. Eastman, N. C. Frey, X. Fu, V. Gharakhanyan, A. S. Krishnapriyan, J. A. Rackers, S. Raja, A. Rizvi, A. S. Rosen, Z. Ulissi, S. Vargas, C. L. Zitnick, S. M. Blau, and B. M. Wood, “The open molecules 2025 (omol25) dataset, evaluations, and models,” (2025).

17S. Ganscha, O. T. Unke, D. Ahlin, H. Maennel, S. Kashubin, and K.-R. Müller, “The qcml dataset, quantum chemistry reference data from 33.5m dft and 14.7b semi-empirical calculations,” Scientific Data 12 (2025), 10.1038/s41597-025-04720-7.

18P. Novelli, L. Bonati, P. J. Buigues, G. Meanti, L. Rosasco, M. Parrinello, and M. Pontil, “Fine-tuning foundation models for molecular dynamics: A data-efficient approach with random features,” in Proceedings of the NeurIPS 2024 Workshop on Machine Learning and the Physical Sciences (2024).

19H. Kaur, F. Della Pia, I. Batatia, X. R. Advincula, B. X. Shi, J. Lan, G. Csányi, A. Michaelides, and V. Kapil, “Data-efficient fine-tuning of foundational models for first-principles quality sublimation enthalpies,” Faraday Discussions 256, 120–138 (2025).

20M. Radova, W. G. Stark, C. S. Allen, R. J. Maurer, and A. P. Bartók, “Fine-tuning foundation models of materials interatomic potentials with frozen transfer learning,” (2025).

21B. Settles, “Active Learning Literature Survey,” Technical Report (Uni- versity of Wisconsin-Madison Department of Computer Sciences, 2009) accepted: 2012-03-15T17:23:56Z.

22P. Ren, Y. Xiao, X. Chang, P.-Y. Huang, Z. Li, B. B. Gupta, X. Chen, and X. Wang, “A survey of deep active learning,” ACM Comput. Surv. 54 (2021), 10.1145/3472291.

23T. A. Young, T. Johnston-Wood, V. L. Deringer, and F. Duarte, “A transfer- able active-learning strategy for reactive molecular force fields,” Chemical Science 12, 10944–10955 (2021).

24W. G. Stark, J. Westermayr, O. A. Douglas-Gallardo, J. Gardner, S. Haber- shon, and R. J. Maurer, “Machine learning interatomic potentials for reac- tive hydrogen dynamics at metal surfaces based on iterative refinement of reaction probabilities,” The Journal of Physical Chemistry C 127, 24168– 24182 (2023), https://doi.org/10.1021/acs.jpcc.3c06648.

25S. Mohanty, J. Stevenson, A. R. Browning, et al., “Development of scal- able and generalizable machine learned force field for polymers,” Scien- tific Reports 13, 17251 (2023).

26K. Kang, T. A. R. Purcell, C. Carbogno, and M. Scheffler, “Accelerat- ing the training and improving the reliability of machine-learned inter- atomic potentials for strongly anharmonic materials through active learn- ing,” (2024).

27L. Zhang, D.-Y. Lin, H. Wang, R. Car, and W. E, “Active learning of uni- formly accurate interatomic potentials for materials simulation,” Physical Review Materials 3 (2019), 10.1103/physrevmaterials.3.023804.

28J. S. Smith, B. Nebgen, N. Lubbers, O. Isayev, and A. E. Roitberg, “Less is more: Sampling chemical space with active learning,” The Journal of Chemical Physics 148 (2018), 10.1063/1.5023802.

29L. Zhang, G. Csányi, E. van der Giessen, and F. Maresca, “Atomistic fracture in bcc iron revealed by active learning of gaussian approxima- tion potential,” npj Computational Materials 9 (2023), 10.1038/s41524- 023-01174-6.

30S. Shambhawi, G. Csányi, for catalysts

and A. A. Lapkin, “Active learn- en- ing ergy fea- tures,” Chemistry–Methods 1, 444–450 (2021), https://chemistry- europe.onlinelibrary.wiley.com/doi/pdf/10.1002/cmtd.202100035.

predicting using

free catalyst

training on

inexpensive

adsorption

perovskite

strategy

o

31G. Sivaraman, A. N. Krishnamoorthy, M. Baur, C. Holm, M. Stan, G. Csányi, C. Benmore, and A. Vazquez-Mayagoitia, “Machine-learned interatomic potentials by active learning: amorphous and liquid hafnium dioxide,” npj Computational Materials 6 (2020), 10.1038/s41524-020- 00367-7.

32D. Kuryla, G. Csányi, A. C. T. van Duin, and A. Michaelides, “Efficient exploration of reaction pathways using reaction databases and active learn- ing,” The Journal of Chemical Physics 162 (2025), 10.1063/5.0235715. 33L. C. Erhard, J. Rohrer, K. Albe, and V. L. Deringer, “Modelling atomic and nanoscale structure in the silicon–oxygen system through active ma- chine learning,” Nature Communications 15 (2024), 10.1038/s41467-024- 45840-9.

34J. Vandermause, A. Johansson, Y. Miao, J. J. Vlassak, and B. Kozinsky, “Phase discovery with active learning: Application to structural phase tran- sitions in equiatomic niti,” (2024).

35B. R. Duschatko, J. Vandermause, N. Molinari, and B. Kozinsky, “Un- certainty driven active learning of coarse grained free energy models,” npj Computational Materials 10 (2024), 10.1038/s41524-023-01183-5.

36Y. Xie, J. Vandermause, S. Ramakers, N. H. Protik, A. Johansson, and B. Kozinsky, “Uncertainty-aware molecular dynamics from bayesian ac- tive learning for phase transformations and thermal transport in sic,” npj Computational Materials 9 (2023), 10.1038/s41524-023-00988-8.

37J. Vandermause, Y. Xie, J. S. Lim, C. J. Owen,

and B. Kozinsky, “Active learning of reactive bayesian force fields applied to heteroge- neous catalysis dynamics of h/pt,” Nature Communications 13 (2022), 10.1038/s41467-022-32294-0.

38A. Johansson, Y. Xie, C. J. Owen, J. S. Lim, L. Sun, J. Vandermause, and B. Kozinsky, “Micron-scale heterogeneous catalysis with bayesian force fields from first principles and active learning,” (2022).

39Y. Xie, J. Vandermause, L. Sun, A. Cepellotti,

and B. Kozinsky, “Bayesian force fields from active learning for simulation of inter- dimensional transformation of stanene,” npj Computational Materials 7 (2021), 10.1038/s41524-021-00510-y.

40N. Matsumura, Y. Yoshimoto, T. Yamazaki, T. Amano, T. Noda, N. Ebata, T. Kasano, and Y. Sakai, “Generator of neural network potential for molecular dynamics: Constructing robust and accurate potentials with ac- tive learning for nanosecond-scale simulations,” Journal of Chemical The- ory and Computation 21, 3832–3846 (2025).

41B. Gurlek, S. Sharma, P. Lazzaroni, A. Rubio,

and M. Rossi, “Ac- curate machine learning interatomic potentials for polyacene molecular crystals: Application to single molecule host-guest systems,” (2025), arXiv:2504.11224 [cond-mat.mtrl-sci].

42G. Kresse and J. Hafner, “Ab initio molecular dynamics for liquid metals,”

Physical Review B 47, 558–561 (1993).

43G. Kresse and J. Furthmüller, “Efficiency of ab-initio total energy calcula- tions for metals and semiconductors using a plane-wave basis set,” Com- putational Materials Science 6, 15–50 (1996).

44G. Kresse and J. Furthmüller, “Efficient iterative schemes forab initiototal- energy calculations using a plane-wave basis set,” Physical Review B 54, 11169–11186 (1996).

45S. J. Clark, M. D. Segall, C. J. Pickard, P. J. Hasnip, M. I. J. Probert, K. Refson, and M. C. Payne, “First principles methods using castep,” Zeitschrift für Kristallographie - Crystalline Materials 220, 567–570 (2005).

46T. K. Stenczel, Z. El-Machachi, G. Liepuoniute, J. D. Morrow, A. P. Bartók, M. I. J. Probert, G. Csányi, and V. L. Deringer, “Machine-learned acceleration for molecular dynamics in castep,” The Journal of Chemical Physics 159 (2023), 10.1063/5.0155621.

47R. Rüger, M. Franchini, T. Trnka, A. Yakovlev, E. van Lenthe, P. Philipsen, T. van Vuren, B. Klumpers, and T. Soini, “AMS 2025.1,” http://www. scm.com (2025), sCM, Theoretical Chemistry, Vrije Universiteit, Amster-

aims-PAX: Parallel Active eXploration for the automated construction of Machine Learning Force Fields

14

dam, The Netherlands.

48J. Vandermause, S. B. Torrisi, S. Batzner, Y. Xie, L. Sun, A. M. Kol- pak, and B. Kozinsky, “On-the-fly active learning of interpretable bayesian force fields for atomistic rare events,” npj Computational Materials 6 (2020), 10.1038/s41524-020-0283-z.

49Y.-P. Liu, Q.-Y. Fan, F.-Q. Gong, and J. Cheng, “CatFlow: An Automated Workflow for Training Machine Learning Potentials to Compute Free En- ergies in Dynamic Catalysis,” J. Phys. Chem. C 129, 1089–1102 (2025), publisher: American Chemical Society.

50V. Zaverkin, D. Holzmüller, H. Christiansen, F. Errica, F. Alesiani, M. Takamoto, M. Niepert, and J. Kästner, “Uncertainty-biased molecu- lar dynamics for learning uniformly accurate interatomic potentials,” npj Computational Materials 10 (2024), 10.1038/s41524-024-01254-1.

51S. Vandenhaute, M. Cools-Ceuppens, S. DeKeyser, T. Verstraelen, and V. Van Speybroeck, “Machine learning potentials for metal-organic frame- works using an incremental learning approach,” npj Computational Mate- rials 9 (2023), 10.1038/s41524-023-00969-x.

52K. Kang, T. A. R. Purcell, C. Carbogno, and M. Scheffler, “Accelerat- ing the training and improving the reliability of machine-learned inter- atomic potentials for strongly anharmonic materials through active learn- ing,” (2024).

53M. R. Schäfer, N. Segreto, F. Zills, C. Holm,

and J. Käst- ner, “Apax: A flexible and performant the de- velopment of machine-learned interatomic potentials,” Journal of Chemical Information and Modeling 0, null (0), pMID: 40734268, https://doi.org/10.1021/acs.jcim.5c01221.

framework for

54C. Zhou, M. Neubert, Y. Koide, Y. Zhang, V.-Q. Vuong, T. Schlöder, S. Dehnen, and P. Friederich, “Pal – parallel active learning for machine- learned potentials,” (2024).

55J. W. Abbott, C. M. Acosta, A. Akkoush, A. Ambrosetti, V. Atalla, A. Bagrets, J. Behler, D. Berger, B. Bieniek, J. Björk, V. Blum, S. Bohloul, C. L. Box, N. Boyer, D. S. Brambila, G. A. Bramley, K. R. Bryenton, M. Camarasa-Gómez, C. Carbogno, F. Caruso, S. Chutia, M. Ceriotti, G. Csányi, W. Dawson, F. A. Delesma, F. D. Sala, B. Delley, R. A. D. Jr., M. Dragoumi, S. Driessen, M. Dvorak, S. Erker, F. Evers, E. Fabi- ano, M. R. Farrow, F. Fiebig, J. Filser, L. Foppa, L. Gallandi, A. Garcia, R. Gehrke, S. Ghan, L. M. Ghiringhelli, M. Glass, S. Goedecker, D. Golze, M. Gramzow, J. A. Green, A. Grisafi, A. Grüneis, J. Günzl, S. Gutzeit, S. J. Hall, F. Hanke, V. Havu, X. He, J. Hekele, O. Hellman, U. Herath, J. Her- mann, D. Hernangómez-Pérez, O. T. Hofmann, J. Hoja, S. Hollweger, L. Hörmann, B. Hourahine, W. B. How, W. P. Huhn, M. Hülsberg, T. Ja- cob, S. P. Jand, H. Jiang, E. R. Johnson, W. Jürgens, J. M. Kahk, Y. Kanai, K. Kang, P. Karpov, E. Keller, R. Kempt, D. Khan, M. Kick, B. P. Klein, J. Kloppenburg, A. Knoll, F. Knoop, F. Knuth, S. S. Köcher, J. Kockläuner, S. Kokott, T. Körzdörfer, H.-H. Kowalski, P. Kratzer, P. K˚us, R. Laasner, B. Lang, B. Lange, M. F. Langer, A. H. Larsen, H. Lederer, S. Lehtola, M.- O. Lenz-Himmer, M. Leucke, S. Levchenko, A. Lewis, O. A. von Lilien- feld, K. Lion, W. Lipsunen, J. Lischner, Y. Litman, C. Liu, Q.-L. Liu, A. J. Logsdail, M. Lorke, Z. Lou, I. Mandzhieva, A. Marek, J. T. Margraf, R. J. Maurer, T. Melson, F. Merz, J. Meyer, G. S. Michelitsch, T. Mi- zoguchi, E. Moerman, D. Morgan, J. Morgenstein, J. Moussa, A. S. Nair, L. Nemec, H. Oberhofer, A. O. de-la Roza, R. L. Panadés-Barrueta, T. Pat- lolla, M. Pogodaeva, A. Pöppl, A. J. A. Price, T. A. R. Purcell, J. Quan, N. Raimbault, M. Rampp, K. Rasim, R. Redmer, X. Ren, K. Reuter, N. A. Richter, S. Ringe, P. Rinke, S. P. Rittmeyer, H. I. Rivera-Arrieta, M. Ropo, M. Rossi, V. Ruiz, N. Rybin, A. Sanfilippo, M. Scheffler, C. Scheurer, C. Schober, F. Schubert, T. Shen, C. Shepard, H. Shang, K. Shibata, A. Sobolev, R. Song, A. Soon, D. T. Speckhard, P. V. Stishenko, M. Tahir, I. Takahara, J. Tang, Z. Tang, T. Theis, F. Theiss, A. Tkatchenko, M. Todor- ovi´c, G. Trenins, O. T. Unke, Álvaro Vázquez-Mayagoitia, O. van Vuren, D. Waldschmidt, H. Wang, Y. Wang, J. Wieferink, J. Wilhelm, S. Wood- ley, J. Xu, Y. Xu, Y. Yao, Y. Yao, M. Yoon, V. W. zhe Yu, Z. Yuan, M. Zacharias, I. Y. Zhang, M.-Y. Zhang, W. Zhang, R. Zhao, S. Zhao, R. Zhou, Y. Zhou, and T. Zhu, “Roadmap on advancements of the fhi- aims software package,” (2025), arXiv:2505.00125 [cond-mat.mtrl-sci].

56I. Batatia, D. P. Kovacs, G. N. C. Simm, C. Ortner,

and G. Csanyi, “MACE: Higher order equivariant message passing neural networks for fast and accurate force fields,” in Advances in Neural Information Process- ing Systems, edited by A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho (2022).

57I. Batatia, S. Batzner, D. P. Kovács, A. Musaelian, G. N. C. Simm, R. Drautz, C. Ortner, B. Kozinsky, and G. Csányi, “The design space of e(3)-equivariant atom-centred interatomic potentials,” Nature Machine Intelligence 7, 56–67 (2025).

58I. Poltavsky, A. Charkin-Gorbulin, M. Puleva, G. Fonseca, I. Batatia, N. J. Browning, S. Chmiela, M. Cui, J. T. Frank, S. Heinen, B. Huang, S. Käser, A. Kabylda, D. Khan, C. Müller, A. J. A. Price, K. Riedmiller, K. Töpfer, T. W. Ko, M. Meuwly, M. Rupp, G. Csányi, O. A. von Lilienfeld, J. T. Margraf, K.-R. Müller, and A. Tkatchenko, “Crash testing machine learn- ing force fields for molecules, materials, and interfaces: model analysis in the tea challenge 2023,” Chem. Sci. 16, 3720–3737 (2025).

59I. Poltavsky, M. Puleva, A. Charkin-Gorbulin, G. Fonseca, I. Batatia, N. J. Browning, S. Chmiela, M. Cui, J. T. Frank, S. Heinen, B. Huang, S. Käser, A. Kabylda, D. Khan, C. Müller, A. J. A. Price, K. Riedmiller, K. Töpfer, T. W. Ko, M. Meuwly, M. Rupp, G. Csányi, O. Anatole von Lilienfeld, J. T. Margraf, K.-R. Müller, and A. Tkatchenko, “Crash testing machine learning force fields for molecules, materials, and interfaces: molecular dynamics in the tea challenge 2023,” Chem. Sci. 16, 3738–3754 (2025). 60Y. Babuji, A. Woodard, Z. Li, D. S. Katz, B. Clifford, R. Kumar, L. Lacin- ski, R. Chard, J. Wozniak, I. Foster, M. Wilde, and K. Chard, “Parsl: Pervasive parallel programming in python,” in 28th ACM International Symposium on High-Performance Parallel and Distributed Computing (HPDC) (2019).

61I. Batatia, P. Benner, Y. Chiang, A. M. Elena, D. P. Kovács, J. Riebe- sell, X. R. Advincula, M. Asta, M. Avaylon, W. J. Baldwin, F. Berger, N. Bernstein, A. Bhowmik, S. M. Blau, V. C˘arare, J. P. Darby, S. De, F. D. Pia, V. L. Deringer, R. Elijošius, Z. El-Machachi, F. Falcioni, E. Fako, A. C. Ferrari, A. Genreith-Schriever, J. George, R. E. A. Goodall, C. P. Grey, P. Grigorev, S. Han, W. Handley, H. H. Heenen, K. Hermansson, C. Holm, J. Jaafar, S. Hofmann, K. S. Jakob, H. Jung, V. Kapil, A. D. Ka- plan, N. Karimitari, J. R. Kermode, N. Kroupa, J. Kullgren, M. C. Kuner, D. Kuryla, G. Liepuoniute, J. T. Margraf, I.-B. Magd˘au, A. Michaelides, J. H. Moore, A. A. Naik, S. P. Niblett, S. W. Norwood, N. O’Neill, C. Or- tner, K. A. Persson, K. Reuter, A. S. Rosen, L. L. Schaaf, C. Schran, B. X. Shi, E. Sivonxay, T. K. Stenczel, V. Svahn, C. Sutton, T. D. Swin- burne, J. Tilly, C. van der Oord, E. Varga-Umbrich, T. Vegge, M. Vondrák, Y. Wang, W. C. Witt, F. Zills, and G. Csányi, “A foundation model for atomistic materials chemistry,” (2024), arXiv:2401.00096 [physics.chem- ph].

62R. French, “Catastrophic forgetting in connectionist networks,” Trends in

Cognitive Sciences 3, 128–135 (1999).

63M. McCloskey and N. J. Cohen, “Catastrophic interference in connection- ist networks: The sequential learning problem,” in Psychology of Learning and Motivation (Elsevier, 1989) p. 109–165.

64J. L. McClelland, B. L. McNaughton, and R. C. O’Reilly, “Why there are complementary learning systems in the hippocampus and neocortex: In- sights from the successes and failures of connectionist models of learning and memory.” Psychological Review 102, 419–457 (1995).

65R. Ratcliff, “Connectionist models of recognition memory: Constraints imposed by learning and forgetting functions.” Psychological Review 97, 285–308 (1990).

66D. Kumaran, D. Hassabis, and J. L. McClelland, “What learning systems do intelligent agents need? complementary learning systems theory up- dated,” Trends in Cognitive Sciences 20, 512–534 (2016).

67J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hass- abis, C. Clopath, D. Kumaran, and R. Hadsell, “Overcoming catastrophic forgetting in neural networks,” Proceedings of the National Academy of Sciences 114, 3521–3526 (2017).

68R. Aljundi, “Continual learning in neural networks,” (2019). 69G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter, “Continual lifelong learning with neural networks: A review,” Neural Networks 113, 54–71 (2019).

70H. S. Seung, M. Opper, and H. Sompolinsky, “Query by committee,” in Proceedings of the fifth annual workshop on Computational learning the- ory, COLT92 (ACM, 1992) p. 287–294.

71C. Schran, K. Brezina, and O. Marsalek, “Committee neural network po- tentials control generalization errors and enable active learning,” The Jour- nal of Chemical Physics 153 (2020).

aims-PAX: Parallel Active eXploration for the automated construction of Machine Learning Force Fields

15

72N. Artrith and J. Behler, “High-dimensional neural network potentials for metal surfaces: A prototype study for copper,” Phys. Rev. B 85, 045439 (2012).

73M. Kulichenko, K. Barros, N. Lubbers, Y. W. Li, R. Messerly, S. Tre- tiak, J. S. Smith, and B. Nebgen, “Uncertainty-driven dynamics for ac- tive learning of interatomic potentials,” Nature Computational Science 3, 230–239 (2023).

74P. V. Stishenko, T. W. Keal, S. M. Woodley, V. Blum, B. Hourahine, R. J. Maurer, and A. J. Logsdail, “Atomic simulation interface (asi): appli- cation programming interface for electronic structure codes,” Journal of Open Source Software 8, 5186 (2023).

75V. Blum, R. Gehrke, F. Hanke, P. Havu, V. Havu, X. Ren, K. Reuter, and M. Scheffler, “Ab initio molecular simulations with numeric atom- centered orbitals,” Computer Physics Communications 180, 2175–2196 (2009).

76A. Hjorth Larsen, J. Jørgen Mortensen, J. Blomqvist, I. E. Castelli, R. Christensen, M. Dułak, J. Friis, M. N. Groves, B. Hammer, C. Har- gus, E. D. Hermes, P. C. Jennings, P. Bjerre Jensen, J. Kermode, J. R. Kitchin, E. Leonhard Kolsbjerg, J. Kubal, K. Kaasbjerg, S. Lysgaard, J. Bergmann Maronsson, T. Maxson, T. Olsen, L. Pastewka, A. Peterson, C. Rostgaard, J. Schiøtz, O. Schütt, M. Strange, K. S. Thygesen, T. Vegge, L. Vilhelmsen, M. Walter, Z. Zeng, and K. W. Jacobsen, “The atomic sim- ulation environment—a python library for working with atoms,” Journal of Physics: Condensed Matter 29, 273002 (2017).

77A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, “Pytorch: an imperative style, high- performance deep learning library,” in Proceedings of the 33rd Interna- tional Conference on Neural Information Processing Systems (Curran As- sociates Inc., Red Hook, NY, USA, 2019).

78J. P. Perdew, K. Burke, and M. Ernzerhof, “Generalized gradient approxi- mation made simple,” Physical Review Letters 77, 3865–3868 (1996). 79J. Hermann and A. Tkatchenko, “Density functional model for van der waals interactions: Unifying many-body atomic approaches with non- local functionals,” Physical Review Letters 124 (2020), 10.1103/phys- revlett.124.146401.

80E. van Lenthe, J. G. Snijders, and E. J. Baerends, “The zero-order regular approximation for relativistic effects: The effect of spin–orbit coupling in closed shell molecules,” The Journal of Chemical Physics 105, 6505–6516 (1996).

81C. Adamo, M. Cossi, and V. Barone, “An accurate density functional method for the study of magnetic properties: the pbe0 model,” Journal of Molecular Structure: THEOCHEM 493, 145–157 (1999).

82H. J. C. Berendsen, J. P. M. Postma, W. F. van Gunsteren, A. DiNola, and J. R. Haak, “Molecular dynamics with coupling to an external bath,” The Journal of Chemical Physics 81, 3684–3690 (1984).

83Y. Eldar, M. Lindenbaum, M. Porat, and Y. Y. Zeevi, “The farthest point strategy for progressive image sampling,” IEEE Transactions on Image Processing 6, 1305–1315 (1997).

84A. Tkatchenko and M. Scheffler, “Accurate molecular van der waals inter- actions from ground-state electron density and free-atom reference data,” Physical Review Letters 102 (2009), 10.1103/physrevlett.102.073005. 85D. J. Evans and B. L. Holian, “The nose–hoover thermostat,” The Journal

of Chemical Physics 83, 4069–4074 (1985).

86M. Parrinello and A. Rahman, “Polymorphic transitions in single crys- tals: A new molecular dynamics method,” Journal of Applied Physics 52, 7182–7190 (1981).

87S. J. Reddi, S. Kale, and S. Kumar, “On the convergence of adam and beyond,” in 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings (2018).

88S. Lu, L. M. Ghiringhelli, C. Carbogno, J. Wang, and M. Scheffler, “On the uncertainty estimates of equivariant-neural-network-ensembles inter- atomic potentials,” (2023).

89G. Imbalzano, Y. Zhuang, V. Kapil, K. Rossi, E. A. Engel, F. Grasselli, and M. Ceriotti, “Uncertainty estimation for molecular dynamics and sam- pling,” The Journal of chemical physics 154 (2021).

90X. Fu, Z. Wu, W. Wang, T. Xie, S. Keten, R. Gomez-Bombarelli, and T. Jaakkola, “Forces are not enough: Benchmark and critical evaluation for machine learning force fields with molecular simulations,” Transactions on Machine Learning Research (2023), survey Certification.

91D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza- tion,” in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings (2015).

92T. Plé, O. Adjoua, A. Benali, E. Posenitskiy, C. Villot, L. Lagardère, and J.-P. Piquemal, “A foundation model for accurate atomistic simulations in drug design,” (2025), 10.26434/chemrxiv-2025-f1hgn-v3.

93J. T. Frank, O. T. Unke, K.-R. Müller, and S. Chmiela, “A euclidean trans- former for fast and stable machine learned force fields,” Nature Commu- nications 15 (2024), 10.1038/s41467-024-50620-6.

94A. Musaelian, S. Batzner, A. Johansson, L. Sun, C. J. Owen, M. Ko- rnbluth, and B. Kozinsky, “Learning local equivariant representations for large-scale atomistic dynamics,” Nature Communications 14 (2023), 10.1038/s41467-023-36329-y.

95S. Batzner, A. Musaelian, L. Sun, M. Geiger, J. P. Mailoa, M. Kornbluth, N. Molinari, T. E. Smidt, and B. Kozinsky, “E(3)-equivariant graph neu- ral networks for data-efficient and accurate interatomic potentials,” Nature Communications 13 (2022), 10.1038/s41467-022-29939-5.

96C. E. Rasmussen and C. K. I. Williams, Gaussian Processes for Machine

Learning (The MIT Press, 2005).

97L. Zhang, D.-Y. Lin, H. Wang, R. Car, and W. E, “Active learning of uni- formly accurate interatomic potentials for materials simulation,” Physical Review Materials 3 (2019), 10.1103/physrevmaterials.3.023804.

98J. Krumland and C. Cocchi, “Conditions for electronic hybridization between transition-metal dichalcogenide monolayers and physisorbed carbon-conjugated molecules,” Electronic Structure 3, 044003 (2021). 99M. Jacobs, K. Fidanyan, M. Rossi, and C. Cocchi, “Impact of nuclear ef- fects on the ultrafast dynamics of an organic/inorganic mixed-dimensional interface,” Electronic Structure 6, 025006 (2024).

100M. Jacobs, J. Krumland, and C. Cocchi, “Laser-controlled charge trans- fer in a two-dimensional organic/inorganic optical coherent nanojunction,” ACS Applied Nano Materials 5, 5187–5195 (2022).

101S. Fu, J. Ding, H. Lv, S. Liu, K. Zhao, Z. Bai, D. He, R. Wang, J. Zhao, X. Wu, et al., “Many-body hybrid excitons in organic-inorganic van der waals heterostructures,” arXiv preprint arXiv:2301.02523 (2023).

