# Abstract

Hydrogen atom transfer (HAT) reactions are essential in many biological processes, such as radical migration in damaged proteins, but their mechanistic pathways remain incompletely understood. Simulating HAT processes is challenging due to the conflicting requirements of quantum chemical accuracy and biologically relevant time and length scales; thus, neither classical force fields nor DFT-based molecular dynamics simulations are applicable. Machine- learned potentials offer an alternative, with the ability to learn potential energy surfaces (PESs) that capture reactions and transitions with near-quantum accuracy. However, training such models to generalize across diverse HAT configurations—especially at radical positions in pro- teins—requires tailored data generation strategies and careful model selection. In this work, we systematically generate HAT reaction configurations in peptides to build large datasets using semiempirical methods as well as DFT. We benchmark three graph neural network architectures, SchNet, Allegro, and MACE, on their ability to learn HAT potential energy surfaces and indirectly predict reaction barriers through direct energy predictions. MACE consistently outperforms the other models in energy, force, and reaction barrier prediction accuracy, achieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT barrier predictions. This level of accuracy will enable integration of ML potentials into large-scale collagen simulations to compute reaction rates from predicted barriers, advancing the mech- anistic understanding of HAT and radical migration in peptides. We analyze scaling laws, model transferability, and cost-performance trade-offs, and outline strategies for improvement through the combination of ML potentials with transition state search algorithms and ac- tive learning. The presented approach is generalizable to other biomolecular systems, offering a method toward quantum-accurate simulations of chemical reactivity in complex biological environments.

1

# Introduction

Hydrogen atom transfer (HAT) is a fundamental process in radical chemistry where a hydrogen atom is abstracted from a donor molecule according to Equation 1, producing a new radical.

AH + B• −−→ A• + BH

(1)

It is a key step in many chemical reactions; the precise mechanical pathway, however, still needs to be fully understood 1. HAT is an important reaction of protein radicals, which are in turn formed in proteins subjected to oxidative stress, such as hydroxyl or superoxide radicals, to radiolysis, or to mechanical stress. Recently, HAT processes have been identified as a crucial step in mitigating damage caused by

i













mechanoradicals produced through homolytic bond scission in strained collagen fibrils 2. Collagen is the most abundant protein in mammals and performs various functions, including strengthening and supporting skin, tendons, and bone tissue 3. The most characteristic feature of collagen I is the triple alpha helix, which consists of three polypeptide chains formed from amino acids. Not only collagen, but virtually any polymer forms mechanoradicals under mechanical stress, followed by radical migration 4. In this process, each fragment keeps one of the initially bonded electrons. Due to these unpaired valence electrons, the resulting radicals are highly reactive and thus potentially damaging to the surrounding environment. Experiments and simulations of mechanoradical formation in collagen I tendons lead to a proposed reaction path leading from primary radicals via hydrogen atom transfer reactions to experimen- tally observable dihydroxyphenylalanine (DOPA) radicals 2,5,6. Primary mechanoradicals were not experimentally detected, leading Zapp et al. to conclude a rapid radical migration 2. They infer that collagen prevents extensive damage from radicals by keeping radical migration through HAT reactions under control, which occur directly after their generation. To enable large-scale collagen simulations that accurately account for the effects of HAT, reaction barrier heights are required. Calculations of these barrier heights for given configurations need to be fast in order to simulate collagen fibrils effectively. Riedmiller et al. 7 pursued an approach that included machine learning (ML). The authors trained an ML model that directly predicts reaction barrier heights based on initial 3D peptide config- urations, allowing for fast inference of the barriers within the collagen simulation. Their model reached a prediction error of 2.4 ± 2.5 kcal/mol on configurations inferred from classical molecular dynamics trajectories and synthetic peptide systems and an error of 4.6 ± 4.8 kcal/mol on out-of- distribution data. The model’s limited prediction accuracy restricts the accuracy of the subsequent simulation and the understanding of the HAT reactions themselves, thus motivating a search for more accurate methods to predict reaction barriers. Barrier heights depend on precise knowledge of reaction paths, which in turn require an under- standing of the potential energy surface (PES). The PES describes the functional relationship between potential energy and atomic positions. If an accurate PES is known, a molecular system’s equilibrium structures or transition states can be found since these correspond to the PES’s min- ima or saddle points. When an ML model is directly trained on barrier heights, information on the reaction path and topology of the PES is thus not included. In this work, we model the PES of HAT reactions in peptides using ML models, which allow us to indirectly predict reaction barrier heights via direct energy predictions. By learning the full PES of HAT reactions in peptide systems, we can predict more accurate reaction barrier heights. An accurate model of the PES will also allow investigations to go further and to understand reaction dynamics. The trained ML models representing the PES can also be used in optimization and transition state search algorithms since they are differentiable. Traditionally, there were two approaches to calculating the energy and forces for molecular systems, i.e., the PES. Ab initio methods, while accurate, are unfeasible for large system sizes due to their computational costs. Classical force fields, instead, are very fast due to their analytical form. The terms in classical force field functions contain many empirical parameters describing bonded and non-bonded interactions (e.g., electrostatic or van der Waals interactions). However, force fields do not allow bonds to break or form, i.e., no chemical reactions can be simulated. Reactive force fields have been developed to counteract this; however, the accuracy is generally lower than ab initio calculations due to the general empirical approximations 8. Since the PES is a multidimensional function, an analytical expression can also be found by mathe- matical fitting to data with ab initio accuracy. With the formulation of the construction of the PES as a function approximation problem, it becomes clear where machine learning (ML) methods can be used as efficient tools in this context: If the relationship between potential energy, including as- sociated analytical derivatives, and atomic positions of a system is constructed using ML methods, the resulting analytical expression is referred to as a machine-learned (ML) potential. The training data for ML potentials consists of coordinates as well as elements of all atoms of a system and the corresponding energies and often forces. Since the quality of the resulting ML potential is directly dependent on the quality and quantity of the training data, the latter is typically calculated using an accurate but affordable ab initio method, e.g. density functional theory (DFT). Compared to classical force field methods, ML methods offer the advantage that no constraining assumptions about the functional form of the PES or bonds are needed - the chemical behavior, including long-range interactions and chemical reactions, is learned from the reference data alone 9.

ii

ML potentials allow the modeling of the PES of a system with high accuracy and reasonable com- putational costs. Therefore, their field of application is vast and theoretically ranges over any material in any state, from biomolecules to crystalline systems. Due to its many potential use cases, the development of ML potentials is a very active research field. Successful ML potentials are based on several different ML architectures, from neural networks to kernel-based methods to graph neural networks (GNNs), with specific advantages and disadvantages 10. Regardless of the exact model architecture, training data for ML potentials initially consist of (Cartesian) atom coordinates, the element type of the atoms of a system, and the associated energies in context- dependent configurations. Often, information about forces is also part of the training data, as adding them can increase the accuracy of the models and reduce the required training set size 11 since there are 3N forces for N atoms instead of just one energy label 12. The calculation of energies and forces requires an ab initio method to guarantee the accuracy of the learned potential, but it can also become a bottleneck if a large training data set is needed. The PES exhibits symmetries, which the ML model should also reflect. For example, the total energy is invariant if a molecule is translated or rotated, or if two atoms of the same element type exchange. These invariances can either be explicitly satisfied by choosing a representation of the geometry (e.g. inverse distances), by including them in the functional form of the machine learning model (inductive bias), or by learning them (e.g. through data augmentation). Currently, the most popular ML model architecture for ML potentials is the graph neural network (GNN), which utilizes the natural graph structure of molecules 13,14. Since the topology of the molecu- lar structure can be considered as an undirected graph, atoms can be associated with nodes and chemical bonds with edges. At first, atom feature vectors contain properties such as element types and positions 15. Information or ’messages’ are then exchanged between atoms through message- passing layers, and the model iteratively learns feature representations of the individual atoms’ local environments, including information about neighbors and more long-range interactions after several message-passing steps. One of the first GNNs to learn PES was SchNet 16, which is based on invariant convolutions over scalars. The model consists of convolutional interaction blocks in which the initial features are up- dated and the final atom embeddings are learned. The model ensures rotational invariance of the output by constructing only scalar features and operating on (scalar) interatomic distances, rather than Cartesian atom coordinates. While SchNet was successfully employed in various chemical applications, the requirement for a lot of ab initio training data was found to be a bottleneck for larger length scales. More recently, equivariant GNNs gained popularity as ML potential models, outperforming pre- vious invariant architectures and displaying higher data efficiency. Equivariant GNNs can encode more physical information about an atomic system by directly acting on vector quantities while preserving known physical symmetries. More specifically, the models are equivariant with respect to transformations under the 3D Euclidean group (rotation, inversion, and translation). This is relevant for preserving force vectors under rotation of the atomic system. Equivariance is achieved by not only learning scalar node representations but also higher-order geometric tensor features. Examples of equivariant GNNs include NequIP 17, Allegro 18, and MACE 19. NequIP utilizes learned scalar and tensor features, and information is propagated via message-passing over relative position vectors. While achieving state-of-the-art accuracies on several benchmark datasets, computational performance, specifically training and evaluation speed, constitutes a drawback when scaling to larger systems. The main disadvantage in this context is the message-passing step since it con- structs many neighbors for each atom, hindering the parallelizability of the model. To combat this, Allegro 18, based on NequIP, learns strictly local equivariant tensor features be- tween edges and employs no message-passing, resulting in an O(N ) scaling with respect to the number of atoms. The embedding of the local environment only leads to receptive fields with fixed sizes, which does not reduce accuracy on benchmark data sets. MACE 19, on the other hand, employs a higher-order message-passing strategy to reduce computational costs. It explic- itly models higher-order interactions by constructing many-body features from radial and spherical harmonics basis functions based on the multi-atomic cluster expansion framework 20. Equivariant messages from these features are then constructed hierarchically via tensor operations. With this construction method, the authors show that the message-passing can be reduced to two iterations, compared to 4-6 for other equivariant models 17. Evaluations on benchmark datasets show that both Allegro and MACE have high accuracies and good transferability to out-of-distribution data. While benchmark data allows a wide range of comparisons between the latest models, many ap-

iii

Figure 1: Workflow overview: We generated training data for HAT reactions in peptides and trained graph neural networks to learn the corresponding PES. We used direct energy predictions from these models to indirectly predict HAT reaction barriers.

plications of interest, especially in biochemistry, are often more complex and more specific than common benchmark datasets, and it is not immediately clear which architecture is the most suit- able or what kind and how much training data is required. Thus, trade-offs between training times, data requirements, and accuracy are not intuitively apparent. One of the challenges in the context of HAT in peptides is the increased complexity that a reaction entails. In addition to training data on equilibrium configurations, global information on the PES, i.e., the reaction’s intermediate steps and transition states, is also required. Accurately training a model thus requires more data, which at the same time needs to be informative to allow a model to capture the increased complexity. Generating this data constitutes a challenge in itself and re- quires an efficient data generation workflow. This is especially true if not only one specific reaction configuration but, as in our case, various combinations of peptides, radical positions, and reaction paths should be learned. Higher data requirements to learn the PES of a reaction also mean that we need more ab initio calculations for energy and force labels. The chosen model must, therefore, be data-efficient. Otherwise, the number of ab initio calculations represents a bottleneck.

In this work, we trained ML potentials to learn potential energy surfaces of HAT reactions in peptides. We developed a workflow to generate training data of reaction configurations for HAT in peptides. Using this workflow, we generated a dataset of 172,000 data points with energy and forces calculated using the semi-empirical method GFN-xTB 21. Additionally, we generated a dataset comprising 125,365 data points at the DFT/bmk/def2-TZVPD level of theory. We ex- plored the performance of the models SchNet, Allegro, and MACE, estimated a scaling law, and investigated their transferability from small to large systems, both on semi-empirical and DFT data. We used the trained models to indirectly predict HAT reaction barriers through direct energy predictions (see Figure 1). The models trained on the PES directly can capture more com- plexity than previous SOTA direct barrier predictions. Our best MACE model, trained on 65,514 DFT/bmk/def2-TZVPD configurations, achieved an MAE of 1.13 kcal/mol in barrier predictions on out-of-distribution data, compared to a previously reported MAE of 4.6 kcal/mol 7. The signifi- cant increase in accuracy achieved here renders the ML model suitable for use in barrier predictions, for example, in radical migration in damaged proteins, as well as in the more conventional fashion as an ML potential for MD simulations of such systems.

2 Methods

2.1 Data generation

2.1.1 Training data.

In the following, we present the data generation workflow used to create reaction configurations for HAT in peptides. The resulting datasets consist of coordinates and corresponding energies and forces of systems of amino acids and dipeptides. In addition to equilibrium structures, we must aim to cover a diverse and informative conformational space relevant to describing HAT in various chemical environments. To achieve this, we combined normal mode sampling and reaction configuration sampling. The library developed in this work allows us to automatically perform each step of the generation process on many different molecules simultaneously. Each step can also be performed individually, and the framework can, in principle, be adapted to any molecular system.

iv

Figure 2a depicts an overview of the steps of the data generation workflow. Starting from SMILES, we generated non-equilibrium structures from which we constructed HAT reaction configurations. We used RDKit to generate 3D coordinates from SMILES representations 22 corresponding to I molecules we want to include in the final training data. We then optimized the initial coordinates using xTB 21 to get minimum energy structures. The minimum energy structures were the starting point for a conformer search performed on each with CREST 23 (Figure 2a). Since this typically results in numerous structures per molecule, we selected the five lowest-energy conformers and five randomly sampled higher-energy conformers, resulting in C = 10 conformer configurations {Ri c} per molecule i, where i ∈ I and c ∈ C. We applied normal mode sampling (Figure 2b to the chosen conformers in the next step to obtain J physically relevant non-equilibrium structures per molecule {Ri j}, where j ∈ J. This allowed us to sample the PES around minima up to a maximum relative energy. In this step, we distorted the molecules along their normal modes based on methods employed by Rupp et al. 24 and Smith et al. 25,26. We used xTB to calculate normal mode coordinates qi c,m for m eigenmodes of each conformer configuration c per molecule i. The force constants were then used to calculate displacements Ri c,m (Figure 2b), with which the sampled configuration was generated according to Equation 2:

c,m and force constants ki

Ri

j = Ri

c +

Ri

c,mqi

c,m.

(2)

(cid:88)

m

The normal mode sampled geometries Ri j are thus superpositions of perturbed normal mode coor- dinates qi c,m that pass relative bond length and total energy checks. This ensures that no bonds are broken and that the new configuration’s total energy is within a set range. Normal mode sampling is only an estimation working within the harmonic approximation; in the context of generating training data for ML potentials, it is still beneficial since it allows fast sampling of structures that cover physically relevant PES areas. The perturbed molecular coordinates Ri j served as initial structures to build radical systems in the subsequent steps. To create radical systems, we transformed a molecule into a radical by removing a hydrogen atom, creating a radical at position r0 (Figure 2c). We consider two types of radical systems in which reactions occur - intramolecular and intermolecular HAT. For intra-HAT in peptides, we assume that a transfer occurs within the same molecule, while for inter-HAT, we assume that a hydrogen atom at position rH moves between two distinct peptides towards a radical at position r0, thus creating a radical at position r1. We implemented a function g that creates the radical systems by performing a selection and geometry modification in the case of inter-HAT systems. For both system types, the function analyzes given molecules and randomly chooses a hydrogen atom for transfer and a radical position, i.e., the start and end position of the reaction. The selection func- tion performs distance checks to prevent clashes and includes conditions under which hydrogen atoms can be transferred, depending on the molecule type and atom environment. The function generates inter-HAT systems by translating and rotating one randomly chosen molecule and one radical, while the distance between hydrogen atom at rH and radical at r0 is randomly drawn from a χ2-distribution with a maximum distance of 4 Å. The two configurations were arranged so that no clashes occurred, and no other hydrogen atom was closer to the radical position than the atom designated for transfer. This step results in the creation of radical system configurations {Ra} for both inter- and intra-HAT. In the last step, we created reaction configurations from the generated radical systems. A function f modifies the geometry of a system by moving the designated hy- drogen atom between the start and end positions. This displacement function takes a generated radical system Rinter, intra and translates the hydrogen atom rH to a point on a sphere with a randomly sampled radius around the center of the start and endpoints of the reaction (Figure 2d). To avoid outlier geometries, we checked again for clashes and energy outliers, i.e., we defined a maximum difference between the minimum energy and the energy of the generated configuration. This scheme creates a diverse set of reaction systems {Rr} with corresponding energies and forces {Er, Fr} that differ in geometry, transfer type (intra or inter), type of peptide, as well as the hydrogen and radical positions.

a

2.1.2 Evaluation data.

To evaluate trained models, we used data generated by the workflow described in Section 2.1.1. We used the generated configurations to directly evaluate the trained models’ ability to predict energy and forces. Due to the randomness in the combinations and finite molecule types we considered, the

v

Figure 2: Overview of the training data generation workflow (a). Starting from SMILES represen- tations of amino acids and dipeptides, we generated 3D coordinates using RDKit. After optimizing to get the minimum energy structures, we generated conformers on which we applied normal mode sampling to obtain non-equilibrium structures (b). These configurations serve as input for gen- erating inter- and intra-HAT radical systems (c). Reaction configurations are then sampled by randomly translating the hydrogen atom designated for transfer. Additional evaluation data for the barriers is generated by linear interpolation of the hydrogen atom (d).

generated systems can contain the same amino acids or dipeptides as the training data; however, radical and hydrogen atom positions, as well as distances and spatial arrangements of molecules, vary. Since our goal is to predict reaction barriers indirectly using the direct energy predictions from trained models, we generated additional data from linear interpolation of the hydrogen atom at rH designated for transfer (Figure 2d) similar to what has been done previously in the literature 7. We generated configurations by moving the hydrogen atom on a linear path between a system’s start and end positions with 10 interpolations per system. Thus, the reaction barriers for a system were calculated as the energy difference between the highest energy configuration on the path and the start and end positions, respectively. We applied the linear interpolation scheme to radical systems using the evaluation data, and for additional analysis, to part of the training data.

2.2 Graph neural networks

For all models, we optimized the recommended hyperparameters for our use case while considering training times and computational resources.

2.2.1 SchNet.

We employed the Keras Graph Convolution Neural Networks (KGCNN) 27 implementation of SchNet with a TensorFlow backend, optimizing energy and force predictions. We used six convo- lutional interaction blocks with 128 feature dimensions and set the distance cutoff to 5 Å. Radial basis function expansion was applied to pairwise distances using 25 Gaussian functions with a dis- . The interaction blocks used shifted softplus tance cutoff of 5 Å and scaling parameter of 0.4 1/Å as the activation function with a pooling method of scatter-sum to aggregate atomic contributions. The models were trained using the Adam optimizer with an initial learning rate of 10−3 and loss weights of 1 for energy and 49 for forces. We applied a linear warmup exponential learning rate scheduler, exponentially decreasing the learning rate by 0.995 per epoch after one warmup epoch. We used a batch size of 32 and trained the model for 1000 epochs. We applied an extensive scaler for scaling per-species energies, performing a linear regression to calculate the mean energy per atom type and standard deviation to remove the atomization energy per atom species.

2

vi

2.2.2 Allegro.

We used the Allegro PyTorch implementation as provided by Musaelian et al. 18 and built all mod- els with three layers, a radial cutoff of 5 Å, and a latent space with 64 channels. The radial basis expansion used eight trainable Bessel functions with a polynomial cutoff p = 6 and maximum spherical harmonics order lmax = 2. The two-body latent MLP consists of four hidden layers with dimensions [128, 256, 512, 1024], utilizing SiLU nonlinearities and uniform weight initialization. For the latent MLP, which processes higher-order features, we used three hidden layers with dimen- sions [1024, 1024, 1024], employing SiLU activations and uniform weight initialization. A residual connection was applied in the scalar latent space to facilitate efficient propagation of scalar infor- mation across layers. The final per-edge energy MLP had a single hidden layer of dimension [128], no nonlinearity, and uniform weight initialization. We trained all Allegro models using the Adam optimizer with default parameters β1 = 0.9, β2 = 0.99, and ϵ = 10−8 without weight decay, using a batch size of 5 and a joint per-atom MSE loss function with weights 1.0 and 1.0 for both energy and forces. The initial learning rate of 0.001 was reduced by 0.8 using an on-plateau scheduler based on the validation MAE of the energy with a patience of 50. Early stopping was employed when either the learning rate reached a value of 10−6, the validation loss did not improve for 50 epochs, or 1000 training epochs were reached. Per-species energy scaling was applied to normalize atomic energies during training using a Gaussian process regression to compute the mean energy per atom type and standard deviation.

2.2.3 MACE.

We employed the MACE PyTorch implementation as provided by Batatia et al. 19, building two- layer models with lmax = 2 in the spherical harmonic expansion, 128 feature channels, and cor- relation order N=3, i.e., exchanging four-body messages. We generated radial features using 8 Bessel basis functions with a polynomial envelope with cutoff p = 5 and set the size of the MLP processing these features for all models to [64,64,64] using SiLu activation functions. The readout function performs a linear transformation in the first layer, while the second layer consists of an MLP with a single layer and 16 dimensions. Models were trained using the Adam optimizer with the AMSGrad variant, with standard parameters β1 = 0.9, β2 = 0.99, and ϵ = 10−8. The learning rate was initially set to 0.005, and an on-plateau scheduler was used to decrease it by a factor of 0.8, with a patience of 50 epochs, based on the validation loss. For the validation set and final model evaluations, we used an exponential moving average with a decay factor of 0.99. We used a weighted loss function as described in Batatia et al. 19. Initially, the weights for energy and forces were set to 1 and 10, respectively. After 650 epochs, we initiated the second training stage with a reduced learning rate of 10−3 and a focus on energy loss, with weights set to 1000 for energy and 100 for forces. We trained all MACE models for 1000 epochs and set the batch size to 5. The per-atom energy and standard deviation were calculated using a least-squares regression, which was used to normalize the data during training.

3 Results and Discussion

3.1 Datasets

We generated datasets for training and evaluating three graph neural network (GNN) architec- tures, SchNet, Allegro, and MACE, on their ability to predict potential energy surfaces for hydro- gen atom transfer (HAT) reactions in peptides. These datasets include both individual reaction configurations and linearly interpolated hydrogen transfer pathways to enable indirect estimation of reaction barriers. All data were generated synthetically through a workflow (see Section 2.1) and calculated both at semi-empirical (xTB) and DFT levels of theory. A semi-empirical tight-binding model (xTB) was initially used to generate a large and diverse dataset for model development, hyperparameter optimization, and scaling law analysis. In total, we generated 172,042 reaction configurations, of which 45,724 correspond to linear interpolations between hydrogen donor and acceptor positions (see Section 2.1.2). The scaling law analysis (Section 3.2) enabled us to ap- proximate the training set size required to achieve mean absolute errors (MAEs) below 40 meV (1 kcal/mol) for reaction barrier predictions. Based on this analysis, we selected a subset of the xTB dataset for more accurate density functional theory (DFT) calculations. Energies and forces

vii

were recalculated at the bmk/def2-TZVPD level using Turbomole, resulting in a DFT dataset with 125,365 configurations, including the full set of 45,724 linear interpolations.

3.1.1 xTB datasets.

We used the semi-empirical tight-binding method xTB 21 with an implicit solvent model (ϵ = 10) to compute energies and forces during dataset generation. An epsilon value of 10.0 was chosen to approximate the dielectric environment of collagen (SI Figure 1). xTB dataset: reaction configurations. SMILES representations of all 20 amino acids and 400 dipeptides (with and without capping groups) were used as the starting point. The capping groups—NH – CH3 at the C-terminus and acetyl at the N-terminus—were chosen to mimic the environment of a type I collagen backbone. To sample relevant conformational space, we applied normal mode sampling to the five lowest-energy and five randomly selected conformers of each molecule. Configurations were retained only if their energy remained within 5.0 eV of the equi- librium structure. Radical systems for HAT were created by generating all possible combinations of intra- and intermolecular donor–acceptor pairs, including amino acids and dipeptides with and without capping groups. This resulted in a total of 126,318 unique radical systems. To create intramolecular HAT systems, we equally sampled normal mode sampling configurations across all molecule types(amino acids, dipeptides capped/uncapped). We considered all possible pairs of molecule types for intermolecular HAT systems, i.e., HAT between two amino acids, amino acid and dipeptide, two dipeptides (capped and uncapped), all equally weighted. From each radical sys- tem, we generated one reaction configuration by randomly sampling a hydrogen position rH using the method described in Section 2.1. Our preliminary tests showed that including a larger variety of systems improves model generalization more effectively than including multiple configurations per system. As a result, only one configuration—either a start, end, or intermediate hydrogen position—was retained per system, yielding a dataset of 126,318 single-point reaction configura- tions. System sizes ranged from 15 atoms (e.g., uncapped single amino acids) to approximately 130 atoms (e.g., capped dipeptide–dipeptide pairs). Energy and distance distributions are provided in the SI (see SI Figure 2). For training and evaluation, the xTB dataset was split into subsets while preserving the distribution of system types (intra- vs. inter-HAT and molecule combinations). The maximum training set size used was 112,191. Detailed dataset statistics and splits are provided in Table 1 in the SI. xTB dataset: linear interpolations. To assess the ML models’ ability to reproduce reaction barriers indirectly, we constructed a separate dataset of linearly interpolated hydrogen positions between donor and acceptor atoms, based on radical systems from the xTB dataset. As described in Section 2.1.2, each interpolation consists of 12 configurations (10 intermediate steps plus start and end points). Sampling equally from all system types, we selected 1,861 radical systems from the training data and 2,164 from the test data, resulting in 21,104 and 24,620 configurations, re- spectively. These datasets were not used for training but only for the evaluation of indirect barrier predictions.

3.1.2 DFT datasets.

We recalculated a subset of the xTB data at DFT level using the BMK functional and the def2- TZVPD basis set, implemented in Turbomole 28. As with the xTB calculations, we used an implicit solvent model with ϵ = 10 to approximate the aqueous peptide environment. The choice of dielectric constant was informed by testing the sensitivity of barrier heights to ϵ; see SI Figure 1 for details. DFT Dataset: Reaction Configurations. A total of 79,641 single-point configurations were selected from the xTB reaction configuration dataset and recalculated at the DFT level. For consistency, we retained the same configuration indices and data splits across both theory levels. Distribution plots and statistics are provided in the SI Table 1. DFT Dataset: Linear Interpolations. The entire xTB interpolation dataset (45,724 configu- rations) was recalculated at the DFT level, preserving the same system identities and splits (1,861 training, 2,164 test). As shown in Figure 3, the DFT barriers are consistently higher than those calculated by xTB, indicating that xTB systematically underestimates HAT barrier heights in peptides (see SI Figure 3).

viii

Figure 3: a) Barrier height distributions from linear interpolation test datasets calculated at the xTB and DFT levels. The linear data was only used in test sets, not in training. xTB system- atically underestimates barrier heights relative to DFT. b) Example interpolation from the test set for intermolecular HAT between capped Arginine–Glutamate and Lysine–Proline dipeptides (98 atoms). xTB barrier: ∆Eleft = 2.50 eV, ∆Eright = 2.85 eV; DFT barrier: ∆Eleft = 3.25 eV, ∆Eright = 3.37 eV.

Figure 4: Learning curves of GNNs: a) Test set force MAE vs. training dataset size. b) Test set barrier MAE vs. training dataset size.

3.2 MACE outperforms other graph neural networks

We investigated the three GNN architectures, SchNet, Allegro, and MACE, for predicting energies, forces, and (indirectly) reaction barriers of HAT reactions. Our comparison focuses on learning efficiency (scaling laws), transferability to larger systems, and training costs. assessing their scaling laws, transferability to larger systems, and training efficiency. Note that all models were trained on reaction configurations only, as described in Section 3.1.1 and 3.1.2. Linear interpolation datasets were only used to evaluate the models. All models were trained on an NVIDIA A100 GPU with 40 GB of memory. Learning curves. To assess learning behavior, we trained each model on increasing subsets of both xTB and DFT datasets, using identical configurations, evaluation splits, and model-specific hyperparameters across experiments. Learning curves for energy and force MAEs, and therefore also barrier MAEs, decrease as expected with increasing dataset size (Figure 4, SI Figure 4a). Across all dataset sizes, MACE consistently achieves the lowest errors, followed by Allegro, with SchNet showing the highest MAEs. When comparing models trained on xTB vs. DFT data, the former consistently exhibits lower errors (Figure 4, SI Figure 4b), suggesting that the xTB PESs are inherently easier for the models to learn. For MACE, the learning curves for xTB and DFT do not run parallel. As the dataset size increases, the gap between the two widens, particularly for force and energy errors. This suggests that DFT-level PESs introduce more complexity, likely requiring higher-order interactions or richer model capacity to be fully captured.

Model transferability. To evaluate model generalization beyond the training distribution, we trained each model on 30,661 DFT configurations with fewer than 50 atoms and tested on a dataset of 34,853 configurations with more than 50 atoms. Performance was also measured on a 3,411- configuration test set of small systems for comparison. All models exhibit limited transferability to larger systems in terms of energy and barrier MAEs (Table 1). Force MAEs, however, remain

ix

Figure 5: MACE is transferrable to different system sizes: Force MAEs and per-atom energy MAEs vs. atom count. The per-atom energy MAE initially increases, then decreases with increasing system size.

stable or even improve with increasing system size (Figure 5b). MACE again outperforms both Allegro and SchNet across all metrics. A deeper analysis of MACE on varying system sizes shows that energy MAEs increase with system size, while force MAEs decrease, due to the additive nature of energy prediction errors and stable local force accuracy. For intermediate-sized systems (60–70 atoms), we observe a slight peak in per-atom energy MAEs, followed by a decrease for even larger systems. No clear trend is visible for barrier estimations based on the energy predictions (SI Figure 5).

Table 1: Model performance on small (≤ 50 atoms) and large (>50 atoms) DFT test systems. All models were trained on 30,661 small configurations.

Model

SchNet (≤50 atoms) Allegro (≤50 atoms) MACE (≤50 atoms)

SchNet (>50 atoms) Allegro (>50 atoms) MACE (>50 atoms)

Energy MAE Force MAE Barrier MAE (meV/Å)

(meV)

(meV)

100 60 50

234 120 100

74 45 33

71 44 31

100 58 47

146 94 66

Training efficiency and resource trade-offs. Training times for Allegro and MACE are sub- stantially longer than for SchNet across all dataset sizes (Figure 6a). MACE requires up to 20 times more GPU hours than SchNet, but achieves comparable or better force accuracy with only half the data (Figure 6b), highlighting a trade-off between data efficiency and computational cost.

3.3 Final DFT model training

To obtain final models trained on all available data, we trained SchNet, Allegro, and MACE on 65,514 DFT configurations and tested on 6,836 unseen configurations. Reaction barriers were derived from direct energy predictions for 2,164 HAT test systems, comprising 24,620 single- point evaluations. MACE achieves the best performance, with the lowest energy (68 meV), force (28 meV/Å), and barrier (49 meV) MAEs (Table 2). Despite higher errors in energy predictions, all models exhibit lower errors in barrier predictions, likely due to systematic error cancellation when models systematically over-/underestimated energies. In some cases, predicted energy profiles closely match DFT reference values (Figure 7a), while in others, consistent over- or underestimation across the pathway leads to accurate relative energies and barriers (Figure 7b).

x

Figure 6: a) Training time vs. dataset size. Training times increase with dataset size. Allegro and MACE need substantially longer training times than SchNet. b) GPU hours vs. test force MAE. MACE and Allegro are more data-efficient but require significantly more compute.

Figure 7: Barrier predictions with MACE: a) Inter HAT system comprising aspartate and alanine with capping groups (45 atoms). Very good agreement between predicted and true single point energies results in low barrier errors |∆Elef t| = 1.5meV, |∆Eright| = 0.82meV b) Selected inter HAT system comprising a lysine-asparagine dipeptide and histidine with capping groups (77 atoms) with high energy and low barrier errors. Error cancellation yields accurate barrier estimates despite offset energy predictions. |∆Elef t| = 2.3meV, |∆Eright| = 41meV .

xi

Table 2: Test error of final models trained on 65,514 DFT configurations and tested on 6,836 unseen configurations and 2,164 barrier evaluations.

Energy MAE Force MAE Barrier MAE (meV/Å)

(meV)

(meV)

97 79 68

58 36 28

96 60 49

Model

SchNet Allegro MACE

3.4 Discussion

We developed a pipeline for generating xTB and DFT-labeled configurations of HAT reactions in peptides and used it to investigate three ML potential architectures: SchNet, Allegro, and MACE. The xTB data served us in our initial tests to estimate sufficiently large training datasets and allowed us to compare learning difficulties between a semi-empirical PES and DFT PES. MACE consistently outperforms the other models regarding energy, force, and barrier MAEs but is also the most computationally demanding. Allegro achieves slightly lower accuracy and has comparable training costs. SchNet trains quickly but suffers from higher prediction errors, especially when the dataset size as well as the budget for training compute is large. Models trained on xTB data achieve lower errors than their DFT-trained counterparts, suggesting that xTB PESs are inherently easier to learn, at least for the HAT/peptide systems investigated here. The approximate nature of the xTB PES might reduce the complexity of the PES compared to DFT. DFT-trained models may better reflect physical reality despite higher energy and force errors. As dataset sizes increase, this difference becomes more pronounced, particularly for MACE, indicating that DFT PESs likely require more complex models or additional model capacity (e.g. higher-body terms). Though hyperparameter searches were conducted for both data types, they were limited to small datasets due to training costs. Improved performance on DFT data might be achievable with more extensive hyperparameter tuning on larger datasets. Energy prediction transferability to larger systems is challenging for all model architectures we investigated, likely because the prediction of global and extensive properties such as total energy suffers from additive errors. The relatively stable or even improved force MAEs suggest that models generalize well locally, and that force predictions benefit from larger local environments or averaging effects in bigger systems. However, very large systems may introduce long-range interactions that are not present during training, further complicating energy and force predictions. Since SchNet, Allegro, and MACE rely on local atomic environments, transferability of energies depends on the presence and diversity of long-range effects in the training data. For small datasets (<10k configurations), SchNet offers a practical trade-off between training time and accuracy. In some cases, if a limited amount of resources is available, it might be more efficient to train more data on SchNet for fewer GPU hours (Figure 6b). For larger datasets (>30k configurations), MACE becomes more advantageous due to its data efficiency, despite longer training times. Allegro falls in between in terms of both cost and performance. All models were trained on a single GPU for consistency, but MACE in particular supports parallel training, which could significantly reduce training time. All models achieve more accurate barrier predictions than direct energy predictions, likely due to error cancellation within reaction pathways. Overall, our results show that machine-learned potentials can accurately predict DFT-level reaction barriers from direct energy predictions, with MACE providing the most reliable and generalizable performance across the tasks considered. In our tests, we used linear interpolations for barrier estimations, which need to be refined via optimization and transition state searches to get more accurate barriers. However, our scheme of indirectly predicting the reaction barriers should work equally well when using refined reaction configurations. More accurate barriers could also be obtained via transition state searches using the trained models directly, since they provide a cheaper way to get Hessians via auto-differentiation. As MACE provides differentiable energy landscapes, it could also be integrated into such optimization schemes. To do so, we would need further tests to ensure accurate Hessian predictions, but initial investigations already suggest this works well 29. Our pretrained models, combined with transition state search algorithms, are also very well suited to be used in an active learning approach to retrain models on relevant PES areas for HAT reactions.

xii

4 Conclusion

In this work, we developed a workflow for generating datasets and training machine learning poten- tials for HAT reactions in peptides, particularly within the biologically relevant context of collagen mechanics. A key motivation for this work was the need to predict reaction rates for large-scale col- lagen simulations, where quantum-mechanical accuracy is computationally prohibitive. By training ML potentials to predict HAT barriers, we enable their integration into kinetic models to estimate reaction rates across collagen’s hierarchical structure. We trained and assessed the performance of three GNN architectures, SchNet, Allegro, and MACE, on both semi-empirical (xTB) and DFT- level potential energy surfaces. We demonstrated that MACE consistently outperforms SchNet and Allegro in energy, force, and reaction barrier prediction accuracy, albeit at a higher computational cost. Our best MACE model achieved a mean absolute error of 1.13 kcal/mol in indirectly predicting DFT-calculated HAT reac- tion barriers, substantially improving upon previous machine learning approaches. This accuracy is critical for reliable reaction rate predictions, as errors in barriers propagate exponentially into rate constants. The trained MACE model is suitable to be used as an emulator, for example, in kinetic Monte Carlo schemes to invoke reactions within a protein. While the training data more closely represents the collagen composition than the composition of other proteins, the model is likely transferable also to other proteins and can be applied to model HAT therein. Despite increased DFT energy errors, we also showed that ML potentials can yield accurate barrier predictions due to systematic error cancellation. Our approach leverages direct energy predictions to model complex PESs and estimate reaction barriers without the need for explicit transition-state data, offering a generalizable alternative to direct barrier prediction schemes. Our analysis of scaling laws, trans- ferability, and training costs highlights the importance of balancing model complexity, dataset size, and computational resources. While xTB PESs are easier to learn and allow fast prototyping, they may limit generalization to high-accuracy regimes. In contrast, DFT-trained models better reflect physical reality and are essential for robust and transferable predictions. The trained models show good transferability of force predictions across system sizes, though total energy errors grow with system size, likely due to a lack of out-of-distribution generalization to larger systems and missing long-range interactions. To address these challenges, future work could explore hybrid training strategies such as xTB pre- training followed by DFT fine-tuning. Transition-state optimization with ML-predicted Hessians could further refine barrier predictions. An active learning strategy combining pretrained models with automated transition state searches may help systematically improve accuracy and broaden the configurational diversity of training data 30. The presented workflow is not limited to HAT in collagen and can be translated to other reactive processes in biomolecular systems. As ML potentials continue to mature, they offer a path to- ward simulating complex chemical reactivity in biologically and chemically realistic environments, bridging the gap between quantum accuracy and large-scale dynamics.

Data availability

The training data, trained models, as well as the code to train all machine learning models and the scripts to reproduce the results of this paper, can be found on https://github.com/aimat-lab/ hat_pes_learning (v1.0) and on Zenodo (https://doi.org/10.5281/zenodo.16572631).

Author contributions

All authors contributed to the idea and the preparation of the manuscript. M.N. implemented the methods and conducted the computational experiments.

Conflicts of interest

There are no conflicts of interest to declare.

Acknowledgements

The present contribution is supported by the Helmholtz Association under the joint research school “HIDSS4Health – Helmholtz Information and Data Science School for Health”.

xiii

We acknowledge funding from the Klaus Tschira Stiftung gGmbH (SIMPLAIX Project 1, to P.F. and F.G.) and the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 101002812) (to F.G.). This work was performed on the HoreKa supercomputer funded by the Ministry of Science, Research and the Arts Baden-Württemberg and by the Federal Ministry of Education and Research. The authors acknowledge support by the state of Baden-Württemberg through bwHPC.

xiv

References

[1] L. Wang and J. Xiao, Topics in current chemistry (Cham), 2016, 374, 17.

[2] C. Zapp, A. Obarska-Kosinska, B. Rennekamp, M. Kurth, D. M. Hudson, D. Mercadante, U. Barayeu, T. P. Dick, V. Denysenkov, T. Prisner, M. Bennati, C. Daday, R. Kappl and F. Gräter, Nature communications, 2020, 11, 2315.

[3] S. Ricard-Blum, Cold Spring Harbor perspectives in biology, 2011, 3, a004978.

[4] M. M. Caruso, D. A. Davis, Q. Shen, S. A. Odom, N. R. Sottos, S. R. White and J. S. Moore,

Chemical reviews, 2009, 109, 5755–5798.

[5] B. Rennekamp, F. Kutzki, A. Obarska-Kosinska, C. Zapp and F. Gräter, Journal of Chemical

Theory and Computation, 2020, 16, 553–563.

[6] B. Rennekamp, C. Karfusehr, M. Kurth, A. Ünal, D. Monego, K. Riedmiller, G. Gryn’ova,

D. M. Hudson and F. Gräter, Nature Communications, 2023, 14, 2075.

[7] K. Riedmiller, P. Reiser, E. Bobkova, K. Maltsev, G. Gryn’ova, P. Friederich and F. Gräter,

Chemical science, 2024, 15, 2518–2527.

[8] F. Vitalini, A. S. J. S. Mey, F. Noé and B. G. Keller, The Journal of Chemical Physics, 2015,

142, 084101.

[9] O. T. Unke, S. Chmiela, H. E. Sauceda, M. Gastegger, I. Poltavsky, K. T. Schütt,

A. Tkatchenko and K.-R. Müller, Chemical reviews, 2021, 121, 10142–10186.

[10] P. Friederich, F. Häse, J. Proppe and A. Aspuru-Guzik, Nature Materials, 2021, 20, 750–761.

[11] K. Yao, J. E. Herr, D. W. Toth, R. Mckintyre and J. Parkhill, Chemical science, 2018, 9,

2261–2269.

[12] F. Noé, A. Tkatchenko, K.-R. Müller and C. Clementi, Annual review of physical chemistry,

2020, 71, 361–390.

[13] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals and G. E. Dahl, Proceedings of the 34th

International Conference on Machine Learning - Volume 70, 2017, p. 1263–1272.

[14] P. Reiser, M. Neubert, A. Eberhard, L. Torresi, C. Zhou, C. Shao, H. Metni, C. van Hoesel, H. Schopmans, T. Sommer and P. Friederich, Communications Materials, 2022, 3, 1–18.

[15] J. Klicpera, J. Groß and S. Günnemann, Directional Message Passing for Molecular Graphs,

2020.

[16] K. T. Schütt, H. E. Sauceda, P.-J. Kindermans, A. Tkatchenko and K.-R. Müller, The Journal

of Chemical Physics, 2018, 148, 241722.

[17] S. Batzner, A. Musaelian, L. Sun, M. Geiger, J. P. Mailoa, M. Kornbluth, N. Molinari, T. E.

Smidt and B. Kozinsky, Nature Communications, 2022, 13, 2453.

[18] A. Musaelian, S. Batzner, A. Johansson, L. Sun, C. J. Owen, M. Kornbluth and B. Kozinsky,

Nature Communications, 2023, 14, 579.

[19] I. Batatia, D. P. Kovacs, G. N. C. Simm, C. Ortner and G. Csanyi, Advances in Neural

Information Processing Systems, 2022.

[20] I. Batatia, S. Batzner, D. P. Kovács, A. Musaelian, G. N. C. Simm, R. Drautz, C. Ortner, B. Kozinsky and G. Csányi, The Design Space of E(3)-Equivariant Atom-Centered Interatomic Potentials, 2022.

[21] C. Bannwarth, E. Caldeweyher, S. Ehlert, A. Hansen, P. Pracht, J. Seibert, S. Spicher and

S. Grimme, WIREs Comput. Mol. Sci., 2020, 11, e01493.

xv

[22] G. Landrum, P. Tosco, B. Kelley, R. Rodriguez, D. Cosgrove, R. Vianello, sriniker, P. Gedeck, G. Jones, NadineSchneider, E. Kawashima, D. Nealschneider, A. Dalke, M. Swain, B. Cole, S. Turk, A. Savelev, A. Vaucher, M. Wójcikowski, I. Take, V. F. Scalfani, R. Walker, K. Uji- hara, D. Probst, tadhurst cdd, g. godin, A. Pahl, J. Lehtivarjo, F. Bérenger and strets123, rdkit/rdkit: 2024_03_6 (Q1 2024) Release, 2024, https://zenodo.org/records/13469390.

[23] P. Pracht, F. Bohle and S. Grimme, Physical Chemistry Chemical Physics, 2020, 22, 7169–

7192.

[24] M. Rupp, R. Ramakrishnan and O. A. von Lilienfeld, The Journal of Physical Chemistry

Letters, 2015, 6, 3309–3313.

[25] J. S. Smith, O. Isayev and A. E. Roitberg, Scientific data, 2017, 4, 170193.

[26] J. S. Smith, O. Isayev and A. E. Roitberg, Chemical Science, 2017, 8, 3192–3203.

[27] P. Reiser, A. Eberhard and P. Friederich, Software Impacts, 2021, 100095.

[28] R. Ahlrichs, M. Bär, M. Häser, H. Horn and C. Kölmel, Chemical Physics Letters, 1989, 162,

165–169.

[29] E. C.-Y. Yuan, A. Kumar, X. Guan, E. D. Hermes, A. S. Rosen, J. Zádor, T. Head-Gordon

and S. M. Blau, Nature Communications, 2024, 15, 8865.

[30] C. Zhou, M. Neubert, Y. Koide, Y. Zhang, V.-Q. Vuong, T. Schlöder, S. Dehnen and

P. Friederich, Digital Discovery, 2025, 4, 1901–1911.

xvi

Supporting Information

SI 1 Datasets

Table SI 1: Summary of dataset composition used, listing the number of molecular configurations available at the xTB and DFT levels of theory. The datasets are divided into training, evaluation, and test sets for both single molecular systems and linear interpolation tasks.

Dataset type

xTB

DFT

Total Single Systems Training Single Systems Evaluation Single Systems Test

172,042 112,191 7,291 6,836

Linear Interpolation Evaluation Linear Interpolation Test

24,620 21,104

125,365 65,514 7,291 6,836

24,620 21,104

Figure SI 1: Dielectric constant tests for implicit solvent calculations. The dielectric constant (ϵ) used in xTB and DFT calculations was selected based on convergence behaviour of reaction barrier heights and considerations of typical protein environments. a) Example energy profile showing that the barrier height stabilized for ϵ > 10.0. b) Barrier height as a function of ϵ, illustrating convergence beyond ϵ = 10.0.

xvii

Figure SI 2: DFT and xTB dataset statistics. Potential energy distributions of the configurations of all a) DFT and b) xTB data. Distribution of c) the number of atoms per configuration and d) the Hydrogen atom transferred - radical distances within the DFT dataset.

Figure SI 3: Comparison between barrier heights (n = 4,025) calculated using xTB vs. DFT for all configurations of the linear interpolation dataset. a) Violin plot: xTB underestimates both left and right HAT reaction barriers. b) xTB underestimates barrier heights for most systems.

xviii

SI 2 Comparative analysis of GNNs

Figure SI 4: Scaling behavior of GNNs: a) Test set energy MAE vs. training dataset size. b) DFT-trained force MAE is higher than xTB-trained force MAE for all trained models.

Figure SI 5: Transferability of MACE to different system sizes: Energy MAEs and per-atom energy MAEs vs. atom count. Energy MAE increases with system size, hinting at additive errors.

SI 3 Final model performance

Table SI 2: Test error of models trained on 65,514 xTB configurations and tested on 6,836 unseen configurations and 2,164 barrier evaluations.

Model Energy MAE (meV) Force MAE (meV/Å) Barrier MAE (meV)

SchNet Allegro MACE

78 60 42

43 30 18

78 58 39

xix