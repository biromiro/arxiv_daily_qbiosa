# Abstract

1

# Introduction

We present GridFF, an efficient method for sim- ulating molecules on rigid substrates, derived from techniques used in protein-ligand dock- ing in biochemistry. By projecting molecule- substrate interactions onto precomputed spa- tial grids with tricubic B-spline interpolation, GridFF reduces the computational cost by orders of magnitude compared to traditional pairwise atomistic models, without compro- mising the accuracy of forces or trajectories. The CPU implementation of GridFF in the open-source FireCore package provides a 100- 1000× speedup over all-atom simulations us- ing LAMMPS, while the GPU implementation – running thousands of system replicas in paral- lel – samples millions of configurations per sec- ond, enabling an exhaustive exploration of the configuration space of small flexible molecules on surfaces within minutes. Furthermore, as demonstrated in our previous application of a similar technique to high-resolution scanning probe microscopy, GridFF can be extended be- yond empirical pairwise potentials to those de- rived from ab initio electron densities. Alto- gether, this unlocks accurate high-throughput modeling of molecular self-assembly, adsorp- tion, and scanning probe manipulation in sur- face science.

The structural characterization and design of organic/inorganic interfaces represent a criti- cal challenge across multiple research and tech- nological fields, including friction and lubrica- tion, 1–3 molecular electronics, 4 photovoltaics, 5 and scanning probe microscopy (SPM). 6 The interaction of organic molecules with surfaces of inorganic crystals plays a key role in the development of molecular nanotechnology com- prising self-assembled monolayers, 7 2D molecu- lar crystals, 8 covalent organic frameworks, 9 and also for the emerging field of on-surface chem- istry. 10 Complex molecular nanostructures are one of the most promising building blocks for next-generation computational devices such as molecular electronics, 11 photonics, 12 and quan- tum cellular automata. 13

The formation of such structures is primarily governed by non-covalent interactions between organic molecules and the templating effects of the inorganic substrate. However, predict- ing the absorption configurations of molecules on surface and self-assembled layers remains an open challenge, akin crystal structure predic- tion in pharmaceuticals 14 and protein folding in biochemistry. 15 The challenge of designing such structures, especially from flexible molecules, arises from the curse of dimensionality, related to soft internal degrees of freedom (e.g., tor- sions along single bonds), which drastically ex-

1













pands their configuration space. Fortunately, the interaction with rigid inorganic substrates can constrain this structural freedom, enabling more systematic and rational design of self-as- sembled motifs from first principles.

Crystalline inorganic substrates also provide an ideal, well-defined support for the exper- imental construction and study of complex nanosystems. 16 Unlike bulk solids or liquid so- lutions, surfaces allow unobstructed access to functional molecular components and atomic- level control through SPM techniques. SPM is not only an indispensable tool for imaging nanostructures with atomic resolution, 17 but it also offers the potential to construct complex supramolecular structures by direct manipula- tion. 18 However, the optimal control of molecu- lar degrees of freedom through interactions with an atomic force microscope (AFM) or a scan- ning tunneling microscope (STM) tip remains a major challenge, heavily dependent on atom- istic simulations. Without such simulations, SPM manipulation becomes a blind trial-and- error process, as the instrument provides min- imal experimental information about molecu- lar configurations during manipulation. 19 Re- cent efforts to automate this laborious pro- cess involve reinforcement learning for robotic AFM/STM machines. 20–22 However, training such robotic systems through direct experimen- tation is prohibitively expensive and time-con- suming due to the vast number of required training examples. This underscores the need for a specialized virtual training nanophysics engine – akin to NVIDIA’s Isaac physics en- gine 23 – capable of generating the data needed for training robotic manipulation of nanoscale objects, possibly parallelized on graphics pro- cessing units (GPU).

An even more ambitious challenge is the computational molecular design of organic molecules that deterministically self-assemble into target patterns or shapes on a substrate, e.g. driven by hydrogen bond formation. 24 In this case, whilst chemical intuition may suffice for designing simple structures based on one or two hydrogen bonds from rigid molecules, creating complex patterns requires molecules informa- with significantly greater structural

tion and an inverse computational design. No- table examples include biopolymers such as DNA, RNA, and proteins, whose self-assembly can be computationally designed using auto- mated tools, often leveraging machine learning approaches like AlphaFold. 25 However, training such design tools requires extensive databases of self-assembled (folded) structures, which in biochemistry are obtained from experimental data or atomistic simulations. 26 In the do- main of molecular surface science, no com- parable database currently exists, and con- structing such a database for newly designed molecules using experimental methods would be prohibitively expensive. Therefore, soft- wares capable of efficiently predicting self-as- sembled patterns for a broad range of novel molecules are essential for the rational design of molecular nanostructures on surfaces.

Moreover, the structure of molecular assem- blies at finite temperatures is influenced not only by enthalpy, but also by entropic contri- butions. For soft, flexible molecules, entropy can play a particularly significant role. The temperature dependence of the entropy term in the Gibbs free energy (∆G = ∆H − T ∆S) is crucial for estimating the melting tempera- ture of self-assembled structures and determin- ing annealing conditions for their reversible for- mation. These are key parameters in computa- tional design, particularly for larger self-assem- bling molecular templates akin to DNA frag- ments or oligopeptides. However, the computa- tional estimation of entropy and free energy re- quires an exhaustive configuration sampling of the partition function, which is extremely de- manding – especially when one aims at screen- ing a large number of candidate molecules.

Consequently, both SPM manipulation of molecules on solid substrates and their self-as- sembly ultimately face the same fundamental challenge: exploring a vast configuration space of soft organic molecules while simultaneously describing their interactions with the substrate (and potentially with an AFM/STM tip) ef- ficiently. The sheer size of this configuration space renders ab initio methods computation- ally unfeasible, necessitating the use of clas- sical force fields. Unfortunately, many exist-

2

ing methods for molecular configuration explo- ration, such as CREST 27 – designed for sys- tems in gas or liquid phases – cannot be directly applied to molecules on surfaces. Likewise, like AMBER, 28 state-of-the-art force fields, CHARMM, 29 GROMOS 30 and OPLS 31 (which can be used on general-purpose simulation pro- grams such as AMBER, 32 CHARMM, 33 GRO- MACS, 34 LAMMPS 35 and NAMD 36), are pri- marily optimized for biological molecules in aqueous environments, making efficient simula- tions of molecular interactions with substrates highly challenging.

To address these limitations and provide an efficient tool for SPM manipulation and self- assembly of small organic molecules on ionic substrates, we have developed a new classical force field simulation software. 37 The software we introduce here is optimized for efficient par- allel configuration sampling, accelerated using modern GPUs, and incorporates a specialized grid-projected force field (GridFF) to enhance the description of molecule-substrate interac- tions. This approach, which is the main focus of this publication, enables precise and computa- tionally feasible predictions of molecular self-as- sembly and manipulation on inorganic surfaces, bridging the gap between theoretical modeling and experimental nanostructure design.

2 Motivations

Simulating organic molecules on inorganic sub- strates, even using classical force fields, can be relatively costly, particularly when using large supercells comprising a large number of atoms. In such systems the number of substrate atoms nS, replicated across multiple unit cells, far exceeds the number of atoms of the organic molecules nM being studied, leading to sub- stantial computational overhead. There is more than one reason why one has to use large super- cells. First of all, in order to properly model a crystal slab, several atomic layers shall be con- sidered. In complex surface reconstructions, the unit cell itself may be also very large. Moreover, if one aims to simulate an isolated molecule on a surface, the sizes of the cell must be large

enough to avoid artifacts due to the interaction between the molecule and its periodic images. On the other hand, even in the case of periodic systems such as self-assembled monolayers, one has to deal with the intrinsic incommensura- bility between the two lattices. For all these reasons, the typical case is nS ≫ nM . Fur- thermore, many classical force fields parame- terizations struggle to simultaneously capture the nuances of both molecule-substrate interac- tions, and bulk mechanics of the crystal. For this reason, in such simulations, it is a com- mon practice to fix the position of the substrate atoms, or at least the bottom atomic layers. Even in this case, the computational overhead is not reduced in most implementations as the number of pairwise interaction scales quadrat- ically with the total number of atoms in the supercell (nM + nS)2. Even if pairwise interac- tions between the atoms of the substrate can be eliminated (which is generally a non-standard feature in classical force field programs 38), the computational cost of evaluating nM nS pair- wise interactions between the molecule and the substrate dominates over the n2 M calculations required to describe the interactions between atoms in the molecule.

Electrostatics interactions, due to their slow convergence in real-space calculations, require a different treatment. They are usually ad- dressed by evaluating the interactions in the Fourier space using techniques derived from the Ewald summation, 39 such as the Particle-Mesh Ewald (PME) method. 40 However, achieving a satisfactory accuracy requires a large plane- wave cutoff and a correspondingly dense grid, making PME a frequent bottleneck in periodic classical force field calculations. This computa- tional cost increases with the supercell size as O(N log(N )) (where N is the number of grid points).

GridFF, inspired by a similar method used typically for rigid protein-ligand docking, 41,42 offers an alternative approach designed to ad- dress these limitations. Unlike traditional force fields, which evaluate all pairwise interactions and PME on-the-fly, GridFF precalculates the molecule-substrate interaction potential on a grid before the actual simulation is performed.

3

This precalculated grid, representing the inter- action potential, is then simply interpolated during the simulation. This largely decreases the computational cost, and allows one to use denser grids, larger supercells, and more com- plex force fields without added computational cost. In addition to this, GridFF does not nec- essarily need to be constructed from pairwise potentials (e.g., as in the D3 van der Waals cor- rections, 43 where three-body terms are consid- ered), but can also be computed directly from electron densities obtained from ab initio wave- functions, as it is routinely done in the so-called full density based model (FDBM) in atomic force microscopy simulations. 44,45

Whilst the computational cost of a traditional pairwise potential scales quadratically with the number of particles in the system, and Ewald summation scales logarithmically (albeit with a significant prefactor), the cost of evaluat- ing the GridFF potential through interpola- tion remains essentially constant. Although cache memory effects might subtly influence the performance for large grids, the computational complexity of the interpolation algorithm itself is independent of the grid size, offering signifi- cant potential for speedup when the molecule- substrate interaction is the computational bot- tleneck.

3 Principles

Grid-based potentials for the description of non-covalent interactions, such as Coulomb and Lennard-Jones, are routinely used in compu- tational biochemistry codes for protein-ligand docking. 46–48 Nevertheless, here we briefly re- call the approach originally published by Good- ford. 49 The core idea lies in the assumption that the total interaction energy E between the molecule and the substrate can be decomposed into a sum of atomic contributions

E =

(cid:88)

i

Ei(ri, ti, Qi),

(1)

where ri, ti and Qi are the position, type and charge of the molecular atom i. Each term Ei can be interpolated from the grid potential us-

ing efficient techniques such as 3D trilinear 50 or tricubic splines, 51 which are particularly well- suited for implementation on GPU hardware. In ligand docking applications, the trilinear ap- proximation prevails due to the minimal com- putational cost, despite the fact that it can- not provide high quality forces. 52 Grid poten- tials are then typically used to evaluate energy- based scoring function only, not to run molec- ular dynamics simulations or force-based opti- mizations. 53 Moreover, these potentials are of- ten artificially softened to effectively mimic flex- ibility and thermal fluctuations of the protein at room temperature. 54 In some approaches, the grid force fields are used for mapping free en- ergy profiles using Monte Carlo sampling at fi- nite temperature. 55–57 In such situations, even- tual inaccuracies in energy and force evaluation are smeared out by thermal fluctuations. This is in contrast to our applications of GridFF for force-based dynamical simulations at low tem- perature, where molecule precisely follows the potential energy surface (PES), and a delicate balance between the gradient of the PES and the driving force (from the AFM tip) deter- mines bifurcations of the trajectory near sad- dle points. Therefore, in this work we opted for higher order interpolation (i.e., tricubic B- splines), and much finer grids (∼0.1 ˚A) to ob- tain high-quality forces. We found that even this computationally more intensive settings for GridFF provide significant speedups compared to all-atom potentials.

As already mentioned, a grid-based potential can be constructed using any desired method, namely a direct projection of pairwise poten- tials, a fitting procedure based on density func- tional theory (DFT) calculations, or a machine- learned potential. In its simplest form, GridFF approximates the molecule-substrate interac- tion by projecting typical non-covalent pair- wise potentials – such as Coulomb, Morse, or Lennard-Jones – onto the grid. In the general case, this involves evaluating the potential

Ei(ri, ti, Qi) =

(cid:88)

j

Vij(|ri − rj|, ti, tj, Qi, Qj)

for each atomic type ti of the molecular atoms

4

interacting with all substrate atoms j. In prin- ciple this is doable; nevertheless, it can be very memory demanding, as one may end up with tens or hundreds of grids Vij for all possible ij combinations, each accounting hundreds of megabytes of memory.

Another consideration to take into account is the efficiency of cache memory (which decreases when reading from distant memory addresses), finally making this formulation practically use- less. For this reason, grid-based potentials of- ten 41,42,54 (but not always 50,51,58) rely on vari- ous factorization schemes where the parameters of the ligand atoms (the molecular adsorbate in our case) can be factored out of the sum- mation over the substrate atoms. This can be done easily for Coulomb and Morse potential with Lorentz-Berthelot mixing rules. For elec- trostatics interactions, this is trivial:

EC

i (ri, Qi) =

(cid:88)

QiQj |ri − rj|

j = QiV C

i (ri).

= Qi

(cid:88)

j

Qj |ri − rj|

(2)

The factorization for a Morse potential can be done by applying basic properties of the expo- nential function (i.e., ea+b = ea · eb) separately to the repulsive (Pauli) and attractive (London dispersion) parts:

EM

i (ri, ti) =

(cid:88)

j

− 2

εiεje−2α(|ri−rj |−Ri−Rj )

εiεje−α(|ri−rj |−Ri−Rj )

(3)

(cid:88)

j

=Pi(ti)V P

i (ri) + Li(ti)V L

i (ri),

where

Pi(ti) = εie2αRi Li(ti) = −2εieαRi V P i (ri) =

(cid:88)

εje−2α(|ri−rj |−Rj )

V L i (ri) =

j (cid:88)

j

εje−α(|ri−rj |−Rj ).

(4)

Therefore, the three grids defined above (V C , i V P i ) do not carry any dependence i on molecular atomic types nor charges. This

and V L

5

means that once the grids have been calculated, then it is possible to simulate the interaction with any molecule, without the need of any other precomputation. It is also worth not- ing that the GridFF approach does not pose any restriction on the number of substrate atom types. Finally, during the evaluation of energies and forces, we simply perform the interpolation of the three separately stored grids (with the co- efficients in Eq. 4) using Eqs. 1, 2 and 3, and considering that E = (cid:80)

i EC

i + EM i

.

4 Periodicity

and

long-

range electrostatics

As anticipated in the previous section, the treatment of long-range Coulomb interactions in classical force field simulations requires some attention. A naive evaluation of electrostatic energy via direct pairwise summation does not converge for periodic systems, and even in neu- tral systems, electrostatic forces converge only very slowly with distance, consuming a relevant amount of computational resources. Fortu- nately, periodic boundary conditions (PBC) al- low the use of efficient reciprocal-space summa- tion techniques based on the Ewald summation. For example in PME, 40 atomic charges are pro- jected onto a grid as a charge density ρ(⃗r), and the electrostatic potential is calculated in the reciprocal space via V (⃗k) = ρ(⃗k)/|⃗k|2, using fast Fourier transforms (FFT) to switch be- tween real and reciprocal space representations. The PME algorithm is so efficient that it is often used even for systems that are not natu- rally periodic (e.g., proteins in water, or a single molecule on a surface), at the cost of introduc- ing artificial interactions between periodic im- ages of the system. Using PME for surfaces is further complicated by the lack of periodic- ity in the z-direction. A well-established and computationally efficient solution (used both in LAMMPS and FireCore) is to add a sufficient vacuum padding above the surface and apply monopole and dipole corrections to the result- ing 3D-periodic solution, assuming that higher- order multipole contributions decay rapidly. 59 For pristine surfaces, due to the exponential de-

cay with z of the electrostatic potential of the substrate and the absence of molecular mul- tipoles, this approach can be particularly effi- cient, allowing the usage of a smaller vacuum padding region.

For an efficient implementation of PME, it is essential to optimally split the charge den- sity into a smooth (low-frequency) component solved in the reciprocal space and a high- frequency residual handled in the real space. This decomposition enables the use of rela- tively coarse grids. Sophisticated smoothing and splitting schemes have been developed for this purpose, 60 but their details are beyond the scope of this article. Nevertheless, even with these optimizations (imposing PBCs on the sys- tem, using highly optimized PME solvers and FFT libraries), electrostatics still accounts for a relevant portion of the computational cost in classical molecular dynamics simulations.

Here, GridFF offers major advantages in both speed and accuracy. Since the electro- static potential of the substrate is precom- puted, the computational cost of solving for the electrostatic potential (via PME, or any other method) is removed entirely from the molecu- lar dynamics (MD) run. In addition, periodic boundary conditions are imposed only on the substrate, and not artificially on the molecular system too. This means that the molecule does not actually interact with its periodic images. Moreover, in fully periodic systems (e.g., a pris- tine surface), all interactions can be folded into the unit cell of the substrate, even if the molecu- lar adsorbate is much larger, and therefore stor- ing all interaction data in a minimal grid. Dur- ing the MD run, the atomic coordinates of the adsorbate are then mapped into the surface unit cell and used for computing energies and forces. In addition, the surface potential becomes neg- ligible just a few ˚A above the substrate, as both Morse and electrostatic components decay ex- ponentially with the height. For example, the potential of a pristine NaCl substrate can be stored in as little as 32 ˚A3 or 256 kB per com- ponent. For non-periodic systems (e.g., point defects, step edges, AFM tips), the required memory footprint is significantly higher, e.g., 100 cubic nanometers and 800 MB per compo-

nent in our largest supercell of 20×20. Overall, aside from the assumption of surface rigidity, the main limitation of GridFF is memory us- age.

In the present work, we test the GridFF ap- proach using a simplified electrostatic solver, which, unlike PME, omits the real-space resid- ual entirely and relies solely on solving the Poisson equation ∇2V = ρ in the reciprocal In this respect, our implementation space. more closely resembles the electrostatic solvers used in density functional theory codes (e.g., SIESTA 61 or VASP 62), than those based on classical force fields like LAMMPS. Besides sim- plicity, our motivation for this approach is the possibility to generate GridFF directly from ab initio charge densities without requiring atomic charge assignment or projection. Thanks to the relatively fine grid spacing (0.1 ˚A), this recip- rocal-space-only solver achieves an accuracy of about 0.01 meV, which is sufficient for most purposes. Nevertheless, in future work, we plan to implement the full PME algorithm including the real-space correction to further improve ac- curacy.

5 Interpolation

Among the many numerical algorithms devel- oped for function approximation and interpola- tion, we seek those that offer optimal perfor- mance on current computing hardware while maintaining sufficient accuracy. Additionally, we require methods that are completely gen- eral (i.e., do not assume any particular func- tional form of the potential) and that can be used not only for pairwise interaction poten- tials, but also for potentials derived from elec- tron densities. These requirements effectively constrain us to low-degree polynomial approx- imations defined on orthogonal uniform grids, which are also compatible with FFT algorithms used in long-range electrostatics.

A crucial consideration for high-performance implementation on modern CPUs and GPUs is the memory access pattern and cache locality. Contemporary hardware can perform hundreds of arithmetic operations (e.g., additions or mul-

6

tiplications) in the time that it takes to read a single number from global memory. To mitigate this bottleneck, fast cache memory is used to pre-load data stored at nearby addresses. Thus, interpolation algorithms can be significantly ac- celerated when the data required to evaluate the potential at a given atomic position are col- located in memory and accessed in contiguous blocks.

In the original application of grid-projected potentials for AFM imaging (ppafm), 45,63 the trilinear interpolation was found sufficient, and even implemented in a hardware-accelerated form with reduced numerical precision. How- ever, for simulations involving self-assembly or nanomanipulation of molecules on surfaces (es- pecially under low-temperature conditions), a higher accuracy is required. Such systems can be sensitive to energy differences on the order of fractions of a meV, which can qualitatively affect trajectory bifurcations and final states.

Another fundamental problem of the trilinear interpolation approach is the inconsistency be- tween interpolated forces and energy. In that implementation, the force components (Fx, Fy, Fz) and energy E were stored and interpolated independently. This not only require storing four times more data in memory (E, Fx, Fy and Fz, instead of just E), but also implies that the resulting force is not the exact gradient of the energy field. This is because the derivative of a piecewise linear energy function is a piecewise constant function, while the interpolated forces are instead piecewise linear. This may cause difficulties to converge forces in molecular dy- namics below a certain threshold, and/or for energy conservation.

To resolve this, we implemented a tricubic interpolation of the energy field, where forces are obtained analytically as the gradient of the piecewise cubic polynomials. In one dimension, this produces force profiles that are piecewise quadratic and therefore continuous and smooth. Formerly, we tested several approaches, includ- ing Hermite cubic polynomials, but ultimately selected cubic B-splines, which provided the best trade-off between interpolation accuracy and computational performance. Tricubic B- splines are also used in some ligand-docking ap-

plications, in cases where high quality forces are required, although typically with coarser grid spacing. 51,64

The one-dimensional cubic B-spline interpo- lation on an interval (xi, xi+1) can be expressed as a linear combination

f (x) =

2 (cid:88)

j=−1

B

(cid:19)

(cid:18) x − xi+j ∆x

bi+j

(5)

of B-spline basis functions B(t) positioned at four points (xi−1, xi, xi+1, xi+2) separated by a grid spacing ∆x. The B-spline basis is a sym- metric piecewise cubic polynomial defined as

B(t) =

 



2 |t|3,

1

2

3 − t2 + 1 6 (2 − |t|)3, 0,

|t| < 1, 1 ≤ |t| < 2, |t| ≥ 2.

(6)

The 3D cubic B-spline interpolation is imple- mented as a Cartesian (tensor) product of 1D interpolations. This means that each evalua- tion accesses data from a 4×4×4 block of the nearest grid points – namely, 64 values in total. For each potential component (i.e., Coulomb, Pauli, and London), this corresponds to read- ing 192 floating-point numbers.

The grid is stored in memory such that the fastest-changing axis (typically z) is laid out contiguously, so blocks of four values along z are collocated in memory. Although the full in- terpolation could be evaluated as a direct sum over 64 basis functions weighted by precom- puted coefficients, more efficient implementa- tions – like ours – decompose the operation into a sequence of 1D interpolations. Specifically, we perform 16 1D interpolations along z, followed by 4 along y, and finally 1 along x. Starting with the fastest axis (z in our case) ensures op- timal cache locality and allows the use of single- instruction-multiple-data vectorization.

According to our benchmarks, this tricubic B-spline interpolation is only about two times slower than a trilinear interpolation (using grids for E, Fx, Fy and Fz), while providing several orders of magnitude higher accuracy. Moreover, in practical simulations, the cost of evaluating molecule-substrate interactions (using either in- terpolation scheme) is anyway about one hun-

7

dred times faster (on CPU) than the cost of evaluating molecule-molecule pairwise interac- tions – even for small systems (∼50 atoms). Therefore, the additional cost of the tricubic in- terpolation is negligible in real workloads. On GPU, the GridFF interpolation consume a sig- nificantly higher share of performance budget (see Section 6) as the GPU performance is lim- ited by global memory access rather than arith- metic operations.

One complication of using B-splines is that the expansion coefficients bi, stored at each grid point, are not known a priori. They must be fit- ted such that the interpolated potential repro- duces the reference values at grid nodes. While this fitting is a linear problem involving a very sparse matrix, the problem size is large: for typ- ical grid spacing (∼0.1 ˚A), a cubic nanometer contains on the order of 106 grid points. This makes iterative solvers more feasible than direct matrix inversion approaches.

Currently, we use a simple gradient descent algorithm to minimize the root mean square er- ror across all grid points. Although this method is not particularly efficient and may require thousands of iterations to reach convergence at the desired accuracy, we do not consider this as a bottleneck. In a typical workflow, the grid is precomputed once for a given substrate, and then reused in many simulations. Therefore, we have not prioritized further optimizations of the coefficient fitting procedure.

6 Accuracy

and

perfor-

mance tests on CPU

In order to benchmark the accuracy of the pro- posed approach, we performed several system- atic tests calculations using GridFF as imple- mented in FireCore 37 and all-atom simulations using the LAMMPS package. 35 The studied system is the desorption and manipulation of a 3,4,9,10-perylenetetracarboxylic dianhydride (PTCDA) molecule on top of a NaCl slab. This system was chosen as it has been extensively studied experimentally by means of SPM tech- niques. 65–67 Also robotic manipulation of the PTCDA molecule (although on metallic sub-

strate) originally motivated our work. 20

First, we performed rigid (i.e., with fixed rela- tive positions of the atoms in the molecule) ver- tical and lateral scans of PTCDA on a 8×8×3 NaCl(001) surface slab to compare individ- ual potential components between GridFF and LAMMPS. Next, we conducted relaxed scans with the same molecular and substrate configu- rations, allowing the system to undergo molec- ular relaxation, therefore more closely mimick- ing a manipulation experiment with AFM. Af- ter validating the relaxed potential behavior, we extended our tests to include defective sub- strates. We introduced a neutral, nearly iso- lated defect by removing 2 atoms from the sur- face of a 20×20×3 NaCl supercell, equivalent to a defect density of approximately 0.08%. Using the same PTCDA molecule, we performed lat- eral scans across this defected surface to test the capability of the method to handle surface irregularities. Finally, we compared the perfor- mance of GridFF and all-atom calculations for increasing substrate sizes ranging from 8×8×3 to 20×20×3 supercells.

The molecular structure of PTCDA was re- trieved from a previous work of one of us on high resolution AFM imaging, 45 and supple- mented with atomic charges determined us- ing the restrained electrostatic potential fit- ting scheme. 68 The NaCl substrate was mod- eled by three fcc atomic layers with lattice spac- ing of 4.0 ˚A, and charges for the Na/Cl ions set to ±0.9e according to the Bader analysis. 69 The intramolecular interactions are modeled us- ing the universal force field (UFF) 70 both in FireCore and LAMMPS. UFF values were also used to obtain the Morse parameters modeling the molecule-substrate interaction (the value of the α parameter was set to 1.5 ˚A−1 for all pairs, and a cutoff of 17 ˚A was applied).

Within the GridFF framework, we generated potential energy grids for the NaCl(001) sub- strate using a fine grid spacing of 0.1 ˚A and setting the maximum iterations to 3000 for the generation of the B-spline parameters, achiev- ing fitting errors on the order of 10−5 for both Morse and Coulomb potentials. For the treat- ment of the slowly decaying electrostatic inter- actions, in FireCore we used a reciprocal-space

8

Poisson solver as reported above, whilst in LAMMPS they were evaluated with the closely related particle-particle particle-mesh (PPPM) method 71 with a real space cutoff of 15 ˚A. The relative accuracy of reciprocal part of PPPM kernel was set to 10−8 in rigid scans to obtain high-accuracy reference energy profiles. Such a stringent tolerance was necessary since a more standard setting (10−6) produced numerical ar- tifacts in LAMMPS calculations larger than the error coming from the reciprocal-space-only Poisson solver in GridFF. Nevertheless, a more relaxed threshold of 10−6 was chosen for relaxed scans with LAMMPS, as these simulations were also used for benchmarking the performance in typical use-cases. Therefore we should note that in case of relaxed scans FireCore simula- tions are actually more accurate then LAMMPS results, while being significantly faster. To ensure a fair comparison, both FireCore and LAMMPS used the same FIRE algorithm 72 for the structural optimization with a convergence threshold for forces of 10−3 eV/˚A.

6.1 Rigid scans of PTCDA on

NaCl

The profiles in Figure 1 show the contribu- tions to the energy variations when a PTCDA molecule is rigidly moved away from the NaCl surface. The decomposition of the total inter- action potential into individual components en- sures that the underlying interaction physics is accurately reproduced by the GridFF ap- proach. The Morse potential component (red) describes the short-range interactions (encom- passing both Pauli repulsion and van der Waals attraction), whilst the Coulomb potential com- ponent (blue) captures the long-range electro- statics between the PTCDA molecular charge distribution and the ionic NaCl substrate. The total potential energy curve (green) represents the superposition of all interaction components, yielding an equilibrium adsorption configura- tion at Z = 3.1 ˚A with a total binding en- ergy of -0.89 eV. All energy profiles overlap perfectly, and therefore, to quantitatively es- timate the accuracy of GridFF approach, we also report the energy differences between the

two methods on the right y-axis. In all cases, the agreement is remarkable (considering the fitting error in the interpolation procedure, and the PPPM accuracy set in the LAMMPS cal- culations), with the Morse potential component exhibiting differences on the order of 10−6 eV, whilst the Coulomb component shows discrep- ancies of approximately 10−5 eV. Overall, the differences are dominated by the electrostat- ics contribution, due to the plane-wave cut-off of the reciprocal Poisson solver and the afore- mentioned omission of real-space residual from PME employed in the GridFF calculations. In both cases, the differences tend to increase at small separation distances (where the slope of the profiles is maximal in absolute values).

Energy profiles

Figure 1: for a PTCDA molecule interacting with a NaCl surface as a function of the separation distance along the z-direction. The total potential is decomposed into Morse (red) and Coulomb (blue) compo- nents, with the total energy (green) shown for the sake of completeness. Calculations using the GridFF approach as implemented in the FireCore code are reported with thin solid lines, whilst all-atom simulations from LAMMPS are in thick dashed lines. The energy differences between the two methods are plotted with semi-transparent lines and with the same color scheme.

Next, we performed lateral two-dimensional rigid scans on the xy-plane by rigidly displac- ing the PTCDA molecule with a step length of

9

0.1 ˚A and at a constant height of 3.3 ˚A from the surface. Figure 2 shows the total interac- tion energy as a function of the in-plane po- sition of the molecule across one unit cell of the NaCl(001) substrate. Such a 2D scan pro- vides information about the preferred adsorp- tion sites and barriers for lateral diffusion or manipulation of the molecule on the surface. The left and center panels show the PES calcu- lated by FireCore and the reference LAMMPS force field, respectively. Both methods produce a qualitatively identical energy landscape, with an energy barrier of 0.46 eV located at the cen- ter of the unit cell. The right panel displays the absolute energy difference between the FireCore and LAMMPS calculations. As in the previous case, the absolute error is on the order of 10−5 eV. In summary, these results demonstrate that FireCore accurately reproduces the whole inter- action energy landscape.

6.2 Relaxed

detachment

of

PTCDA from NaCl

In order to simulate more realistically the ma- nipulation of PTCDA with an SPM tip on a surface at low temperature, we performed cal- culations where we restrained the position of one atom (a carboxylic oxygen at the corner) of the molecule and let the position of the other atoms relax. In the first set of simulations, at each step we displaced the z-component of the position of the selected atom by 0.1 ˚A, starting from 1.3 to 20 ˚A above the NaCl(001) surface. Such a computational setup effectively mim- ics the experimental protocols where a func- tionalized tip is used to grasp and manipu- late individual molecules on surfaces by form- ing a mechanical contact with a specific atomic site. 19,20 The resulting relaxed potential energy curve reported in Figure 3 exhibits significantly different characteristics compared to the rigid scan. The global minimum occurs at approx- imately Z = 2.8 ˚A with a binding energy of -0.94 eV, representing the equilibrium adsorp- tion configuration. The larger binding energy (compared to the rigid scan) demonstrates the importance of molecular flexibility in achieving optimal molecule-surface interactions through

conformational adaptation. The unbinding en- ergy profile is also qualitatively different, pre- senting multiple local minima and discontinu- ities, particularly in the intermediate separation range (6-15 ˚A). These features arise from the in- terplay between attractive molecule-surface in- teractions and internal molecular strain. The molecular relaxation allows for rotation, tilt- ing, and conformational changes (that can cre- ate metastable adsorption states not accessible in rigid scan calculations), and eventually lead- ing to the sudden detachment of the molecule from the substrate. As already pointed out, in the proximity of the sudden changes of molec- ular orientation/conformation, the relaxation dynamics is more sensitive to small force differ- ences, leading to bifurcation in the taken path. Nevertheless, also in this case the quantitative agreement between FireCore and LAMMPS is good, with energy differences remaining below 0.9 meV across the entire scan range. These results validate the ability of the GridFF ap- proach to accurately capture the energetics and the structural response of a molecule under me- chanical manipulation, a critical requirement for predicting AFM-based molecular device fab- rication and single-molecule manipulation pro- tocols.

6.3 Dragging the PTCDA molecule

over a defect

The goal of this set of simulations is to mimic the dragging of a PTCDA molecule by a SPM molecule over a NaCl surface along the diagonal of the substrate cell. As done before, this was achieved by fixing the position of one atom of the PTCDA molecule (marked by the red dot in Figure 4), and systematically displacing it along the scan direction while allowing the en- tire molecular system to relax at each step. We have considered a 20x20x3 NaCl substrate with 2400 atoms for the pristine surface, and a sys- tem with a neutral defect (i.e., by removing one Na and one Cl atoms next to each other from the topmost atomic layer) placed at the center of the cell.

The top panels of Figure 4 illustrate the slid- ing behavior on a defect-free NaCl surface. The

10

Figure 2: Rigid PES of a PTCDA molecule interacting with a NaCl surface at a separation height of 3.3 ˚A, obtained with the GridFF approach as implemented in the FireCore (a), and all-atom calculations from LAMMPS (b). Panel (c) reports the energy difference between FireCore and LAMMPS.

Figure 3: (a) Energy profiles for both GridFF (FireCore) and all-atom (LAMMPS) calculation, and (b) selected configurations of a PTCDA molecule lifted up from a NaCl surface. In panel (b), the red and blue dots represent the atom that is displaced vertically during the scan, and the carbonyl oxygen at the opposite position of the PTCDA molecule, respectively. The six molecular configurations shown in (b) correspond to the marked dots in (a). Na and Cl ions are depicted in orange and cyan, respectively (only the topmost layer is shown).

11

Figure 4: Comparison between GridFF (Firecore) and all-atom (LAMMPS) calculations for a PTCDA molecule dragged over a NaCl surface with and without the presence of a defect. (a) Energy profile and (b) the corresponding set of relaxed configurations on a pristine 20x20x3 NaCl substrate. (c) Energy profile and (d) the corresponding set of relaxed configurations on the same substrate in the presence of an isolated neutral defect at the center of the cell. In the energy plots (a, c), the potentials calculated by FireCore (dashed orange) and LAMMPS (solid blue) are shown on the left axis, with their absolute difference (red) on the right axis. In the trajectory plots (b, d), the path of the restrained atom (red) and of the atom in the opposite corner of the molecule (blue) are shown with dots, illustrating the path taken by the molecule over the Na (orange) and Cl (cyan) ions of the top substrate layer.

12

(a)(b)(c)(d)energy profile in Figure 4a shows a quantitative agreement between FireCore and LAMMPS, with the difference remaining consistently be- low 0.9 meV. The energy minima correspond to the PTCDA molecule settling into energeti- cally favorable adsorption sites that align with the underlying Na and Cl ion lattice. The en- ergy maxima represent the potential barriers that the molecule must overcome to move be- tween these stable sites. The non-smooth na- ture of the energy profile arises from the in- terplay between the static interaction field gen- erated by the substrate and the orientational and conformational degrees of freedom of the PTCDA molecule. As it is pulled across the sur- face, the molecule continuously adjusts its ori- entation and internal geometry to minimize the total energy, leading to the characteristic ’stick- slip’ motion (reminiscent of different systems studied by AFM experiments and MD simu- lations 73,74) visually depicted in the trajectory plot (Figure 4b). The path of the unconstrained opposite corner atom (blue dot) clearly deviates from the straight-line path of the fixed atom, hopping between adjacent lattice sites.

The bottom panels of Figure 4 shows the ca- pability of FireCore to handle localized and chemically complex features, such as a neutral point defect in the substrate. It can be observed from the graph that, far from the defect, the en- ergy profile in Figure 4c retains the periodic cor- rugation of the pristine surface. However, as the molecule approaches the defect location (at the mid-point of the scan length), the energy pro- file changes drastically. A sharp, deep potential well emerges, indicating a strong pinning of the molecule to the defect site. This interaction is significantly stronger than the regular surface corrugation, with an energy stabilization of over 0.77 eV. Figure 4d provides an intuitive real- space visualization of this event: the path taken by the molecule shows a lateral deviation as it is influenced by the presence of the defect. The PTCDA molecule reorients itself to maximize its favorable interaction with the defect before being pulled away, which requires overcoming a substantial energy barrier. Also in this case, the energy profile obtained with GridFF per- fectly matches the one from reference calcula-

tions, with maximum deviations in the order of 0.9 meV. Overall, this demonstrates the capa- bility of the proposed approach to accurately model the molecule-surface interaction even in the presence of more complicated features such as a point defect, making it a powerful tool for predictive materials simulation.

6.4 Performance comparison

The lateral relaxed scan on the pristine surface described in the previous section was also used to benchmark the performance of the CPU im- plementation of GridFF in FireCore with re- spect to LAMMPS. The dynamical relaxation with the FIRE algorithm 72 is a good choice for such comparison, as the overheads (e.g., setup and initialization of the system) are amortized over the thousands of steps required for the structure relaxation. To ensure a reliable com- parison between the two approaches, and avoid any dependence on the details of the relaxation algorithm, the number of relaxation steps was set to the same value (5000), and with a tiny threshold on the force convergence criterion to ensure that the maximum number of relaxation steps is always reached. In this way we can di- rectly compare the calculation walltimes needed to complete the dragging path, as in both cases the same number of relaxation steps (and there- fore of force and energy evaluations) are per- formed. It must be stressed that, in order to have a fair comparison, the PPPM accuracy tol- erance in LAMMPS was increased to 10−6 (with respect to 10−8 used for rigid-scan calculations). Moreover, again for the sake of fairness, we per- formed calculations also directly excluding the computation of pairwise interactions within the substrate, by removing the appropriate atoms from the neighbor lists.

Figure 5 shows the comparison of the com- putational performance between LAMMPS and Firecore, together with the scaling with respect to the system size (ranging from 8×8 to 20×20 NaCl unit cells, corresponding to 384 to 2400 substrate atoms). All calculations were per- formed on the same computer equipped with an AMD EPYC 7513 CPU (2.6 GHz) using a sin- gle core. Firecore achieves good speedup factors

13

ranging from 113× for the smallest system to 751× for the largest, reducing execution times from hours to minutes, as also shown in Table 1. Notably, the largest system, that requires over 45 hours in LAMMPS, completes in less than 4 minutes with Firecore. Moreover, as one can notice from the different slopes in Figure 5, GridFF shows also a superior scaling behavior with respect to the system size, if compared to all-atom calculations. In fact, by moving from the smallest to the largest systems, the total number of atoms increases by about 6 times. Correspondingly, the execution time (normal- ized by the total number of path points) in- creases by almost 8 times in LAMMPS, while in FireCore the increment is only about 20 %. Such dependence of speedup factors on the sys- tem size indicate that the algorithmic advan- tages of GridFF may become even more pro- nounced for larger systems (such as pulling of graphene ribbons 3 or DNA), enabling practical high-throughput screening and statistical sam- pling for several applications.

Figure 5: Execution walltimes for FireCore and LAMMPS as a function of the system size. The green solid and dashed lines repre- sent LAMMPS calculations with and without excluding the computation of pairwise interac- tions within the substrate atoms, respectively.

14

7 Configuration sampling

on GPU

While the simulations of dragging of the PTCDA molecule using CPU allows a side- by-side comparison of the accuracy and per- formance of GridFF/FireCore with LAMMPS, the main strength of our approach lies in its ability to accelerate the exploration of large configuration spaces of flexible molecules on surfaces. This includes tasks such as finding the most stable binding configuration (i.e., the global energy minimum in the configuration space, which is a hard problem), or comput- ing the binding potential of mean force, which requires sampling all energetically relevant con- figurations.

To illustrate the performance of FireCore in such applications, we conducted a case study of the adsorption of a xylitol molecule on a sodium chloride surface with a single chlorine vacancy. The presence of five hydroxyl groups in the xyl- itol molecule creates numerous possibilities for hydrogen bonding with the ionic surface. Com- bined with molecular flexibility due to free rota- tions around sigma bonds, this results in a com- plex energy landscape characterized by many local minima.

To comprehensively explore this vast confor- mational space, we employed a minima hopping technique adapted for surface adsorption. This method systematically samples different con- figurations by perturbing the molecular struc- ture (i.e., by performing 1000 steps of Langevin molecular dynamics at a relatively high temper- ature of 300 K, followed by dynamical relax- ation to the nearest local minimum. Each en- ergy minimization was carried out until forces converged below 0.1 meV/˚A, ensuring an accu- rate representation of the stable configuration. Efficient implementation of molecular dynam- ics for small molecules like this on GPU faces several challenges. Modern GPUs are equipped with thousands of cores, which is significantly more than the number of atoms in such sys- tems (typically 50-100 atoms; rigid substrate atoms represented by GridFF are excluded). Although parallelization over individual pair-

Table 1: Execution times and computational speedups for FireCore and LAMMPS for different system sizes.

LAMMPS

System Total steps 2265000 3395000 4525000 5660000

size 8x8 12x12 16x16 20x20

with exclusion

Firecore without exclusion Time (s) Time (s) Speedup Time (s) Speedup 113 262 706 751

8240 27549 117259 162257

14256 101655 164526 255390

195 968 991 1182

73 105 166 216

wise interactions (rather than atoms) is possi- ble, synchronized output of forces (i.e., reduc- tion) from different threads to global memory would require thread synchronization, reducing performance. We address this challenge by sim- ulating multiple replicas of the same system in parallel. Testing showed that the optimal performance is achieved with ∼5000 replicas, in the case of the xylitol molecule. Moreover, the time required to evaluate forces (i.e., sum- ming over all bonding and non-covalent inter- actions) for such small systems is often shorter than the time required to transfer atomic co- ordinates (and forces) to and from the GPU. Therefore, the entire molecular dynamics loop – including force evaluation and integration of the equations of motion – must be executed entirely on the GPU, eliminating the need for costly synchronization with the CPU.

While Langevin dynamics is performed fully on the GPU and downloaded only every few hundred steps for visualization, the dynamical relaxation requires global properties which in- volve reductions over all atoms – i.e., thread synchronization. In particular, for dynamical relaxations, one needs to calculate the norm of the whole force vector ⃗F in order to check the force convergence criterion, and to set to zero all velocities (⃗v = 0) if the system inertially moves up the hill (⟨ ⃗F | ⃗v⟩ < 0). To handle this, we download the system state every 100 steps and perform these operations on the CPU (al- though in principle, such reductions could also be performed on the GPU but at the cost of more complicated kernels). At any rate, not performing the check at every step does not seem to significantly hamper the overall perfor-

mance of the algorithm. We attribute this to the smoothness of the trajectory near the min- imum (which often is a long narrow “valley”) and to the fact that a relaxation typically takes several thousand steps anyway.

To identify unique structures, each optimized geometry is compared with all previously min- imized structures using a root-mean-square de- viation criterion (with a threshold of 0.1 ˚A), accounting for both conformational differences and variations in adsorption site and orienta- tion relative to the surface. For this purpose, geometries are downloaded from the GPU and the comparison is performed on the CPU, typi- cally only once per several thousand steps. Al- though the computational cost of comparison with hundreds or thousands of previously found local minima is substantial (using only CPU), it is amortized in the dominating cost of thermal- ization and relaxation, and therefore we did not attempt to implement this sub-task on GPU yet.

The simulation can be run either as a batch calculation via the FireCore Python API – suit- able for supercomputer production runs – or through a graphical user interface (GUI), use- ful for debugging and educational purposes on desktop computers. Figure 6a shows a screen- shot from such an interactive simulation, high- lighting a selected configuration near the va- cancy, while the other replicas are shown as transparent skeletons in the background.

In Figure 6b,c we present performance met- rics for this simulation using both the GridFF approach and a naive all-atom FireCore GPU MD-loop implementation. In the all-atom simulation, molecule-substrate interactions are computed as a direct sum of pairwise interac-

15

Figure 6: Analysis of xylitol adsorption on NaCl using GridFF. (a) Visualization of a 2000-replica simulation of representative xylitol molecule configurations adsorbed on the NaCl(001) surface with a Cl-hole. (b) Number of total (solid) and unique (dashed) structures found in a short run using FireCore. Blue and green lines represent the outcomes of simulations done on 16×16×3 surface with a Cl-hole using the GridFF and standard all-atom approaches, respectively. (c) Number of MD steps per second using GridFF (blue) and all-atom (green) calculations, and for an isolated xylitol molecule (red) for reference, as a function of the system size. Solid and dashed lines corresponds to calculations with different settings.

tions (i.e., without Ewald summation as used in LAMMPS). These are summed over the near- est periodic images of the surface supercell, while the interactions between surface atoms are omitted. Figure 6c reports the total num- ber of substrate atoms interacting with the molecule (i.e., the number of atoms in the su- percell plus its 9 periodic images used for the direct-sum evaluation). Notably, GridFF-based calculations are not only significantly faster but also more accurate, owing to the use of Ewald summation in constructing the GridFF.

Figure 6b presents a quantitative analysis of the sampling efficiency, showing the number of converged and unique structures found as a function of the execution time. The num- ber of converged structures increases propor- tionally with the number of attempts, indicat- ing the consistent performance of the energy minimization protocol. In contrast, the num- ber of unique structures rises rapidly at early times and plateaus around 800 structures, de- spite continued sampling. This saturation sug- gests that our approach exhaustively explored the relevant conformational space of the xylitol-

NaCl system under the chosen computational model.

Looking closely at Figure 6b, we observe that, in the all-atom simulation, the first batch of structures is found only after ∼10 seconds, re- flecting the time required for the initial ther- malization. This synchronization delay also explains why the green curve deviates from a steady-state sampling behavior around 20 sec- onds after the simulation start. A similar be- havior is seen in the GridFF simulation, but it is compressed into the first 1-2 seconds due to its ∼20× higher performance.

Figure 6c provides a systematic analysis of how performance scales with system size. GridFF shows clear performance superiority for larger supercells, achieving a throughput of ap- proximately 7.5 million MD steps per second for the largest systems, compared to only 0.3 million steps per second for the all-atom ap- proach. Notably, GridFF performance remains stable regardless of the system size, while the all-atom approach degrades steadily with sys- tem size, making the performance advantage of GridFF even more pronounced for larger sys-

16

(a)(b)(c)tems.

python3 generate_scans.py

For small supercells (<200 atoms in the su- percell, <1800 atoms in 9 images), GridFF shows a slight increase in performance, likely due to an improved cache efficiency as neighbor- ing voxel memory addresses are more contigu- ous. This effect could potentially be exploited further by using coarser grids.

8 FireCore from the user

perspective

FireCore is written in C++ and OpenCL lan- guages, with python binding providing a con- venient scripting interface. The generation of the surface potential can be done internally on CPU, or using a standalone pyOpenCL acceler- ated python script, which takes the structure of the substrate (provided by the user in the xyz or mol format), and the desired voxel size for the grid, and it generates and stores the grid potential. The syntax for generating the grid potentials presented in this paper is:

python3 generate_grid.py

The main simulation engine of FireCore can perform energy optimization or molecular dy- namics calculations of molecules on surfaces. To start a run, is sufficient to provide the ge- ometries of the molecule and the substrate. Supported formats are xyz and mol. In the case that a plain xyz file is provided for the molecule, FireCore automatically detect the bonding topology. FireCore accordingly as- signs the atomic types and all parameters needed for bonded and non-bonded interac- tions. Currently are available only UFF and sp3FF (our home brew potential which was not yet published). The user can also directly cus- tomize the parameter files for bonded and non- bonded interactions. Next, the user can decide whether to run grid-based or all-atom simula- tions. A folder (which also contains a README file with commands and explanations) with all files needed to run the examples reported in this paper is available in the FireCore repository. 75 To reproduce the calculations presented in Sec- tion 6, one can run the following command:

17

Along with the command line interface, FireCore also provides a GUI for running in- teractive simulations, real-time visualization of structural relaxations and molecular dynamics runs, including the multiple-replica feature de- scribed in Section 7. The GUI allows the user to visualize atomic types, charges, and electro- static potentials. It also enables the interactive manipulation of molecules by picking and drag- ging atoms with the mouse (resembling what is done in AFM manipulation of molecules). The GUI can be run using the command:

bash FireCore/tests/tMolGUIapp/run.sh

Beside the above-mentioned functionalities, FireCore also implements other advanced fea- tures like the charge equilibration scheme, 76 a workflow for fitting hydrogen bond corrections from reference data (currently under develop- ment), the thermodynamic integration method for the calculation of free energy differences, and a QM/MM framework with an interface to the Fireball DFT(B) package 77,78 and inte- grated high-resolution AFM simulations.

9 Conclusions and Outlook

In this work, we have demonstrated that grid- projected force fields – originally inspired by methods used for ligand docking in molec- ular biology – can be successfully applied to surface science, particularly for simulating molecular adsorption and manipulation on sur- faces using scanning probe microscopy at low temperatures. As implemented in the new open-source simulation package FireCore, the method achieves a speedup of 2-3 orders of mag- nitude (compared to conventional all-atom sim- ulations) for a PTCDA molecule on a NaCl sur- face using a single CPU, while maintaining a high accuracy of the results. We further show- case the CPU and GPU implementations of the method in FireCore which enables sampling of millions of molecular configurations per second, making it possible to exhaustively explore all local minima of small flexible molecules (e.g.,

xylitol) within just a few minutes on a standard desktop GPU.

The main limitation of the method is the as- sumption of a rigid substrate. However, this is a common approximation even in traditional all-atom simulations, particularly due to the lack of accurate force fields for ionic crystal surfaces. This limitation can be partially miti- gated by adjusting force field parameters to em- ulate the effective (softened) potential resulting from substrate atom deflections – a strategy commonly employed in ligand docking. 54 In a future work, we aim to address this limitation more rigorously by incorporating additional force field components that allow for local po- larization and substrate deflection, based on lin- ear response theory. 79–81 We are also developing a hydrogen-bond correction scheme within the GridFF framework, as well as density-derived potentials such as FDBM, 44 which promise to deliver accuracy far beyond traditional pairwise interactions, like Coulomb, Morse, or Lennard- Jones, while retaining unparalleled computa- tional performance. Although in this study we opted for a conservative approach to GridFF interpolation using cubic B-splines with very fine grid spacing, we recognize opportunities to further optimize the memory footprint and po- tentially improve the speed via a better cache locality. This could be achieved by experiment- ing with alternative interpolation strategies, including power-transformed interpolations, 52 different forms of potential factorization, or non-uniform grid spacing schemes. 82

authors

Acknowledgement The

thank Mithun Manikandan for the RESP calcula- tions. This work was supported by the Czech Science Foundation, Project 22-06008M, and co-funded by the European Union (Physics for Future – Grant Agreement No. 101081515). A part of the computational resources was pro- vided by the e-INFRA CZ project (ID:90254), supported by the Ministry of Education, Youth and Sports of the Czech Republic.

References

(1) Krim, J. Friction and energy dissipa- tion mechanisms in adsorbed molecules and molecularly thin films. Advances in Physics 2012, 61, 155–323.

(2) Villa, N. S.; Bonoldi, L.; Assanelli, G.; Notari, M.; Lucotti, A.; Tommasini, M.; Cuppen, H. M.; Galimberti, D. R. Digging into the friction reduction mechanism of organic friction modifiers on steel surfaces: Chains packing vs. molecule–metal in- teractions. Tribology International 2024, 195, 109649.

K.;

Pawlak, R.;

(3) Kawai, S.; Benassi, A.; Gnecco, E.; Feng, X.; S¨ode, H.; M¨ullen, D.; Pignedoli, C. A.; Ruffieux, P.; Fasel, R.; Meyer, E. Superlubricity of graphene nanoribbons on gold surfaces. Science 2016, 351, 957–961.

Passerone,

(4) Chen, H.; Stoddart, J. F. From molecu- lar to supramolecular electronics. Nature Reviews Materials 2021, 6, 804–828.

M.

M. Schryver, F. C.;

(5) Surin, M.; Lecl`ere, P.; De Feyter, S.; S.; Abdel-Mottaleb, De Henze, O.; Feast, W. J.; Lazzaroni, R. Molecule– versus Molecule–Substrate Molecule Interactions in the Assembly of Olig- othiophenes at Surfaces. The Journal of Physical Chemistry B 2006, 110, 7898–7908.

(6) Hieulle, J.; Stecker, C.; Ohmann, R.; Ono, L. K.; Qi, Y. Scanning Probe Mi- croscopy Applied to Organic–Inorganic Halide Perovskite Materials and Solar Cells. Small Methods 2018, 2, 1700295.

(7) Heimel, G.; Romaner, L.; Zojer, E.; Bredas, J.-L. The Interface Energetics of Self-Assembled Monolayers on Metals. Ac- counts of Chemical Research 2008, 41, 721–729.

(8) Feng, X.; Peng, X.; Peng, B.; Li, Z.; Huang, W.; Yang, S.; Pei, K.; Sun, Z.;

18

Huang, F.; Li, H.; Shuai, Z.; Zhai, T. Effect of Strong Intermolecular Interac- tion in 2D Inorganic Molecular Crystals. Journal of the American Chemical Society 2021, 143, 20192–20201.

(9) Zhu, Y.; Yan, Y.; Feng, Y.; Liu, Y.; Lin, C.-Y.; Ai, Q.; Zhai, T.; Shin, B.; Xu, R.; Shen, H.; Fang, Q.; Zhang, X.; Bhagwandin, D.; Han, Y.; Zhu, H.; Glavin, N. R.; Ajayan, P. M.; Li, Q.; Lou, J. A General Synthesis Method for Covalent Organic Framework and Inor- ganic 2D Materials Hybrids. Precision Chemistry 2024, 2, 398–405.

(10) George, S. M.; Yoon, B.; Dameron, A. A. Surface Chemistry for Molecular Layer Deposition and Hybrid Organic–Inorganic Polymers. Accounts of Chemical Research 2009, 42, 498–508.

of Organic

(11) Yuan, M.; Qiu, Y.; Gao, H.; Feng, J.; Jiang, L.; Wu, Y. Molecular Electron- ics: From Nanostructure Assembly to De- vice Integration. Journal of the American Chemical Society 2024, 146, 7885–7904.

(12) Shi, Y.-L.; Wang, X.-D. 1D Organic Micro/Nanostructures for Photonics. Ad- vanced Functional Materials 2021, 31, 2008149.

(13) Chen, H.; Zhao, L. Quantum-dot cellular automata as a potential technology for de- signing nano-scale computers: Exploring the state-of-the-art techniques and sug- gesting the opportunities for the future. Optik 2022, 265, 169431.

(14) Beran, G. J. O. Frontiers of molecular crystal structure prediction for pharma- ceuticals and functional organic materials. Chemical Science 2023, 14, 13290–13312.

(15) Nassar, R.; Dignon, G. L.; Razban, R. M.; Dill, K. A. The Protein Folding Problem: The Role of Theory. Journal of Molecular Biology 2021, 433, 167126.

(16) Li, Y.; Liang, D.; Wang, R.; Yang, S.; Liu, W.; Sang, Q.; Pu, J.; Wang, Y.;

Qian, K. Interfacial Self-Assembly Nanos- tructures: Constructions and Applica- tions. Small 2024, 20, 2405318.

(17) Sumaiya, S. A.; Baykara, M. Z. Atomic- scale imaging and spectroscopy via scan- ning probe microscopy: An overview. Journal of Vacuum Science & Technology B 2023, 41, 060802.

(18) Xiong, W.; Lu, J.; Geng, J.; Ruan, Z.; Zhang, H.; Zhang, Y.; Niu, G.; Fu, B.; Zhang, Y.; Sun, S.; Gao, L.; Cai, J. Atomic-scale construction and character- ization of quantum dots array and poly- fluorene chains via 2,7-dibromofluorene on Au(111). Applied Surface Science 2023, 609, 155315.

(19) Scheidt,

J.;

A.;

Diener,

Mai- worm, M.; M¨uller, K.-R.; Findeisen, R.; Driessens, K.; Tautz, F. S.; Wagner, C. Concept for the Real-Time Monitoring of Molecular Configurations during Manipu- lation with a Scanning Probe Microscope. The Journal of Physical Chemistry C 2023, 127, 13817–13836.

(20) Leinen, P.; Esders, M.; Sch¨utt, K. T.; Wagner, C.; M¨uller, K.-R.; Tautz, F. S. Autonomous robotic nanofabrication with reinforcement learning. Science Advances 2020, 6, eabb6987.

(21) Ramsauer, B.; Simpson, G. J.; Car- tus, J. J.; Jeindl, A.; Garc´ıa-L´opez, V.; Tour, J. M.; Grill, L.; Hofmann, O. T. Au- tonomous Single-Molecule Manipulation Based on Reinforcement Learning. The Journal of Physical Chemistry A 2023, 127, 2041–2050.

(22) Ramsauer, B.; Cartus, J. J.; Hof- mann, O. T. MAM-STM: A software for autonomous control of single moieties to- wards specific surface positions. Com- puter Physics Communications 2024, 303, 109264.

(23) Liang, J.; Makoviychuk, V.; Handa, A.; Chentanez, N.; Macklin, M.; Fox, D.

19

GPU-accelerated robotic simulation for distributed reinforcement learning. Con- ference on Robot Learning. 2018; pp 270– 282.

(24) Manikandan, M.; Nicolini, P.; Hapala, P. Computational Design of Photosensitive Polymer Templates To Drive Molecular Nanofabrication. ACS Nano 2024, 18, 9969–9979.

Figurnov, M.;

(25) Jumper, J.; Evans, R.; Pritzel, A.; Ron- Green, T.; Tunyasuvunakool, K.; neberger, O.; Bates, R.; ˇZ´ıdek, A.; Potapenko, A.; Bridgland, A.; Meyer, C.; Kohl, S. A. A.; Ballard, A. J.; Cowie, A.; S.; B.; Romera-Paredes, Jain, R.; Adler, J.; Back, T.; Pe- tersen, S.; Reiman, D.; Clancy, E.; Zielinski, M.; Steinegger, M.; Pachol- ska, M.; Berghammer, T.; Bodenstein, S.; Silver, D.; Vinyals, O.; Senior, A. W.; Kavukcuoglu, K.; Kohli, P.; Hassabis, D. Highly accurate protein structure predic- tion with AlphaFold. Nature 2021, 596, 583–589.

Nikolov,

(26) wwPDB consortium Protein Data Bank: the single global archive for 3D macro- molecular structure data. Nucleic Acids Research 2018, 47, D520–D528.

(27) Pracht, P.; Grimme, S.; Bannwarth, C.; Bohle, F.; Ehlert, S.; Feldmann, G.; Gorges, J.; M¨uller, M.; Neudecker, T.; Plett, C.; Spicher, S.; Steinbach, P.; Weso(cid:32)lowski, P. A.; Zeller, F. CREST—A program for the exploration of low-energy molecular chemical space. The Journal of Chemical Physics 2024, 160, 114110.

(28) Wang, J.; Wolf, R. M.; Caldwell, J. W.; Kollman, P. A.; Case, D. A. Develop- ment and testing of a general amber force field. Journal of Computational Chemistry 2004, 25, 1157–1174.

(29) Vanommeslaeghe, K.;

Hatcher, E.; Acharya, C.; Kundu, S.; Zhong, S.; Shim, J.; Darian, E.; Guvench, O.;

20

Lopes, P.; I.; Mack- Vorobyov, erell Jr., A. D. CHARMM general force field: A force field for drug-like molecules compatible with the CHARMM all-atom additive biological force fields. Journal of Computational Chemistry 2010, 31, 671–690.

(30) Oostenbrink, C.; Villa, A.; Mark, A. E.; Van Gunsteren, W. F. A biomolecular force field based on the free enthalpy of hydration and solvation: The GRO- MOS force-field parameter sets 53A5 and 53A6. Journal of Computational Chem- istry 2004, 25, 1656–1676.

(31) Jorgensen, W. L.; Maxwell, D. S.; Tirado- Rives, J. Development and Testing of the OPLS All-Atom Force Field on Con- formational Energetics and Properties of Organic Liquids. Journal of the Ameri- can Chemical Society 1996, 118, 11225– 11236.

(32) Case, D. A.; Cheatham III, T. E.; Darden, T.; Gohlke, H.; Luo, R.; Merz Jr., K. M.; Onufriev, A.; Sim- merling, C.; Wang, B.; Woods, R. J. The Amber biomolecular simulation pro- grams. Journal of Computational Chem- istry 2005, 26, 1668–1688.

(33) Hwang, W.; Austin, S. L.; Blondel, A.; Boittier, E. D.; Boresch, S.; Buck, M.; Buckner, J.; Caflisch, A.; Chang, H.-T.; Cheng, X.; Choi, Y. K.; Chu, J.-W.; Crowley, M. F.; Cui, Q.; Damjanovic, A.; Deng, Y.; Devereux, M.; Ding, X.; Feig, M. F.; Gao, J.; Glowacki, D. R.; Gonzales, J. E. I.; Hamaneh, M. B.; Harder, E. D.; Hayes, R. L.; Huang, J.; Huang, Y.; Hudson, P. S.; Im, W.; Is- lam, S. M.; Jiang, W.; Jones, M. R.; K¨aser, S.; Kearns, F. L.; Kern, N. R.; Klauda, J. B.; Lazaridis, T.; Lee, J.; Lemkul, J. A.; Liu, X.; Luo, Y.; MacKerell, A. D. J.; Major, D. T.; Meuwly, M.; Nam, K.; Nilsson, L.; Ovchinnikov, V.; Paci, E.; Park, S.; Pas- tor, R. W.; Pittman, A. R.; Post, C. B.;

Prasad, S.; Pu, J.; Qi, Y.; Rathinave- lan, T.; Roe, D. R.; Roux, B.; Row- ley, C. N.; Shen, J.; Simmonett, A. C.; Sodt, A. J.; T¨opfer, K.; Upadhyay, M.; van der Vaart, A.; Vazquez-Salazar, L. I.; Venable, R. M.; Warrensford, L. C.; Woodcock, H. L.; Wu, Y.; Brooks, C. L. I.; Brooks, B. R.; Karplus, M. CHARMM at 45: Enhancements in Accessibility, Func- tionality, and Speed. The Journal of Phys- ical Chemistry B 2024, 128, 9976–10042.

(34) Abraham, M. J.; Murtola, T.; Schulz, R.; P´all, S.; Smith, J. C.; Hess, B.; Lin- dahl, E. GROMACS: High performance molecular simulations through multi-level parallelism from laptops to supercomput- ers. SoftwareX 2015, 1–2, 19–25.

Bolintineanu, D.

(35) Thompson, A. P.; Aktulga, H. M.; S.; Berger, R.; Brown, W. M.; Crozier, P. S.; in ’t Veld, P. J.; Kohlmeyer, A.; Moore, S. G.; Nguyen, T. D.; Shan, R.; Stevens, M. J.; Tranchida, J.; Trott, C.; Plimpton, S. J. LAMMPS - a flexible simulation tool for particle-based materials modeling at the atomic, meso, and continuum scales. Computer Physics Communication 2022, 271, 108171.

(36) Phillips, J. C.; Hardy, D. J.; Maia, J. D. C.; Stone, J. E.; Ribeiro, J. V.; Bernardi, R. C.; Buch, R.; Fiorin, G.; H´enin, J.; Jiang, W.; McGreevy, R.; Melo, M. C. R.; Radak, B. K.; Skeel, R. D.; Singharoy, A.; Wang, Y.; Roux, B.; Aksimentiev, A.; Luthey- Schulten, Z.; Kal´e, L. V.; Schulten, K.; Chipot, C.; Tajkhorshid, E. Scalable molecular dynamics on CPU and GPU architectures with NAMD. The Journal of Chemical Physics 2020, 153, 044130.

(37) FireCore.

ProkopHapala/FireCore, 23/07/2025.

https://github.com/ Accessed:

(38) In LAMMPS this can be easily done using the neigh modify exclude com- mand, while in the other codes there is

21

generally no native support, even though workarounds exist.

(39) Ewald, P. P. Die Berechnung optis- cher und elektrostatischer Gitterpoten- tiale. Annalen der Physik 1921, 369, 253– 287.

(40) Darden, T.; York, D.; Pedersen, L. Parti- cle mesh Ewald: An N·log(N) method for Ewald sums in large systems. The Jour- nal of Chemical Physics 1993, 98, 10089– 10092.

(41) Pattabiraman, N.;

Levitt, M.; Fer- rin, T. E.; Langridge, R. Computer graph- ics in real-time docking with energy calcu- lation and minimization. Journal of Com- putational Chemistry 1985, 6, 432–436.

(42) Meng, E. C.; Shoichet, B. K.; Kuntz, I. D. Automated docking with grid-based en- ergy evaluation. Journal of Computational Chemistry 1992, 13, 505–524.

(43) Grimme, S.; Antony, J.; Ehrlich, S.; Krieg, H. A consistent and accurate ab ini- tio parametrization of density functional dispersion correction (DFT-D) for the 94 elements H-Pu. The Journal of Chemical Physics 2010, 132, 154104.

(44) Ellner, M.; Pou, P.; P´erez, R. Molecu- lar Identification, Bond Order Discrimi- nation, and Apparent Intermolecular Fea- tures in Atomic Force Microscopy Stud- ied with a Charge Density Based Method. ACS Nano 2019, 13, 786–795.

(45) Oinonen, N.; Yakutovich, A. V.; Gal- lardo, A.; Ondr´aˇcek, M.; Hapala, P.; Krejˇc´ı, O. Advancing scanning probe mi- croscopy simulations: A decade of devel- opment in probe-particle models. Com- puter Physics Communications 2024, 305, 109341.

(46) Friesner, R. A.; Banks, J. L.; Mur- phy, R. B.; Halgren, T. A.; Kli- cic, J. J.; Mainz, D. T.; Repasky, M. P.; Knoll, E. H.; Shelley, M.; Perry, J. K.; Shaw, D. E.; Francis, P.; Shenkin, P. S.

Glide: A New Approach for Rapid, Accu- rate Docking and Scoring. 1. Method and Assessment of Docking Accuracy. Journal of Medicinal Chemistry 2004, 47, 1739– 1749.

(47) Allen, W. J.; Balius, T. E.; Mukher- jee, S.; Brozell, S. R.; Moustakas, D. T.; Lang, P. T.; Case, D. A.; Kuntz, I. D.; Rizzo, R. C. DOCK 6: Impact of new features and current docking perfor- mance. Journal of Computational Chem- istry 2015, 36, 1132–1156.

(48) Eberhardt,

J.;

Santos-Martins, D.; Tillack, A. F.; Forli, S. AutoDock Vina 1.2.0: New Docking Methods, Expanded and Python Bindings. Force Field, Journal of Chemical Information and Modeling 2021, 61, 3891–3898.

(49) Goodford, P. J. A computational proce- dure for determining energetically favor- able binding sites on biologically impor- tant macromolecules. Journal of Medici- nal Chemistry 1985, 28, 849–857.

(50) Diller, D. J.; Verlinde, C. L. M. J. A crit- ical evaluation of several global optimiza- tion algorithms for the purpose of molec- ular docking. Journal of Computational Chemistry 1999, 20, 1740–1751.

(51) Oberlin, D.; Scheraga, H. A. B-spline method for energy minimization in grid- based molecular mechanics calculations. Journal of Computational Chemistry 1998, 19, 71–85.

(52) Minh, D. D. L. Power transformations im- prove interpolation of grids for molecu- lar mechanics interaction energies. Jour- nal of Computational Chemistry 2018, 39, 1200–1207.

(53) Tomioka, N.; Itai, A. GREEN: A pro- gram package for docking studies in ra- tional drug design. Journal of Computer- Aided Molecular Design 1994, 8, 347–366.

(54) Venkatachalam, C. M.; Jiang, X.; Old- field, T.; Waldman, M. LigandFit: a novel

method for the shape-directed rapid dock- ing of ligands to protein active sites. Jour- nal of Molecular Graphics and Modelling 2003, 21, 289–307.

(55) Forouzesh, N.; Izadi, S.; Onufriev, A. V. Grid-Based Surface Generalized Born Model for Calculation of Electrostatic Binding Free Energies. Journal of Chem- ical Information and Modeling 2017, 57, 2505–2513.

(56) Minh, D. D. L. Alchemical Grid Dock (AlGDock): Binding Free Energy Cal- culations between Flexible Ligands and Rigid Receptors. Journal of Computa- tional Chemistry 2020, 41, 715–730.

(57) Ren, E.; Coudert, F.-X. Enhancing Gas Separation Selectivity Prediction through Geometrical and Chemical Descriptors. Chemistry of Materials 2023, 35, 6771– 6781.

(58) Wu,

D.

G.;

Robertson,

H.; Brooks III, C. L.; Vieth, M. Detailed analysis of grid-based molecular dock- ing: A case study of CDOCKER—A CHARMm-based MD docking algorithm. Journal of Computational Chemistry 2003, 24, 1549–1562.

(59) Yeh, I.-C.; Berkowitz, M. L. Ewald sum- mation for systems with slab geometry. The Journal of Chemical Physics 1999, 111, 3155–3162.

(60) Arnold, A.; Fahrenberger, F.; Holm, C.; Lenz, O.; Bolten, M.; Dachsel, H.; Halver, R.; Kabadshow, I.; G¨ahler, F.; Heber, F.; J.; Hof- mann, M.; Pippig, M.; Potts, D.; Sut- mann, G. Comparison of scalable fast methods for long-range interactions. Phys- ical Review E 2013, 88, 063308.

Iseringhausen,

(61) Soler, J. M.; Artacho, E.; Gale, J. D.; Garc´ıa, A.; Junquera, J.; Ordej´on, P.; S´anchez-Portal, D. The SIESTA method for ab initio order-N materials simulation. Journal of Physics: Condensed Matter 2002, 14, 2745.

22

(62) Kresse, G.; Furthm¨uller, J. Efficient it- erative schemes for ab initio total-energy calculations using a plane-wave basis set. Physical Review B 1996, 54, 11169– 11186.

(63) Hapala, P.; Temirov, R.; Tautz, F. S.; Jel´ınek, P. Origin of High-Resolution IETS-STM Images of Organic Molecules with Functionalized Tips. Physical Review Letters 2014, 113, 226101.

(64) Trosset, J.-Y.; Scheraga, H. A. Reaching the global minimum in docking simula- tions: A Monte Carlo energy minimiza- tion approach using Bezier splines. Pro- ceedings of the National Academy of Sci- ences 1998, 95, 8011–8015.

(65) Karacuban, H.; Koch, S.; Fendrich, M.; Wagner, T.; M¨oller, R. PTCDA on Cu(111) partially covered with NaCl. Nanotechnology 2011, 22, 295305.

(66) Swart, I.; Sonnleitner, T.; Niedenf¨uhr, J.; Repp, J. Controlled Lateral Manipulation of Molecules on Insulating Films by STM. Nano Letters 2012, 12, 1070–1074.

(67) de Campos Ferreira, R. C.; Sagwal, A.; Doleˇzal, J.; Canola, S.; Merino, P.; Neuman, T.; ˇSvec, M. Resonant Tip- Enhanced Raman Spectroscopy of a Single-Molecule Kondo System. ACS Nano 2024, 18, 13164–13170.

(68) Bayly, C. I.; Cieplak, P.; Cornell, W.; Kollman, P. A. A well-behaved elec- trostatic potential based method us- ing charge restraints for deriving atomic charges: the RESP model. The Journal of Physical Chemistry 1993, 97, 10269– 10280.

(69) Bader, R. F. W. Atoms in Molecules: a Quantum Theory; Oxford University Press: New York, 1990.

molecular dynamics simulations. Journal of the American Chemical Society 1992, 114, 10024–10035.

(71) Hockney, R.; Eastwood, J. Computer Sim- ulation Using Particles; Adam Hilger: New York, 1988.

(72) Bitzek, E.; Koskinen, P.; G¨ahler, F.; Moseler, M.; Gumbsch, P. Structural Re- laxation Made Simple. Physical Review Letters 2006, 97, 170201.

(73) Ouyang, W.; Mandelli, D.; Urbakh, M.; Graphene Hod, O. Nanoserpents: Nanoribbon Motion on Two-Dimensional Hexagonal Materials. Nano Letters 2018, 18, 6009–6016.

(74) Vilhena, J. G.; Pawlak, R.; D’Astolfo, P.; Liu, X.; Gnecco, E.; Kisiel, M.; Glatzel, T.; P´erez, R.; H¨aner, R.; De- curtins, S.; Baratoff, A.; Prampolini, G.; Liu, S.-X.; Meyer, E. Flexible Superlu- bricity Unveiled in Sidewinding Motion of Individual Polymeric Chains. Physical Review Letters 2022, 128, 216102.

(75) GridFF folder.

https://github.com/

ProkopHapala/FireCore/tree/debug/ grid_geration_and_test/tests/ tGridFF, Accessed: 23/07/2025.

(76) Rappe, A. K.; Goddard, W. A. Charge equilibration for molecular dynamics sim- ulations. The Journal of Physical Chem- istry 1991, 95, 3358–3363.

(77) Lewis, J. P.; Jel´ınek, P.; Ortega, J.; Demkov, A. A.; Trabada, D. G.; Hay- cock, B.; Wang, H.; Adams, G.; Tom- fohr, J. K.; Abad, E.; Wang, H.; Drabold, D. A. Advances and applica- tions in the F¡scp¿IREBALL¡/scp¿ab ini- tio tight-binding molecular-dynamics for- malism. physica status solidi (b) 2011, 248, 1989–2007.

(70) Rappe, A. K.; Casewit, C. J.; Col- well, K. S.; Goddard, W. A. I.; Skiff, W. M. UFF, a full periodic table force field for molecular mechanics and

(78) Mendieta-Moreno, J. I.; Walker, R. C.; Lewis, J. P.; G´omez-Puertas, P.; Mendi- eta, J.; Ortega, J. FIREBALL/AMBER: An Efficient Local-Orbital DFT QM/MM

23

Method for Biomolecular Systems. Jour- nal of Chemical Theory and Computation 2014, 10, 2185–2193.

(79) Evans, W. A. B.; Heyes, D. M.; Powles, J. G.; Rickayzen, G. Parametriza- tion of linear dielectric response. Physical Review E 2011, 83, 046601.

(80) Tabacchi, G.; Mundy, C. J.; Hutter, J.; Parrinello, M. Classical polarizable force fields parametrized from ab initio calcu- lations. The Journal of Chemical Physics 2002, 117, 1416–1433.

(81) Harshan, A. K.; Bronson, M. J. J.; Jensen, L. Local-Field Effects in Linear Response Properties within a Polarizable Frozen Density Embedding Method. Jour- nal of Chemical Theory and Computation 2022, 18, 380–393.

(82) Piegl, L.; Tiller, W. The NURBS book ;

Springer: Berlin, 1997.

24

TOC Graphic

For Table of Contents Only

25

projectionB-splineinterpolationGridFF~700x faster