[
    {
        "title": "AR-RAG: Autoregressive Retrieval Augmentation for Image Generation",
        "summary": "We introduce Autoregressive Retrieval Augmentation (AR-RAG), a novel paradigm\nthat enhances image generation by autoregressively incorporating knearest\nneighbor retrievals at the patch level. Unlike prior methods that perform a\nsingle, static retrieval before generation and condition the entire generation\non fixed reference images, AR-RAG performs context-aware retrievals at each\ngeneration step, using prior-generated patches as queries to retrieve and\nincorporate the most relevant patch-level visual references, enabling the model\nto respond to evolving generation needs while avoiding limitations (e.g.,\nover-copying, stylistic bias, etc.) prevalent in existing methods. To realize\nAR-RAG, we propose two parallel frameworks: (1) Distribution-Augmentation in\nDecoding (DAiD), a training-free plug-and-use decoding strategy that directly\nmerges the distribution of model-predicted patches with the distribution of\nretrieved patches, and (2) Feature-Augmentation in Decoding (FAiD), a\nparameter-efficient fine-tuning method that progressively smooths the features\nof retrieved patches via multi-scale convolution operations and leverages them\nto augment the image generation process. We validate the effectiveness of\nAR-RAG on widely adopted benchmarks, including Midjourney-30K, GenEval and\nDPG-Bench, demonstrating significant performance gains over state-of-the-art\nimage generation models.",
        "url": "http://arxiv.org/abs/2506.06962v1",
        "published_date": "2025-06-08T01:33:05+00:00",
        "updated_date": "2025-06-08T01:33:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingyuan Qi",
            "Zhiyang Xu",
            "Qifan Wang",
            "Lifu Huang"
        ],
        "tldr": "The paper introduces AR-RAG, a new image generation paradigm that autoregressively incorporates nearest neighbor retrievals at the patch level, using two frameworks: Distribution-Augmentation in Decoding (DAiD) and Feature-Augmentation in Decoding (FAiD). It demonstrates performance gains over SOTA models on several benchmarks.",
        "tldr_zh": "该论文介绍了 AR-RAG，一种新型图像生成范式，它以自回归方式在补丁级别整合最近邻检索，使用两个框架：解码中的分布增强 (DAiD) 和解码中的特征增强 (FAiD)。 结果表明，在多个基准测试中，其性能优于 SOTA 模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer",
        "summary": "Recent advances in multimodal foundation models unifying image understanding\nand generation have opened exciting avenues for tackling a wide range of\nvision-language tasks within a single framework. Despite progress, existing\nunified models typically require extensive pretraining and struggle to achieve\nthe same level of performance compared to models dedicated to each task.\nAdditionally, many of these models suffer from slow image generation speeds,\nlimiting their practical deployment in real-time or resource-constrained\nsettings. In this work, we propose Layerwise Timestep-Expert Flow-based\nTransformer (LaTtE-Flow), a novel and efficient architecture that unifies image\nunderstanding and generation within a single multimodal model. LaTtE-Flow\nbuilds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong\nmultimodal understanding capabilities, and extends them with a novel Layerwise\nTimestep Experts flow-based architecture for efficient image generation.\nLaTtE-Flow distributes the flow-matching process across specialized groups of\nTransformer layers, each responsible for a distinct subset of timesteps. This\ndesign significantly improves sampling efficiency by activating only a small\nsubset of layers at each sampling timestep. To further enhance performance, we\npropose a Timestep-Conditioned Residual Attention mechanism for efficient\ninformation reuse across layers. Experiments demonstrate that LaTtE-Flow\nachieves strong performance on multimodal understanding tasks, while achieving\ncompetitive image generation quality with around 6x faster inference speed\ncompared to recent unified multimodal models.",
        "url": "http://arxiv.org/abs/2506.06952v1",
        "published_date": "2025-06-08T00:15:32+00:00",
        "updated_date": "2025-06-08T00:15:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ying Shen",
            "Zhiyang Xu",
            "Jiuhai Chen",
            "Shizhe Diao",
            "Jiaxin Zhang",
            "Yuguang Yao",
            "Joy Rimchala",
            "Ismini Lourentzou",
            "Lifu Huang"
        ],
        "tldr": "The paper introduces LaTtE-Flow, a novel flow-based transformer architecture for efficient multimodal image understanding and generation, achieving competitive image generation quality with significantly faster inference speeds.",
        "tldr_zh": "该论文介绍了LaTtE-Flow，一种新颖的基于流的Transformer架构，用于高效的多模态图像理解和生成，在实现有竞争力的图像生成质量的同时，显著提高了推理速度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]