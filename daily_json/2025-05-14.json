[
    {
        "title": "Aya Vision: Advancing the Frontier of Multilingual Multimodality",
        "summary": "Building multimodal language models is fundamentally challenging: it requires\naligning vision and language modalities, curating high-quality instruction\ndata, and avoiding the degradation of existing text-only capabilities once\nvision is introduced. These difficulties are further magnified in the\nmultilingual setting, where the need for multimodal data in different languages\nexacerbates existing data scarcity, machine translation often distorts meaning,\nand catastrophic forgetting is more pronounced. To address the aforementioned\nchallenges, we introduce novel techniques spanning both data and modeling.\nFirst, we develop a synthetic annotation framework that curates high-quality,\ndiverse multilingual multimodal instruction data, enabling Aya Vision models to\nproduce natural, human-preferred responses to multimodal inputs across many\nlanguages. Complementing this, we propose a cross-modal model merging technique\nthat mitigates catastrophic forgetting, effectively preserving text-only\ncapabilities while simultaneously enhancing multimodal generative performance.\nAya-Vision-8B achieves best-in-class performance compared to strong multimodal\nmodels such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger\nLlama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which\noutperforms models more than twice its size, such as Molmo-72B and\nLLaMA-3.2-90B-Vision. Our work advances multilingual progress on the\nmulti-modal frontier, and provides insights into techniques that effectively\nbend the need for compute while delivering extremely high performance.",
        "url": "http://arxiv.org/abs/2505.08751v1",
        "published_date": "2025-05-13T17:03:48+00:00",
        "updated_date": "2025-05-13T17:03:48+00:00",
        "categories": [
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Saurabh Dash",
            "Yiyang Nan",
            "John Dang",
            "Arash Ahmadian",
            "Shivalika Singh",
            "Madeline Smith",
            "Bharat Venkitesh",
            "Vlad Shmyhlo",
            "Viraat Aryabumi",
            "Walter Beller-Morales",
            "Jeremy Pekmez",
            "Jason Ozuzu",
            "Pierre Richemond",
            "Acyr Locatelli",
            "Nick Frosst",
            "Phil Blunsom",
            "Aidan Gomez",
            "Ivan Zhang",
            "Marzieh Fadaee",
            "Manoj Govindassamy",
            "Sudip Roy",
            "Matthias Gallé",
            "Beyza Ermis",
            "Ahmet Üstün",
            "Sara Hooker"
        ],
        "tldr": "This paper introduces Aya Vision, a multilingual multimodal model family (8B and 32B) that leverages synthetic data and cross-modal model merging to achieve state-of-the-art performance in multilingual multimodal generation, even surpassing larger models.",
        "tldr_zh": "该论文介绍了Aya Vision，一个多语言多模态模型系列（8B和32B），利用合成数据和跨模态模型融合，在多语言多模态生成中实现了最先进的性能，甚至超越了更大的模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]