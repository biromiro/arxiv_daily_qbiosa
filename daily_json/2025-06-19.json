[
    {
        "title": "Show-o2: Improved Native Unified Multimodal Models",
        "summary": "This paper presents improved native unified multimodal models, \\emph{i.e.,}\nShow-o2, that leverage autoregressive modeling and flow matching. Built upon a\n3D causal variational autoencoder space, unified visual representations are\nconstructed through a dual-path of spatial (-temporal) fusion, enabling\nscalability across image and video modalities while ensuring effective\nmultimodal understanding and generation. Based on a language model,\nautoregressive modeling and flow matching are natively applied to the language\nhead and flow head, respectively, to facilitate text token prediction and\nimage/video generation. A two-stage training recipe is designed to effectively\nlearn and scale to larger models. The resulting Show-o2 models demonstrate\nversatility in handling a wide range of multimodal understanding and generation\ntasks across diverse modalities, including text, images, and videos. Code and\nmodels are released at https://github.com/showlab/Show-o.",
        "url": "http://arxiv.org/abs/2506.15564v1",
        "published_date": "2025-06-18T15:39:15+00:00",
        "updated_date": "2025-06-18T15:39:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinheng Xie",
            "Zhenheng Yang",
            "Mike Zheng Shou"
        ],
        "tldr": "Show-o2 introduces improved unified multimodal models using autoregressive modeling and flow matching within a 3D causal VAE framework, achieving versatility in text, image, and video tasks.",
        "tldr_zh": "Show-o2 引入了改进的统一多模态模型，该模型在3D因果VAE框架中使用自回归建模和流匹配，从而在文本、图像和视频任务中实现了多功能性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]