[
    {
        "title": "UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation",
        "summary": "Unified multimodal large language models (MLLMs) have shown promise in\njointly advancing multimodal understanding and generation, with visual\ncodebooks discretizing images into tokens for autoregressive modeling. Existing\ncodebook-based methods either rely on small vocabularies (~16K entries) that\nlack fine-grained semantics or naively scale up, resulting in low token\nutilization and unstable training. We propose UniCode$^2$, a cascaded codebook\nframework enabling large-scale, semantically aligned, and stable visual\ntokenization. By clustering millions of SigLIP sequence embeddings, we build a\n500K-entry codebook that preserves vision-language alignment while expanding\ncapacity. Stability is ensured via a cascaded design: a frozen codebook anchors\nthe embedding space, and a trainable codebook refines task-specific semantics.\nThis decoupling promotes high utilization and robust learning. Moreover, the\nalignment of our visual tokens with textual semantics enables seamless\nintegration with pretrained diffusion decoders, supporting high-quality visual\nsynthesis with minimal adaptation. UniCode^2 delivers strong performance across\ndiverse benchmarks, demonstrating the viability of scaling visual token spaces\nwithout sacrificing stability, semantics, or modularity.",
        "url": "http://arxiv.org/abs/2506.20214v1",
        "published_date": "2025-06-25T07:57:09+00:00",
        "updated_date": "2025-06-25T07:57:09+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Yanzhe Chen",
            "Huasong Zhong",
            "Yan Li",
            "Zhenheng Yang"
        ],
        "tldr": "The paper introduces UniCode$^2$, a cascaded codebook framework for improved visual tokenization in multimodal LLMs, achieving stable training and high-quality visual synthesis using a large-scale, semantically aligned codebook.",
        "tldr_zh": "该论文介绍了UniCode$^2$，一个级联码本框架，用于改进多模态LLM中的视觉标记化，通过使用大规模、语义对齐的码本，实现稳定的训练和高质量的视觉合成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]