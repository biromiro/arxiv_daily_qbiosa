[
    {
        "title": "Mixture of Contexts for Long Video Generation",
        "summary": "Long video generation is fundamentally a long context memory problem: models\nmust retain and retrieve salient events across a long range without collapsing\nor drifting. However, scaling diffusion transformers to generate long-context\nvideos is fundamentally limited by the quadratic cost of self-attention, which\nmakes memory and computation intractable and difficult to optimize for long\nsequences. We recast long-context video generation as an internal information\nretrieval task and propose a simple, learnable sparse attention routing module,\nMixture of Contexts (MoC), as an effective long-term memory retrieval engine.\nIn MoC, each query dynamically selects a few informative chunks plus mandatory\nanchors (caption, local windows) to attend to, with causal routing that\nprevents loop closures. As we scale the data and gradually sparsify the\nrouting, the model allocates compute to salient history, preserving identities,\nactions, and scenes over minutes of content. Efficiency follows as a byproduct\nof retrieval (near-linear scaling), which enables practical training and\nsynthesis, and the emergence of memory and consistency at the scale of minutes.",
        "url": "http://arxiv.org/abs/2508.21058v1",
        "published_date": "2025-08-28T17:57:55+00:00",
        "updated_date": "2025-08-28T17:57:55+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Shengqu Cai",
            "Ceyuan Yang",
            "Lvmin Zhang",
            "Yuwei Guo",
            "Junfei Xiao",
            "Ziyan Yang",
            "Yinghao Xu",
            "Zhenheng Yang",
            "Alan Yuille",
            "Leonidas Guibas",
            "Maneesh Agrawala",
            "Lu Jiang",
            "Gordon Wetzstein"
        ],
        "tldr": "This paper introduces Mixture of Contexts (MoC), a sparse attention routing module, to address the quadratic cost of self-attention in long video generation, enabling the creation of longer, more consistent videos.",
        "tldr_zh": "本文介绍了一种稀疏注意力路由模块 Mixture of Contexts (MoC)，旨在解决长视频生成中自注意力的二次方成本问题，从而能够创建更长、更一致的视频。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models",
        "summary": "The field of video diffusion generation faces critical bottlenecks in\nsampling efficiency, especially for large-scale models and long sequences.\nExisting video acceleration methods adopt image-based techniques but suffer\nfrom fundamental limitations: they neither model the temporal coherence of\nvideo frames nor provide single-step distillation for large-scale video models.\nTo bridge this gap, we propose POSE (Phased One-Step Equilibrium), a\ndistillation framework that reduces the sampling steps of large-scale video\ndiffusion models, enabling the generation of high-quality videos in a single\nstep. POSE employs a carefully designed two-phase process to distill video\nmodels:(i) stability priming: a warm-up mechanism to stabilize adversarial\ndistillation that adapts the high-quality trajectory of the one-step generator\nfrom high to low signal-to-noise ratio regimes, optimizing the video quality of\nsingle-step mappings near the endpoints of flow trajectories. (ii) unified\nadversarial equilibrium: a flexible self-adversarial distillation mechanism\nthat promotes stable single-step adversarial training towards a Nash\nequilibrium within the Gaussian noise space, generating realistic single-step\nvideos close to real videos. For conditional video generation, we propose (iii)\nconditional adversarial consistency, a method to improve both semantic\nconsistency and frame consistency between conditional frames and generated\nframes. Comprehensive experiments demonstrate that POSE outperforms other\nacceleration methods on VBench-I2V by average 7.15% in semantic alignment,\ntemporal conference and frame quality, reducing the latency of the pre-trained\nmodel by 100$\\times$, from 1000 seconds to 10 seconds, while maintaining\ncompetitive performance.",
        "url": "http://arxiv.org/abs/2508.21019v1",
        "published_date": "2025-08-28T17:20:01+00:00",
        "updated_date": "2025-08-28T17:20:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaxiang Cheng",
            "Bing Ma",
            "Xuhua Ren",
            "Hongyi Jin",
            "Kai Yu",
            "Peng Zhang",
            "Wenyue Li",
            "Yuan Zhou",
            "Tianxiang Zheng",
            "Qinglin Lu"
        ],
        "tldr": "The paper introduces POSE, a novel distillation framework for video diffusion models that significantly reduces sampling steps and latency while maintaining high-quality video generation through a phased adversarial training approach.",
        "tldr_zh": "该论文介绍了POSE，一种用于视频扩散模型的新型提炼框架，通过分阶段的对抗训练方法，显著减少了采样步骤和延迟，同时保持了高质量的视频生成。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning",
        "summary": "In this paper, we introduce OneReward, a unified reinforcement learning\nframework that enhances the model's generative capabilities across multiple\ntasks under different evaluation criteria using only \\textit{One Reward} model.\nBy employing a single vision-language model (VLM) as the generative reward\nmodel, which can distinguish the winner and loser for a given task and a given\nevaluation criterion, it can be effectively applied to multi-task generation\nmodels, particularly in contexts with varied data and diverse task objectives.\nWe utilize OneReward for mask-guided image generation, which can be further\ndivided into several sub-tasks such as image fill, image extend, object\nremoval, and text rendering, involving a binary mask as the edit area. Although\nthese domain-specific tasks share same conditioning paradigm, they differ\nsignificantly in underlying data distributions and evaluation metrics. Existing\nmethods often rely on task-specific supervised fine-tuning (SFT), which limits\ngeneralization and training efficiency. Building on OneReward, we develop\nSeedream 3.0 Fill, a mask-guided generation model trained via multi-task\nreinforcement learning directly on a pre-trained base model, eliminating the\nneed for task-specific SFT. Experimental results demonstrate that our unified\nedit model consistently outperforms both commercial and open-source\ncompetitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across\nmultiple evaluation dimensions. Code and model are available at:\nhttps://one-reward.github.io",
        "url": "http://arxiv.org/abs/2508.21066v1",
        "published_date": "2025-08-28T17:59:46+00:00",
        "updated_date": "2025-08-28T17:59:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuan Gong",
            "Xionghui Wang",
            "Jie Wu",
            "Shiyin Wang",
            "Yitong Wang",
            "Xinglong Wu"
        ],
        "tldr": "The paper introduces OneReward, a unified reinforcement learning framework using a single vision-language model to improve multi-task mask-guided image generation without task-specific fine-tuning, achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了 OneReward，一个统一的强化学习框架，它使用单个视觉-语言模型来改进多任务掩码引导的图像生成，无需针对特定任务进行微调，并实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets",
        "summary": "Text-to-image diffusion models enable high-quality image generation but are\ncomputationally expensive. While prior work optimizes per-inference efficiency,\nwe explore an orthogonal approach: reducing redundancy across correlated\nprompts. Our method leverages the coarse-to-fine nature of diffusion models,\nwhere early denoising steps capture shared structures among similar prompts. We\npropose a training-free approach that clusters prompts based on semantic\nsimilarity and shares computation in early diffusion steps. Experiments show\nthat for models trained conditioned on image embeddings, our approach\nsignificantly reduces compute cost while improving image quality. By leveraging\nUnClip's text-to-image prior, we enhance diffusion step allocation for greater\nefficiency. Our method seamlessly integrates with existing pipelines, scales\nwith prompt sets, and reduces the environmental and financial burden of\nlarge-scale text-to-image generation. Project page:\nhttps://ddecatur.github.io/hierarchical-diffusion/",
        "url": "http://arxiv.org/abs/2508.21032v1",
        "published_date": "2025-08-28T17:35:03+00:00",
        "updated_date": "2025-08-28T17:35:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dale Decatur",
            "Thibault Groueix",
            "Wang Yifan",
            "Rana Hanocka",
            "Vladimir Kim",
            "Matheus Gadelha"
        ],
        "tldr": "This paper introduces a training-free method for efficient text-to-image diffusion by clustering similar prompts and sharing computation in early diffusion steps, leading to reduced compute cost and improved image quality, especially for models conditioned on image embeddings.",
        "tldr_zh": "该论文提出了一种无需训练的方法，通过对相似提示进行聚类并在早期扩散步骤中共享计算，来实现高效的文本到图像扩散，从而降低计算成本并提高图像质量，尤其适用于以图像嵌入为条件的模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DrivingGaussian++: Towards Realistic Reconstruction and Editable Simulation for Surrounding Dynamic Driving Scenes",
        "summary": "We present DrivingGaussian++, an efficient and effective framework for\nrealistic reconstructing and controllable editing of surrounding dynamic\nautonomous driving scenes. DrivingGaussian++ models the static background using\nincremental 3D Gaussians and reconstructs moving objects with a composite\ndynamic Gaussian graph, ensuring accurate positions and occlusions. By\nintegrating a LiDAR prior, it achieves detailed and consistent scene\nreconstruction, outperforming existing methods in dynamic scene reconstruction\nand photorealistic surround-view synthesis. DrivingGaussian++ supports\ntraining-free controllable editing for dynamic driving scenes, including\ntexture modification, weather simulation, and object manipulation, leveraging\nmulti-view images and depth priors. By integrating large language models (LLMs)\nand controllable editing, our method can automatically generate dynamic object\nmotion trajectories and enhance their realism during the optimization process.\nDrivingGaussian++ demonstrates consistent and realistic editing results and\ngenerates dynamic multi-view driving scenarios, while significantly enhancing\nscene diversity. More results and code can be found at the project site:\nhttps://xiong-creator.github.io/DrivingGaussian_plus.github.io",
        "url": "http://arxiv.org/abs/2508.20965v1",
        "published_date": "2025-08-28T16:22:54+00:00",
        "updated_date": "2025-08-28T16:22:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yajiao Xiong",
            "Xiaoyu Zhou",
            "Yongtao Wan",
            "Deqing Sun",
            "Ming-Hsuan Yang"
        ],
        "tldr": "DrivingGaussian++ introduces a novel framework for reconstructing and editing dynamic autonomous driving scenes using 3D Gaussians and LiDAR priors, enabling realistic scene synthesis and controllable editing.",
        "tldr_zh": "DrivingGaussian++提出了一种新颖的框架，利用3D高斯分布和激光雷达先验来重建和编辑动态自动驾驶场景，从而实现了逼真的场景合成和可控编辑。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI",
        "summary": "We introduce SAFEMax, a novel method for Machine Unlearning in diffusion\nmodels. Grounded in information-theoretic principles, SAFEMax maximizes the\nentropy in generated images, causing the model to generate Gaussian noise when\nconditioned on impermissible classes by ultimately halting its denoising\nprocess. Also, our method controls the balance between forgetting and retention\nby selectively focusing on the early diffusion steps, where class-specific\ninformation is prominent. Our results demonstrate the effectiveness of SAFEMax\nand highlight its substantial efficiency gains over state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2508.20773v1",
        "published_date": "2025-08-28T13:29:21+00:00",
        "updated_date": "2025-08-28T13:29:21+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Christoforos N. Spartalis",
            "Theodoros Semertzidis",
            "Petros Daras",
            "Efstratios Gavves"
        ],
        "tldr": "This paper introduces SAFEMax, a new machine unlearning method for diffusion models that maximizes entropy to selectively forget impermissible classes efficiently.",
        "tldr_zh": "本文介绍了一种名为SAFEMax 的新型扩散模型机器卸载方法，该方法通过最大化熵来高效地选择性遗忘不可接受的类别。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning",
        "summary": "Recent advancements highlight the importance of GRPO-based reinforcement\nlearning methods and benchmarking in enhancing text-to-image (T2I) generation.\nHowever, current methods using pointwise reward models (RM) for scoring\ngenerated images are susceptible to reward hacking. We reveal that this happens\nwhen minimal score differences between images are amplified after\nnormalization, creating illusory advantages that drive the model to\nover-optimize for trivial gains, ultimately destabilizing the image generation\nprocess. To address this, we propose Pref-GRPO, a pairwise preference\nreward-based GRPO method that shifts the optimization objective from score\nmaximization to preference fitting, ensuring more stable training. In\nPref-GRPO, images are pairwise compared within each group using preference RM,\nand the win rate is used as the reward signal. Extensive experiments\ndemonstrate that PREF-GRPO differentiates subtle image quality differences,\nproviding more stable advantages and mitigating reward hacking. Additionally,\nexisting T2I benchmarks are limited by coarse evaluation criteria, hindering\ncomprehensive model assessment. To solve this, we introduce UniGenBench, a\nunified T2I benchmark comprising 600 prompts across 5 main themes and 20\nsubthemes. It evaluates semantic consistency through 10 primary and 27\nsub-criteria, leveraging MLLM for benchmark construction and evaluation. Our\nbenchmarks uncover the strengths and weaknesses of both open and closed-source\nT2I models and validate the effectiveness of Pref-GRPO.",
        "url": "http://arxiv.org/abs/2508.20751v1",
        "published_date": "2025-08-28T13:11:24+00:00",
        "updated_date": "2025-08-28T13:11:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yibin Wang",
            "Zhimin Li",
            "Yuhang Zang",
            "Yujie Zhou",
            "Jiazi Bu",
            "Chunyu Wang",
            "Qinglin Lu",
            "Cheng Jin",
            "Jiaqi Wang"
        ],
        "tldr": "The paper introduces Pref-GRPO, a pairwise preference reward-based GRPO method for more stable text-to-image reinforcement learning, and UniGenBench, a new T2I benchmark for comprehensive model assessment.",
        "tldr_zh": "该论文介绍了Pref-GRPO，一种基于成对偏好奖励的GRPO方法，用于实现更稳定的文本到图像强化学习，以及UniGenBench，一个新的T2I基准测试，用于全面评估模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improving Alignment in LVLMs with Debiased Self-Judgment",
        "summary": "The rapid advancements in Large Language Models (LLMs) and Large\nVisual-Language Models (LVLMs) have opened up new opportunities for integrating\nvisual and linguistic modalities. However, effectively aligning these\nmodalities remains challenging, often leading to hallucinations--where\ngenerated outputs are not grounded in the visual input--and raising safety\nconcerns across various domains. Existing alignment methods, such as\ninstruction tuning and preference tuning, often rely on external datasets,\nhuman annotations, or complex post-processing, which limit scalability and\nincrease costs. To address these challenges, we propose a novel approach that\ngenerates the debiased self-judgment score, a self-evaluation metric created\ninternally by the model without relying on external resources. This enables the\nmodel to autonomously improve alignment. Our method enhances both decoding\nstrategies and preference tuning processes, resulting in reduced\nhallucinations, enhanced safety, and improved overall capability. Empirical\nresults show that our approach significantly outperforms traditional methods,\noffering a more effective solution for aligning LVLMs.",
        "url": "http://arxiv.org/abs/2508.20655v1",
        "published_date": "2025-08-28T11:01:33+00:00",
        "updated_date": "2025-08-28T11:01:33+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Sihan Yang",
            "Chenhang Cui",
            "Zihao Zhao",
            "Yiyang Zhou",
            "Weilong Yan",
            "Ying Wei",
            "Huaxiu Yao"
        ],
        "tldr": "This paper introduces a novel debiased self-judgment method for improving alignment in LVLMs, reducing hallucinations and enhancing safety without relying on external resources.",
        "tldr_zh": "该论文提出了一种新的去偏自我判断方法，用于提高大型视觉语言模型的对齐能力，减少幻觉并增强安全性，而无需依赖外部资源。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent",
        "summary": "Despite the progress in text-to-image generation, semantic image editing\nremains a challenge. Inversion-based algorithms unavoidably introduce\nreconstruction errors, while instruction-based models mainly suffer from\nlimited dataset quality and scale. To address these problems, we propose a\ndescriptive-prompt-based editing framework, named DescriptiveEdit. The core\nidea is to re-frame `instruction-based image editing' as `reference-image-based\ntext-to-image generation', which preserves the generative power of well-trained\nText-to-Image models without architectural modifications or inversion.\nSpecifically, taking the reference image and a prompt as input, we introduce a\nCross-Attentive UNet, which newly adds attention bridges to inject reference\nimage features into the prompt-to-edit-image generation process. Owing to its\ntext-to-image nature, DescriptiveEdit overcomes limitations in instruction\ndataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other\nextensions, and is more scalable. Experiments on the Emu Edit benchmark show it\nimproves editing accuracy and consistency.",
        "url": "http://arxiv.org/abs/2508.20505v1",
        "published_date": "2025-08-28T07:45:08+00:00",
        "updated_date": "2025-08-28T07:45:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "En Ci",
            "Shanyan Guan",
            "Yanhao Ge",
            "Yilin Zhang",
            "Wei Li",
            "Zhenyu Zhang",
            "Jian Yang",
            "Ying Tai"
        ],
        "tldr": "The paper introduces DescriptiveEdit, a descriptive-prompt-based image editing framework that leverages pre-trained text-to-image models by reframing instruction-based editing as reference-image-based generation. It improves editing accuracy and consistency by introducing a Cross-Attentive UNet.",
        "tldr_zh": "该论文介绍了一种名为DescriptiveEdit的基于描述性提示的图像编辑框架，通过将基于指令的编辑重新定义为基于参考图像的生成，从而利用预训练的文本到图像模型。它通过引入一个Cross-Attentive UNet来提高编辑的准确性和一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation",
        "summary": "Corner cases are crucial for training and validating autonomous driving\nsystems, yet collecting them from the real world is often costly and hazardous.\nEditing objects within captured sensor data offers an effective alternative for\ngenerating diverse scenarios, commonly achieved through 3D Gaussian Splatting\nor image generative models. However, these approaches often suffer from limited\nvisual fidelity or imprecise pose control. To address these issues, we propose\nG^2Editor, a framework designed for photorealistic and precise object editing\nin driving videos. Our method leverages a 3D Gaussian representation of the\nedited object as a dense prior, injected into the denoising process to ensure\naccurate pose control and spatial consistency. A scene-level 3D bounding box\nlayout is employed to reconstruct occluded areas of non-target objects.\nFurthermore, to guide the appearance details of the edited object, we\nincorporate hierarchical fine-grained features as additional conditions during\ngeneration. Experiments on the Waymo Open Dataset demonstrate that G^2Editor\neffectively supports object repositioning, insertion, and deletion within a\nunified framework, outperforming existing methods in both pose controllability\nand visual quality, while also benefiting downstream data-driven tasks.",
        "url": "http://arxiv.org/abs/2508.20471v1",
        "published_date": "2025-08-28T06:39:53+00:00",
        "updated_date": "2025-08-28T06:39:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiusi Li",
            "Jackson Jiang",
            "Jinyu Miao",
            "Miao Long",
            "Tuopu Wen",
            "Peijin Jia",
            "Shengxiang Liu",
            "Chunlei Yu",
            "Maolin Liu",
            "Yuzhan Cai",
            "Kun Jiang",
            "Mengmeng Yang",
            "Diange Yang"
        ],
        "tldr": "The paper introduces G^2Editor, a framework for photorealistic and controllable object editing in driving videos using 3D Gaussian representations and scene layout information, outperforming existing methods in pose controllability and visual quality.",
        "tldr_zh": "该论文介绍了G^2Editor，一个用于在驾驶视频中进行逼真且可控的物体编辑的框架，它利用3D高斯表示和场景布局信息，在姿态可控性和视觉质量上都优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation",
        "summary": "Scaling laws have validated the success and promise of large-data-trained\nmodels in creative generation across text, image, and video domains. However,\nthis paradigm faces data scarcity in the 3D domain, as there is far less of it\navailable on the internet compared to the aforementioned modalities.\nFortunately, there exist adequate videos that inherently contain commonsense\npriors, offering an alternative supervisory signal to mitigate the\ngeneralization bottleneck caused by limited native 3D data. On the one hand,\nvideos capturing multiple views of an object or scene provide a spatial\nconsistency prior for 3D generation. On the other hand, the rich semantic\ninformation contained within the videos enables the generated content to be\nmore faithful to the text prompts and semantically plausible. This paper\nexplores how to apply the video modality in 3D asset generation, spanning\ndatasets to models. We introduce Droplet3D-4M, the first large-scale video\ndataset with multi-view level annotations, and train Droplet3D, a generative\nmodel supporting both image and dense text input. Extensive experiments\nvalidate the effectiveness of our approach, demonstrating its ability to\nproduce spatially consistent and semantically plausible content. Moreover, in\ncontrast to the prevailing 3D solutions, our approach exhibits the potential\nfor extension to scene-level applications. This indicates that the commonsense\npriors from the videos significantly facilitate 3D creation. We have\nopen-sourced all resources including the dataset, code, technical framework,\nand model weights: https://dropletx.github.io/.",
        "url": "http://arxiv.org/abs/2508.20470v1",
        "published_date": "2025-08-28T06:39:41+00:00",
        "updated_date": "2025-08-28T06:39:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaochuan Li",
            "Guoguang Du",
            "Runze Zhang",
            "Liang Jin",
            "Qi Jia",
            "Lihua Lu",
            "Zhenhua Guo",
            "Yaqian Zhao",
            "Haiyang Liu",
            "Tianqi Wang",
            "Changsheng Li",
            "Xiaoli Gong",
            "Rengang Li",
            "Baoyu Fan"
        ],
        "tldr": "The paper introduces Droplet3D, a method for 3D asset generation using videos as commonsense priors, along with a new large-scale video dataset, Droplet3D-4M, demonstrating improved spatial consistency and semantic plausibility in generated 3D content.",
        "tldr_zh": "该论文介绍了Droplet3D，一种利用视频作为常识先验进行3D资产生成的方法，并发布了一个新的大型视频数据集Droplet3D-4M，展示了生成3D内容中更好的空间一致性和语义合理性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Audio-Guided Visual Editing with Complex Multi-Modal Prompts",
        "summary": "Visual editing with diffusion models has made significant progress but often\nstruggles with complex scenarios that textual guidance alone could not\nadequately describe, highlighting the need for additional non-text editing\nprompts. In this work, we introduce a novel audio-guided visual editing\nframework that can handle complex editing tasks with multiple text and audio\nprompts without requiring additional training. Existing audio-guided visual\nediting methods often necessitate training on specific datasets to align audio\nwith text, limiting their generalization to real-world situations. We leverage\na pre-trained multi-modal encoder with strong zero-shot capabilities and\nintegrate diverse audio into visual editing tasks, by alleviating the\ndiscrepancy between the audio encoder space and the diffusion model's prompt\nencoder space. Additionally, we propose a novel approach to handle complex\nscenarios with multiple and multi-modal editing prompts through our separate\nnoise branching and adaptive patch selection. Our comprehensive experiments on\ndiverse editing tasks demonstrate that our framework excels in handling\ncomplicated editing scenarios by incorporating rich information from audio,\nwhere text-only approaches fail.",
        "url": "http://arxiv.org/abs/2508.20379v1",
        "published_date": "2025-08-28T03:00:30+00:00",
        "updated_date": "2025-08-28T03:00:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hyeonyu Kim",
            "Seokhoon Jeong",
            "Seonghee Han",
            "Chanhyuk Choi",
            "Taehwan Kim"
        ],
        "tldr": "This paper introduces a novel audio-guided visual editing framework that leverages a pre-trained multi-modal encoder to handle complex editing tasks with multiple text and audio prompts, without requiring additional training, outperforming text-only approaches.",
        "tldr_zh": "本文介绍了一种新颖的音频引导视觉编辑框架，该框架利用预训练的多模态编码器来处理具有多个文本和音频提示的复杂编辑任务，无需额外训练，并且优于仅文本方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MobileCLIP2: Improving Multi-Modal Reinforced Training",
        "summary": "Foundation image-text models such as CLIP with zero-shot capabilities enable\na wide array of applications. MobileCLIP is a recent family of image-text\nmodels at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot\naccuracy. The main ingredients in MobileCLIP were its low-latency and light\narchitectures and a novel multi-modal reinforced training that made knowledge\ndistillation from multiple caption-generators and CLIP teachers efficient,\nscalable, and reproducible. In this paper, we improve the multi-modal\nreinforced training of MobileCLIP through: 1) better CLIP teacher ensembles\ntrained on the DFN dataset, 2) improved captioner teachers trained on the DFN\ndataset and fine-tuned on a diverse selection of high-quality image-caption\ndatasets. We discover new insights through ablations such as the importance of\ntemperature tuning in contrastive knowledge distillation, the effectiveness of\ncaption-generator fine-tuning for caption diversity, and the additive\nimprovement from combining synthetic captions generated by multiple models. We\ntrain a new family of models called MobileCLIP2 and achieve state-of-the-art\nImageNet-1k zero-shot accuracies at low latencies. In particular, we observe\n2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with\nMobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot\naccuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\\times$ smaller and\nimproves on DFN ViT-L/14 at 2.5$\\times$ lower latency. We release our\npretrained models (https://github.com/apple/ml-mobileclip) and the data\ngeneration code (https://github.com/apple/ml-mobileclip-dr). The data\ngeneration code makes it easy to create new reinforced datasets with arbitrary\nteachers using distributed scalable processing.",
        "url": "http://arxiv.org/abs/2508.20691v1",
        "published_date": "2025-08-28T11:50:22+00:00",
        "updated_date": "2025-08-28T11:50:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Fartash Faghri",
            "Pavan Kumar Anasosalu Vasu",
            "Cem Koc",
            "Vaishaal Shankar",
            "Alexander Toshev",
            "Oncel Tuzel",
            "Hadi Pouransari"
        ],
        "tldr": "The paper introduces MobileCLIP2, an improved version of MobileCLIP with better multi-modal reinforced training. It achieves state-of-the-art ImageNet-1k zero-shot accuracy at low latencies by enhancing teacher ensembles and captioners, and releases pretrained models and data generation code.",
        "tldr_zh": "该论文介绍了MobileCLIP2，一个通过改进多模态强化训练的MobileCLIP的优化版本。通过增强教师模型和captioner，它在低延迟下实现了最先进的ImageNet-1k零样本精度，并发布了预训练模型和数据生成代码。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Physics Informed Generative Models for Magnetic Field Images",
        "summary": "In semiconductor manufacturing, defect detection and localization are\ncritical to ensuring product quality and yield. While X-ray imaging is a\nreliable non-destructive testing method, it is memory-intensive and\ntime-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a\nmore efficient means to localize regions of interest (ROI) for targeted X-ray\nscanning. However, the limited availability of MFI datasets due to proprietary\nconcerns presents a significant bottleneck for training machine learning (ML)\nmodels using MFI. To address this challenge, we consider an ML-driven approach\nleveraging diffusion models with two physical constraints. We propose Physics\nInformed Generative Models for Magnetic Field Images (PI-GenMFI) to generate\nsynthetic MFI samples by integrating specific physical information. We generate\nMFI images for the most common defect types: power shorts. These synthetic\nimages will serve as training data for ML algorithms designed to localize\ndefect areas efficiently. To evaluate generated MFIs, we compare our model to\nSOTA generative models from both variational autoencoder (VAE) and diffusion\nmethods. We present a domain expert evaluation to assess the generated samples.\nIn addition, we present qualitative and quantitative evaluation using various\nmetrics used for image generation and signal processing, showing promising\nresults to optimize the defect localization process.",
        "url": "http://arxiv.org/abs/2508.20612v1",
        "published_date": "2025-08-28T10:00:23+00:00",
        "updated_date": "2025-08-28T10:00:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aye Phyu Phyu Aung",
            "Lucas Lum",
            "Zhansen Shi",
            "Wen Qiu",
            "Bernice Zee",
            "JM Chin",
            "Yeow Kheng Lim",
            "J. Senthilnath"
        ],
        "tldr": "The paper introduces Physics Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate synthetic MFI data for semiconductor defect localization, addressing the issue of limited real MFI datasets. It shows promising results in domain expert evaluation and quantitative metrics.",
        "tldr_zh": "该论文介绍了用于磁场图像的物理信息生成模型 (PI-GenMFI)，旨在生成用于半导体缺陷定位的合成 MFI 数据，解决了现实 MFI 数据集有限的问题。 领域专家评估和定量指标显示出令人鼓舞的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion",
        "summary": "Generating 3D human motions from text is a challenging yet valuable task. The\nkey aspects of this task are ensuring text-motion consistency and achieving\ngeneration diversity. Although recent advancements have enabled the generation\nof precise and high-quality human motions from text, achieving diversity in the\ngenerated motions remains a significant challenge. In this paper, we aim to\novercome the above challenge by designing a simple yet effective text-to-motion\ngeneration method, \\textit{i.e.}, Diverse-T2M. Our method introduces\nuncertainty into the generation process, enabling the generation of highly\ndiverse motions while preserving the semantic consistency of the text.\nSpecifically, we propose a novel perspective that utilizes noise signals as\ncarriers of diversity information in transformer-based methods, facilitating a\nexplicit modeling of uncertainty. Moreover, we construct a latent space where\ntext is projected into a continuous representation, instead of a rigid\none-to-one mapping, and integrate a latent space sampler to introduce\nstochastic sampling into the generation process, thereby enhancing the\ndiversity and uncertainty of the outputs. Our results on text-to-motion\ngeneration benchmark datasets~(HumanML3D and KIT-ML) demonstrate that our\nmethod significantly enhances diversity while maintaining state-of-the-art\nperformance in text consistency.",
        "url": "http://arxiv.org/abs/2508.20604v1",
        "published_date": "2025-08-28T09:49:27+00:00",
        "updated_date": "2025-08-28T09:49:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zheng Qin",
            "Yabing Wang",
            "Minghui Yang",
            "Sanping Zhou",
            "Ming Yang",
            "Le Wang"
        ],
        "tldr": "The paper introduces Diverse-T2M, a text-to-motion generation method that uses noise signals and latent space sampling to enhance diversity while maintaining text consistency, achieving state-of-the-art results on HumanML3D and KIT-ML datasets.",
        "tldr_zh": "该论文介绍了Diverse-T2M，一种文本到动作生成方法，该方法使用噪声信号和潜在空间采样来增强多样性，同时保持文本一致性，并在HumanML3D和KIT-ML数据集上实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP Generator",
        "summary": "Labeled handwriting data is often scarce, limiting the effectiveness of\nrecognition systems that require diverse, style-consistent training samples.\nHandwriting synthesis offers a promising solution by generating artificial data\nto augment training. However, current methods face two major limitations.\nFirst, most are built on conventional convolutional architectures, which\nstruggle to model long-range dependencies and complex stroke patterns. Second,\nthey largely ignore the crucial role of frequency information, which is\nessential for capturing fine-grained stylistic and structural details in\nhandwriting. To address these challenges, we propose FW-GAN, a one-shot\nhandwriting synthesis framework that generates realistic, writer-consistent\ntext from a single example. Our generator integrates a phase-aware Wave-MLP to\nbetter capture spatial relationships while preserving subtle stylistic cues. We\nfurther introduce a frequency-guided discriminator that leverages\nhigh-frequency components to enhance the authenticity detection of generated\nsamples. Additionally, we introduce a novel Frequency Distribution Loss that\naligns the frequency characteristics of synthetic and real handwriting, thereby\nenhancing visual fidelity. Experiments on Vietnamese and English handwriting\ndatasets demonstrate that FW-GAN generates high-quality, style-consistent\nhandwriting, making it a valuable tool for augmenting data in low-resource\nhandwriting recognition (HTR) pipelines. Official implementation is available\nat https://github.com/DAIR-Group/FW-GAN",
        "url": "http://arxiv.org/abs/2508.21040v1",
        "published_date": "2025-08-28T17:44:52+00:00",
        "updated_date": "2025-08-28T17:44:52+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Huynh Tong Dang Khoa",
            "Dang Hoai Nam",
            "Vo Nguyen Le Duy"
        ],
        "tldr": "FW-GAN introduces a novel handwriting synthesis framework using a Wave-MLP generator and frequency-guided discriminator, achieving high-quality, style-consistent handwriting generation for data augmentation in handwriting recognition.",
        "tldr_zh": "FW-GAN 提出了一种新的手写体合成框架，它利用 Wave-MLP 生成器和频率引导的判别器，实现了高质量、风格一致的手写体生成，用于手写体识别中的数据增强。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts",
        "summary": "Simulating detector responses is a crucial part of understanding the inner\nworkings of particle collisions in the Large Hadron Collider at CERN. Such\nsimulations are currently performed with statistical Monte Carlo methods, which\nare computationally expensive and put a significant strain on CERN's\ncomputational grid. Therefore, recent proposals advocate for generative machine\nlearning methods to enable more efficient simulations. However, the\ndistribution of the data varies significantly across the simulations, which is\nhard to capture with out-of-the-box methods. In this study, we present\nExpertSim - a deep learning simulation approach tailored for the Zero Degree\nCalorimeter in the ALICE experiment. Our method utilizes a\nMixture-of-Generative-Experts architecture, where each expert specializes in\nsimulating a different subset of the data. This allows for a more precise and\nefficient generation process, as each expert focuses on a specific aspect of\nthe calorimeter response. ExpertSim not only improves accuracy, but also\nprovides a significant speedup compared to the traditional Monte-Carlo methods,\noffering a promising solution for high-efficiency detector simulations in\nparticle physics experiments at CERN. We make the code available at\nhttps://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts.",
        "url": "http://arxiv.org/abs/2508.20991v1",
        "published_date": "2025-08-28T16:53:03+00:00",
        "updated_date": "2025-08-28T16:53:03+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Patryk Będkowski",
            "Jan Dubiński",
            "Filip Szatkowski",
            "Kamil Deja",
            "Przemysław Rokita",
            "Tomasz Trzciński"
        ],
        "tldr": "The paper introduces ExpertSim, a Mixture-of-Generative-Experts approach for fast and efficient particle detector simulation in the ALICE experiment at CERN, achieving significant speedups compared to traditional Monte Carlo methods.",
        "tldr_zh": "该论文介绍了ExpertSim，一种混合生成专家方法，用于CERN ALICE实验中快速高效的粒子探测器模拟，与传统的蒙特卡罗方法相比，实现了显著的加速。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 6
    },
    {
        "title": "Webly-Supervised Image Manipulation Localization via Category-Aware Auto-Annotation",
        "summary": "Images manipulated using image editing tools can mislead viewers and pose\nsignificant risks to social security. However, accurately localizing the\nmanipulated regions within an image remains a challenging problem. One of the\nmain barriers in this area is the high cost of data acquisition and the severe\nlack of high-quality annotated datasets. To address this challenge, we\nintroduce novel methods that mitigate data scarcity by leveraging readily\navailable web data. We utilize a large collection of manually forged images\nfrom the web, as well as automatically generated annotations derived from a\nsimpler auxiliary task, constrained image manipulation localization.\nSpecifically, we introduce a new paradigm CAAAv2, which automatically and\naccurately annotates manipulated regions at the pixel level. To further improve\nannotation quality, we propose a novel metric, QES, which filters out\nunreliable annotations. Through CAAA v2 and QES, we construct MIMLv2, a\nlarge-scale, diverse, and high-quality dataset containing 246,212 manually\nforged images with pixel-level mask annotations. This is over 120x larger than\nexisting handcrafted datasets like IMD20. Additionally, we introduce Object\nJitter, a technique that further enhances model training by generating\nhigh-quality manipulation artifacts. Building on these advances, we develop a\nnew model, Web-IML, designed to effectively leverage web-scale supervision for\nthe image manipulation localization task. Extensive experiments demonstrate\nthat our approach substantially alleviates the data scarcity problem and\nsignificantly improves the performance of various models on multiple real-world\nforgery benchmarks. With the proposed web supervision, Web-IML achieves a\nstriking performance gain of 31% and surpasses previous SOTA TruFor by 24.1\naverage IoU points. The dataset and code will be made publicly available at\nhttps://github.com/qcf-568/MIML.",
        "url": "http://arxiv.org/abs/2508.20987v1",
        "published_date": "2025-08-28T16:44:40+00:00",
        "updated_date": "2025-08-28T16:44:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenfan Qu",
            "Yiwu Zhong",
            "Bin Li",
            "Lianwen Jin"
        ],
        "tldr": "This paper introduces a webly-supervised approach, CAAAv2 and QES, to generate a large-scale dataset (MIMLv2) for image manipulation localization, and a novel model (Web-IML) leveraging this dataset, achieving significant performance improvements.",
        "tldr_zh": "本文提出了一种基于Web监督的方法CAAAv2和QES，用于生成大规模图像篡改定位数据集（MIMLv2），以及一种利用该数据集的新模型（Web-IML），实现了显著的性能提升。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting",
        "summary": "Generalizable Gaussian Splatting aims to synthesize novel views for unseen\nscenes without per-scene optimization. In particular, recent advancements\nutilize feed-forward networks to predict per-pixel Gaussian parameters,\nenabling high-quality synthesis from sparse input views. However, existing\napproaches fall short in encoding discriminative, multi-view consistent\nfeatures for Gaussian predictions, which struggle to construct accurate\ngeometry with sparse views. To address this, we propose $\\mathbf{C}^{3}$-GS, a\nframework that enhances feature learning by incorporating context-aware,\ncross-dimension, and cross-scale constraints. Our architecture integrates three\nlightweight modules into a unified rendering pipeline, improving feature fusion\nand enabling photorealistic synthesis without requiring additional supervision.\nExtensive experiments on benchmark datasets validate that $\\mathbf{C}^{3}$-GS\nachieves state-of-the-art rendering quality and generalization ability. Code is\navailable at: https://github.com/YuhsiHu/C3-GS.",
        "url": "http://arxiv.org/abs/2508.20754v1",
        "published_date": "2025-08-28T13:12:18+00:00",
        "updated_date": "2025-08-28T13:12:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuxi Hu",
            "Jun Zhang",
            "Kuangyi Chen",
            "Zhe Zhang",
            "Friedrich Fraundorfer"
        ],
        "tldr": "The paper introduces C3-GS, a novel framework for generalizable Gaussian Splatting that uses context-aware, cross-dimension, and cross-scale feature learning to improve novel view synthesis from sparse inputs, achieving state-of-the-art rendering quality and generalization ability.",
        "tldr_zh": "该论文介绍了一种名为C3-GS的新框架，用于可泛化的高斯溅射（Gaussian Splatting），它利用上下文感知、跨维度和跨尺度的特征学习来改进从稀疏输入的新视角合成，实现了最先进的渲染质量和泛化能力。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]