[
    {
        "title": "Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper",
        "summary": "Recent Video-to-Audio (V2A) generation relies on extracting semantic and\ntemporal features from video to condition generative models. Training these\nmodels from scratch is resource intensive. Consequently, leveraging foundation\nmodels (FMs) has gained traction due to their cross-modal knowledge transfer\nand generalization capabilities. One prior work has explored fine-tuning a\nlightweight mapper network to connect a pre-trained visual encoder with a\ntext-to-audio generation model for V2A. Inspired by this, we introduce the\nMultiple Foundation Model Mapper (MFM-Mapper). Compared to the previous mapper\napproach, MFM-Mapper benefits from richer semantic and temporal information by\nfusing features from dual visual encoders. Furthermore, by replacing a linear\nmapper with GPT-2, MFM-Mapper improves feature alignment, drawing parallels\nbetween cross-modal features mapping and autoregressive translation tasks. Our\nMFM-Mapper exhibits remarkable training efficiency. It achieves better\nperformance in semantic and temporal consistency with fewer training consuming,\nrequiring only 16\\% of the training scale compared to previous mapper-based\nwork, yet achieves competitive performance with models trained on a much larger\nscale.",
        "url": "http://arxiv.org/abs/2509.04957v1",
        "published_date": "2025-09-05T09:24:08+00:00",
        "updated_date": "2025-09-05T09:24:08+00:00",
        "categories": [
            "cs.CV",
            "cs.MM",
            "cs.SD",
            "eess.AS"
        ],
        "authors": [
            "Gehui Chen",
            "Guan'an Wang",
            "Xiaowen Huang",
            "Jitao Sang"
        ],
        "tldr": "The paper proposes an efficient video-to-audio generation method called MFM-Mapper, which leverages multiple foundation models and GPT-2 for improved feature alignment and training efficiency.",
        "tldr_zh": "该论文提出了一种高效的视频到音频生成方法MFM-Mapper，它利用多个基础模型和GPT-2来提高特征对齐和训练效率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous GPUs",
        "summary": "The escalating adoption of diffusion models for applications such as image\ngeneration demands efficient parallel inference techniques to manage their\nsubstantial computational cost. However, existing diffusion parallelism\ninference schemes often underutilize resources in heterogeneous multi-GPU\nenvironments, where varying hardware capabilities or background tasks cause\nworkload imbalance. This paper introduces Spatio-Temporal Adaptive Diffusion\nInference (STADI), a novel framework to accelerate diffusion model inference in\nsuch settings. At its core is a hybrid scheduler that orchestrates fine-grained\nparallelism across both temporal and spatial dimensions. Temporally, STADI\nintroduces a novel computation-aware step allocator applied after warmup\nphases, using a least-common-multiple-minimizing quantization technique to\nreduce denoising steps on slower GPUs and execution synchronization. To further\nminimize GPU idle periods, STADI executes an elastic patch parallelism\nmechanism that allocates variably sized image patches to GPUs according to\ntheir computational capability, ensuring balanced workload distribution through\na complementary spatial mechanism. Extensive experiments on both\nload-imbalanced and heterogeneous multi-GPU clusters validate STADI's efficacy,\ndemonstrating improved load balancing and mitigation of performance\nbottlenecks. Compared to patch parallelism, a state-of-the-art diffusion\ninference framework, our method significantly reduces end-to-end inference\nlatency by up to 45% and significantly improves resource utilization on\nheterogeneous GPUs.",
        "url": "http://arxiv.org/abs/2509.04719v1",
        "published_date": "2025-09-05T00:25:40+00:00",
        "updated_date": "2025-09-05T00:25:40+00:00",
        "categories": [
            "cs.DC",
            "cs.CV"
        ],
        "authors": [
            "Han Liang",
            "Jiahui Zhou",
            "Zicheng Zhou",
            "Xiaoxi Zhang",
            "Xu Chen"
        ],
        "tldr": "The paper introduces STADI, a novel framework for accelerating diffusion model inference in heterogeneous multi-GPU environments by using fine-grained spatiotemporal parallelism with a hybrid scheduler that orchestrates variable patch allocation.",
        "tldr_zh": "该论文介绍了 STADI，一种新的框架，通过细粒度的时空并行性和混合调度器协调可变补丁分配，从而加速异构多 GPU 环境中的扩散模型推理。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping",
        "summary": "Drag-based image editing has emerged as a powerful paradigm for intuitive\nimage manipulation. However, existing approaches predominantly rely on\nmanipulating the latent space of generative models, leading to limited\nprecision, delayed feedback, and model-specific constraints. Accordingly, we\npresent Inpaint4Drag, a novel framework that decomposes drag-based editing into\npixel-space bidirectional warping and image inpainting. Inspired by elastic\nobject deformation in the physical world, we treat image regions as deformable\nmaterials that maintain natural shape under user manipulation. Our method\nachieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at\n512x512 resolution, significantly improving the interaction experience compared\nto existing methods that require minutes per edit. By transforming drag inputs\ndirectly into standard inpainting formats, our approach serves as a universal\nadapter for any inpainting model without architecture modification,\nautomatically inheriting all future improvements in inpainting technology.\nExtensive experiments demonstrate that our method achieves superior visual\nquality and precise control while maintaining real-time performance. Project\npage: https://visual-ai.github.io/inpaint4drag/",
        "url": "http://arxiv.org/abs/2509.04582v1",
        "published_date": "2025-09-04T18:04:47+00:00",
        "updated_date": "2025-09-04T18:04:47+00:00",
        "categories": [
            "cs.CV",
            "I.3.6; I.3.3"
        ],
        "authors": [
            "Jingyi Lu",
            "Kai Han"
        ],
        "tldr": "The paper introduces Inpaint4Drag, a novel framework for drag-based image editing that uses bidirectional warping and image inpainting, achieving real-time performance and compatibility with any inpainting model.",
        "tldr_zh": "该论文介绍了一种名为 Inpaint4Drag 的新型拖拽式图像编辑框架，该框架采用双向扭曲和图像修复技术，实现了实时性能并与任何修复模型兼容。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation",
        "summary": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n$90\\times$ increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18",
        "url": "http://arxiv.org/abs/2509.05263v1",
        "published_date": "2025-09-05T17:22:33+00:00",
        "updated_date": "2025-09-05T17:22:33+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yinglin Duan",
            "Zhengxia Zou",
            "Tongwei Gu",
            "Wei Jia",
            "Zhan Zhao",
            "Luyi Xu",
            "Xinzhu Liu",
            "Hao Jiang",
            "Kang Chen",
            "Shuang Qiu"
        ],
        "tldr": "LatticeWorld is a framework using LLMs (LLaMA-2-7B) and industry-grade rendering engines (Unreal Engine 5) to generate interactive 3D worlds from text and visual instructions, demonstrating increased production efficiency with high fidelity.",
        "tldr_zh": "LatticeWorld是一个框架，使用LLM（LLaMA-2-7B）和工业级渲染引擎（Unreal Engine 5）通过文本和视觉指令生成交互式3D世界，并在保持高保真度的同时，显著提高了生产效率。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]