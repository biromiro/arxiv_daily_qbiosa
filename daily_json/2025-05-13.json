[
    {
        "title": "DanceGRPO: Unleashing GRPO on Visual Generation",
        "summary": "Recent breakthroughs in generative models-particularly diffusion models and\nrectified flows-have revolutionized visual content creation, yet aligning model\noutputs with human preferences remains a critical challenge. Existing\nreinforcement learning (RL)-based methods for visual generation face critical\nlimitations: incompatibility with modern Ordinary Differential Equations\n(ODEs)-based sampling paradigms, instability in large-scale training, and lack\nof validation for video generation. This paper introduces DanceGRPO, the first\nunified framework to adapt Group Relative Policy Optimization (GRPO) to visual\ngeneration paradigms, unleashing one unified RL algorithm across two generative\nparadigms (diffusion models and rectified flows), three tasks (text-to-image,\ntext-to-video, image-to-video), four foundation models (Stable Diffusion,\nHunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video\naesthetics, text-image alignment, video motion quality, and binary reward). To\nour knowledge, DanceGRPO is the first RL-based unified framework capable of\nseamless adaptation across diverse generative paradigms, tasks, foundational\nmodels, and reward models. DanceGRPO demonstrates consistent and substantial\nimprovements, which outperform baselines by up to 181% on benchmarks such as\nHPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can\nstabilize policy optimization for complex video generation, but also enables\ngenerative policy to better capture denoising trajectories for Best-of-N\ninference scaling and learn from sparse binary feedback. Our results establish\nDanceGRPO as a robust and versatile solution for scaling Reinforcement Learning\nfrom Human Feedback (RLHF) tasks in visual generation, offering new insights\ninto harmonizing reinforcement learning and visual synthesis. The code will be\nreleased.",
        "url": "http://arxiv.org/abs/2505.07818v1",
        "published_date": "2025-05-12T17:59:34+00:00",
        "updated_date": "2025-05-12T17:59:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeyue Xue",
            "Jie Wu",
            "Yu Gao",
            "Fangyuan Kong",
            "Lingting Zhu",
            "Mengzhao Chen",
            "Zhiheng Liu",
            "Wei Liu",
            "Qiushan Guo",
            "Weilin Huang",
            "Ping Luo"
        ],
        "tldr": "dancegrpo adapts group relative policy optimization (grpo) to visual generation, creating a unified rl framework applicable across diverse generative paradigms, tasks, models, and reward structures, showing significant performance improvements in text-to-image and video generation tasks.",
        "tldr_zh": "dancegrpo将group relative policy optimization (grpo)应用于视觉生成，创建了一个统一的rl框架，适用于各种生成范式、任务、模型和奖励结构，在文本到图像和视频生成任务中表现出显著的性能改进。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Generative Pre-trained Autoregressive Diffusion Transformer",
        "summary": "In this work, we present GPDiT, a Generative Pre-trained Autoregressive\nDiffusion Transformer that unifies the strengths of diffusion and\nautoregressive modeling for long-range video synthesis, within a continuous\nlatent space. Instead of predicting discrete tokens, GPDiT autoregressively\npredicts future latent frames using a diffusion loss, enabling natural modeling\nof motion dynamics and semantic consistency across frames. This continuous\nautoregressive framework not only enhances generation quality but also endows\nthe model with representation capabilities. Additionally, we introduce a\nlightweight causal attention variant and a parameter-free rotation-based\ntime-conditioning mechanism, improving both the training and inference\nefficiency. Extensive experiments demonstrate that GPDiT achieves strong\nperformance in video generation quality, video representation ability, and\nfew-shot learning tasks, highlighting its potential as an effective framework\nfor video modeling in continuous space.",
        "url": "http://arxiv.org/abs/2505.07344v1",
        "published_date": "2025-05-12T08:32:39+00:00",
        "updated_date": "2025-05-12T08:32:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuan Zhang",
            "Jiacheng Jiang",
            "Guoqing Ma",
            "Zhiying Lu",
            "Haoyang Huang",
            "Jianlong Yuan",
            "Nan Duan"
        ],
        "tldr": "gpdit combines diffusion and autoregressive modeling with a transformer architecture for improved long-range video synthesis in a continuous latent space, achieving strong performance in video generation, representation, and few-shot learning.",
        "tldr_zh": "gpdit结合了扩散模型和自回归模型，并采用transformer架构，在连续潜在空间中实现了改进的长程视频合成，在视频生成、表示和少样本学习方面表现出色。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Continuous Visual Autoregressive Generation via Score Maximization",
        "summary": "Conventional wisdom suggests that autoregressive models are used to process\ndiscrete data. When applied to continuous modalities such as visual data,\nVisual AutoRegressive modeling (VAR) typically resorts to quantization-based\napproaches to cast the data into a discrete space, which can introduce\nsignificant information loss. To tackle this issue, we introduce a Continuous\nVAR framework that enables direct visual autoregressive generation without\nvector quantization. The underlying theoretical foundation is strictly proper\nscoring rules, which provide powerful statistical tools capable of evaluating\nhow well a generative model approximates the true distribution. Within this\nframework, all we need is to select a strictly proper score and set it as the\ntraining objective to optimize. We primarily explore a class of training\nobjectives based on the energy score, which is likelihood-free and thus\novercomes the difficulty of making probabilistic predictions in the continuous\nspace. Previous efforts on continuous autoregressive generation, such as GIVT\nand diffusion loss, can also be derived from our framework using other strictly\nproper scores. Source code: https://github.com/shaochenze/EAR.",
        "url": "http://arxiv.org/abs/2505.07812v1",
        "published_date": "2025-05-12T17:58:14+00:00",
        "updated_date": "2025-05-12T17:58:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenze Shao",
            "Fandong Meng",
            "Jie Zhou"
        ],
        "tldr": "this paper introduces a continuous visual autoregressive (var) framework that directly generates visual data without quantization by using strictly proper scoring rules, particularly the energy score, as the training objective.",
        "tldr_zh": "本文介绍了一种连续视觉自回归（var）框架，该框架通过使用严格正确的评分规则（特别是能量得分）作为训练目标，直接生成视觉数据，而无需量化。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models",
        "summary": "Current diffusion-based text-to-video methods are limited to producing short\nvideo clips of a single shot and lack the capability to generate multi-shot\nvideos with discrete transitions where the same character performs distinct\nactivities across the same or different backgrounds. To address this limitation\nwe propose a framework that includes a dataset collection pipeline and\narchitectural extensions to video diffusion models to enable text-to-multi-shot\nvideo generation. Our approach enables generation of multi-shot videos as a\nsingle video with full attention across all frames of all shots, ensuring\ncharacter and background consistency, and allows users to control the number,\nduration, and content of shots through shot-specific conditioning. This is\nachieved by incorporating a transition token into the text-to-video model to\ncontrol at which frames a new shot begins and a local attention masking\nstrategy which controls the transition token's effect and allows shot-specific\nprompting. To obtain training data we propose a novel data collection pipeline\nto construct a multi-shot video dataset from existing single-shot video\ndatasets. Extensive experiments demonstrate that fine-tuning a pre-trained\ntext-to-video model for a few thousand iterations is enough for the model to\nsubsequently be able to generate multi-shot videos with shot-specific control,\noutperforming the baselines. You can find more details in\nhttps://shotadapter.github.io/",
        "url": "http://arxiv.org/abs/2505.07652v1",
        "published_date": "2025-05-12T15:22:28+00:00",
        "updated_date": "2025-05-12T15:22:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ozgur Kara",
            "Krishna Kumar Singh",
            "Feng Liu",
            "Duygu Ceylan",
            "James M. Rehg",
            "Tobias Hinz"
        ],
        "tldr": "the paper introduces shotadapter, a framework for generating multi-shot videos with diffusion models. it addresses the limitation of existing methods that only produce single-shot video clips by proposing a data collection pipeline and architectural extensions, enabling control over shot number, duration, and content.",
        "tldr_zh": "本文介绍了shotadapter，一个使用扩散模型生成多镜头视频的框架。它通过提出数据收集流程和架构扩展，解决了现有方法只能生成单镜头视频片段的局限性，从而能够控制镜头的数量、持续时间和内容。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unified Continuous Generative Models",
        "summary": "Recent advances in continuous generative models, including multi-step\napproaches like diffusion and flow-matching (typically requiring 8-1000\nsampling steps) and few-step methods such as consistency models (typically 1-8\nsteps), have demonstrated impressive generative performance. However, existing\nwork often treats these approaches as distinct paradigms, resulting in separate\ntraining and sampling methodologies. We introduce a unified framework for\ntraining, sampling, and analyzing these models. Our implementation, the Unified\nContinuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves\nstate-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a\n675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID\nin 20 steps and a few-step model reaching 1.42 FID in just 2 steps.\nAdditionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at\n250 steps) improves performance to 1.06 FID in only 40 steps. Code is available\nat: https://github.com/LINs-lab/UCGM.",
        "url": "http://arxiv.org/abs/2505.07447v1",
        "published_date": "2025-05-12T11:15:39+00:00",
        "updated_date": "2025-05-12T11:15:39+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Peng Sun",
            "Yi Jiang",
            "Tao Lin"
        ],
        "tldr": "the paper introduces a unified framework (ucgm) for training and sampling continuous generative models, achieving state-of-the-art performance in image generation by combining multi-step and few-step approaches.",
        "tldr_zh": "该论文介绍了一个统一的框架 (ucgm) 用于训练和采样连续生成模型，通过结合多步和少步方法，在图像生成方面实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]