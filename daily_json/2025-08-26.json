[
    {
        "title": "ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models",
        "summary": "3D inpainting often relies on multi-view 2D image inpainting, where the\ninherent inconsistencies across different inpainted views can result in blurred\ntextures, spatial discontinuities, and distracting visual artifacts. These\ninconsistencies pose significant challenges when striving for accurate and\nrealistic 3D object completion, particularly in applications that demand high\nfidelity and structural coherence. To overcome these limitations, we propose\nObjFiller-3D, a novel method designed for the completion and editing of\nhigh-quality and consistent 3D objects. Instead of employing a conventional 2D\nimage inpainting model, our approach leverages a curated selection of\nstate-of-the-art video editing model to fill in the masked regions of 3D\nobjects. We analyze the representation gap between 3D and videos, and propose\nan adaptation of a video inpainting model for 3D scene inpainting. In addition,\nwe introduce a reference-based 3D inpainting method to further enhance the\nquality of reconstruction. Experiments across diverse datasets show that\ncompared to previous methods, ObjFiller-3D produces more faithful and\nfine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of\n0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for\npractical deployment in real-world 3D editing applications. Project page:\nhttps://objfiller3d.github.io/ Code:\nhttps://github.com/objfiller3d/ObjFiller-3D .",
        "url": "http://arxiv.org/abs/2508.18271v1",
        "published_date": "2025-08-25T17:59:40+00:00",
        "updated_date": "2025-08-25T17:59:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haitang Feng",
            "Jie Liu",
            "Jie Tang",
            "Gangshan Wu",
            "Beiqi Chen",
            "Jianhuang Lai",
            "Guangcong Wang"
        ],
        "tldr": "ObjFiller-3D uses video diffusion models for consistent multi-view 3D inpainting, addressing inconsistencies found in traditional 2D-based methods and achieving superior reconstruction quality.",
        "tldr_zh": "ObjFiller-3D 利用视频扩散模型进行一致的多视角 3D 修复，解决了传统基于 2D 方法中存在的不一致性问题，并实现了卓越的重建质量。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency",
        "summary": "We introduce InternVL 3.5, a new family of open-source multimodal models that\nsignificantly advances versatility, reasoning capability, and inference\nefficiency along the InternVL series. A key innovation is the Cascade\nReinforcement Learning (Cascade RL) framework, which enhances reasoning through\na two-stage process: offline RL for stable convergence and online RL for\nrefined alignment. This coarse-to-fine training strategy leads to substantial\nimprovements on downstream reasoning tasks, e.g., MMMU and MathVista. To\noptimize efficiency, we propose a Visual Resolution Router (ViR) that\ndynamically adjusts the resolution of visual tokens without compromising\nperformance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)\nstrategy separates the vision encoder and language model across different GPUs,\neffectively balancing computational load. These contributions collectively\nenable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning\nperformance and a 4.05$\\times$ inference speedup compared to its predecessor,\ni.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as\nGUI interaction and embodied agency. Notably, our largest model, i.e.,\nInternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs\nacross general multimodal, reasoning, text, and agentic tasks -- narrowing the\nperformance gap with leading commercial models like GPT-5. All models and code\nare publicly released.",
        "url": "http://arxiv.org/abs/2508.18265v1",
        "published_date": "2025-08-25T17:58:17+00:00",
        "updated_date": "2025-08-25T17:58:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weiyun Wang",
            "Zhangwei Gao",
            "Lixin Gu",
            "Hengjun Pu",
            "Long Cui",
            "Xingguang Wei",
            "Zhaoyang Liu",
            "Linglin Jing",
            "Shenglong Ye",
            "Jie Shao",
            "Zhaokai Wang",
            "Zhe Chen",
            "Hongjie Zhang",
            "Ganlin Yang",
            "Haomin Wang",
            "Qi Wei",
            "Jinhui Yin",
            "Wenhao Li",
            "Erfei Cui",
            "Guanzhou Chen",
            "Zichen Ding",
            "Changyao Tian",
            "Zhenyu Wu",
            "Jingjing Xie",
            "Zehao Li",
            "Bowen Yang",
            "Yuchen Duan",
            "Xuehui Wang",
            "Songze Li",
            "Xiangyu Zhao",
            "Haodong Duan",
            "Nianchen Deng",
            "Bin Fu",
            "Yinan He",
            "Yi Wang",
            "Conghui He",
            "Botian Shi",
            "Junjun He",
            "Yingtong Xiong",
            "Han Lv",
            "Lijun Wu",
            "Wenqi Shao",
            "Kaipeng Zhang",
            "Huipeng Deng",
            "Biqing Qi",
            "Jiaye Ge",
            "Qipeng Guo",
            "Wenwei Zhang",
            "Wanli Ouyang",
            "Limin Wang",
            "Min Dou",
            "Xizhou Zhu",
            "Tong Lu",
            "Dahua Lin",
            "Jifeng Dai",
            "Bowen Zhou",
            "Weijie Su",
            "Kai Chen",
            "Yu Qiao",
            "Wenhai Wang",
            "Gen Luo"
        ],
        "tldr": "InternVL3.5 introduces a new open-source multimodal model family with advancements in reasoning, efficiency, and versatility, achieving state-of-the-art results among open-source models and closing the gap with commercial models like GPT-5.",
        "tldr_zh": "InternVL3.5 推出了一系列新的开源多模态模型，在推理、效率和通用性方面取得了进展，在开源模型中取得了最先进的成果，并缩小了与 GPT-5 等商业模型的差距。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "SpotEdit: Evaluating Visually-Guided Image Editing Methods",
        "summary": "Visually-guided image editing, where edits are conditioned on both visual\ncues and textual prompts, has emerged as a powerful paradigm for fine-grained,\ncontrollable content generation. Although recent generative models have shown\nremarkable capabilities, existing evaluations remain simple and insufficiently\nrepresentative of real-world editing challenges. We present SpotEdit, a\ncomprehensive benchmark designed to systematically assess visually-guided image\nediting methods across diverse diffusion, autoregressive, and hybrid generative\nmodels, uncovering substantial performance disparities. To address a critical\nyet underexplored challenge, our benchmark includes a dedicated component on\nhallucination, highlighting how leading models, such as GPT-4o, often\nhallucinate the existence of a visual cue and erroneously perform the editing\ntask. Our code and benchmark are publicly released at\nhttps://github.com/SaraGhazanfari/SpotEdit.",
        "url": "http://arxiv.org/abs/2508.18159v1",
        "published_date": "2025-08-25T16:08:57+00:00",
        "updated_date": "2025-08-25T16:08:57+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Sara Ghazanfari",
            "Wei-An Lin",
            "Haitong Tian",
            "Ersin Yumer"
        ],
        "tldr": "SpotEdit is a new benchmark designed to evaluate visually-guided image editing methods, revealing performance disparities and highlighting the issue of hallucination in current models like GPT-4o.",
        "tldr_zh": "SpotEdit是一个新的基准，旨在评估视觉引导的图像编辑方法，揭示了性能差异，并突出了GPT-4o等当前模型中存在的幻觉问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Incorporating Pre-trained Diffusion Models in Solving the Schrödinger Bridge Problem",
        "summary": "This paper aims to unify Score-based Generative Models (SGMs), also known as\nDiffusion models, and the Schr\\\"odinger Bridge (SB) problem through three\nreparameterization techniques: Iterative Proportional Mean-Matching (IPMM),\nIterative Proportional Terminus-Matching (IPTM), and Iterative Proportional\nFlow-Matching (IPFM). These techniques significantly accelerate and stabilize\nthe training of SB-based models. Furthermore, the paper introduces novel\ninitialization strategies that use pre-trained SGMs to effectively train\nSB-based models. By using SGMs as initialization, we leverage the advantages of\nboth SB-based models and SGMs, ensuring efficient training of SB-based models\nand further improving the performance of SGMs. Extensive experiments\ndemonstrate the significant effectiveness and improvements of the proposed\nmethods. We believe this work contributes to and paves the way for future\nresearch on generative models.",
        "url": "http://arxiv.org/abs/2508.18095v1",
        "published_date": "2025-08-25T14:56:16+00:00",
        "updated_date": "2025-08-25T14:56:16+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zhicong Tang",
            "Tiankai Hang",
            "Shuyang Gu",
            "Dong Chen",
            "Baining Guo"
        ],
        "tldr": "The paper introduces reparameterization and initialization techniques using pre-trained diffusion models to improve the training and performance of Schrödinger Bridge-based generative models, demonstrating significant effectiveness through experiments.",
        "tldr_zh": "该论文介绍了使用预训练扩散模型的重参数化和初始化技术，以改善Schrödinger Bridge生成模型的训练和性能，并通过实验证明了其显著有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation",
        "summary": "Despite the promising progress of recent autoregressive models in\ntext-to-image (T2I) generation, their ability to handle multi-attribute and\nambiguous prompts remains limited. To address these limitations, existing works\nhave applied chain-of-thought (CoT) to enable stage-aware visual synthesis and\nemployed reinforcement learning (RL) to improve reasoning capabilities.\nHowever, most models provide reward signals only at the end of the generation\nstage. This monolithic final-only guidance makes it difficult to identify which\nstages contribute positively to the final outcome and may lead to suboptimal\npolicies. To tackle this issue, we propose a Visual-Chain of Guidance\n(Visual-CoG) paradigm consisting of three stages: semantic reasoning, process\nrefining, and outcome evaluation, with stage-aware rewards providing immediate\nguidance throughout the image generation pipeline. We further construct a\nvisual cognition benchmark, VisCog-Bench, which comprises four subtasks to\nevaluate the effectiveness of semantic reasoning. Comprehensive evaluations on\nGenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%,\n5%, and 19%, respectively, demonstrating the superior performance of the\nproposed Visual-CoG. We will release all the resources soon.",
        "url": "http://arxiv.org/abs/2508.18032v1",
        "published_date": "2025-08-25T13:53:02+00:00",
        "updated_date": "2025-08-25T13:53:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yaqi Li",
            "Peng Chen",
            "Mingyang Han",
            "Bu Pi",
            "Haoxiang Shi",
            "Runzhou Zhao",
            "Yang Yao",
            "Xuan Zhang",
            "Jun Song"
        ],
        "tldr": "The paper introduces Visual-CoG, a stage-aware reinforcement learning approach for text-to-image generation that provides intermediate rewards to guide the generation process and achieves improvements on benchmark datasets.",
        "tldr_zh": "本文提出了一种名为Visual-CoG的阶段感知强化学习方法，用于文本到图像的生成。该方法通过提供中间奖励来指导生成过程，并在基准数据集上取得了改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniAPO: Unified Multimodal Automated Prompt Optimization",
        "summary": "Prompting is fundamental to unlocking the full potential of large language\nmodels. To automate and enhance this process, automatic prompt optimization\n(APO) has been developed, demonstrating effectiveness primarily in text-only\ninput scenarios. However, extending existing APO methods to multimodal tasks,\nsuch as video-language generation introduces two core challenges: (i) visual\ntoken inflation, where long visual token sequences restrict context capacity\nand result in insufficient feedback signals; (ii) a lack of process-level\nsupervision, as existing methods focus on outcome-level supervision and\noverlook intermediate supervision, limiting prompt optimization. We present\nUniAPO: Unified Multimodal Automated Prompt Optimization, the first framework\ntailored for multimodal APO. UniAPO adopts an EM-inspired optimization process\nthat decouples feedback modeling and prompt refinement, making the optimization\nmore stable and goal-driven. To further address the aforementioned challenges,\nwe introduce a short-long term memory mechanism: historical feedback mitigates\ncontext limitations, while historical prompts provide directional guidance for\neffective prompt optimization. UniAPO achieves consistent gains across text,\nimage, and video benchmarks, establishing a unified framework for efficient and\ntransferable prompt optimization.",
        "url": "http://arxiv.org/abs/2508.17890v1",
        "published_date": "2025-08-25T10:56:39+00:00",
        "updated_date": "2025-08-25T10:56:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qipeng Zhu",
            "Yanzhe Chen",
            "Huasong Zhong",
            "Yan Li",
            "Jie Chen",
            "Zhixin Zhang",
            "Junping Zhang",
            "Zhenheng Yang"
        ],
        "tldr": "UniAPO introduces a unified framework for multimodal automated prompt optimization, addressing challenges in visual token inflation and process-level supervision using an EM-inspired optimization process and a short-long term memory mechanism.",
        "tldr_zh": "UniAPO 提出了一个统一的多模态自动提示优化框架，利用 EM 启发的优化过程和短长期记忆机制，解决了视觉标记膨胀和过程级监督方面的挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation",
        "summary": "In Text-to-Image (T2I) generation, the complexity of entities and their\nintricate interactions pose a significant challenge for T2I method based on\ndiffusion model: how to effectively control entity and their interactions to\nproduce high-quality images. To address this, we propose CEIDM, a image\ngeneration method based on diffusion model with dual controls for entity and\ninteraction. First, we propose an entity interactive relationships mining\napproach based on Large Language Models (LLMs), extracting reasonable and rich\nimplicit interactive relationships through chain of thought to guide diffusion\nmodels to generate high-quality images that are closer to realistic logic and\nhave more reasonable interactive relationships. Furthermore, We propose an\ninteractive action clustering and offset method to cluster and offset the\ninteractive action features contained in each text prompts. By constructing\nglobal and local bidirectional offsets, we enhance semantic understanding and\ndetail supplementation of original actions, making the model's understanding of\nthe concept of interactive \"actions\" more accurate and generating images with\nmore accurate interactive actions. Finally, we design an entity control network\nwhich generates masks with entity semantic guidance, then leveraging\nmulti-scale convolutional network to enhance entity feature and dynamic network\nto fuse feature. It effectively controls entities and significantly improves\nimage quality. Experiments show that the proposed CEIDM method is better than\nthe most representative existing methods in both entity control and their\ninteraction control.",
        "url": "http://arxiv.org/abs/2508.17760v1",
        "published_date": "2025-08-25T07:58:57+00:00",
        "updated_date": "2025-08-25T07:58:57+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Mingyue Yang",
            "Dianxi Shi",
            "Jialu Zhou",
            "Xinyu Wei",
            "Leqian Li",
            "Shaowu Yang",
            "Chunping Qiu"
        ],
        "tldr": "The paper introduces CEIDM, a diffusion-based text-to-image generation method with dual controls for entities and their interactions, using LLMs for relationship mining and action clustering to enhance image quality and realism.",
        "tldr_zh": "该论文介绍了CEIDM，一种基于扩散模型的文本到图像生成方法，它具有实体及其交互的双重控制，利用大型语言模型进行关系挖掘和动作聚类，以提高图像质量和真实感。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Instant Preference Alignment for Text-to-Image Diffusion Models",
        "summary": "Text-to-image (T2I) generation has greatly enhanced creative expression, yet\nachieving preference-aligned generation in a real-time and training-free manner\nremains challenging. Previous methods often rely on static, pre-collected\npreferences or fine-tuning, limiting adaptability to evolving and nuanced user\nintents. In this paper, we highlight the need for instant preference-aligned\nT2I generation and propose a training-free framework grounded in multimodal\nlarge language model (MLLM) priors. Our framework decouples the task into two\ncomponents: preference understanding and preference-guided generation. For\npreference understanding, we leverage MLLMs to automatically extract global\npreference signals from a reference image and enrich a given prompt using\nstructured instruction design. Our approach supports broader and more\nfine-grained coverage of user preferences than existing methods. For\npreference-guided generation, we integrate global keyword-based control and\nlocal region-aware cross-attention modulation to steer the diffusion model\nwithout additional training, enabling precise alignment across both global\nattributes and local elements. The entire framework supports multi-round\ninteractive refinement, facilitating real-time and context-aware image\ngeneration. Extensive experiments on the Viper dataset and our collected\nbenchmark demonstrate that our method outperforms prior approaches in both\nquantitative metrics and human evaluations, and opens up new possibilities for\ndialog-based generation and MLLM-diffusion integration.",
        "url": "http://arxiv.org/abs/2508.17718v1",
        "published_date": "2025-08-25T06:51:15+00:00",
        "updated_date": "2025-08-25T06:51:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yang Li",
            "Songlin Yang",
            "Xiaoxuan Han",
            "Wei Wang",
            "Jing Dong",
            "Yueming Lyu",
            "Ziyu Xue"
        ],
        "tldr": "This paper introduces a training-free framework for instant preference-aligned text-to-image generation using multimodal large language models to understand preferences and guide diffusion models, enabling real-time interactive refinement.",
        "tldr_zh": "本文介绍了一种无需训练的框架，用于即时偏好对齐的文本到图像生成，该框架利用多模态大型语言模型来理解偏好并指导扩散模型，从而实现实时的交互式改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Language-Guided Temporal Token Pruning for Efficient VideoLLM Processing",
        "summary": "Vision Language Models (VLMs) struggle with long-form videos due to the\nquadratic complexity of attention mechanisms. We propose Language-Guided\nTemporal Token Pruning (LGTTP), which leverages temporal cues from queries to\nadaptively prune video tokens, preserving contextual continuity while reducing\ncomputational overhead. Unlike uniform pruning or keyframe selection, LGTTP\nretains higher token density in temporally relevant segments. Our\nmodel-agnostic framework integrates with TimeChat and LLaVA-Video, achieving a\n65% reduction in computation while preserving 97-99% of the original\nperformance. On QVHighlights, LGTTP improves HIT@1 by +9.5%, and on\nCharades-STA, it retains 99.6% of R@1. It excels on queries with explicit\ntemporal markers and remains effective across general video understanding\ntasks.",
        "url": "http://arxiv.org/abs/2508.17686v1",
        "published_date": "2025-08-25T05:51:21+00:00",
        "updated_date": "2025-08-25T05:51:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yogesh Kumar"
        ],
        "tldr": "The paper introduces Language-Guided Temporal Token Pruning (LGTTP) to reduce computational cost in VideoLLMs by adaptively pruning video tokens based on language queries, thus achieving significant computational savings while preserving performance.",
        "tldr_zh": "该论文提出了语言引导的时间令牌剪枝 (LGTTP)，通过根据语言查询自适应地剪枝视频令牌，从而降低 VideoLLM 的计算成本，并在保持性能的同时显著节省计算量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HERO: Hierarchical Extrapolation and Refresh for Efficient World Models",
        "summary": "Generation-driven world models create immersive virtual environments but\nsuffer slow inference due to the iterative nature of diffusion models. While\nrecent advances have improved diffusion model efficiency, directly applying\nthese techniques to world models introduces limitations such as quality\ndegradation. In this paper, we present HERO, a training-free hierarchical\nacceleration framework tailored for efficient world models. Owing to the\nmulti-modal nature of world models, we identify a feature coupling phenomenon,\nwherein shallow layers exhibit high temporal variability, while deeper layers\nyield more stable feature representations. Motivated by this, HERO adopts\nhierarchical strategies to accelerate inference: (i) In shallow layers, a\npatch-wise refresh mechanism efficiently selects tokens for recomputation. With\npatch-wise sampling and frequency-aware tracking, it avoids extra metric\ncomputation and remain compatible with FlashAttention. (ii) In deeper layers, a\nlinear extrapolation scheme directly estimates intermediate features. This\ncompletely bypasses the computations in attention modules and feed-forward\nnetworks. Our experiments show that HERO achieves a 1.73$\\times$ speedup with\nminimal quality degradation, significantly outperforming existing diffusion\nacceleration methods.",
        "url": "http://arxiv.org/abs/2508.17588v1",
        "published_date": "2025-08-25T01:22:15+00:00",
        "updated_date": "2025-08-25T01:22:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Quanjian Song",
            "Xinyu Wang",
            "Donghao Zhou",
            "Jingyu Lin",
            "Cunjian Chen",
            "Yue Ma",
            "Xiu Li"
        ],
        "tldr": "The paper introduces HERO, a training-free hierarchical acceleration framework for diffusion-based world models that leverages feature coupling to achieve a 1.73x speedup with minimal quality degradation.",
        "tldr_zh": "该论文介绍了HERO，一个无需训练的分层加速框架，用于基于扩散的世界模型。它利用特征耦合来实现1.73倍的加速，同时质量损失最小。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "HLG: Comprehensive 3D Room Construction via Hierarchical Layout Generation",
        "summary": "Realistic 3D indoor scene generation is crucial for virtual reality, interior\ndesign, embodied intelligence, and scene understanding. While existing methods\nhave made progress in coarse-scale furniture arrangement, they struggle to\ncapture fine-grained object placements, limiting the realism and utility of\ngenerated environments. This gap hinders immersive virtual experiences and\ndetailed scene comprehension for embodied AI applications. To address these\nissues, we propose Hierarchical Layout Generation (HLG), a novel method for\nfine-grained 3D scene generation. HLG is the first to adopt a coarse-to-fine\nhierarchical approach, refining scene layouts from large-scale furniture\nplacement to intricate object arrangements. Specifically, our fine-grained\nlayout alignment module constructs a hierarchical layout through vertical and\nhorizontal decoupling, effectively decomposing complex 3D indoor scenes into\nmultiple levels of granularity. Additionally, our trainable layout optimization\nnetwork addresses placement issues, such as incorrect positioning, orientation\nerrors, and object intersections, ensuring structurally coherent and physically\nplausible scene generation. We demonstrate the effectiveness of our approach\nthrough extensive experiments, showing superior performance in generating\nrealistic indoor scenes compared to existing methods. This work advances the\nfield of scene generation and opens new possibilities for applications\nrequiring detailed 3D environments. We will release our code upon publication\nto encourage future research.",
        "url": "http://arxiv.org/abs/2508.17832v1",
        "published_date": "2025-08-25T09:32:57+00:00",
        "updated_date": "2025-08-25T09:32:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiping Wang",
            "Yuxi Wang",
            "Mengqi Zhou",
            "Junsong Fan",
            "Zhaoxiang Zhang"
        ],
        "tldr": "The paper proposes Hierarchical Layout Generation (HLG), a novel coarse-to-fine approach for generating realistic 3D indoor scenes, addressing limitations in detail found in prior work.",
        "tldr_zh": "该论文提出了一种新颖的分层布局生成（HLG）方法，用于生成逼真的3D室内场景，解决了现有工作中细节不足的问题。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]