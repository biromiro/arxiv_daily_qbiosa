[
    {
        "title": "YoChameleon: Personalized Vision and Language Generation",
        "summary": "Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into\npowerful tools with millions of users. However, they remain generic models and\nlack personalized knowledge of specific user concepts. Previous work has\nexplored personalization for text generation, yet it remains unclear how these\nmethods can be adapted to new modalities, such as image generation. In this\npaper, we introduce Yo'Chameleon, the first attempt to study personalization\nfor large multimodal models. Given 3-5 images of a particular concept,\nYo'Chameleon leverages soft-prompt tuning to embed subject-specific information\nto (i) answer questions about the subject and (ii) recreate pixel-level details\nto produce images of the subject in new contexts. Yo'Chameleon is trained with\n(i) a self-prompting optimization mechanism to balance performance across\nmultiple modalities, and (ii) a ``soft-positive\" image generation approach to\nenhance image quality in a few-shot setting.",
        "url": "http://arxiv.org/abs/2504.20998v1",
        "published_date": "2025-04-29T17:59:57+00:00",
        "updated_date": "2025-04-29T17:59:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Thao Nguyen",
            "Krishna Kumar Singh",
            "Jing Shi",
            "Trung Bui",
            "Yong Jae Lee",
            "Yuheng Li"
        ],
        "tldr": "yo'chameleon introduces a method to personalize large multimodal models using soft-prompt tuning and few-shot learning to answer questions and generate images of specific subjects, addressing a gap in personalized multimodal generation.",
        "tldr_zh": "yo'chameleon 提出了一种使用软提示调整和少样本学习个性化大型多模态模型的方法，以回答问题并生成特定对象的图像，弥补了个性化多模态生成方面的空白。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models",
        "summary": "We propose X-Fusion, a framework that extends pretrained Large Language\nModels (LLMs) for multimodal tasks while preserving their language\ncapabilities. X-Fusion employs a dual-tower design with modality-specific\nweights, keeping the LLM's parameters frozen while integrating vision-specific\ninformation for both understanding and generation. Our experiments demonstrate\nthat X-Fusion consistently outperforms alternative architectures on both\nimage-to-text and text-to-image tasks. We find that incorporating\nunderstanding-focused data improves generation quality, reducing image data\nnoise enhances overall performance, and feature alignment accelerates\nconvergence for smaller models but has minimal impact on larger ones. Our\nfindings provide valuable insights into building efficient unified multimodal\nmodels.",
        "url": "http://arxiv.org/abs/2504.20996v1",
        "published_date": "2025-04-29T17:59:45+00:00",
        "updated_date": "2025-04-29T17:59:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sicheng Mo",
            "Thao Nguyen",
            "Xun Huang",
            "Siddharth Srinivasan Iyer",
            "Yijun Li",
            "Yuchen Liu",
            "Abhishek Tandon",
            "Eli Shechtman",
            "Krishna Kumar Singh",
            "Yong Jae Lee",
            "Bolei Zhou",
            "Yuheng Li"
        ],
        "tldr": "x-fusion is a framework that extends frozen llms for multimodal tasks using modality-specific weights, achieving state-of-the-art performance in image-to-text and text-to-image generation while preserving language capabilities. the paper gives valuable insights into efficiently unified multimodal models.",
        "tldr_zh": "x-fusion是一种框架，它使用特定于模态的权重来扩展冻结的llm，以用于多模态任务，在图像到文本和文本到图像生成中实现了最先进的性能，同时保留了语言能力。该论文为构建高效的统一多模态模型提供了宝贵的见解。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TesserAct: Learning 4D Embodied World Models",
        "summary": "This paper presents an effective approach for learning novel 4D embodied\nworld models, which predict the dynamic evolution of 3D scenes over time in\nresponse to an embodied agent's actions, providing both spatial and temporal\nconsistency. We propose to learn a 4D world model by training on RGB-DN (RGB,\nDepth, and Normal) videos. This not only surpasses traditional 2D models by\nincorporating detailed shape, configuration, and temporal changes into their\npredictions, but also allows us to effectively learn accurate inverse dynamic\nmodels for an embodied agent. Specifically, we first extend existing robotic\nmanipulation video datasets with depth and normal information leveraging\noff-the-shelf models. Next, we fine-tune a video generation model on this\nannotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for\neach frame. We then present an algorithm to directly convert generated RGB,\nDepth, and Normal videos into a high-quality 4D scene of the world. Our method\nensures temporal and spatial coherence in 4D scene predictions from embodied\nscenarios, enables novel view synthesis for embodied environments, and\nfacilitates policy learning that significantly outperforms those derived from\nprior video-based world models.",
        "url": "http://arxiv.org/abs/2504.20995v1",
        "published_date": "2025-04-29T17:59:30+00:00",
        "updated_date": "2025-04-29T17:59:30+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Haoyu Zhen",
            "Qiao Sun",
            "Hongxin Zhang",
            "Junyan Li",
            "Siyuan Zhou",
            "Yilun Du",
            "Chuang Gan"
        ],
        "tldr": "the paper introduces tesseract, a method for learning 4d embodied world models from rgb-dn videos, which enables improved spatial and temporal consistency compared to 2d models and facilitates better policy learning.",
        "tldr_zh": "该论文介绍了 tesseract，一种通过 rgb-dn 视频学习 4d 具身世界模型的方法，与 2d 模型相比，它能够提高空间和时间一致性，并促进更好的策略学习。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities",
        "summary": "Retrieval-Augmented Generation (RAG) has shown substantial promise in\nimproving factual accuracy by grounding model responses with external knowledge\nrelevant to queries. However, most existing RAG approaches are limited to a\ntext-only corpus, and while recent efforts have extended RAG to other\nmodalities such as images and videos, they typically operate over a single\nmodality-specific corpus. In contrast, real-world queries vary widely in the\ntype of knowledge they require, which a single type of knowledge source cannot\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\ndiverse modalities and granularities. Specifically, motivated by the\nobservation that forcing all modalities into a unified representation space\nderived from a single combined corpus causes a modality gap, where the\nretrieval tends to favor items from the same modality as the query, we propose\na modality-aware routing mechanism that dynamically identifies the most\nappropriate modality-specific corpus and performs targeted retrieval within it.\nAlso, beyond modality, we organize each modality into multiple granularity\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\nmodalities, showing its superiority over modality-specific and unified\nbaselines.",
        "url": "http://arxiv.org/abs/2504.20734v1",
        "published_date": "2025-04-29T13:18:58+00:00",
        "updated_date": "2025-04-29T13:18:58+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.IR",
            "cs.LG"
        ],
        "authors": [
            "Woongyeong Yeo",
            "Kangsan Kim",
            "Soyeong Jeong",
            "Jinheon Baek",
            "Sung Ju Hwang"
        ],
        "tldr": "the paper introduces universalrag, a retrieval-augmented generation framework that retrieves knowledge from diverse modalities and granularities, addressing the limitations of existing rag approaches that typically focus on a single modality. it uses a modality-aware routing mechanism and granularity levels for fine-tuned retrieval.",
        "tldr_zh": "该论文介绍了universalrag，一个检索增强生成框架，可以从不同的模态和粒度中检索知识，解决了现有rag方法通常只关注单一模态的局限性。它使用了一种模态感知的路由机制和粒度级别，以实现细粒度的检索。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer",
        "summary": "Instruction-based image editing enables robust image modification via natural\nlanguage prompts, yet current methods face a precision-efficiency tradeoff.\nFine-tuning methods demand significant computational resources and large\ndatasets, while training-free techniques struggle with instruction\ncomprehension and edit quality. We resolve this dilemma by leveraging\nlarge-scale Diffusion Transformer (DiT)' enhanced generation capacity and\nnative contextual awareness. Our solution introduces three contributions: (1)\nan in-context editing framework for zero-shot instruction compliance using\nin-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning\nstrategy that enhances flexibility with efficient adaptation and dynamic expert\nrouting, without extensive retraining; and (3) an early filter inference-time\nscaling method using vision-language models (VLMs) to select better initial\nnoise early, improving edit quality. Extensive evaluations demonstrate our\nmethod's superiority: it outperforms state-of-the-art approaches while\nrequiring only 0.5% training data and 1% trainable parameters compared to\nconventional baselines. This work establishes a new paradigm that enables\nhigh-precision yet efficient instruction-guided editing. Codes and demos can be\nfound in https://river-zhang.github.io/ICEdit-gh-pages/.",
        "url": "http://arxiv.org/abs/2504.20690v1",
        "published_date": "2025-04-29T12:14:47+00:00",
        "updated_date": "2025-04-29T12:14:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zechuan Zhang",
            "Ji Xie",
            "Yu Lu",
            "Zongxin Yang",
            "Yi Yang"
        ],
        "tldr": "this paper introduces an efficient and precise instruction-based image editing method called in-context edit, which leverages a diffusion transformer with in-context prompting, lora-moe tuning, and early filter inference-time scaling to outperform existing methods with significantly less training data and parameters.",
        "tldr_zh": "本文介绍了一种高效且精确的基于指令的图像编辑方法，名为 in-context edit。该方法利用扩散transformer（dit），通过上下文提示、lora-moe tuning 以及 early filter inference-time scaling，能够在显著减少训练数据和参数的情况下，超越现有的图像编辑方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Listener: Dyadic Facial Motion Synthesis via Action Diffusion",
        "summary": "Generating realistic listener facial motions in dyadic conversations remains\nchallenging due to the high-dimensional action space and temporal dependency\nrequirements. Existing approaches usually consider extracting 3D Morphable\nModel (3DMM) coefficients and modeling in the 3DMM space. However, this makes\nthe computational speed of the 3DMM a bottleneck, making it difficult to\nachieve real-time interactive responses. To tackle this problem, we propose\nFacial Action Diffusion (FAD), which introduces the diffusion methods from the\nfield of image generation to achieve efficient facial action generation. We\nfurther build the Efficient Listener Network (ELNet) specially designed to\naccommodate both the visual and audio information of the speaker as input.\nConsidering of FAD and ELNet, the proposed method learns effective listener\nfacial motion representations and leads to improvements of performance over the\nstate-of-the-art methods while reducing 99% computational time.",
        "url": "http://arxiv.org/abs/2504.20685v1",
        "published_date": "2025-04-29T12:08:02+00:00",
        "updated_date": "2025-04-29T12:08:02+00:00",
        "categories": [
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Zesheng Wang",
            "Alexandre Bruckert",
            "Patrick Le Callet",
            "Guangtao Zhai"
        ],
        "tldr": "the paper introduces facial action diffusion (fad) and efficient listener network (elnet) for real-time listener facial motion generation in dyadic conversations, achieving significant speed improvements compared to existing 3dmm-based methods.",
        "tldr_zh": "该论文介绍了面部动作扩散（fad）和高效监听器网络（elnet），用于在二元对话中生成实时监听者面部动作，与现有的基于3dmm的方法相比，实现了显著的速度提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LDPoly: Latent Diffusion for Polygonal Road Outline Extraction in Large-Scale Topographic Mapping",
        "summary": "Polygonal road outline extraction from high-resolution aerial images is an\nimportant task in large-scale topographic mapping, where roads are represented\nas vectorized polygons, capturing essential geometric features with minimal\nvertex redundancy. Despite its importance, no existing method has been\nexplicitly designed for this task. While polygonal building outline extraction\nhas been extensively studied, the unique characteristics of roads, such as\nbranching structures and topological connectivity, pose challenges to these\nmethods. To address this gap, we introduce LDPoly, the first dedicated\nframework for extracting polygonal road outlines from high-resolution aerial\nimages. Our method leverages a novel Dual-Latent Diffusion Model with a\nChannel-Embedded Fusion Module, enabling the model to simultaneously generate\nroad masks and vertex heatmaps. A tailored polygonization method is then\napplied to obtain accurate vectorized road polygons with minimal vertex\nredundancy. We evaluate LDPoly on a new benchmark dataset, Map2ImLas, which\ncontains detailed polygonal annotations for various topographic objects in\nseveral Dutch regions. Our experiments include both in-region and cross-region\nevaluations, with the latter designed to assess the model's generalization\nperformance on unseen regions. Quantitative and qualitative results demonstrate\nthat LDPoly outperforms state-of-the-art polygon extraction methods across\nvarious metrics, including pixel-level coverage, vertex efficiency, polygon\nregularity, and road connectivity. We also design two new metrics to assess\npolygon simplicity and boundary smoothness. Moreover, this work represents the\nfirst application of diffusion models for extracting precise vectorized object\noutlines without redundant vertices from remote-sensing imagery, paving the way\nfor future advancements in this field.",
        "url": "http://arxiv.org/abs/2504.20645v1",
        "published_date": "2025-04-29T11:13:33+00:00",
        "updated_date": "2025-04-29T11:13:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weiqin Jiao",
            "Hao Cheng",
            "George Vosselman",
            "Claudio Persello"
        ],
        "tldr": "the paper introduces ldpoly, a novel dual-latent diffusion model for extracting polygonal road outlines from aerial images, demonstrating superior performance on a new benchmark dataset. it pioneers the use of diffusion models for precise vectorized object outline extraction in remote sensing.",
        "tldr_zh": "该论文介绍了ldpoly，一种新型双潜变量扩散模型，用于从航空图像中提取多边形道路轮廓，并在新的基准数据集上展示了卓越的性能。它开创了使用扩散模型从遥感图像中提取精确矢量化对象轮廓的先河。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models",
        "summary": "Recent studies have revealed that text-to-image diffusion models are\nvulnerable to backdoor attacks, where attackers implant stealthy textual\ntriggers to manipulate model outputs. Previous backdoor detection methods\nprimarily focus on the static features of backdoor samples. However, a vital\nproperty of diffusion models is their inherent dynamism. This study introduces\na novel backdoor detection perspective named Dynamic Attention Analysis (DAA),\nshowing that these dynamic characteristics serve as better indicators for\nbackdoor detection. Specifically, by examining the dynamic evolution of\ncross-attention maps, we observe that backdoor samples exhibit distinct feature\nevolution patterns at the $<$EOS$>$ token compared to benign samples. To\nquantify these dynamic anomalies, we first introduce DAA-I, which treats the\ntokens' attention maps as spatially independent and measures dynamic feature\nusing the Frobenius norm. Furthermore, to better capture the interactions\nbetween attention maps and refine the feature, we propose a dynamical\nsystem-based approach, referred to as DAA-S. This model formulates the spatial\ncorrelations among attention maps using a graph-based state equation and we\ntheoretically analyze the global asymptotic stability of this method. Extensive\nexperiments across five representative backdoor attack scenarios demonstrate\nthat our approach significantly surpasses existing detection methods, achieving\nan average F1 Score of 79.49% and an AUC of 87.67%. The code is available at\nhttps://github.com/Robin-WZQ/DAA.",
        "url": "http://arxiv.org/abs/2504.20518v1",
        "published_date": "2025-04-29T07:59:35+00:00",
        "updated_date": "2025-04-29T07:59:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhongqi Wang",
            "Jie Zhang",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "tldr": "this paper introduces a novel approach, dynamic attention analysis (daa), for detecting backdoor attacks in text-to-image diffusion models by analyzing the dynamic evolution of cross-attention maps. experiments show significant improvement over existing methods.",
        "tldr_zh": "本文提出了一种名为动态注意力分析 (daa) 的新方法，通过分析交叉注意力图的动态演变来检测文本到图像扩散模型中的后门攻击。实验表明，该方法比现有方法有显著改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LMM4Gen3DHF: Benchmarking and Evaluating Multimodal 3D Human Face Generation with LMMs",
        "summary": "The rapid advancement in generative artificial intelligence have enabled the\ncreation of 3D human faces (HFs) for applications including media production,\nvirtual reality, security, healthcare, and game development, etc. However,\nassessing the quality and realism of these AI-generated 3D human faces remains\na significant challenge due to the subjective nature of human perception and\ninnate perceptual sensitivity to facial features. To this end, we conduct a\ncomprehensive study on the quality assessment of AI-generated 3D human faces.\nWe first introduce Gen3DHF, a large-scale benchmark comprising 2,000 videos of\nAI-Generated 3D Human Faces along with 4,000 Mean Opinion Scores (MOS)\ncollected across two dimensions, i.e., quality and authenticity, 2,000\ndistortion-aware saliency maps and distortion descriptions. Based on Gen3DHF,\nwe propose LMME3DHF, a Large Multimodal Model (LMM)-based metric for Evaluating\n3DHF capable of quality and authenticity score prediction, distortion-aware\nvisual question answering, and distortion-aware saliency prediction.\nExperimental results show that LMME3DHF achieves state-of-the-art performance,\nsurpassing existing methods in both accurately predicting quality scores for\nAI-generated 3D human faces and effectively identifying distortion-aware\nsalient regions and distortion types, while maintaining strong alignment with\nhuman perceptual judgments. Both the Gen3DHF database and the LMME3DHF will be\nreleased upon the publication.",
        "url": "http://arxiv.org/abs/2504.20466v1",
        "published_date": "2025-04-29T07:00:06+00:00",
        "updated_date": "2025-04-29T07:00:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Woo Yi Yang",
            "Jiarui Wang",
            "Sijing Wu",
            "Huiyu Duan",
            "Yuxin Zhu",
            "Liu Yang",
            "Kang Fu",
            "Guangtao Zhai",
            "Xiongkuo Min"
        ],
        "tldr": "the paper introduces a new large-scale benchmark (gen3dhf) for evaluating ai-generated 3d human faces and proposes a large multimodal model (lmme3dhf) for assessing quality and authenticity, demonstrating state-of-the-art performance.",
        "tldr_zh": "该论文介绍了一个新的大规模基准测试（gen3dhf），用于评估人工智能生成的3d人脸，并提出了一个大型多模态模型（lmme3dhf）来评估质量和真实性，展示了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Inception: Jailbreak the Memory Mechanism of Text-to-Image Generation Systems",
        "summary": "Currently, the memory mechanism has been widely and successfully exploited in\nonline text-to-image (T2I) generation systems ($e.g.$, DALL$\\cdot$E 3) for\nalleviating the growing tokenization burden and capturing key information in\nmulti-turn interactions. Despite its practicality, its security analyses have\nfallen far behind. In this paper, we reveal that this mechanism exacerbates the\nrisk of jailbreak attacks. Different from previous attacks that fuse the unsafe\ntarget prompt into one ultimate adversarial prompt, which can be easily\ndetected or may generate non-unsafe images due to under- or over-optimization,\nwe propose Inception, the first multi-turn jailbreak attack against the memory\nmechanism in real-world text-to-image generation systems. Inception embeds the\nmalice at the inception of the chat session turn by turn, leveraging the\nmechanism that T2I generation systems retrieve key information in their memory.\nSpecifically, Inception mainly consists of two modules. It first segments the\nunsafe prompt into chunks, which are subsequently fed to the system in multiple\nturns, serving as pseudo-gradients for directive optimization. Specifically, we\ndevelop a series of segmentation policies that ensure the images generated are\nsemantically consistent with the target prompt. Secondly, after segmentation,\nto overcome the challenge of the inseparability of minimum unsafe words, we\npropose recursion, a strategy that makes minimum unsafe words subdivisible.\nCollectively, segmentation and recursion ensure that all the request prompts\nare benign but can lead to malicious outcomes. We conduct experiments on the\nreal-world text-to-image generation system ($i.e.$, DALL$\\cdot$E 3) to validate\nthe effectiveness of Inception. The results indicate that Inception surpasses\nthe state-of-the-art by a 14\\% margin in attack success rate.",
        "url": "http://arxiv.org/abs/2504.20376v1",
        "published_date": "2025-04-29T02:40:36+00:00",
        "updated_date": "2025-04-29T02:40:36+00:00",
        "categories": [
            "cs.CV",
            "cs.CR"
        ],
        "authors": [
            "Shiqian Zhao",
            "Jiayang Liu",
            "Yiming Li",
            "Runyi Hu",
            "Xiaojun Jia",
            "Wenshu Fan",
            "Xinfeng Li",
            "Jie Zhang",
            "Wei Dong",
            "Tianwei Zhang",
            "Luu Anh Tuan"
        ],
        "tldr": "the paper introduces 'inception,' a novel multi-turn jailbreak attack against memory mechanisms in text-to-image generation systems (dall·e 3), achieving a 14% improvement in attack success rate compared to state-of-the-art methods.",
        "tldr_zh": "本文介绍了一种名为“inception”的新型多轮越狱攻击，针对文本到图像生成系统（dall·e 3）中的记忆机制，与现有技术相比，攻击成功率提高了 14%。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Picture is Worth a Thousand Prompts? Efficacy of Iterative Human-Driven Prompt Refinement in Image Regeneration Tasks",
        "summary": "With AI-generated content becoming ubiquitous across the web, social media,\nand other digital platforms, it is vital to examine how such content are\ninspired and generated. The creation of AI-generated images often involves\nrefining the input prompt iteratively to achieve desired visual outcomes. This\nstudy focuses on the relatively underexplored concept of image regeneration\nusing AI, in which a human operator attempts to closely recreate a specific\ntarget image by iteratively refining their prompt. Image regeneration is\ndistinct from normal image generation, which lacks any predefined visual\nreference. A separate challenge lies in determining whether existing image\nsimilarity metrics (ISMs) can provide reliable, objective feedback in iterative\nworkflows, given that we do not fully understand if subjective human judgments\nof similarity align with these metrics. Consequently, we must first validate\ntheir alignment with human perception before assessing their potential as a\nfeedback mechanism in the iterative prompt refinement process. To address these\nresearch gaps, we present a structured user study evaluating how iterative\nprompt refinement affects the similarity of regenerated images relative to\ntheir targets, while also examining whether ISMs capture the same improvements\nperceived by human observers. Our findings suggest that incremental prompt\nadjustments substantially improve alignment, verified through both subjective\nevaluations and quantitative measures, underscoring the broader potential of\niterative workflows to enhance generative AI content creation across various\napplication domains.",
        "url": "http://arxiv.org/abs/2504.20340v1",
        "published_date": "2025-04-29T01:21:16+00:00",
        "updated_date": "2025-04-29T01:21:16+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Khoi Trinh",
            "Scott Seidenberger",
            "Raveen Wijewickrama",
            "Murtuza Jadliwala",
            "Anindya Maiti"
        ],
        "tldr": "this paper investigates iterative prompt refinement for ai image regeneration (recreating a target image) and evaluates the alignment between image similarity metrics (isms) and human perception of similarity improvements during this process, finding that iterative refinement improves alignment based on both subjective and quantitative measures.",
        "tldr_zh": "该论文研究了ai图像再生（重新创建目标图像）的迭代提示细化方法，并评估了图像相似性指标（ism）与人类对迭代过程中相似性提高的感知之间的一致性，发现基于主观和定量指标，迭代细化都能改进一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Image Interpolation with Score-based Riemannian Metrics of Diffusion Models",
        "summary": "Diffusion models excel in content generation by implicitly learning the data\nmanifold, yet they lack a practical method to leverage this manifold - unlike\nother deep generative models equipped with latent spaces. This paper introduces\na novel framework that treats the data space of pre-trained diffusion models as\na Riemannian manifold, with a metric derived from the score function.\nExperiments with MNIST and Stable Diffusion show that this geometry-aware\napproach yields image interpolations that are more realistic, less noisy, and\nmore faithful to prompts than existing methods, demonstrating its potential for\nimproved content generation and editing.",
        "url": "http://arxiv.org/abs/2504.20288v1",
        "published_date": "2025-04-28T22:04:20+00:00",
        "updated_date": "2025-04-28T22:04:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shinnosuke Saito",
            "Takashi Matsubara"
        ],
        "tldr": "this paper introduces a riemannian manifold framework utilizing the score function of diffusion models to achieve geometry-aware image interpolation, resulting in more realistic and faithful outputs. it unlocks practical applications based on the data manifold learned by the diffusion model.",
        "tldr_zh": "本文提出了一种黎曼流形框架，利用扩散模型的得分函数实现几何感知的图像插值，从而产生更逼真和真实的输出。它解锁了基于扩散模型学习的数据流形的实际应用。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Physics-Informed Diffusion Models for SAR Ship Wake Generation from Text Prompts",
        "summary": "Detecting ship presence via wake signatures in SAR imagery is attracting\nconsiderable research interest, but limited annotated data availability poses\nsignificant challenges for supervised learning. Physics-based simulations are\ncommonly used to address this data scarcity, although they are slow and\nconstrain end-to-end learning. In this work, we explore a new direction for\nmore efficient and end-to-end SAR ship wake simulation using a diffusion model\ntrained on data generated by a physics-based simulator. The training dataset is\nbuilt by pairing images produced by the simulator with text prompts derived\nfrom simulation parameters. Experimental result show that the model generates\nrealistic Kelvin wake patterns and achieves significantly faster inference than\nthe physics-based simulator. These results highlight the potential of diffusion\nmodels for fast and controllable wake image generation, opening new\npossibilities for end-to-end downstream tasks in maritime SAR analysis.",
        "url": "http://arxiv.org/abs/2504.20241v1",
        "published_date": "2025-04-28T20:21:05+00:00",
        "updated_date": "2025-04-28T20:21:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kamirul Kamirul",
            "Odysseas Pappas",
            "Alin Achim"
        ],
        "tldr": "this paper introduces a physics-informed diffusion model for generating sar ship wake images from text prompts, offering a faster alternative to physics-based simulators and enabling end-to-end learning for maritime sar analysis.",
        "tldr_zh": "本文介绍了一种物理信息引导的扩散模型，用于从文本提示生成 sar 船舶尾流图像，为基于物理的模拟器提供了一种更快的替代方案，并支持海事 sar 分析的端到端学习。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Integration Flow Models",
        "summary": "Ordinary differential equation (ODE) based generative models have emerged as\na powerful approach for producing high-quality samples in many applications.\nHowever, the ODE-based methods either suffer the discretization error of\nnumerical solvers of ODE, which restricts the quality of samples when only a\nfew NFEs are used, or struggle with training instability. In this paper, we\nproposed Integration Flow, which directly learns the integral of ODE-based\ntrajectory paths without solving the ODE functions. Moreover, Integration Flow\nexplicitly incorporates the target state $\\mathbf{x}_0$ as the anchor state in\nguiding the reverse-time dynamics. We have theoretically proven this can\ncontribute to both stability and accuracy. To the best of our knowledge,\nIntegration Flow is the first model with a unified structure to estimate\nODE-based generative models and the first to show the exact straightness of\n1-Rectified Flow without reflow. Through theoretical analysis and empirical\nevaluations, we show that Integration Flows achieve improved performance when\nit is applied to existing ODE-based models, such as diffusion models, Rectified\nFlows, and PFGM++. Specifically, Integration Flow achieves one-step generation\non CIFAR10 with FIDs of 2.86 for the Variance Exploding (VE) diffusion model,\n3.36 for rectified flow without reflow, and 2.91 for PFGM++; and on ImageNet\nwith FIDs of 4.09 for VE diffusion model, 4.35 for rectified flow without\nreflow and 4.15 for PFGM++.",
        "url": "http://arxiv.org/abs/2504.20179v1",
        "published_date": "2025-04-28T18:29:15+00:00",
        "updated_date": "2025-04-28T18:29:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Jingjing Wang",
            "Dan Zhang",
            "Joshua Luo",
            "Yin Yang",
            "Feng Luo"
        ],
        "tldr": "the paper introduces 'integration flow,' a novel approach to improve ode-based generative models by directly learning the integral of trajectory paths, leading to enhanced stability, accuracy, and one-step generation performance across various models like diffusion models and rectified flows.",
        "tldr_zh": "该论文介绍了“积分流”（integration flow），一种通过直接学习轨迹路径的积分来改进基于常微分方程（ode）的生成模型的新方法，从而提高了稳定性和准确性，并在扩散模型和纠正流等多种模型中实现了单步生成性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report Generation with Multi-Agent RAG and Concept Bottleneck Models",
        "summary": "Advancements in generative Artificial Intelligence (AI) hold great promise\nfor automating radiology workflows, yet challenges in interpretability and\nreliability hinder clinical adoption. This paper presents an automated\nradiology report generation framework that combines Concept Bottleneck Models\n(CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge\nAI performance with clinical explainability. CBMs map chest X-ray features to\nhuman-understandable clinical concepts, enabling transparent disease\nclassification. Meanwhile, the RAG system integrates multi-agent collaboration\nand external knowledge to produce contextually rich, evidence-based reports.\nOur demonstration showcases the system's ability to deliver interpretable\npredictions, mitigate hallucinations, and generate high-quality, tailored\nreports with an interactive interface addressing accuracy, trust, and usability\nchallenges. This framework provides a pathway to improving diagnostic\nconsistency and empowering radiologists with actionable insights.",
        "url": "http://arxiv.org/abs/2504.20898v1",
        "published_date": "2025-04-29T16:14:55+00:00",
        "updated_date": "2025-04-29T16:14:55+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.IR"
        ],
        "authors": [
            "Hasan Md Tusfiqur Alam",
            "Devansh Srivastav",
            "Abdulrahman Mohamed Selim",
            "Md Abdul Kadir",
            "Md Moktadiurl Hoque Shuvo",
            "Daniel Sonntag"
        ],
        "tldr": "this paper introduces a framework combining concept bottleneck models (cbms) with a multi-agent retrieval-augmented generation (rag) system for interpretable radiology report generation, aiming to improve diagnostic consistency and provide actionable insights for radiologists.",
        "tldr_zh": "本文介绍了一个框架，该框架结合概念颈瓶模型（cbm）和多代理检索增强生成（rag）系统，用于生成可解释的放射学报告，旨在提高诊断一致性并为放射科医生提供可操作的见解。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "PixelHacker: Image Inpainting with Structural and Semantic Consistency",
        "summary": "Image inpainting is a fundamental research area between image editing and\nimage generation. Recent state-of-the-art (SOTA) methods have explored novel\nattention mechanisms, lightweight architectures, and context-aware modeling,\ndemonstrating impressive performance. However, they often struggle with complex\nstructure (e.g., texture, shape, spatial relations) and semantics (e.g., color\nconsistency, object restoration, and logical correctness), leading to artifacts\nand inappropriate generation. To address this challenge, we design a simple yet\neffective inpainting paradigm called latent categories guidance, and further\npropose a diffusion-based model named PixelHacker. Specifically, we first\nconstruct a large dataset containing 14 million image-mask pairs by annotating\nforeground and background (potential 116 and 21 categories, respectively).\nThen, we encode potential foreground and background representations separately\nthrough two fixed-size embeddings, and intermittently inject these features\ninto the denoising process via linear attention. Finally, by pre-training on\nour dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker.\nExtensive experiments show that PixelHacker comprehensively outperforms the\nSOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits\nremarkable consistency in both structure and semantics. Project page at\nhttps://hustvl.github.io/projects/PixelHacker.",
        "url": "http://arxiv.org/abs/2504.20438v1",
        "published_date": "2025-04-29T05:28:36+00:00",
        "updated_date": "2025-04-29T05:28:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziyang Xu",
            "Kangsheng Duan",
            "Xiaolei Shen",
            "Zhifeng Ding",
            "Wenyu Liu",
            "Xiaohu Ruan",
            "Xiaoxin Chen",
            "Xinggang Wang"
        ],
        "tldr": "pixelhacker, a diffusion-based image inpainting model, uses latent category guidance and a pre-trained dataset of image-mask pairs to achieve state-of-the-art performance with improved structural and semantic consistency.",
        "tldr_zh": "pixelhacker是一个基于扩散模型的图像修复模型，它使用潜在类别引导和一个预训练的图像-掩码对数据集，实现了最先进的性能，并改进了结构和语义的一致性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "GarmentX: Autoregressive Parametric Representations for High-Fidelity 3D Garment Generation",
        "summary": "This work presents GarmentX, a novel framework for generating diverse,\nhigh-fidelity, and wearable 3D garments from a single input image. Traditional\ngarment reconstruction methods directly predict 2D pattern edges and their\nconnectivity, an overly unconstrained approach that often leads to severe\nself-intersections and physically implausible garment structures. In contrast,\nGarmentX introduces a structured and editable parametric representation\ncompatible with GarmentCode, ensuring that the decoded sewing patterns always\nform valid, simulation-ready 3D garments while allowing for intuitive\nmodifications of garment shape and style. To achieve this, we employ a masked\nautoregressive model that sequentially predicts garment parameters, leveraging\nautoregressive modeling for structured generation while mitigating\ninconsistencies in direct pattern prediction. Additionally, we introduce\nGarmentX dataset, a large-scale dataset of 378,682 garment parameter-image\npairs, constructed through an automatic data generation pipeline that\nsynthesizes diverse and high-quality garment images conditioned on parametric\ngarment representations. Through integrating our method with GarmentX dataset,\nwe achieve state-of-the-art performance in geometric fidelity and input image\nalignment, significantly outperforming prior approaches. We will release\nGarmentX dataset upon publication.",
        "url": "http://arxiv.org/abs/2504.20409v1",
        "published_date": "2025-04-29T04:15:33+00:00",
        "updated_date": "2025-04-29T04:15:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingfeng Guo",
            "Jinnan Chen",
            "Weikai Chen",
            "Zhenyu Sun",
            "Lanjiong Li",
            "Baozhu Zhao",
            "Lingting Zhu",
            "Xin Wang",
            "Qi Liu"
        ],
        "tldr": "garmentx introduces a novel autoregressive framework for generating high-fidelity, wearable 3d garments from images, using a structured parametric representation and a large-scale dataset, achieving state-of-the-art performance.",
        "tldr_zh": "garmentx 提出了一个新颖的自回归框架，用于从图像生成高保真、可穿戴的 3d 服装。该框架使用结构化的参数化表示和一个大规模数据集，并实现了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MicarVLMoE: A Modern Gated Cross-Aligned Vision-Language Mixture of Experts Model for Medical Image Captioning and Report Generation",
        "summary": "Medical image reporting (MIR) aims to generate structured clinical\ndescriptions from radiological images. Existing methods struggle with\nfine-grained feature extraction, multimodal alignment, and generalization\nacross diverse imaging types, often relying on vanilla transformers and\nfocusing primarily on chest X-rays. We propose MicarVLMoE, a vision-language\nmixture-of-experts model with gated cross-aligned fusion, designed to address\nthese limitations. Our architecture includes: (i) a multiscale vision encoder\n(MSVE) for capturing anatomical details at varying resolutions, (ii) a\nmultihead dual-branch latent attention (MDLA) module for vision-language\nalignment through latent bottleneck representations, and (iii) a modulated\nmixture-of-experts (MoE) decoder for adaptive expert specialization. We extend\nMIR to CT scans, retinal imaging, MRI scans, and gross pathology images,\nreporting state-of-the-art results on COVCTR, MMR, PGROSS, and ROCO datasets.\nExtensive experiments and ablations confirm improved clinical accuracy,\ncross-modal alignment, and model interpretability. Code is available at\nhttps://github.com/AI-14/micar-vl-moe.",
        "url": "http://arxiv.org/abs/2504.20343v1",
        "published_date": "2025-04-29T01:26:02+00:00",
        "updated_date": "2025-04-29T01:26:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Amaan Izhar",
            "Nurul Japar",
            "Norisma Idris",
            "Ting Dang"
        ],
        "tldr": "the paper introduces micarvlmoe, a vision-language mixture-of-experts model for medical image report generation, achieving state-of-the-art results across multiple medical imaging modalities and datasets.",
        "tldr_zh": "该论文介绍了 micarvlmoe，一种用于医学图像报告生成的视觉-语言混合专家模型，在多种医学成像模式和数据集上取得了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains",
        "summary": "Vision-language models (VLMs) achieve remarkable success in single-image\ntasks. However, real-world scenarios often involve intricate multi-image\ninputs, leading to a notable performance decline as models struggle to\ndisentangle critical information scattered across complex visual features. In\nthis work, we propose Focus-Centric Visual Chain, a novel paradigm that\nenhances VLMs'perception, comprehension, and reasoning abilities in multi-image\nscenarios. To facilitate this paradigm, we propose Focus-Centric Data\nSynthesis, a scalable bottom-up approach for synthesizing high-quality data\nwith elaborate reasoning paths. Through this approach, We construct VISC-150K,\na large-scale dataset with reasoning data in the form of Focus-Centric Visual\nChain, specifically designed for multi-image tasks. Experimental results on\nseven multi-image benchmarks demonstrate that our method achieves average\nperformance gains of 3.16% and 2.24% across two distinct model architectures,\nwithout compromising the general vision-language capabilities. our study\nrepresents a significant step toward more robust and capable vision-language\nsystems that can handle complex visual scenarios.",
        "url": "http://arxiv.org/abs/2504.20199v1",
        "published_date": "2025-04-28T19:02:18+00:00",
        "updated_date": "2025-04-28T19:02:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Juntian Zhang",
            "Chuanqi cheng",
            "Yuhan Liu",
            "Wei Liu",
            "Jian Luan",
            "Rui Yan"
        ],
        "tldr": "the paper introduces a novel focus-centric visual chain paradigm and a corresponding dataset (visc-150k) to improve vlms' performance on multi-image tasks, showing performance gains on multiple benchmarks.",
        "tldr_zh": "该论文提出了一种新颖的焦点中心视觉链范式和一个相应的数据集（visc-150k），以提高视觉语言模型在多图像任务上的性能，并在多个基准测试中显示出了性能提升。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception",
        "summary": "Large Vision-Language Models (LVLMs) have achieved impressive results across\nvarious cross-modal tasks. However, hallucinations, i.e., the models generating\ncounterfactual responses, remain a challenge. Though recent studies have\nattempted to alleviate object perception hallucinations, they focus on the\nmodels' response generation, and overlooking the task question itself. This\npaper discusses the vulnerability of LVLMs in solving counterfactual\npresupposition questions (CPQs), where the models are prone to accept the\npresuppositions of counterfactual objects and produce severe hallucinatory\nresponses. To this end, we introduce \"Antidote\", a unified, synthetic\ndata-driven post-training framework for mitigating both types of hallucination\nabove. It leverages synthetic data to incorporate factual priors into questions\nto achieve self-correction, and decouple the mitigation process into a\npreference optimization problem. Furthermore, we construct \"CP-Bench\", a novel\nbenchmark to evaluate LVLMs' ability to correctly handle CPQs and produce\nfactual responses. Applied to the LLaVA series, Antidote can simultaneously\nenhance performance on CP-Bench by over 50%, POPE by 1.8-3.3%, and CHAIR & SHR\nby 30-50%, all without relying on external supervision from stronger LVLMs or\nhuman feedback and introducing noticeable catastrophic forgetting issues.",
        "url": "http://arxiv.org/abs/2504.20468v1",
        "published_date": "2025-04-29T07:05:24+00:00",
        "updated_date": "2025-04-29T07:05:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanchen Wu",
            "Lu Zhang",
            "Hang Yao",
            "Junlong Du",
            "Ke Yan",
            "Shouhong Ding",
            "Yunsheng Wu",
            "Xiaoqiang Li"
        ],
        "tldr": "the paper introduces \"antidote,\" a post-training framework and a new benchmark \"cp-bench\" to mitigate hallucinations in lvlms, specifically in counterfactual presupposition and object perception tasks, through synthetic data and preference optimization.",
        "tldr_zh": "该论文介绍了 \"antidote\"，一个后训练框架和一个新的基准 \"cp-bench\"，旨在通过合成数据和偏好优化来减轻 lvlm 中的幻觉，特别是在反事实预设和对象感知任务中。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification",
        "summary": "Recent advances in reasoning-enhanced large language models (LLMs) and\nmultimodal LLMs (MLLMs) have significantly improved performance in complex\ntasks, yet medical AI models often overlook the structured reasoning processes\ninherent in clinical practice. In this work, we present ChestX-Reasoner, a\nradiology diagnosis MLLM designed to leverage process supervision mined\ndirectly from clinical reports, reflecting the step-by-step reasoning followed\nby radiologists. We construct a large dataset by extracting and refining\nreasoning chains from routine radiology reports. Our two-stage training\nframework combines supervised fine-tuning and reinforcement learning guided by\nprocess rewards to better align model reasoning with clinical standards. We\nintroduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual\nquestion answering samples with 301K clinically validated reasoning steps, and\npropose RadRScore, a metric evaluating reasoning factuality, completeness, and\neffectiveness. ChestX-Reasoner outperforms existing medical and general-domain\nMLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,\nand 18% improvements in reasoning ability compared to the best medical MLLM,\nthe best general MLLM, and its base model, respectively, as well as 3.3%, 24%,\nand 27% improvements in outcome accuracy. All resources are open-sourced to\nfacilitate further research in medical reasoning MLLMs.",
        "url": "http://arxiv.org/abs/2504.20930v1",
        "published_date": "2025-04-29T16:48:23+00:00",
        "updated_date": "2025-04-29T16:48:23+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Ziqing Fan",
            "Cheng Liang",
            "Chaoyi Wu",
            "Ya Zhang",
            "Yanfeng Wang",
            "Weidi Xie"
        ],
        "tldr": "chestx-reasoner introduces a radiology diagnosis mllm trained with process supervision from clinical reports, along with a new benchmark and metric for evaluating reasoning in medical ai, demonstrating significant improvements in diagnostic accuracy and reasoning ability.",
        "tldr_zh": "chestx-reasoner 引入了一个利用临床报告进行过程监督训练的放射诊断多模态大语言模型，以及一个新的用于评估医学人工智能推理能力的基准和指标，在诊断准确性和推理能力方面表现出显著的改进。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "TTTFusion: A Test-Time Training-Based Strategy for Multimodal Medical Image Fusion in Surgical Robots",
        "summary": "With the increasing use of surgical robots in clinical practice, enhancing\ntheir ability to process multimodal medical images has become a key research\nchallenge. Although traditional medical image fusion methods have made progress\nin improving fusion accuracy, they still face significant challenges in\nreal-time performance, fine-grained feature extraction, and edge\npreservation.In this paper, we introduce TTTFusion, a Test-Time Training\n(TTT)-based image fusion strategy that dynamically adjusts model parameters\nduring inference to efficiently fuse multimodal medical images. By adapting the\nmodel during the test phase, our method optimizes the parameters based on the\ninput image data, leading to improved accuracy and better detail preservation\nin the fusion results.Experimental results demonstrate that TTTFusion\nsignificantly enhances the fusion quality of multimodal images compared to\ntraditional fusion methods, particularly in fine-grained feature extraction and\nedge preservation. This approach not only improves image fusion accuracy but\nalso offers a novel technical solution for real-time image processing in\nsurgical robots.",
        "url": "http://arxiv.org/abs/2504.20362v1",
        "published_date": "2025-04-29T02:00:08+00:00",
        "updated_date": "2025-04-29T02:00:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qinhua Xie",
            "Hao Tang"
        ],
        "tldr": "the paper introduces tttfusion, a test-time training-based method for multimodal medical image fusion in surgical robots, improving accuracy and detail preservation during real-time image processing.",
        "tldr_zh": "该论文介绍了tttfusion，一种基于测试时训练的多模态医学图像融合方法，应用于手术机器人，旨在提高实时图像处理过程中的准确性和细节保持。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "A Transformer-based Multimodal Fusion Model for Efficient Crowd Counting Using Visual and Wireless Signals",
        "summary": "Current crowd-counting models often rely on single-modal inputs, such as\nvisual images or wireless signal data, which can result in significant\ninformation loss and suboptimal recognition performance. To address these\nshortcomings, we propose TransFusion, a novel multimodal fusion-based\ncrowd-counting model that integrates Channel State Information (CSI) with image\ndata. By leveraging the powerful capabilities of Transformer networks,\nTransFusion effectively combines these two distinct data modalities, enabling\nthe capture of comprehensive global contextual information that is critical for\naccurate crowd estimation. However, while transformers are well capable of\ncapturing global features, they potentially fail to identify finer-grained,\nlocal details essential for precise crowd counting. To mitigate this, we\nincorporate Convolutional Neural Networks (CNNs) into the model architecture,\nenhancing its ability to extract detailed local features that complement the\nglobal context provided by the Transformer. Extensive experimental evaluations\ndemonstrate that TransFusion achieves high accuracy with minimal counting\nerrors while maintaining superior efficiency.",
        "url": "http://arxiv.org/abs/2504.20178v1",
        "published_date": "2025-04-28T18:26:28+00:00",
        "updated_date": "2025-04-28T18:26:28+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zhe Cui",
            "Yuli Li",
            "Le-Nam Tran"
        ],
        "tldr": "the paper introduces transfusion, a novel crowd-counting model that fuses visual and wireless signals using a transformer network enhanced with cnns for improved accuracy and efficiency.",
        "tldr_zh": "该论文介绍了一种名为transfusion的新型人群计数模型，该模型使用transformer网络融合视觉和无线信号，并结合cnn，以提高准确性和效率。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "FLIM-based Salient Object Detection Networks with Adaptive Decoders",
        "summary": "Salient Object Detection (SOD) methods can locate objects that stand out in\nan image, assign higher values to their pixels in a saliency map, and binarize\nthe map outputting a predicted segmentation mask. A recent tendency is to\ninvestigate pre-trained lightweight models rather than deep neural networks in\nSOD tasks, coping with applications under limited computational resources. In\nthis context, we have investigated lightweight networks using a methodology\nnamed Feature Learning from Image Markers (FLIM), which assumes that the\nencoder's kernels can be estimated from marker pixels on discriminative regions\nof a few representative images. This work proposes flyweight networks, hundreds\nof times lighter than lightweight models, for SOD by combining a FLIM encoder\nwith an adaptive decoder, whose weights are estimated for each input image by a\ngiven heuristic function. Such FLIM networks are trained from three to four\nrepresentative images only and without backpropagation, making the models\nsuitable for applications under labeled data constraints as well. We study five\nadaptive decoders; two of them are introduced here. Differently from the\nprevious ones that rely on one neuron per pixel with shared weights, the\nheuristic functions of the new adaptive decoders estimate the weights of each\nneuron per pixel. We compare FLIM models with adaptive decoders for two\nchallenging SOD tasks with three lightweight networks from the\nstate-of-the-art, two FLIM networks with decoders trained by backpropagation,\nand one FLIM network whose labeled markers define the decoder's weights. The\nexperiments demonstrate the advantages of the proposed networks over the\nbaselines, revealing the importance of further investigating such methods in\nnew applications.",
        "url": "http://arxiv.org/abs/2504.20872v1",
        "published_date": "2025-04-29T15:44:02+00:00",
        "updated_date": "2025-04-29T15:44:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Gilson Junior Soares",
            "Matheus Abrantes Cerqueira",
            "Jancarlo F. Gomes",
            "Laurent Najman",
            "Silvio Jamil F. Guimarães",
            "Alexandre Xavier Falcão"
        ],
        "tldr": "this paper introduces flyweight networks for salient object detection (sod) using feature learning from image markers (flim) and adaptive decoders, trained on limited data and without backpropagation. the proposed networks outperform existing lightweight models while being significantly smaller.",
        "tldr_zh": "本文介绍了一种用于显著目标检测（sod）的轻量级网络，该网络使用图像标记特征学习（flim）和自适应解码器，在有限的数据上训练且无需反向传播。所提出的网络在性能上优于现有的轻量级模型，且体积更小。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    },
    {
        "title": "FBRT-YOLO: Faster and Better for Real-Time Aerial Image Detection",
        "summary": "Embedded flight devices with visual capabilities have become essential for a\nwide range of applications. In aerial image detection, while many existing\nmethods have partially addressed the issue of small target detection,\nchallenges remain in optimizing small target detection and balancing detection\naccuracy with efficiency. These issues are key obstacles to the advancement of\nreal-time aerial image detection. In this paper, we propose a new family of\nreal-time detectors for aerial image detection, named FBRT-YOLO, to address the\nimbalance between detection accuracy and efficiency. Our method comprises two\nlightweight modules: Feature Complementary Mapping Module (FCM) and\nMulti-Kernel Perception Unit(MKP), designed to enhance object perception for\nsmall targets in aerial images. FCM focuses on alleviating the problem of\ninformation imbalance caused by the loss of small target information in deep\nnetworks. It aims to integrate spatial positional information of targets more\ndeeply into the network,better aligning with semantic information in the deeper\nlayers to improve the localization of small targets. We introduce MKP, which\nleverages convolutions with kernels of different sizes to enhance the\nrelationships between targets of various scales and improve the perception of\ntargets at different scales. Extensive experimental results on three major\naerial image datasets, including Visdrone, UAVDT, and AI-TOD,demonstrate that\nFBRT-YOLO outperforms various real-time detectors in terms of performance and\nspeed.",
        "url": "http://arxiv.org/abs/2504.20670v1",
        "published_date": "2025-04-29T11:53:54+00:00",
        "updated_date": "2025-04-29T11:53:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yao Xiao",
            "Tingfa Xu",
            "Yu Xin",
            "Jianan Li"
        ],
        "tldr": "the paper introduces fbrt-yolo, a new real-time object detector optimized for aerial images, using feature complementary mapping and a multi-kernel perception unit to improve small object detection accuracy and efficiency.",
        "tldr_zh": "该论文介绍了fbrt-yolo，一种新的实时目标检测器，针对航空图像进行了优化，使用特征互补映射和多核感知单元，以提高小目标检测的准确性和效率。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 3
    }
]