[
    {
        "title": "ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction",
        "summary": "In recent years, video generation has seen significant advancements. However,\nchallenges still persist in generating complex motions and interactions. To\naddress these challenges, we introduce ReVision, a plug-and-play framework that\nexplicitly integrates parameterized 3D physical knowledge into a pretrained\nconditional video generation model, significantly enhancing its ability to\ngenerate high-quality videos with complex motion and interactions.\nSpecifically, ReVision consists of three stages. First, a video diffusion model\nis used to generate a coarse video. Next, we extract a set of 2D and 3D\nfeatures from the coarse video to construct a 3D object-centric representation,\nwhich is then refined by our proposed parameterized physical prior model to\nproduce an accurate 3D motion sequence. Finally, this refined motion sequence\nis fed back into the same video diffusion model as additional conditioning,\nenabling the generation of motion-consistent videos, even in scenarios\ninvolving complex actions and interactions. We validate the effectiveness of\nour approach on Stable Video Diffusion, where ReVision significantly improves\nmotion fidelity and coherence. Remarkably, with only 1.5B parameters, it even\noutperforms a state-of-the-art video generation model with over 13B parameters\non complex video generation by a substantial margin. Our results suggest that,\nby incorporating 3D physical knowledge, even a relatively small video diffusion\nmodel can generate complex motions and interactions with greater realism and\ncontrollability, offering a promising solution for physically plausible video\ngeneration.",
        "url": "http://arxiv.org/abs/2504.21855v1",
        "published_date": "2025-04-30T17:59:56+00:00",
        "updated_date": "2025-04-30T17:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qihao Liu",
            "Ju He",
            "Qihang Yu",
            "Liang-Chieh Chen",
            "Alan Yuille"
        ],
        "tldr": "the paper introduces revision, a framework that enhances video generation by integrating 3d physical knowledge into a pretrained video diffusion model, leading to improved motion fidelity and performance compared to larger models.",
        "tldr_zh": "该论文提出了一种名为revision的框架，通过将3d物理知识整合到预训练的视频扩散模型中来增强视频生成，从而提高了运动保真度，并且性能优于更大的模型。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "A Survey of Interactive Generative Video",
        "summary": "Interactive Generative Video (IGV) has emerged as a crucial technology in\nresponse to the growing demand for high-quality, interactive video content\nacross various domains. In this paper, we define IGV as a technology that\ncombines generative capabilities to produce diverse high-quality video content\nwith interactive features that enable user engagement through control signals\nand responsive feedback. We survey the current landscape of IGV applications,\nfocusing on three major domains: 1) gaming, where IGV enables infinite\nexploration in virtual worlds; 2) embodied AI, where IGV serves as a\nphysics-aware environment synthesizer for training agents in multimodal\ninteraction with dynamically evolving scenes; and 3) autonomous driving, where\nIGV provides closed-loop simulation capabilities for safety-critical testing\nand validation. To guide future development, we propose a comprehensive\nframework that decomposes an ideal IGV system into five essential modules:\nGeneration, Control, Memory, Dynamics, and Intelligence. Furthermore, we\nsystematically analyze the technical challenges and future directions in\nrealizing each component for an ideal IGV system, such as achieving real-time\ngeneration, enabling open-domain control, maintaining long-term coherence,\nsimulating accurate physics, and integrating causal reasoning. We believe that\nthis systematic analysis will facilitate future research and development in the\nfield of IGV, ultimately advancing the technology toward more sophisticated and\npractical applications.",
        "url": "http://arxiv.org/abs/2504.21853v1",
        "published_date": "2025-04-30T17:59:02+00:00",
        "updated_date": "2025-04-30T17:59:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiwen Yu",
            "Yiran Qin",
            "Haoxuan Che",
            "Quande Liu",
            "Xintao Wang",
            "Pengfei Wan",
            "Di Zhang",
            "Kun Gai",
            "Hao Chen",
            "Xihui Liu"
        ],
        "tldr": "this paper surveys interactive generative video (igv) techniques across gaming, embodied ai, and autonomous driving. it proposes a framework and analyzes challenges for future igv development.",
        "tldr_zh": "本文综述了交互式生成视频（igv）技术在游戏、具身人工智能和自动驾驶等领域的应用。它提出了一个框架，并分析了igv未来发展面临的挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Why Compress What You Can Generate? When GPT-4o Generation Ushers in Image Compression Fields",
        "summary": "The rapid development of AIGC foundation models has revolutionized the\nparadigm of image compression, which paves the way for the abandonment of most\npixel-level transform and coding, compelling us to ask: why compress what you\ncan generate if the AIGC foundation model is powerful enough to faithfully\ngenerate intricate structure and fine-grained details from nothing more than\nsome compact descriptors, i.e., texts, or cues. Fortunately, recent GPT-4o\nimage generation of OpenAI has achieved impressive cross-modality generation,\nediting, and design capabilities, which motivates us to answer the above\nquestion by exploring its potential in image compression fields. In this work,\nwe investigate two typical compression paradigms: textual coding and multimodal\ncoding (i.e., text + extremely low-resolution image), where all/most\npixel-level information is generated instead of compressing via the advanced\nGPT-4o image generation function. The essential challenge lies in how to\nmaintain semantic and structure consistency during the decoding process. To\novercome this, we propose a structure raster-scan prompt engineering mechanism\nto transform the image into textual space, which is compressed as the condition\nof GPT-4o image generation. Extensive experiments have shown that the\ncombination of our designed structural raster-scan prompts and GPT-4o's image\ngeneration function achieved the impressive performance compared with recent\nmultimodal/generative image compression at ultra-low bitrate, further\nindicating the potential of AIGC generation in image compression fields.",
        "url": "http://arxiv.org/abs/2504.21814v1",
        "published_date": "2025-04-30T17:20:14+00:00",
        "updated_date": "2025-04-30T17:20:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yixin Gao",
            "Xiaohan Pan",
            "Xin Li",
            "Zhibo Chen"
        ],
        "tldr": "this paper explores using gpt-4o for image compression by generating images from text prompts and low-resolution images, achieving comparable performance to existing methods at ultra-low bitrates.",
        "tldr_zh": "本文探讨了使用gpt-4o通过文本提示和低分辨率图像生成图像来进行图像压缩的方法，并在超低比特率下实现了与现有方法相当的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation",
        "summary": "The rapid advancement of diffusion models holds the promise of\nrevolutionizing the application of VR and AR technologies, which typically\nrequire scene-level 4D assets for user experience. Nonetheless, existing\ndiffusion models predominantly concentrate on modeling static 3D scenes or\nobject-level dynamics, constraining their capacity to provide truly immersive\nexperiences. To address this issue, we propose HoloTime, a framework that\nintegrates video diffusion models to generate panoramic videos from a single\nprompt or reference image, along with a 360-degree 4D scene reconstruction\nmethod that seamlessly transforms the generated panoramic video into 4D assets,\nenabling a fully immersive 4D experience for users. Specifically, to tame video\ndiffusion models for generating high-fidelity panoramic videos, we introduce\nthe 360World dataset, the first comprehensive collection of panoramic videos\nsuitable for downstream 4D scene reconstruction tasks. With this curated\ndataset, we propose Panoramic Animator, a two-stage image-to-video diffusion\nmodel that can convert panoramic images into high-quality panoramic videos.\nFollowing this, we present Panoramic Space-Time Reconstruction, which leverages\na space-time depth estimation method to transform the generated panoramic\nvideos into 4D point clouds, enabling the optimization of a holistic 4D\nGaussian Splatting representation to reconstruct spatially and temporally\nconsistent 4D scenes. To validate the efficacy of our method, we conducted a\ncomparative analysis with existing approaches, revealing its superiority in\nboth panoramic video generation and 4D scene reconstruction. This demonstrates\nour method's capability to create more engaging and realistic immersive\nenvironments, thereby enhancing user experiences in VR and AR applications.",
        "url": "http://arxiv.org/abs/2504.21650v1",
        "published_date": "2025-04-30T13:55:28+00:00",
        "updated_date": "2025-04-30T13:55:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haiyang Zhou",
            "Wangbo Yu",
            "Jiawen Guan",
            "Xinhua Cheng",
            "Yonghong Tian",
            "Li Yuan"
        ],
        "tldr": "the paper introduces holotime, a framework that uses video diffusion models to generate panoramic videos from prompts or images, which are then reconstructed into 4d scene assets for immersive vr/ar experiences, using a new dataset and novel method for space-time reconstruction.",
        "tldr_zh": "该论文介绍了holotime，一个利用视频扩散模型从提示或图像生成全景视频的框架，然后将这些视频重构为4d场景资产，用于沉浸式vr/ar体验。该框架使用了一个新的数据集和一种用于时空重建的新方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance",
        "summary": "In this paper, we propose a method for video face reenactment that integrates\na 3D face parametric model into a latent diffusion framework, aiming to improve\nshape consistency and motion control in existing video-based face generation\napproaches. Our approach employs the FLAME (Faces Learned with an Articulated\nModel and Expressions) model as the 3D face parametric representation,\nproviding a unified framework for modeling face expressions and head pose. This\nenables precise extraction of detailed face geometry and motion features from\ndriving videos. Specifically, we enhance the latent diffusion model with rich\n3D expression and detailed pose information by incorporating depth maps, normal\nmaps, and rendering maps derived from FLAME sequences. A multi-layer face\nmovements fusion module with integrated self-attention mechanisms is used to\ncombine identity and motion latent features within the spatial domain. By\nutilizing the 3D face parametric model as motion guidance, our method enables\nparametric alignment of face identity between the reference image and the\nmotion captured from the driving video. Experimental results on benchmark\ndatasets show that our method excels at generating high-quality face animations\nwith precise expression and head pose variation modeling. In addition, it\ndemonstrates strong generalization performance on out-of-domain images. Code is\npublicly available at https://github.com/weimengting/MagicPortrait.",
        "url": "http://arxiv.org/abs/2504.21497v1",
        "published_date": "2025-04-30T10:30:46+00:00",
        "updated_date": "2025-04-30T10:30:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mengting Wei",
            "Yante Li",
            "Tuomas Varanka",
            "Yan Jiang",
            "Licai Sun",
            "Guoying Zhao"
        ],
        "tldr": "magicportrait introduces a novel video face reenactment method that integrates a 3d face parametric model (flame) into a latent diffusion framework for improved shape consistency and motion control, demonstrating strong results on benchmark datasets and out-of-domain images.",
        "tldr_zh": "magicportrait 提出了一种新的视频人脸重演方法，该方法将 3d 人脸参数模型 (flame) 集成到潜在扩散框架中，以提高形状一致性和运动控制能力，并在基准数据集和域外图像上展示了强大的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers",
        "summary": "Garment sewing patterns are fundamental design elements that bridge the gap\nbetween design concepts and practical manufacturing. The generative modeling of\nsewing patterns is crucial for creating diversified garments. However, existing\napproaches are limited either by reliance on a single input modality or by\nsuboptimal generation efficiency. In this work, we present\n\\textbf{\\textit{GarmentDiffusion}}, a new generative model capable of producing\ncentimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text,\nimage, and incomplete sewing pattern). Our method efficiently encodes 3D sewing\npattern parameters into compact edge token representations, achieving a\nsequence length that is $\\textbf{10}\\times$ shorter than that of the\nautoregressive SewingGPT in DressCode. By employing a diffusion transformer, we\nsimultaneously denoise all edge tokens along the temporal axis, while\nmaintaining a constant number of denoising steps regardless of dataset-specific\nedge and panel statistics. With all combination of designs of our model, the\nsewing pattern generation speed is accelerated by $\\textbf{100}\\times$ compared\nto SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well\nas on the largest sewing pattern dataset, namely GarmentCodeData. The project\nwebsite is available at https://shenfu-research.github.io/Garment-Diffusion/.",
        "url": "http://arxiv.org/abs/2504.21476v1",
        "published_date": "2025-04-30T09:56:59+00:00",
        "updated_date": "2025-04-30T09:56:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xinyu Li",
            "Qi Yao",
            "Yuanda Wang"
        ],
        "tldr": "garmentdiffusion introduces a multimodal diffusion transformer for generating 3d garment sewing patterns from text, images, and incomplete patterns, achieving significant speedups and state-of-the-art results compared to existing methods.",
        "tldr_zh": "garmentdiffusion 提出了一种多模态扩散 transformer，用于从文本、图像和不完整的图案生成 3d 服装缝纫图案，与现有方法相比，实现了显著的加速和最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Sparse-to-Sparse Training of Diffusion Models",
        "summary": "Diffusion models (DMs) are a powerful type of generative models that have\nachieved state-of-the-art results in various image synthesis tasks and have\nshown potential in other domains, such as natural language processing and\ntemporal data modeling. Despite their stable training dynamics and ability to\nproduce diverse high-quality samples, DMs are notorious for requiring\nsignificant computational resources, both in the training and inference stages.\nPrevious work has focused mostly on increasing the efficiency of model\ninference. This paper introduces, for the first time, the paradigm of\nsparse-to-sparse training to DMs, with the aim of improving both training and\ninference efficiency. We focus on unconditional generation and train sparse DMs\nfrom scratch (Latent Diffusion and ChiroDiff) on six datasets using three\ndifferent methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of\nsparsity in model performance. Our experiments show that sparse DMs are able to\nmatch and often outperform their Dense counterparts, while substantially\nreducing the number of trainable parameters and FLOPs. We also identify safe\nand effective values to perform sparse-to-sparse training of DMs.",
        "url": "http://arxiv.org/abs/2504.21380v1",
        "published_date": "2025-04-30T07:28:11+00:00",
        "updated_date": "2025-04-30T07:28:11+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Inês Cardoso Oliveira",
            "Decebal Constantin Mocanu",
            "Luis A. Leiva"
        ],
        "tldr": "this paper introduces sparse-to-sparse training for diffusion models to improve training and inference efficiency, showing that sparse models can match or outperform dense models with fewer resources.",
        "tldr_zh": "本文介绍了扩散模型的稀疏到稀疏训练方法，旨在提高训练和推理效率，实验表明稀疏模型可以用更少的资源达到甚至超过密集模型的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing",
        "summary": "Unified multimodal large language models (MLLMs) aim to integrate multimodal\nunderstanding and generation abilities through a single framework. Despite\ntheir versatility, existing open-source unified models exhibit performance gaps\nagainst domain-specific architectures. To bridge this gap, we present\nNexus-Gen, a unified model that synergizes the language reasoning capabilities\nof LLMs with the image synthesis power of diffusion models. To align the\nembedding space of the LLM and diffusion model, we conduct a dual-phase\nalignment training process. (1) The autoregressive LLM learns to predict image\nembeddings conditioned on multimodal inputs, while (2) the vision decoder is\ntrained to reconstruct high-fidelity images from these embeddings. During\ntraining the LLM, we identified a critical discrepancy between the\nautoregressive paradigm's training and inference phases, where error\naccumulation in continuous embedding space severely degrades generation\nquality. To avoid this issue, we introduce a prefilled autoregression strategy\nthat prefills input sequence with position-embedded special tokens instead of\ncontinuous embeddings. Through dual-phase training, Nexus-Gen has developed the\nintegrated capability to comprehensively address the image understanding,\ngeneration and editing tasks. All models, datasets, and codes are published at\nhttps://github.com/modelscope/Nexus-Gen.git to facilitate further advancements\nacross the field.",
        "url": "http://arxiv.org/abs/2504.21356v1",
        "published_date": "2025-04-30T06:30:48+00:00",
        "updated_date": "2025-04-30T06:30:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hong Zhang",
            "Zhongjie Duan",
            "Xingjun Wang",
            "Yingda Chen",
            "Yuze Zhao",
            "Yu Zhang"
        ],
        "tldr": "nexus-gen is a unified multimodal large language model (mllm) that integrates image understanding, generation, and editing using a dual-phase training approach to align llm and diffusion model embedding spaces, addressing performance gaps in existing open-source unified models.",
        "tldr_zh": "nexus-gen是一个统一的多模态大型语言模型（mllm），它使用双阶段训练方法整合了图像理解、生成和编辑功能，以对齐llm和扩散模型的嵌入空间，从而弥补了现有开源统一模型中的性能差距。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation",
        "summary": "Multi-modal interpretation of biomedical images opens up novel opportunities\nin biomedical image analysis. Conventional AI approaches typically rely on\ndisjointed training, i.e., Large Language Models (LLMs) for clinical text\ngeneration and segmentation models for target extraction, which results in\ninflexible real-world deployment and a failure to leverage holistic biomedical\ninformation. To this end, we introduce UniBiomed, the first universal\nfoundation model for grounded biomedical image interpretation. UniBiomed is\nbased on a novel integration of Multi-modal Large Language Model (MLLM) and\nSegment Anything Model (SAM), which effectively unifies the generation of\nclinical texts and the segmentation of corresponding biomedical objects for\ngrounded interpretation. In this way, UniBiomed is capable of tackling a wide\nrange of biomedical tasks across ten diverse biomedical imaging modalities. To\ndevelop UniBiomed, we curate a large-scale dataset comprising over 27 million\ntriplets of images, annotations, and text descriptions across ten imaging\nmodalities. Extensive validation on 84 internal and external datasets\ndemonstrated that UniBiomed achieves state-of-the-art performance in\nsegmentation, disease recognition, region-aware diagnosis, visual question\nanswering, and report generation. Moreover, unlike previous models that rely on\nclinical experts to pre-diagnose images and manually craft precise textual or\nvisual prompts, UniBiomed can provide automated and end-to-end grounded\ninterpretation for biomedical image analysis. This represents a novel paradigm\nshift in clinical workflows, which will significantly improve diagnostic\nefficiency. In summary, UniBiomed represents a novel breakthrough in biomedical\nAI, unlocking powerful grounded interpretation capabilities for more accurate\nand efficient biomedical image analysis.",
        "url": "http://arxiv.org/abs/2504.21336v1",
        "published_date": "2025-04-30T05:51:48+00:00",
        "updated_date": "2025-04-30T05:51:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Linshan Wu",
            "Yuxiang Nie",
            "Sunan He",
            "Jiaxin Zhuang",
            "Hao Chen"
        ],
        "tldr": "the paper introduces unibiomed, a universal foundation model integrating mllm and sam for grounded biomedical image interpretation, achieving state-of-the-art performance across diverse tasks and modalities with a large-scale curated dataset.",
        "tldr_zh": "该论文介绍了一种通用基础模型unibiomed，它集成了mllm和sam，用于基于大型数据集的生物医学图像解释，并在各种任务和模式下实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "AGHI-QA: A Subjective-Aligned Dataset and Metric for AI-Generated Human Images",
        "summary": "The rapid development of text-to-image (T2I) generation approaches has\nattracted extensive interest in evaluating the quality of generated images,\nleading to the development of various quality assessment methods for\ngeneral-purpose T2I outputs. However, existing image quality assessment (IQA)\nmethods are limited to providing global quality scores, failing to deliver\nfine-grained perceptual evaluations for structurally complex subjects like\nhumans, which is a critical challenge considering the frequent anatomical and\ntextural distortions in AI-generated human images (AGHIs). To address this gap,\nwe introduce AGHI-QA, the first large-scale benchmark specifically designed for\nquality assessment of AGHIs. The dataset comprises 4,000 images generated from\n400 carefully crafted text prompts using 10 state of-the-art T2I models. We\nconduct a systematic subjective study to collect multidimensional annotations,\nincluding perceptual quality scores, text-image correspondence scores, visible\nand distorted body part labels. Based on AGHI-QA, we evaluate the strengths and\nweaknesses of current T2I methods in generating human images from multiple\ndimensions. Furthermore, we propose AGHI-Assessor, a novel quality metric that\nintegrates the large multimodal model (LMM) with domain-specific human features\nfor precise quality prediction and identification of visible and distorted body\nparts in AGHIs. Extensive experimental results demonstrate that AGHI-Assessor\nshowcases state-of-the-art performance, significantly outperforming existing\nIQA methods in multidimensional quality assessment and surpassing leading LMMs\nin detecting structural distortions in AGHIs.",
        "url": "http://arxiv.org/abs/2504.21308v1",
        "published_date": "2025-04-30T04:36:56+00:00",
        "updated_date": "2025-04-30T04:36:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunhao Li",
            "Sijing Wu",
            "Wei Sun",
            "Zhichao Zhang",
            "Yucheng Zhu",
            "Zicheng Zhang",
            "Huiyu Duan",
            "Xiongkuo Min",
            "Guangtao Zhai"
        ],
        "tldr": "the paper introduces aghi-qa, a new dataset and metric (aghi-assessor) for evaluating the quality of ai-generated human images, addressing the limitations of existing iqa methods in capturing fine-grained structural details and distortions.",
        "tldr_zh": "该论文介绍了aghi-qa，一个新的数据集和评估指标 (aghi-assessor) 用于评估ai生成的人像质量，解决了现有图像质量评估方法在捕捉细粒度结构细节和失真方面的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Can We Achieve Efficient Diffusion without Self-Attention? Distilling Self-Attention into Convolutions",
        "summary": "Contemporary diffusion models built upon U-Net or Diffusion Transformer (DiT)\narchitectures have revolutionized image generation through transformer-based\nattention mechanisms. The prevailing paradigm has commonly employed\nself-attention with quadratic computational complexity to handle global spatial\nrelationships in complex images, thereby synthesizing high-fidelity images with\ncoherent visual semantics.Contrary to conventional wisdom, our systematic\nlayer-wise analysis reveals an interesting discrepancy: self-attention in\npre-trained diffusion models predominantly exhibits localized attention\npatterns, closely resembling convolutional inductive biases. This suggests that\nglobal interactions in self-attention may be less critical than commonly\nassumed.Driven by this, we propose \\(\\Delta\\)ConvFusion to replace conventional\nself-attention modules with Pyramid Convolution Blocks\n(\\(\\Delta\\)ConvBlocks).By distilling attention patterns into localized\nconvolutional operations while keeping other components frozen,\n\\(\\Delta\\)ConvFusion achieves performance comparable to transformer-based\ncounterparts while reducing computational cost by 6929$\\times$ and surpassing\nLinFusion by 5.42$\\times$ in efficiency--all without compromising generative\nfidelity.",
        "url": "http://arxiv.org/abs/2504.21292v1",
        "published_date": "2025-04-30T03:57:28+00:00",
        "updated_date": "2025-04-30T03:57:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "ZiYi Dong",
            "Chengxing Zhou",
            "Weijian Deng",
            "Pengxu Wei",
            "Xiangyang Ji",
            "Liang Lin"
        ],
        "tldr": "this paper proposes replacing self-attention in diffusion models with a convolutional approach (Δconvfusion) that achieves comparable performance with significantly reduced computational cost, suggesting self-attention may be less crucial than currently believed.",
        "tldr_zh": "本文提出用卷积方法(Δconvfusion)替代扩散模型中的自注意力机制，该方法在计算成本显著降低的情况下，实现了可比的性能，这表明自注意力可能没有当前认为的那么重要。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Anatomical Similarity as a New Metric to Evaluate Brain Generative Models",
        "summary": "Generative models enhance neuroimaging through data augmentation, quality\nimprovement, and rare condition studies. Despite advances in realistic\nsynthetic MRIs, evaluations focus on texture and perception, lacking\nsensitivity to crucial anatomical fidelity. This study proposes a new metric,\ncalled WASABI (Wasserstein-Based Anatomical Brain Index), to assess the\nanatomical realism of synthetic brain MRIs. WASABI leverages \\textit{SynthSeg},\na deep learning-based brain parcellation tool, to derive volumetric measures of\nbrain regions in each MRI and uses the multivariate Wasserstein distance to\ncompare distributions between real and synthetic anatomies. Based on controlled\nexperiments on two real datasets and synthetic MRIs from five generative\nmodels, WASABI demonstrates higher sensitivity in quantifying anatomical\ndiscrepancies compared to traditional image-level metrics, even when synthetic\nimages achieve near-perfect visual quality. Our findings advocate for shifting\nthe evaluation paradigm beyond visual inspection and conventional metrics,\nemphasizing anatomical fidelity as a crucial benchmark for clinically\nmeaningful brain MRI synthesis. Our code is available at\nhttps://github.com/BahramJafrasteh/wasabi-mri.",
        "url": "http://arxiv.org/abs/2504.21771v1",
        "published_date": "2025-04-30T16:16:14+00:00",
        "updated_date": "2025-04-30T16:16:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bahram Jafrasteh",
            "Wei Peng",
            "Cheng Wan",
            "Yimin Luo",
            "Ehsan Adeli",
            "Qingyu Zhao"
        ],
        "tldr": "the paper introduces wasabi, a new metric for evaluating the anatomical accuracy of synthetic brain mris using wasserstein distance on brain region volumes, demonstrating its superior sensitivity compared to image-level metrics.",
        "tldr_zh": "该论文介绍了一种名为wasabi 的新指标，用于评估合成脑部 mri 的解剖学准确性，该指标使用 wasserstein 距离衡量脑区体积，并证明其比图像级指标具有更高的灵敏度。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VividListener: Expressive and Controllable Listener Dynamics Modeling for Multi-Modal Responsive Interaction",
        "summary": "Generating responsive listener head dynamics with nuanced emotions and\nexpressive reactions is crucial for practical dialogue modeling in various\nvirtual avatar animations. Previous studies mainly focus on the direct\nshort-term production of listener behavior. They overlook the fine-grained\ncontrol over motion variations and emotional intensity, especially in\nlong-sequence modeling. Moreover, the lack of long-term and large-scale paired\nspeaker-listener corpora including head dynamics and fine-grained\nmulti-modality annotations (e.g., text-based expression descriptions, emotional\nintensity) also limits the application of dialogue modeling.Therefore, we first\nnewly collect a large-scale multi-turn dataset of 3D dyadic conversation\ncontaining more than 1.4M valid frames for multi-modal responsive interaction,\ndubbed ListenerX. Additionally, we propose VividListener, a novel framework\nenabling fine-grained, expressive and controllable listener dynamics modeling.\nThis framework leverages multi-modal conditions as guiding principles for\nfostering coherent interactions between speakers and listeners.Specifically, we\ndesign the Responsive Interaction Module (RIM) to adaptively represent the\nmulti-modal interactive embeddings. RIM ensures the listener dynamics achieve\nfine-grained semantic coordination with textual descriptions and adjustments,\nwhile preserving expressive reaction with speaker behavior. Meanwhile, we\ndesign the Emotional Intensity Tags (EIT) for emotion intensity editing with\nmulti-modal information integration, applying to both text descriptions and\nlistener motion amplitude.Extensive experiments conducted on our newly\ncollected ListenerX dataset demonstrate that VividListener achieves\nstate-of-the-art performance, realizing expressive and controllable listener\ndynamics.",
        "url": "http://arxiv.org/abs/2504.21718v1",
        "published_date": "2025-04-30T15:05:12+00:00",
        "updated_date": "2025-04-30T15:05:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shiying Li",
            "Xingqun Qi",
            "Bingkun Yang",
            "Chen Weile",
            "Zezhao Tian",
            "Muyi Sun",
            "Qifeng Liu",
            "Man Zhang",
            "Zhenan Sun"
        ],
        "tldr": "the paper introduces vividlistener, a framework and a new large-scale dataset (listenerx) for generating expressive and controllable listener head dynamics in multi-modal dialogues, achieving state-of-the-art performance.",
        "tldr_zh": "该论文介绍了vividlistener，一个用于在多模态对话中生成富有表现力和可控的听者头部动态的框架，以及一个新的大型数据集(listenerx)，并实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Diffusion-based Adversarial Identity Manipulation for Facial Privacy Protection",
        "summary": "The success of face recognition (FR) systems has led to serious privacy\nconcerns due to potential unauthorized surveillance and user tracking on social\nnetworks. Existing methods for enhancing privacy fail to generate natural face\nimages that can protect facial privacy. In this paper, we propose\ndiffusion-based adversarial identity manipulation (DiffAIM) to generate natural\nand highly transferable adversarial faces against malicious FR systems. To be\nspecific, we manipulate facial identity within the low-dimensional latent space\nof a diffusion model. This involves iteratively injecting gradient-based\nadversarial identity guidance during the reverse diffusion process,\nprogressively steering the generation toward the desired adversarial faces. The\nguidance is optimized for identity convergence towards a target while promoting\nsemantic divergence from the source, facilitating effective impersonation while\nmaintaining visual naturalness. We further incorporate structure-preserving\nregularization to preserve facial structure consistency during manipulation.\nExtensive experiments on both face verification and identification tasks\ndemonstrate that compared with the state-of-the-art, DiffAIM achieves stronger\nblack-box attack transferability while maintaining superior visual quality. We\nalso demonstrate the effectiveness of the proposed approach for commercial FR\nAPIs, including Face++ and Aliyun.",
        "url": "http://arxiv.org/abs/2504.21646v1",
        "published_date": "2025-04-30T13:49:59+00:00",
        "updated_date": "2025-04-30T13:49:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liqin Wang",
            "Qianyue Hu",
            "Wei Lu",
            "Xiangyang Luo"
        ],
        "tldr": "this paper introduces diffaim, a diffusion-based method for generating adversarial faces that protect privacy by manipulating identity in the latent space, achieving strong black-box attack transferability and superior visual quality against face recognition systems.",
        "tldr_zh": "本文介绍了一种基于扩散的对抗性人脸操纵方法 diffaim，通过在潜在空间中操纵身份来生成保护隐私的对抗性人脸，从而实现强大的黑盒攻击可迁移性和针对人脸识别系统的卓越视觉质量。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling for Image Restoration",
        "summary": "Diffusion models have achieved remarkable progress in universal image\nrestoration. While existing methods speed up inference by reducing sampling\nsteps, substantial step intervals often introduce cumulative errors. Moreover,\nthey struggle to balance the commonality of degradation representations and\nrestoration quality. To address these challenges, we introduce\n\\textbf{DGSolver}, a diffusion generalist solver with universal posterior\nsampling. We first derive the exact ordinary differential equations for\ngeneralist diffusion models and tailor high-order solvers with a queue-based\naccelerated sampling strategy to improve both accuracy and efficiency. We then\nintegrate universal posterior sampling to better approximate\nmanifold-constrained gradients, yielding a more accurate noise estimation and\ncorrecting errors in inverse inference. Extensive experiments show that\nDGSolver outperforms state-of-the-art methods in restoration accuracy,\nstability, and scalability, both qualitatively and quantitatively. Code and\nmodels will be available at https://github.com/MiliLab/DGSolver.",
        "url": "http://arxiv.org/abs/2504.21487v1",
        "published_date": "2025-04-30T10:12:48+00:00",
        "updated_date": "2025-04-30T10:12:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hebaixu Wang",
            "Jing Zhang",
            "Haonan Guo",
            "Di Wang",
            "Jiayi Ma",
            "Bo Du"
        ],
        "tldr": "the paper introduces dgsolver, a diffusion model-based image restoration method that uses high-order solvers and universal posterior sampling to improve accuracy and efficiency, outperforming state-of-the-art methods.",
        "tldr_zh": "该论文介绍了dgsolver，一种基于扩散模型的图像恢复方法，它使用高阶求解器和通用后验采样来提高准确性和效率，优于目前最好的方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Revisiting Diffusion Autoencoder Training for Image Reconstruction Quality",
        "summary": "Diffusion autoencoders (DAEs) are typically formulated as a noise prediction\nmodel and trained with a linear-$\\beta$ noise schedule that spends much of its\nsampling steps at high noise levels. Because high noise levels are associated\nwith recovering large-scale image structures and low noise levels with\nrecovering details, this configuration can result in low-quality and blurry\nimages. However, it should be possible to improve details while spending fewer\nsteps recovering structures because the latent code should already contain\nstructural information. Based on this insight, we propose a new DAE training\nmethod that improves the quality of reconstructed images. We divide training\ninto two phases. In the first phase, the DAE is trained as a vanilla\nautoencoder by always setting the noise level to the highest, forcing the\nencoder and decoder to populate the latent code with structural information. In\nthe second phase, we incorporate a noise schedule that spends more time in the\nlow-noise region, allowing the DAE to learn how to perfect the details. Our\nmethod results in images that have accurate high-level structures and low-level\ndetails while still preserving useful properties of the latent codes.",
        "url": "http://arxiv.org/abs/2504.21368v1",
        "published_date": "2025-04-30T07:00:33+00:00",
        "updated_date": "2025-04-30T07:00:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Pramook Khungurn",
            "Sukit Seripanitkarn",
            "Phonphrm Thawatdamrongkit",
            "Supasorn Suwajanakorn"
        ],
        "tldr": "the paper proposes a two-phase training method for diffusion autoencoders (daes), first focusing on structural information and then on fine details, to improve image reconstruction quality.",
        "tldr_zh": "该论文提出了一种扩散自编码器（dae）的两阶段训练方法，首先侧重于结构信息，然后侧重于精细细节，以提高图像重建质量。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "Text-Conditioned Diffusion Model for High-Fidelity Korean Font Generation",
        "summary": "Automatic font generation (AFG) is the process of creating a new font using\nonly a few examples of the style images. Generating fonts for complex languages\nlike Korean and Chinese, particularly in handwritten styles, presents\nsignificant challenges. Traditional AFGs, like Generative adversarial networks\n(GANs) and Variational Auto-Encoders (VAEs), are usually unstable during\ntraining and often face mode collapse problems. They also struggle to capture\nfine details within font images. To address these problems, we present a\ndiffusion-based AFG method which generates high-quality, diverse Korean font\nimages using only a single reference image, focusing on handwritten and printed\nstyles. Our approach refines noisy images incrementally, ensuring stable\ntraining and visually appealing results. A key innovation is our text encoder,\nwhich processes phonetic representations to generate accurate and contextually\ncorrect characters, even for unseen characters. We used a pre-trained style\nencoder from DG FONT to effectively and accurately encode the style images. To\nfurther enhance the generation quality, we used perceptual loss that guides the\nmodel to focus on the global style of generated images. Experimental results on\nover 2000 Korean characters demonstrate that our model consistently generates\naccurate and detailed font images and outperforms benchmark methods, making it\na reliable tool for generating authentic Korean fonts across different styles.",
        "url": "http://arxiv.org/abs/2504.21325v1",
        "published_date": "2025-04-30T05:24:49+00:00",
        "updated_date": "2025-04-30T05:24:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Abdul Sami",
            "Avinash Kumar",
            "Irfanullah Memon",
            "Youngwon Jo",
            "Muhammad Rizwan",
            "Jaeyoung Choi"
        ],
        "tldr": "this paper introduces a text-conditioned diffusion model for high-fidelity korean font generation, addressing the challenges of generating fonts, particularly handwritten styles, for complex languages and demonstrating improved performance over gans and vaes.",
        "tldr_zh": "本文介绍了一种文本条件扩散模型，用于生成高保真的韩国字体，解决了生成字体（尤其是手写风格字体）的难题，并且展示了相比gans和vaes的性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "T2ID-CAS: Diffusion Model and Class Aware Sampling to Mitigate Class Imbalance in Neck Ultrasound Anatomical Landmark Detection",
        "summary": "Neck ultrasound (US) plays a vital role in airway management by providing\nnon-invasive, real-time imaging that enables rapid and precise interventions.\nDeep learning-based anatomical landmark detection in neck US can further\nfacilitate procedural efficiency. However, class imbalance within datasets,\nwhere key structures like tracheal rings and vocal folds are underrepresented,\npresents significant challenges for object detection models. To address this,\nwe propose T2ID-CAS, a hybrid approach that combines a text-to-image latent\ndiffusion model with class-aware sampling to generate high-quality synthetic\nsamples for underrepresented classes. This approach, rarely explored in the\nultrasound domain, improves the representation of minority classes.\nExperimental results using YOLOv9 for anatomical landmark detection in neck US\ndemonstrated that T2ID-CAS achieved a mean Average Precision of 88.2,\nsignificantly surpassing the baseline of 66. This highlights its potential as a\ncomputationally efficient and scalable solution for mitigating class imbalance\nin AI-assisted ultrasound-guided interventions.",
        "url": "http://arxiv.org/abs/2504.21231v1",
        "published_date": "2025-04-29T23:46:21+00:00",
        "updated_date": "2025-04-29T23:46:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Manikanta Varaganti",
            "Amulya Vankayalapati",
            "Nour Awad",
            "Gregory R. Dion",
            "Laura J. Brattain"
        ],
        "tldr": "this paper introduces t2id-cas, a hybrid approach combining text-to-image diffusion with class-aware sampling to address class imbalance in neck ultrasound anatomical landmark detection, showing significant improvement over a baseline yolov9 model.",
        "tldr_zh": "本文介绍了一种名为t2id-cas的混合方法，该方法结合了文本到图像的扩散模型和类感知采样，以解决颈部超声解剖标志物检测中的类别不平衡问题，并且相比yolov9基线模型有显著改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Embracing Collaboration Over Competition: Condensing Multiple Prompts for Visual In-Context Learning",
        "summary": "Visual In-Context Learning (VICL) enables adaptively solving vision tasks by\nleveraging pixel demonstrations, mimicking human-like task completion through\nanalogy. Prompt selection is critical in VICL, but current methods assume the\nexistence of a single \"ideal\" prompt in a pool of candidates, which in practice\nmay not hold true. Multiple suitable prompts may exist, but individually they\noften fall short, leading to difficulties in selection and the exclusion of\nuseful context. To address this, we propose a new perspective: prompt\ncondensation. Rather than relying on a single prompt, candidate prompts\ncollaborate to efficiently integrate informative contexts without sacrificing\nresolution. We devise Condenser, a lightweight external plugin that compresses\nrelevant fine-grained context across multiple prompts. Optimized end-to-end\nwith the backbone, Condenser ensures accurate integration of contextual cues.\nExperiments demonstrate Condenser outperforms state-of-the-arts across\nbenchmark tasks, showing superior context compression, scalability with more\nprompts, and enhanced computational efficiency compared to ensemble methods,\npositioning it as a highly competitive solution for VICL. Code is open-sourced\nat https://github.com/gimpong/CVPR25-Condenser.",
        "url": "http://arxiv.org/abs/2504.21263v1",
        "published_date": "2025-04-30T02:43:03+00:00",
        "updated_date": "2025-04-30T02:43:03+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.MM"
        ],
        "authors": [
            "Jinpeng Wang",
            "Tianci Luo",
            "Yaohua Zha",
            "Yan Feng",
            "Ruisheng Luo",
            "Bin Chen",
            "Tao Dai",
            "Long Chen",
            "Yaowei Wang",
            "Shu-Tao Xia"
        ],
        "tldr": "the paper introduces 'condenser,' a novel plugin for visual in-context learning (vicl) that compresses information from multiple prompts, outperforming existing methods on benchmark tasks.",
        "tldr_zh": "该论文介绍了一种名为'condenser'的新型插件，用于视觉上下文学习（vicl），它可以压缩来自多个提示的信息，并在基准任务上优于现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "LoC-LIC: Low Complexity Learned Image Coding Using Hierarchical Feature Transforms",
        "summary": "Current learned image compression models typically exhibit high complexity,\nwhich demands significant computational resources. To overcome these\nchallenges, we propose an innovative approach that employs hierarchical feature\nextraction transforms to significantly reduce complexity while preserving bit\nrate reduction efficiency. Our novel architecture achieves this by using fewer\nchannels for high spatial resolution inputs/feature maps. On the other hand,\nfeature maps with a large number of channels have reduced spatial dimensions,\nthereby cutting down on computational load without sacrificing performance.\nThis strategy effectively reduces the forward pass complexity from \\(1256 \\,\n\\text{kMAC/Pixel}\\) to just \\(270 \\, \\text{kMAC/Pixel}\\). As a result, the\nreduced complexity model can open the way for learned image compression models\nto operate efficiently across various devices and pave the way for the\ndevelopment of new architectures in image compression technology.",
        "url": "http://arxiv.org/abs/2504.21778v1",
        "published_date": "2025-04-30T16:30:06+00:00",
        "updated_date": "2025-04-30T16:30:06+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Ayman A. Ameen",
            "Thomas Richter",
            "André Kaup"
        ],
        "tldr": "this paper proposes a low-complexity learned image compression method using hierarchical feature transforms, significantly reducing computational requirements while maintaining bit rate reduction efficiency.",
        "tldr_zh": "本文提出了一种基于分层特征变换的低复杂度图像压缩方法，显著降低了计算需求，同时保持了比特率降低效率。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "eNCApsulate: NCA for Precision Diagnosis on Capsule Endoscopes",
        "summary": "Wireless Capsule Endoscopy is a non-invasive imaging method for the entire\ngastrointestinal tract, and is a pain-free alternative to traditional\nendoscopy. It generates extensive video data that requires significant review\ntime, and localizing the capsule after ingestion is a challenge. Techniques\nlike bleeding detection and depth estimation can help with localization of\npathologies, but deep learning models are typically too large to run directly\non the capsule. Neural Cellular Automata (NCA) for bleeding segmentation and\ndepth estimation are trained on capsule endoscopic images. For monocular depth\nestimation, we distill a large foundation model into the lean NCA architecture,\nby treating the outputs of the foundation model as pseudo ground truth. We then\nport the trained NCA to the ESP32 microcontroller, enabling efficient image\nprocessing on hardware as small as a camera capsule. NCA are more accurate\n(Dice) than other portable segmentation models, while requiring more than 100x\nfewer parameters stored in memory than other small-scale models. The visual\nresults of NCA depth estimation look convincing, and in some cases beat the\nrealism and detail of the pseudo ground truth. Runtime optimizations on the\nESP32-S3 accelerate the average inference speed significantly, by more than\nfactor 3. With several algorithmic adjustments and distillation, it is possible\nto eNCApsulate NCA models into microcontrollers that fit into wireless capsule\nendoscopes. This is the first work that enables reliable bleeding segmentation\nand depth estimation on a miniaturized device, paving the way for precise\ndiagnosis combined with visual odometry as a means of precise localization of\nthe capsule -- on the capsule.",
        "url": "http://arxiv.org/abs/2504.21562v1",
        "published_date": "2025-04-30T12:06:56+00:00",
        "updated_date": "2025-04-30T12:06:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Henry John Krumb",
            "Anirban Mukhopadhyay"
        ],
        "tldr": "this paper introduces an efficient neural cellular automata (nca) approach for bleeding segmentation and depth estimation on capsule endoscopes, enabling on-device processing with significantly reduced parameters and improved accuracy compared to other portable models.",
        "tldr_zh": "该论文介绍了一种高效的神经元胞自动机（nca）方法，用于在胶囊内窥镜上进行出血分割和深度估计，与其它便携式模型相比，它能够以显著减少的参数和提高的精度实现设备上的处理。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 4
    },
    {
        "title": "Towards Improved Cervical Cancer Screening: Vision Transformer-Based Classification and Interpretability",
        "summary": "We propose a novel approach to cervical cell image classification for\ncervical cancer screening using the EVA-02 transformer model. We developed a\nfour-step pipeline: fine-tuning EVA-02, feature extraction, selecting important\nfeatures through multiple machine learning models, and training a new\nartificial neural network with optional loss weighting for improved\ngeneralization. With this design, our best model achieved an F1-score of\n0.85227, outperforming the baseline EVA-02 model (0.84878). We also utilized\nKernel SHAP analysis and identified key features correlating with cell\nmorphology and staining characteristics, providing interpretable insights into\nthe decision-making process of the fine-tuned model. Our code is available at\nhttps://github.com/Khoa-NT/isbi2025_ps3c.",
        "url": "http://arxiv.org/abs/2504.21340v1",
        "published_date": "2025-04-30T05:59:56+00:00",
        "updated_date": "2025-04-30T05:59:56+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Khoa Tuan Nguyen",
            "Ho-min Park",
            "Gaeun Oh",
            "Joris Vankerschaver",
            "Wesley De Neve"
        ],
        "tldr": "this paper presents a vision transformer-based pipeline for cervical cancer screening, achieving a slightly improved f1-score compared to the baseline and providing interpretability through shap analysis. the code is publicly available.",
        "tldr_zh": "本文提出了一种基于vision transformer的宫颈癌筛查流程，与基线相比，f1分数略有提高，并通过shap分析提供可解释性。代码已公开。",
        "relevance_score": 2,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 4
    },
    {
        "title": "Legilimens: Performant Video Analytics on the System-on-Chip Edge",
        "summary": "Continually retraining models has emerged as a primary technique to enable\nhigh-accuracy video analytics on edge devices. Yet, existing systems employ\nsuch adaptation by relying on the spare compute resources that traditional\n(memory-constrained) edge servers afford. In contrast, mobile edge devices such\nas drones and dashcams offer a fundamentally different resource profile:\nweak(er) compute with abundant unified memory pools. We present Legilimens, a\ncontinuous learning system for the mobile edge's System-on-Chip GPUs. Our\ndriving insight is that visually distinct scenes that require retraining\nexhibit substantial overlap in model embeddings; if captured into a base model\non device memory, specializing to each new scene can become lightweight,\nrequiring very few samples. To practically realize this approach, Legilimens\npresents new, compute-efficient techniques to (1) select high-utility data\nsamples for retraining specialized models, (2) update the base model without\ncomplete retraining, and (3) time-share compute resources between retraining\nand live inference for maximal accuracy. Across diverse workloads, Legilimens\nlowers retraining costs by 2.8-10x compared to existing systems, resulting in\n18-45% higher accuracies.",
        "url": "http://arxiv.org/abs/2504.21136v1",
        "published_date": "2025-04-29T19:45:33+00:00",
        "updated_date": "2025-04-29T19:45:33+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Murali Ramanujam",
            "Yinwei Dai",
            "Kyle Jamieson",
            "Ravi Netravali"
        ],
        "tldr": "this paper presents legilimens, a continuous learning system for video analytics on mobile edge devices, which reduces retraining costs and improves accuracy by exploiting overlapping model embeddings and employing compute-efficient techniques.",
        "tldr_zh": "本文介绍了legilimens，一个用于移动边缘设备上视频分析的持续学习系统。该系统通过利用重叠的模型嵌入和采用计算高效的技术，从而降低了重新训练的成本并提高了准确性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    }
]