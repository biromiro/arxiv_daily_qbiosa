[
    {
        "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
        "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from $63\\%$ to $95\\%$. In visual text rendering, its accuracy\nimproves from $59\\%$ to $92\\%$, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments.",
        "url": "http://arxiv.org/abs/2505.05470v1",
        "published_date": "2025-05-08T17:58:45+00:00",
        "updated_date": "2025-05-08T17:58:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jie Liu",
            "Gongye Liu",
            "Jiajun Liang",
            "Yangguang Li",
            "Jiaheng Liu",
            "Xintao Wang",
            "Pengfei Wan",
            "Di Zhang",
            "Wanli Ouyang"
        ],
        "tldr": "flow-grpo integrates online reinforcement learning with flow matching models, using ode-to-sde conversion and denoising reduction to improve sampling efficiency and performance in text-to-image tasks, achieving significant gains in accuracy and alignment.",
        "tldr_zh": "flow-grpo 将在线强化学习与流匹配模型相结合，通过 ode 到 sde 的转换和降噪减少来提高文本到图像任务中的采样效率和性能，从而在准确性和对齐方面取得显著提高。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models",
        "summary": "Thanks to recent advancements in scalable deep architectures and large-scale\npretraining, text-to-video generation has achieved unprecedented capabilities\nin producing high-fidelity, instruction-following content across a wide range\nof styles, enabling applications in advertising, entertainment, and education.\nHowever, these models' ability to render precise on-screen text, such as\ncaptions or mathematical formulas, remains largely untested, posing significant\nchallenges for applications requiring exact textual accuracy. In this work, we\nintroduce T2VTextBench, the first human-evaluation benchmark dedicated to\nevaluating on-screen text fidelity and temporal consistency in text-to-video\nmodels. Our suite of prompts integrates complex text strings with dynamic scene\nchanges, testing each model's ability to maintain detailed instructions across\nframes. We evaluate ten state-of-the-art systems, ranging from open-source\nsolutions to commercial offerings, and find that most struggle to generate\nlegible, consistent text. These results highlight a critical gap in current\nvideo generators and provide a clear direction for future research aimed at\nenhancing textual manipulation in video synthesis.",
        "url": "http://arxiv.org/abs/2505.04946v1",
        "published_date": "2025-05-08T04:49:52+00:00",
        "updated_date": "2025-05-08T04:49:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Xuyang Guo",
            "Jiayan Huo",
            "Zhenmei Shi",
            "Zhao Song",
            "Jiahao Zhang",
            "Jiale Zhao"
        ],
        "tldr": "the paper introduces t2vtextbench, a new human evaluation benchmark for assessing the fidelity and temporal consistency of on-screen text in text-to-video generation models, revealing a current weakness in generating legible and consistent text.",
        "tldr_zh": "该论文介绍了 t2vtextbench，一个新的用于评估文本到视频生成模型中屏幕文本的保真度和时间一致性的人工评估基准，揭示了当前在生成清晰且一致的文本方面的弱点。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 10,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with Video Diffusion and Data Augmentation",
        "summary": "Creating high-quality animatable 3D human avatars from a single image remains\na significant challenge in computer vision due to the inherent difficulty of\nreconstructing complete 3D information from a single viewpoint. Current\napproaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods\nproduce high-quality results but require multiple views or video sequences,\nwhile video diffusion models can generate animations from single images but\nstruggle with consistency and identity preservation. We present SVAD, a novel\napproach that addresses these limitations by leveraging complementary strengths\nof existing techniques. Our method generates synthetic training data through\nvideo diffusion, enhances it with identity preservation and image restoration\nmodules, and utilizes this refined data to train 3DGS avatars. Comprehensive\nevaluations demonstrate that SVAD outperforms state-of-the-art (SOTA)\nsingle-image methods in maintaining identity consistency and fine details\nacross novel poses and viewpoints, while enabling real-time rendering\ncapabilities. Through our data augmentation pipeline, we overcome the\ndependency on dense monocular or multi-view training data typically required by\ntraditional 3DGS approaches. Extensive quantitative, qualitative comparisons\nshow our method achieves superior performance across multiple metrics against\nbaseline models. By effectively combining the generative power of diffusion\nmodels with both the high-quality results and rendering efficiency of 3DGS, our\nwork establishes a new approach for high-fidelity avatar generation from a\nsingle image input.",
        "url": "http://arxiv.org/abs/2505.05475v1",
        "published_date": "2025-05-08T17:59:58+00:00",
        "updated_date": "2025-05-08T17:59:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yonwoo Choi"
        ],
        "tldr": "the paper introduces svad, a novel method for generating high-fidelity animatable 3d avatars from a single image by combining video diffusion for synthetic data generation with 3d gaussian splatting for high-quality rendering.",
        "tldr_zh": "该论文介绍了svad，一种新颖的方法，通过结合视频扩散生成合成数据和3d高斯溅射实现高质量渲染，从而仅从单张图像生成高保真可动画的3d头像。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation",
        "summary": "Recent progress in unified models for image understanding and generation has\nbeen impressive, yet most approaches remain limited to single-modal generation\nconditioned on multiple modalities. In this paper, we present Mogao, a unified\nframework that advances this paradigm by enabling interleaved multi-modal\ngeneration through a causal approach. Mogao integrates a set of key technical\nimprovements in architecture design, including a deep-fusion design, dual\nvision encoders, interleaved rotary position embeddings, and multi-modal\nclassifier-free guidance, which allow it to harness the strengths of both\nautoregressive models for text generation and diffusion models for high-quality\nimage synthesis. These practical improvements also make Mogao particularly\neffective to process interleaved sequences of text and images arbitrarily. To\nfurther unlock the potential of unified models, we introduce an efficient\ntraining strategy on a large-scale, in-house dataset specifically curated for\njoint text and image generation. Extensive experiments show that Mogao not only\nachieves state-of-the-art performance in multi-modal understanding and\ntext-to-image generation, but also excels in producing high-quality, coherent\ninterleaved outputs. Its emergent capabilities in zero-shot image editing and\ncompositional generation highlight Mogao as a practical omni-modal foundation\nmodel, paving the way for future development and scaling the unified\nmulti-modal systems.",
        "url": "http://arxiv.org/abs/2505.05472v1",
        "published_date": "2025-05-08T17:58:57+00:00",
        "updated_date": "2025-05-08T17:58:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chao Liao",
            "Liyang Liu",
            "Xun Wang",
            "Zhengxiong Luo",
            "Xinyu Zhang",
            "Wenliang Zhao",
            "Jie Wu",
            "Liang Li",
            "Zhi Tian",
            "Weilin Huang"
        ],
        "tldr": "the paper introduces mogao, a unified framework for interleaved multi-modal generation, combining autoregressive and diffusion models with novel architectural designs and a curated dataset to achieve sota performance in understanding, text-to-image, and interleaved generation tasks.",
        "tldr_zh": "该论文介绍了一种名为mogao的统一框架，用于交错多模态生成，它结合了自回归和扩散模型，并采用了新颖的架构设计和精心策划的数据集，从而在理解、文本到图像以及交错生成任务中实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant",
        "summary": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.",
        "url": "http://arxiv.org/abs/2505.05467v1",
        "published_date": "2025-05-08T17:57:40+00:00",
        "updated_date": "2025-05-08T17:57:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Haibo Wang",
            "Bo Feng",
            "Zhengfeng Lai",
            "Mingze Xu",
            "Shiyu Li",
            "Weifeng Ge",
            "Afshin Dehghan",
            "Meng Cao",
            "Ping Huang"
        ],
        "tldr": "the paper introduces streambridge, a framework that adapts offline video-llms for streaming video understanding by incorporating memory and proactive response mechanisms, and presents a corresponding dataset, stream-it, demonstrating improvements over existing models.",
        "tldr_zh": "该论文介绍了streambridge，一个将离线视频大语言模型适配于流式视频理解的框架，通过结合记忆机制和主动响应机制，并提出了相应的数据集stream-it，实验证明该框架优于现有模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation",
        "summary": "Pioneering token-based works such as Chameleon and Emu3 have established a\nfoundation for multimodal unification but face challenges of high training\ncomputational overhead and limited comprehension performance due to a lack of\nhigh-level semantics. In this paper, we introduce TokLIP, a visual tokenizer\nthat enhances comprehension by semanticizing vector-quantized (VQ) tokens and\nincorporating CLIP-level semantics while enabling end-to-end multimodal\nautoregressive training with standard VQ tokens. TokLIP integrates a low-level\ndiscrete VQ tokenizer with a ViT-based token encoder to capture high-level\ncontinuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize\nhigh-level features, TokLIP disentangles training objectives for comprehension\nand generation, allowing the direct application of advanced VQ tokenizers\nwithout the need for tailored quantization operations. Our empirical results\ndemonstrate that TokLIP achieves exceptional data efficiency, empowering visual\ntokens with high-level semantic understanding while enhancing low-level\ngenerative capacity, making it well-suited for autoregressive Transformers in\nboth comprehension and generation tasks. The code and models are available at\nhttps://github.com/TencentARC/TokLIP.",
        "url": "http://arxiv.org/abs/2505.05422v1",
        "published_date": "2025-05-08T17:12:19+00:00",
        "updated_date": "2025-05-08T17:12:19+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Haokun Lin",
            "Teng Wang",
            "Yixiao Ge",
            "Yuying Ge",
            "Zhichao Lu",
            "Ying Wei",
            "Qingfu Zhang",
            "Zhenan Sun",
            "Ying Shan"
        ],
        "tldr": "toklip introduces a novel visual tokenizer that integrates low-level discrete vq tokens with a vit-based token encoder to enhance multimodal comprehension and generation by incorporating clip-level semantics, achieving data efficiency and improved generative capacity.",
        "tldr_zh": "toklip 引入了一种新的视觉分词器，它将低级离散 vq 标记与基于 vit 的标记编码器集成，通过结合 clip 级别的语义来增强多模态理解和生成，从而实现数据效率和改进的生成能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EAM: Enhancing Anything with Diffusion Transformers for Blind Super-Resolution",
        "summary": "Utilizing pre-trained Text-to-Image (T2I) diffusion models to guide Blind\nSuper-Resolution (BSR) has become a predominant approach in the field. While\nT2I models have traditionally relied on U-Net architectures, recent\nadvancements have demonstrated that Diffusion Transformers (DiT) achieve\nsignificantly higher performance in this domain. In this work, we introduce\nEnhancing Anything Model (EAM), a novel BSR method that leverages DiT and\noutperforms previous U-Net-based approaches. We introduce a novel block,\n$\\Psi$-DiT, which effectively guides the DiT to enhance image restoration. This\nblock employs a low-resolution latent as a separable flow injection control,\nforming a triple-flow architecture that effectively leverages the prior\nknowledge embedded in the pre-trained DiT. To fully exploit the prior guidance\ncapabilities of T2I models and enhance their generalization in BSR, we\nintroduce a progressive Masked Image Modeling strategy, which also reduces\ntraining costs. Additionally, we propose a subject-aware prompt generation\nstrategy that employs a robust multi-modal model in an in-context learning\nframework. This strategy automatically identifies key image areas, provides\ndetailed descriptions, and optimizes the utilization of T2I diffusion priors.\nOur experiments demonstrate that EAM achieves state-of-the-art results across\nmultiple datasets, outperforming existing methods in both quantitative metrics\nand visual quality.",
        "url": "http://arxiv.org/abs/2505.05209v1",
        "published_date": "2025-05-08T13:03:07+00:00",
        "updated_date": "2025-05-08T13:03:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haizhen Xie",
            "Kunpeng Du",
            "Qiangyu Yan",
            "Sen Lu",
            "Jianhong Han",
            "Hanting Chen",
            "Hailin Hu",
            "Jie Hu"
        ],
        "tldr": "the paper introduces eam, a novel blind super-resolution method using diffusion transformers (dit) and a triple-flow architecture with a new block called $\\psi$-dit, guided by a progressive masked image modeling strategy and subject-aware prompt generation, achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了一种新的盲超分辨率方法eam，它使用扩散transformer (dit) 和具有一个名为$\\psi$-dit的新模块的三流架构，并通过渐进式掩码图像建模策略和主题感知提示生成进行指导，从而实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via Diffusion Models",
        "summary": "Multi-object editing aims to modify multiple objects or regions in complex\nscenes while preserving structural coherence. This task faces significant\nchallenges in scenarios involving overlapping or interacting objects: (1)\nInaccurate localization of target objects due to attention misalignment,\nleading to incomplete or misplaced edits; (2) Attribute-object mismatch, where\ncolor or texture changes fail to align with intended regions due to\ncross-attention leakage, creating semantic conflicts (\\textit{e.g.}, color\nbleeding into non-target areas). Existing methods struggle with these\nchallenges: approaches relying on global cross-attention mechanisms suffer from\nattention dilution and spatial interference between objects, while mask-based\nmethods fail to bind attributes to geometrically accurate regions due to\nfeature entanglement in multi-object scenarios. To address these limitations,\nwe propose a training-free, inference-stage optimization approach that enables\nprecise localized image manipulation in complex multi-object scenes, named\nMDE-Edit. MDE-Edit optimizes the noise latent feature in diffusion models via\ntwo key losses: Object Alignment Loss (OAL) aligns multi-layer cross-attention\nwith segmentation masks for precise object positioning, and Color Consistency\nLoss (CCL) amplifies target attribute attention within masks while suppressing\nleakage to adjacent regions. This dual-loss design ensures localized and\ncoherent multi-object edits. Extensive experiments demonstrate that MDE-Edit\noutperforms state-of-the-art methods in editing accuracy and visual quality,\noffering a robust solution for complex multi-object image manipulation tasks.",
        "url": "http://arxiv.org/abs/2505.05101v1",
        "published_date": "2025-05-08T10:01:14+00:00",
        "updated_date": "2025-05-08T10:01:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongyang Zhu",
            "Haipeng Liu",
            "Bo Fu",
            "Yang Wang"
        ],
        "tldr": "the paper introduces mde-edit, a training-free, inference-stage optimization method for multi-object image editing using diffusion models, which addresses the challenges of inaccurate localization and attribute-object mismatch through object alignment loss (oal) and color consistency loss (ccl). it outperforms sota methods in editing accuracy and visual quality.",
        "tldr_zh": "该论文介绍了mde-edit，一种无需训练的推理阶段优化方法，用于使用扩散模型进行多对象图像编辑，通过对象对齐损失 (oal) 和颜色一致性损失 (ccl) 解决了不准确的定位和属性-对象不匹配的挑战。 它在编辑准确性和视觉质量方面优于 sota 方法。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Inter-Diffusion Generation Model of Speakers and Listeners for Effective Communication",
        "summary": "Full-body gestures play a pivotal role in natural interactions and are\ncrucial for achieving effective communication. Nevertheless, most existing\nstudies primarily focus on the gesture generation of speakers, overlooking the\nvital role of listeners in the interaction process and failing to fully explore\nthe dynamic interaction between them. This paper innovatively proposes an\nInter-Diffusion Generation Model of Speakers and Listeners for Effective\nCommunication. For the first time, we integrate the full-body gestures of\nlisteners into the generation framework. By devising a novel inter-diffusion\nmechanism, this model can accurately capture the complex interaction patterns\nbetween speakers and listeners during communication. In the model construction\nprocess, based on the advanced diffusion model architecture, we innovatively\nintroduce interaction conditions and the GAN model to increase the denoising\nstep size. As a result, when generating gesture sequences, the model can not\nonly dynamically generate based on the speaker's speech information but also\nrespond in realtime to the listener's feedback, enabling synergistic\ninteraction between the two. Abundant experimental results demonstrate that\ncompared with the current state-of-the-art gesture generation methods, the\nmodel we proposed has achieved remarkable improvements in the naturalness,\ncoherence, and speech-gesture synchronization of the generated gestures. In the\nsubjective evaluation experiments, users highly praised the generated\ninteraction scenarios, believing that they are closer to real life human\ncommunication situations. Objective index evaluations also show that our model\noutperforms the baseline methods in multiple key indicators, providing more\npowerful support for effective communication.",
        "url": "http://arxiv.org/abs/2505.04996v1",
        "published_date": "2025-05-08T07:00:58+00:00",
        "updated_date": "2025-05-08T07:00:58+00:00",
        "categories": [
            "cs.GR",
            "cs.CV",
            "cs.SD",
            "eess.AS"
        ],
        "authors": [
            "Jinhe Huang",
            "Yongkang Cheng",
            "Yuming Hang",
            "Gaoge Han",
            "Jinewei Li",
            "Jing Zhang",
            "Xingjian Gu"
        ],
        "tldr": "this paper introduces an inter-diffusion model for generating full-body gestures of both speakers and listeners, capturing their interaction dynamics for more realistic communication. it incorporates listener feedback into the generation process, demonstrating improved naturalness and coherence compared to existing methods.",
        "tldr_zh": "本文介绍了一种交互扩散模型，用于生成说话者和听众的全身手势，捕捉他们之间的互动动态，从而实现更真实的交流。它将听众的反馈纳入生成过程，与现有方法相比，展示了更高的自然性和连贯性。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing",
        "summary": "Scene text editing, a subfield of image editing, requires modifying texts in\nimages while preserving style consistency and visual coherence with the\nsurrounding environment. While diffusion-based methods have shown promise in\ntext generation, they still struggle to produce high-quality results. These\nmethods often generate distorted or unrecognizable characters, particularly\nwhen dealing with complex characters like Chinese. In such systems, characters\nare composed of intricate stroke patterns and spatial relationships that must\nbe precisely maintained. We present GlyphMastero, a specialized glyph encoder\ndesigned to guide the latent diffusion model for generating texts with\nstroke-level precision. Our key insight is that existing methods, despite using\npretrained OCR models for feature extraction, fail to capture the hierarchical\nnature of text structures - from individual strokes to stroke-level\ninteractions to overall character-level structure. To address this, our glyph\nencoder explicitly models and captures the cross-level interactions between\nlocal-level individual characters and global-level text lines through our novel\nglyph attention module. Meanwhile, our model implements a feature pyramid\nnetwork to fuse the multi-scale OCR backbone features at the global-level.\nThrough these cross-level and multi-scale fusions, we obtain more detailed\nglyph-aware guidance, enabling precise control over the scene text generation\nprocess. Our method achieves an 18.02\\% improvement in sentence accuracy over\nthe state-of-the-art multi-lingual scene text editing baseline, while\nsimultaneously reducing the text-region Fr\\'echet inception distance by\n53.28\\%.",
        "url": "http://arxiv.org/abs/2505.04915v1",
        "published_date": "2025-05-08T03:11:58+00:00",
        "updated_date": "2025-05-08T03:11:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tong Wang",
            "Ting Liu",
            "Xiaochao Qu",
            "Chengjing Wu",
            "Luoqi Liu",
            "Xiaolin Hu"
        ],
        "tldr": "the paper introduces glyphmastero, a glyph encoder that enhances diffusion-based scene text editing by capturing hierarchical text structures and improving character generation fidelity, particularly for complex characters like chinese.",
        "tldr_zh": "该论文介绍了glyphmastero，一种字形编码器，通过捕捉分层文本结构并提高字符生成保真度（尤其是对于像中文这样的复杂字符），从而增强了基于扩散的场景文本编辑。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Lay-Your-Scene: Natural Scene Layout Generation with Diffusion Transformers",
        "summary": "We present Lay-Your-Scene (shorthand LayouSyn), a novel text-to-layout\ngeneration pipeline for natural scenes. Prior scene layout generation methods\nare either closed-vocabulary or use proprietary large language models for\nopen-vocabulary generation, limiting their modeling capabilities and broader\napplicability in controllable image generation. In this work, we propose to use\nlightweight open-source language models to obtain scene elements from text\nprompts and a novel aspect-aware diffusion Transformer architecture trained in\nan open-vocabulary manner for conditional layout generation. Extensive\nexperiments demonstrate that LayouSyn outperforms existing methods and achieves\nstate-of-the-art performance on challenging spatial and numerical reasoning\nbenchmarks. Additionally, we present two applications of LayouSyn. First, we\nshow that coarse initialization from large language models can be seamlessly\ncombined with our method to achieve better results. Second, we present a\npipeline for adding objects to images, demonstrating the potential of LayouSyn\nin image editing applications.",
        "url": "http://arxiv.org/abs/2505.04718v1",
        "published_date": "2025-05-07T18:07:57+00:00",
        "updated_date": "2025-05-07T18:07:57+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Divyansh Srivastava",
            "Xiang Zhang",
            "He Wen",
            "Chenru Wen",
            "Zhuowen Tu"
        ],
        "tldr": "the paper introduces lay-your-scene, a new text-to-layout generation pipeline using open-source language models and a diffusion transformer architecture. it outperforms existing methods in spatial and numerical reasoning and demonstrates applications in image editing.",
        "tldr_zh": "该论文介绍了 lay-your-scene，一种新的文本到布局生成流程，它使用开源语言模型和扩散transformer架构。 该方法在空间和数值推理方面优于现有方法，并展示了其在图像编辑中的应用。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Diffusion Model Quantization: A Review",
        "summary": "Recent success of large text-to-image models has empirically underscored the\nexceptional performance of diffusion models in generative tasks. To facilitate\ntheir efficient deployment on resource-constrained edge devices, model\nquantization has emerged as a pivotal technique for both compression and\nacceleration. This survey offers a thorough review of the latest advancements\nin diffusion model quantization, encapsulating and analyzing the current state\nof the art in this rapidly advancing domain. First, we provide an overview of\nthe key challenges encountered in the quantization of diffusion models,\nincluding those based on U-Net architectures and Diffusion Transformers (DiT).\nWe then present a comprehensive taxonomy of prevalent quantization techniques,\nengaging in an in-depth discussion of their underlying principles.\nSubsequently, we perform a meticulous analysis of representative diffusion\nmodel quantization schemes from both qualitative and quantitative perspectives.\nFrom a quantitative standpoint, we rigorously benchmark a variety of methods\nusing widely recognized datasets, delivering an extensive evaluation of the\nmost recent and impactful research in the field. From a qualitative standpoint,\nwe categorize and synthesize the effects of quantization errors, elucidating\nthese impacts through both visual analysis and trajectory examination. In\nconclusion, we outline prospective avenues for future research, proposing novel\ndirections for the quantization of generative models in practical applications.\nThe list of related papers, corresponding codes, pre-trained models and\ncomparison results are publicly available at the survey project homepage\nhttps://github.com/TaylorJocelyn/Diffusion-Model-Quantization.",
        "url": "http://arxiv.org/abs/2505.05215v1",
        "published_date": "2025-05-08T13:09:34+00:00",
        "updated_date": "2025-05-08T13:09:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qian Zeng",
            "Chenggong Hu",
            "Mingli Song",
            "Jie Song"
        ],
        "tldr": "this paper surveys recent advancements in diffusion model quantization, providing a taxonomy of techniques and an analysis of their qualitative and quantitative impacts, with a focus on deployment on resource-constrained devices.",
        "tldr_zh": "本文综述了扩散模型量化的最新进展，提供了技术分类，并分析了其定性和定量影响，重点关注在资源受限设备上的部署。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "PIDiff: Image Customization for Personalized Identities with Diffusion Models",
        "summary": "Text-to-image generation for personalized identities aims at incorporating\nthe specific identity into images using a text prompt and an identity image.\nBased on the powerful generative capabilities of DDPMs, many previous works\nadopt additional prompts, such as text embeddings and CLIP image embeddings, to\nrepresent the identity information, while they fail to disentangle the identity\ninformation and background information. As a result, the generated images not\nonly lose key identity characteristics but also suffer from significantly\nreduced diversity. To address this issue, previous works have combined the W+\nspace from StyleGAN with diffusion models, leveraging this space to provide a\nmore accurate and comprehensive representation of identity features through\nmulti-level feature extraction. However, the entanglement of identity and\nbackground information in in-the-wild images during training prevents accurate\nidentity localization, resulting in severe semantic interference between\nidentity and background. In this paper, we propose a novel fine-tuning-based\ndiffusion model for personalized identities text-to-image generation, named\nPIDiff, which leverages the W+ space and an identity-tailored fine-tuning\nstrategy to avoid semantic entanglement and achieves accurate feature\nextraction and localization. Style editing can also be achieved by PIDiff\nthrough preserving the characteristics of identity features in the W+ space,\nwhich vary from coarse to fine. Through the combination of the proposed\ncross-attention block and parameter optimization strategy, PIDiff preserves the\nidentity information and maintains the generation capability for in-the-wild\nimages of the pre-trained model during inference. Our experimental results\nvalidate the effectiveness of our method in this task.",
        "url": "http://arxiv.org/abs/2505.05081v1",
        "published_date": "2025-05-08T09:26:28+00:00",
        "updated_date": "2025-05-08T09:26:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinyu Gu",
            "Haipeng Liu",
            "Meng Wang",
            "Yang Wang"
        ],
        "tldr": "the paper introduces pidiff, a fine-tuning-based diffusion model for personalized text-to-image generation aimed at disentangling identity and background information using w+ space and an identity-tailored fine-tuning strategy, improving identity preservation and generation diversity.",
        "tldr_zh": "该论文介绍了pidiff，一种基于微调的扩散模型，用于个性化的文本到图像生成，旨在利用w+空间和定制的身份微调策略来分离身份和背景信息，从而提高身份保持和生成的多样性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SOAP: Style-Omniscient Animatable Portraits",
        "summary": "Creating animatable 3D avatars from a single image remains challenging due to\nstyle limitations (realistic, cartoon, anime) and difficulties in handling\naccessories or hairstyles. While 3D diffusion models advance single-view\nreconstruction for general objects, outputs often lack animation controls or\nsuffer from artifacts because of the domain gap. We propose SOAP, a\nstyle-omniscient framework to generate rigged, topology-consistent avatars from\nany portrait. Our method leverages a multiview diffusion model trained on 24K\n3D heads with multiple styles and an adaptive optimization pipeline to deform\nthe FLAME mesh while maintaining topology and rigging via differentiable\nrendering. The resulting textured avatars support FACS-based animation,\nintegrate with eyeballs and teeth, and preserve details like braided hair or\naccessories. Extensive experiments demonstrate the superiority of our method\nover state-of-the-art techniques for both single-view head modeling and\ndiffusion-based generation of Image-to-3D. Our code and data are publicly\navailable for research purposes at https://github.com/TingtingLiao/soap.",
        "url": "http://arxiv.org/abs/2505.05022v1",
        "published_date": "2025-05-08T07:56:16+00:00",
        "updated_date": "2025-05-08T07:56:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tingting Liao",
            "Yujian Zheng",
            "Adilbek Karmanov",
            "Liwen Hu",
            "Leyang Jin",
            "Yuliang Xiu",
            "Hao Li"
        ],
        "tldr": "soap introduces a style-omniscient framework that generates animatable and rigged 3d avatars from single portrait images, overcoming style and accessory limitations found in previous methods using a multi-view diffusion model and adaptive optimization.",
        "tldr_zh": "soap 提出了一种风格全知的框架，可以从单张肖像图像生成可动画和可装配的 3d 头像，通过使用多视图扩散模型和自适应优化，克服了以往方法在风格和配饰方面的限制。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CAG-VLM: Fine-Tuning of a Large-Scale Model to Recognize Angiographic Images for Next-Generation Diagnostic Systems",
        "summary": "Coronary angiography (CAG) is the gold-standard imaging modality for\nevaluating coronary artery disease, but its interpretation and subsequent\ntreatment planning rely heavily on expert cardiologists. To enable AI-based\ndecision support, we introduce a two-stage, physician-curated pipeline and a\nbilingual (Japanese/English) CAG image-report dataset. First, we sample 14,686\nframes from 539 exams and annotate them for key-frame detection and left/right\nlaterality; a ConvNeXt-Base CNN trained on this data achieves 0.96 F1 on\nlaterality classification, even on low-contrast frames. Second, we apply the\nCNN to 243 independent exams, extract 1,114 key frames, and pair each with its\npre-procedure report and expert-validated diagnostic and treatment summary,\nyielding a parallel corpus. We then fine-tune three open-source VLMs\n(PaliGemma2, Gemma3, and ConceptCLIP-enhanced Gemma3) via LoRA and evaluate\nthem using VLScore and cardiologist review. Although PaliGemma2 w/LoRA attains\nthe highest VLScore, Gemma3 w/LoRA achieves the top clinician rating (mean\n7.20/10); we designate this best-performing model as CAG-VLM. These results\ndemonstrate that specialized, fine-tuned VLMs can effectively assist\ncardiologists in generating clinical reports and treatment recommendations from\nCAG images.",
        "url": "http://arxiv.org/abs/2505.04964v1",
        "published_date": "2025-05-08T05:44:52+00:00",
        "updated_date": "2025-05-08T05:44:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuto Nakamura",
            "Satoshi Kodera",
            "Haruki Settai",
            "Hiroki Shinohara",
            "Masatsugu Tamura",
            "Tomohiro Noguchi",
            "Tatsuki Furusawa",
            "Ryo Takizawa",
            "Tempei Kabayama",
            "Norihiko Takeda"
        ],
        "tldr": "the paper presents cag-vlm, a fine-tuned large-scale vision-language model (vlm) for assisting cardiologists in interpreting coronary angiograms and generating treatment recommendations by leveraging a physician-curated bilingual dataset and lora fine-tuning of open-source vlms.",
        "tldr_zh": "该论文介绍了cag-vlm，这是一种微调的大型视觉语言模型（vlm），通过利用医生策划的双语数据集和对开源vlms的lora微调，来帮助心脏病专家解释冠状动脉造影图像并生成治疗建议。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "ViCTr: Vital Consistency Transfer for Pathology Aware Image Synthesis",
        "summary": "Synthesizing medical images remains challenging due to limited annotated\npathological data, modality domain gaps, and the complexity of representing\ndiffuse pathologies such as liver cirrhosis. Existing methods often struggle to\nmaintain anatomical fidelity while accurately modeling pathological features,\nfrequently relying on priors derived from natural images or inefficient\nmulti-step sampling. In this work, we introduce ViCTr (Vital Consistency\nTransfer), a novel two-stage framework that combines a rectified flow\ntrajectory with a Tweedie-corrected diffusion process to achieve high-fidelity,\npathology-aware image synthesis. First, we pretrain ViCTr on the ATLAS-8k\ndataset using Elastic Weight Consolidation (EWC) to preserve critical\nanatomical structures. We then fine-tune the model adversarially with Low-Rank\nAdaptation (LoRA) modules for precise control over pathology severity. By\nreformulating Tweedie's formula within a linear trajectory framework, ViCTr\nsupports one-step sampling, reducing inference from 50 steps to just 4, without\nsacrificing anatomical realism. We evaluate ViCTr on BTCV (CT), AMOS (MRI), and\nCirrMRI600+ (cirrhosis) datasets. Results demonstrate state-of-the-art\nperformance, achieving a Medical Frechet Inception Distance (MFID) of 17.01 for\ncirrhosis synthesis 28% lower than existing approaches and improving nnUNet\nsegmentation by +3.8% mDSC when used for data augmentation. Radiologist reviews\nindicate that ViCTr-generated liver cirrhosis MRIs are clinically\nindistinguishable from real scans. To our knowledge, ViCTr is the first method\nto provide fine-grained, pathology-aware MRI synthesis with graded severity\ncontrol, closing a critical gap in AI-driven medical imaging research.",
        "url": "http://arxiv.org/abs/2505.04963v1",
        "published_date": "2025-05-08T05:44:16+00:00",
        "updated_date": "2025-05-08T05:44:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Onkar Susladkar",
            "Gayatri Deshmukh",
            "Yalcin Tur",
            "Ulas Bagci"
        ],
        "tldr": "victr is a two-stage framework for high-fidelity, pathology-aware medical image synthesis, achieving state-of-the-art results in generating liver cirrhosis mris with fine-grained severity control.",
        "tldr_zh": "victr是一个两阶段框架，用于高保真、感知病理的医学图像合成，在生成具有细粒度严重程度控制的肝硬化mri方面取得了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Canny2Palm: Realistic and Controllable Palmprint Generation for Large-scale Pre-training",
        "summary": "Palmprint recognition is a secure and privacy-friendly method of biometric\nidentification. One of the major challenges to improve palmprint recognition\naccuracy is the scarcity of palmprint data. Recently, a popular line of\nresearch revolves around the synthesis of virtual palmprints for large-scale\npre-training purposes. In this paper, we propose a novel synthesis method named\nCanny2Palm that extracts palm textures with Canny edge detector and uses them\nto condition a Pix2Pix network for realistic palmprint generation. By\nre-assembling palmprint textures from different identities, we are able to\ncreate new identities by seeding the generator with new assemblies. Canny2Palm\nnot only synthesizes realistic data following the distribution of real\npalmprints but also enables controllable diversity to generate large-scale new\nidentities. On open-set palmprint recognition benchmarks, models pre-trained\nwith Canny2Palm synthetic data outperform the state-of-the-art with up to 7.2%\nhigher identification accuracy. Moreover, the performance of models pre-trained\nwith Canny2Palm continues to improve given 10,000 synthetic IDs while those\nwith existing methods already saturate, demonstrating the potential of our\nmethod for large-scale pre-training.",
        "url": "http://arxiv.org/abs/2505.04922v1",
        "published_date": "2025-05-08T03:37:07+00:00",
        "updated_date": "2025-05-08T03:37:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xingzeng Lan",
            "Xing Duan",
            "Chen Chen",
            "Weiyu Lin",
            "Bo Wang"
        ],
        "tldr": "the paper introduces canny2palm, a novel method for generating realistic and controllable palmprints using a canny edge detector and pix2pix network, demonstrating improved performance in palmprint recognition through large-scale pre-training.",
        "tldr_zh": "该论文介绍了一种名为canny2palm的新颖方法，它使用canny边缘检测器和pix2pix网络生成逼真且可控的掌纹，通过大规模预训练，在掌纹识别方面表现出更高的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models",
        "summary": "Reasoning lies at the heart of intelligence, shaping the ability to make\ndecisions, draw conclusions, and generalize across domains. In artificial\nintelligence, as systems increasingly operate in open, uncertain, and\nmultimodal environments, reasoning becomes essential for enabling robust and\nadaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a\npromising paradigm, integrating modalities such as text, images, audio, and\nvideo to support complex reasoning capabilities and aiming to achieve\ncomprehensive perception, precise understanding, and deep reasoning. As\nresearch advances, multimodal reasoning has rapidly evolved from modular,\nperception-driven pipelines to unified, language-centric frameworks that offer\nmore coherent cross-modal understanding. While instruction tuning and\nreinforcement learning have improved model reasoning, significant challenges\nremain in omni-modal generalization, reasoning depth, and agentic behavior. To\naddress these issues, we present a comprehensive and structured survey of\nmultimodal reasoning research, organized around a four-stage developmental\nroadmap that reflects the field's shifting design philosophies and emerging\ncapabilities. First, we review early efforts based on task-specific modules,\nwhere reasoning was implicitly embedded across stages of representation,\nalignment, and fusion. Next, we examine recent approaches that unify reasoning\ninto multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT)\nand multimodal reinforcement learning enabling richer and more structured\nreasoning chains. Finally, drawing on empirical insights from challenging\nbenchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the\nconceptual direction of native large multimodal reasoning models (N-LMRMs),\nwhich aim to support scalable, agentic, and adaptive reasoning and planning in\ncomplex, real-world environments.",
        "url": "http://arxiv.org/abs/2505.04921v1",
        "published_date": "2025-05-08T03:35:23+00:00",
        "updated_date": "2025-05-08T03:35:23+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Yunxin Li",
            "Zhenyu Liu",
            "Zitao Li",
            "Xuanyu Zhang",
            "Zhenran Xu",
            "Xinyu Chen",
            "Haoyuan Shi",
            "Shenyuan Jiang",
            "Xintong Wang",
            "Jifang Wang",
            "Shouzheng Huang",
            "Xinping Zhao",
            "Borui Jiang",
            "Lanqing Hong",
            "Longyue Wang",
            "Zhuotao Tian",
            "Baoxing Huai",
            "Wenhan Luo",
            "Weihua Luo",
            "Zheng Zhang",
            "Baotian Hu",
            "Min Zhang"
        ],
        "tldr": "this paper surveys large multimodal reasoning models (lmrms), tracing their development from modular pipelines to unified, language-centric frameworks and discussing future directions like native lmrms with agentic capabilities. it highlights challenges in omni-modal generalization and reasoning depth.",
        "tldr_zh": "本文综述了大型多模态推理模型（lmrm），追溯了它们从模块化管道到统一的、以语言为中心的框架的发展历程，并讨论了像具有代理能力的原生lmrm这样的未来方向。它强调了全模态泛化和推理深度方面的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "OWT: A Foundational Organ-Wise Tokenization Framework for Medical Imaging",
        "summary": "Recent advances in representation learning often rely on holistic, black-box\nembeddings that entangle multiple semantic components, limiting\ninterpretability and generalization. These issues are especially critical in\nmedical imaging. To address these limitations, we propose an Organ-Wise\nTokenization (OWT) framework with a Token Group-based Reconstruction (TGR)\ntraining paradigm. Unlike conventional approaches that produce holistic\nfeatures, OWT explicitly disentangles an image into separable token groups,\neach corresponding to a distinct organ or semantic entity. Our design ensures\neach token group encapsulates organ-specific information, boosting\ninterpretability, generalization, and efficiency while allowing fine-grained\ncontrol in downstream tasks. Experiments on CT and MRI datasets demonstrate the\neffectiveness of OWT in not only achieving strong image reconstruction and\nsegmentation performance, but also enabling novel semantic-level generation and\nretrieval applications that are out of reach for standard holistic embedding\nmethods. These findings underscore the potential of OWT as a foundational\nframework for semantically disentangled representation learning, offering broad\nscalability and applicability to real-world medical imaging scenarios and\nbeyond.",
        "url": "http://arxiv.org/abs/2505.04899v1",
        "published_date": "2025-05-08T02:30:44+00:00",
        "updated_date": "2025-05-08T02:30:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sifan Song",
            "Siyeop Yoon",
            "Pengfei Jin",
            "Sekeun Kim",
            "Matthew Tivnan",
            "Yujin Oh",
            "Runqi Meng",
            "Ling Chen",
            "Zhiliang Lyu",
            "Dufan Wu",
            "Ning Guo",
            "Xiang Li",
            "Quanzheng Li"
        ],
        "tldr": "the paper introduces owt, a novel organ-wise tokenization framework for medical imaging that disentangles images into organ-specific tokens to improve interpretability, generalization, and efficiency, enabling semantic-level generation and retrieval. it could potentially advance medical image understanding and generation.",
        "tldr_zh": "该论文介绍了 owt，一种用于医学影像的新的器官级分词框架，它将图像分解为器官特定的标记，以提高可解释性、泛化性和效率，从而实现语义级别的生成和检索。它可能推动医学图像理解和生成。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation",
        "summary": "Learning bimanual manipulation is challenging due to its high dimensionality\nand tight coordination required between two arms. Eye-in-hand imitation\nlearning, which uses wrist-mounted cameras, simplifies perception by focusing\non task-relevant views. However, collecting diverse demonstrations remains\ncostly, motivating the need for scalable data augmentation. While prior work\nhas explored visual augmentation in single-arm settings, extending these\napproaches to bimanual manipulation requires generating viewpoint-consistent\nobservations across both arms and producing corresponding action labels that\nare both valid and feasible. In this work, we propose Diffusion for COordinated\nDual-arm Data Augmentation (D-CODA), a method for offline data augmentation\ntailored to eye-in-hand bimanual imitation learning that trains a diffusion\nmodel to synthesize novel, viewpoint-consistent wrist-camera images for both\narms while simultaneously generating joint-space action labels. It employs\nconstrained optimization to ensure that augmented states involving\ngripper-to-object contacts adhere to constraints suitable for bimanual\ncoordination. We evaluate D-CODA on 5 simulated and 3 real-world tasks. Our\nresults across 2250 simulation trials and 300 real-world trials demonstrate\nthat it outperforms baselines and ablations, showing its potential for scalable\ndata augmentation in eye-in-hand bimanual manipulation. Our project website is\nat: https://dcodaaug.github.io/D-CODA/.",
        "url": "http://arxiv.org/abs/2505.04860v1",
        "published_date": "2025-05-08T00:03:04+00:00",
        "updated_date": "2025-05-08T00:03:04+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "I-Chun Arthur Liu",
            "Jason Chen",
            "Gaurav Sukhatme",
            "Daniel Seita"
        ],
        "tldr": "the paper introduces d-coda, a diffusion model for generating augmented data for bimanual manipulation tasks using eye-in-hand imitation learning, ensuring viewpoint consistency and valid actions.",
        "tldr_zh": "本文介绍了 d-coda，一种扩散模型，用于生成双臂操作任务的增强数据，使用手眼模仿学习，确保视点一致性和有效动作。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CRAFT: Cultural Russian-Oriented Dataset Adaptation for Focused Text-to-Image Generation",
        "summary": "Despite the fact that popular text-to-image generation models cope well with\ninternational and general cultural queries, they have a significant knowledge\ngap regarding individual cultures. This is due to the content of existing large\ntraining datasets collected on the Internet, which are predominantly based on\nWestern European or American popular culture. Meanwhile, the lack of cultural\nadaptation of the model can lead to incorrect results, a decrease in the\ngeneration quality, and the spread of stereotypes and offensive content. In an\neffort to address this issue, we examine the concept of cultural code and\nrecognize the critical importance of its understanding by modern image\ngeneration models, an issue that has not been sufficiently addressed in the\nresearch community to date. We propose the methodology for collecting and\nprocessing the data necessary to form a dataset based on the cultural code, in\nparticular the Russian one. We explore how the collected data affects the\nquality of generations in the national domain and analyze the effectiveness of\nour approach using the Kandinsky 3.1 text-to-image model. Human evaluation\nresults demonstrate an increase in the level of awareness of Russian culture in\nthe model.",
        "url": "http://arxiv.org/abs/2505.04851v1",
        "published_date": "2025-05-07T23:29:28+00:00",
        "updated_date": "2025-05-07T23:29:28+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.CY",
            "cs.LG"
        ],
        "authors": [
            "Viacheslav Vasilev",
            "Vladimir Arkhipkin",
            "Julia Agafonova",
            "Tatiana Nikulina",
            "Evelina Mironova",
            "Alisa Shichanina",
            "Nikolai Gerasimenko",
            "Mikhail Shoytov",
            "Denis Dimitrov"
        ],
        "tldr": "the paper introduces a methodology for creating culturally-specific datasets (specifically, russian) to improve text-to-image generation models' awareness and quality in that cultural domain, demonstrating improved cultural awareness with the kandinsky 3.1 model.",
        "tldr_zh": "该论文介绍了一种创建文化特定数据集（特别是俄语）的方法，以提高文本到图像生成模型在该文化领域的意识和质量，并使用 kandinsky 3.1 模型展示了文化意识的提高。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Replay to Remember (R2R): An Efficient Uncertainty-driven Unsupervised Continual Learning Framework Using Generative Replay",
        "summary": "Continual Learning entails progressively acquiring knowledge from new data\nwhile retaining previously acquired knowledge, thereby mitigating\n``Catastrophic Forgetting'' in neural networks. Our work presents a novel\nuncertainty-driven Unsupervised Continual Learning framework using Generative\nReplay, namely ``Replay to Remember (R2R)''. The proposed R2R architecture\nefficiently uses unlabelled and synthetic labelled data in a balanced\nproportion using a cluster-level uncertainty-driven feedback mechanism and a\nVLM-powered generative replay module. Unlike traditional memory-buffer methods\nthat depend on pretrained models and pseudo-labels, our R2R framework operates\nwithout any prior training. It leverages visual features from unlabeled data\nand adapts continuously using clustering-based uncertainty estimation coupled\nwith dynamic thresholding. Concurrently, a generative replay mechanism along\nwith DeepSeek-R1 powered CLIP VLM produces labelled synthetic data\nrepresentative of past experiences, resembling biological visual thinking that\nreplays memory to remember and act in new, unseen tasks. Extensive experimental\nanalyses are carried out in CIFAR-10, CIFAR-100, CINIC-10, SVHN and\nTinyImageNet datasets. Our proposed R2R approach improves knowledge retention,\nachieving a state-of-the-art performance of 98.13%, 73.06%, 93.41%, 95.18%,\n59.74%, respectively, surpassing state-of-the-art performance by over 4.36%.",
        "url": "http://arxiv.org/abs/2505.04787v1",
        "published_date": "2025-05-07T20:29:31+00:00",
        "updated_date": "2025-05-07T20:29:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Sriram Mandalika",
            "Harsha Vardhan",
            "Athira Nambiar"
        ],
        "tldr": "the paper introduces replay to remember (r2r), an unsupervised continual learning framework using generative replay and cluster-level uncertainty to mitigate catastrophic forgetting, achieving state-of-the-art performance on several datasets.",
        "tldr_zh": "该论文介绍了replay to remember (r2r)，一种利用生成式重放和聚类级别不确定性的无监督持续学习框架，旨在减轻灾难性遗忘，并在多个数据集上实现了最先进的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion",
        "summary": "Current Structure-from-Motion (SfM) methods typically follow a two-stage\npipeline, combining learned or geometric pairwise reasoning with a subsequent\nglobal optimization step. In contrast, we propose a data-driven multi-view\nreasoning approach that directly infers 3D scene geometry and camera poses from\nmulti-view images. Our framework, DiffusionSfM, parameterizes scene geometry\nand cameras as pixel-wise ray origins and endpoints in a global frame and\nemploys a transformer-based denoising diffusion model to predict them from\nmulti-view inputs. To address practical challenges in training diffusion models\nwith missing data and unbounded scene coordinates, we introduce specialized\nmechanisms that ensure robust learning. We empirically validate DiffusionSfM on\nboth synthetic and real datasets, demonstrating that it outperforms classical\nand learning-based approaches while naturally modeling uncertainty.",
        "url": "http://arxiv.org/abs/2505.05473v1",
        "published_date": "2025-05-08T17:59:47+00:00",
        "updated_date": "2025-05-08T17:59:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qitao Zhao",
            "Amy Lin",
            "Jeff Tan",
            "Jason Y. Zhang",
            "Deva Ramanan",
            "Shubham Tulsiani"
        ],
        "tldr": "the paper introduces diffusionsfm, a novel data-driven approach using diffusion models to directly infer 3d scene geometry and camera poses from multi-view images, outperforming existing methods while modeling uncertainty.",
        "tldr_zh": "该论文介绍了 diffusionsfm，一种新颖的数据驱动方法，使用扩散模型直接从多视图图像中推断 3d 场景几何体和相机姿态，优于现有方法，同时对不确定性进行建模。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal Bidirectional Warps",
        "summary": "We retarget video stitching to an emerging issue, named warping shake, which\nunveils the temporal content shakes induced by sequentially unsmooth warps when\nextending image stitching to video stitching. Even if the input videos are\nstable, the stitched video can inevitably cause undesired warping shakes and\naffect the visual experience. To address this issue, we propose StabStitch++, a\nnovel video stitching framework to realize spatial stitching and temporal\nstabilization with unsupervised learning simultaneously. First, different from\nexisting learning-based image stitching solutions that typically warp one image\nto align with another, we suppose a virtual midplane between original image\nplanes and project them onto it. Concretely, we design a differentiable\nbidirectional decomposition module to disentangle the homography transformation\nand incorporate it into our spatial warp, evenly spreading alignment burdens\nand projective distortions across two views. Then, inspired by camera paths in\nvideo stabilization, we derive the mathematical expression of stitching\ntrajectories in video stitching by elaborately integrating spatial and temporal\nwarps. Finally, a warp smoothing model is presented to produce stable stitched\nvideos with a hybrid loss to simultaneously encourage content alignment,\ntrajectory smoothness, and online collaboration. Compared with StabStitch that\nsacrifices alignment for stabilization, StabStitch++ makes no compromise and\noptimizes both of them simultaneously, especially in the online mode. To\nestablish an evaluation benchmark and train the learning framework, we build a\nvideo stitching dataset with a rich diversity in camera motions and scenes.\nExperiments exhibit that StabStitch++ surpasses current solutions in stitching\nperformance, robustness, and efficiency, offering compelling advancements in\nthis field by building a real-time online video stitching system.",
        "url": "http://arxiv.org/abs/2505.05001v1",
        "published_date": "2025-05-08T07:12:23+00:00",
        "updated_date": "2025-05-08T07:12:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Lang Nie",
            "Chunyu Lin",
            "Kang Liao",
            "Yun Zhang",
            "Shuaicheng Liu",
            "Yao Zhao"
        ],
        "tldr": "stabstitch++ introduces an unsupervised online video stitching framework that addresses warping shake by using bidirectional warps and a warp smoothing model, achieving superior performance in stitching, robustness, and efficiency.",
        "tldr_zh": "stabstitch++ 提出了一个无监督的在线视频拼接框架，通过双向扭曲和扭曲平滑模型来解决扭曲抖动问题，在拼接性能、鲁棒性和效率方面都表现出色。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "Integrated Image Reconstruction and Target Recognition based on Deep Learning Technique",
        "summary": "Computational microwave imaging (CMI) has gained attention as an alternative\ntechnique for conventional microwave imaging techniques, addressing their\nlimitations such as hardware-intensive physical layer and slow data collection\nacquisition speed to name a few. Despite these advantages, CMI still encounters\nnotable computational bottlenecks, especially during the image reconstruction\nstage. In this setting, both image recovery and object classification present\nsignificant processing demands. To address these challenges, our previous work\nintroduced ClassiGAN, which is a generative deep learning model designed to\nsimultaneously reconstruct images and classify targets using only\nback-scattered signals. In this study, we build upon that framework by\nincorporating attention gate modules into ClassiGAN. These modules are intended\nto refine feature extraction and improve the identification of relevant\ninformation. By dynamically focusing on important features and suppressing\nirrelevant ones, the attention mechanism enhances the overall model\nperformance. The proposed architecture, named Att-ClassiGAN, significantly\nreduces the reconstruction time compared to traditional CMI approaches.\nFurthermore, it outperforms current advanced methods, delivering improved\nNormalized Mean Squared Error (NMSE), higher Structural Similarity Index\n(SSIM), and better classification outcomes for the reconstructed targets.",
        "url": "http://arxiv.org/abs/2505.04836v1",
        "published_date": "2025-05-07T22:34:32+00:00",
        "updated_date": "2025-05-07T22:34:32+00:00",
        "categories": [
            "eess.SP",
            "cs.CV"
        ],
        "authors": [
            "Cien Zhang",
            "Jiaming Zhang",
            "Jiajun He",
            "Okan Yurduseven"
        ],
        "tldr": "this paper introduces att-classigan, an improved deep learning model with attention gates for simultaneous image reconstruction and target classification in computational microwave imaging, showing improvements in speed and accuracy over existing methods.",
        "tldr_zh": "本文介绍了att-classigan，一种改进的深度学习模型，具有注意力门，用于计算微波成像中的同步图像重建和目标分类，显示出比现有方法在速度和准确性方面的提高。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "Direct Image Classification from Fourier Ptychographic Microscopy Measurements without Reconstruction",
        "summary": "The computational imaging technique of Fourier Ptychographic Microscopy (FPM)\nenables high-resolution imaging with a wide field of view and can serve as an\nextremely valuable tool, e.g. in the classification of cells in medical\napplications. However, reconstructing a high-resolution image from tens or even\nhundreds of measurements is computationally expensive, particularly for a wide\nfield of view. Therefore, in this paper, we investigate the idea of classifying\nthe image content in the FPM measurements directly without performing a\nreconstruction step first. We show that Convolutional Neural Networks (CNN) can\nextract meaningful information from measurement sequences, significantly\noutperforming the classification on a single band-limited image (up to 12 %)\nwhile being significantly more efficient than a reconstruction of a\nhigh-resolution image. Furthermore, we demonstrate that a learned multiplexing\nof several raw measurements allows maintaining the classification accuracy\nwhile reducing the amount of data (and consequently also the acquisition time)\nsignificantly.",
        "url": "http://arxiv.org/abs/2505.05054v1",
        "published_date": "2025-05-08T08:46:28+00:00",
        "updated_date": "2025-05-08T08:46:28+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Navya Sonal Agarwal",
            "Jan Philipp Schneider",
            "Kanchana Vaishnavi Gandikota",
            "Syed Muhammad Kazim",
            "John Meshreki",
            "Ivo Ihrke",
            "Michael Moeller"
        ],
        "tldr": "this paper explores using convolutional neural networks to directly classify images from fourier ptychographic microscopy measurements without reconstructing the high-resolution image, improving efficiency and potentially reducing data acquisition time.",
        "tldr_zh": "本文探讨了使用卷积神经网络直接对傅里叶叠层显微镜测量数据进行图像分类，而无需重建高分辨率图像，从而提高效率并可能减少数据采集时间。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    },
    {
        "title": "Adaptive Contextual Embedding for Robust Far-View Borehole Detection",
        "summary": "In controlled blasting operations, accurately detecting densely distributed\ntiny boreholes from far-view imagery is critical for operational safety and\nefficiency. However, existing detection methods often struggle due to small\nobject scales, highly dense arrangements, and limited distinctive visual\nfeatures of boreholes. To address these challenges, we propose an adaptive\ndetection approach that builds upon existing architectures (e.g., YOLO) by\nexplicitly leveraging consistent embedding representations derived through\nexponential moving average (EMA)-based statistical updates.\n  Our method introduces three synergistic components: (1) adaptive augmentation\nutilizing dynamically updated image statistics to robustly handle illumination\nand texture variations; (2) embedding stabilization to ensure consistent and\nreliable feature extraction; and (3) contextual refinement leveraging spatial\ncontext for improved detection accuracy. The pervasive use of EMA in our method\nis particularly advantageous given the limited visual complexity and small\nscale of boreholes, allowing stable and robust representation learning even\nunder challenging visual conditions. Experiments on a challenging proprietary\nquarry-site dataset demonstrate substantial improvements over baseline\nYOLO-based architectures, highlighting our method's effectiveness in realistic\nand complex industrial scenarios.",
        "url": "http://arxiv.org/abs/2505.05008v1",
        "published_date": "2025-05-08T07:25:42+00:00",
        "updated_date": "2025-05-08T07:25:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuesong Liu",
            "Tianyu Hao",
            "Emmett J. Ientilucci"
        ],
        "tldr": "the paper presents an adaptive borehole detection method using ema-based statistical updates and contextual refinement to improve accuracy in challenging far-view imagery, demonstrating significant improvements over yolo baselines on a proprietary quarry-site dataset. it focuses on improving object detection accuracy, especially for small and densely packed objects.",
        "tldr_zh": "该论文提出了一种自适应的孔洞检测方法，利用基于 ema 的统计更新和上下文细化，提高了在具有挑战性的远景图像中的检测精度。实验结果表明，该方法在专有的采石场数据集上显著优于 yolo 基线，主要目标是提升目标检测的精确性，尤其是在小而密集的物体上。",
        "relevance_score": 2,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 3
    }
]