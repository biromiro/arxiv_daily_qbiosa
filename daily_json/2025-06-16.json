[
    {
        "title": "Performance Plateaus in Inference-Time Scaling for Text-to-Image Diffusion Without External Models",
        "summary": "Recently, it has been shown that investing computing resources in searching\nfor good initial noise for a text-to-image diffusion model helps improve\nperformance. However, previous studies required external models to evaluate the\nresulting images, which is impossible on GPUs with small VRAM. For these\nreasons, we apply Best-of-N inference-time scaling to algorithms that optimize\nthe initial noise of a diffusion model without external models across multiple\ndatasets and backbones. We demonstrate that inference-time scaling for\ntext-to-image diffusion models in this setting quickly reaches a performance\nplateau, and a relatively small number of optimization steps suffices to\nachieve the maximum achievable performance with each algorithm.",
        "url": "http://arxiv.org/abs/2506.12633v1",
        "published_date": "2025-06-14T21:25:08+00:00",
        "updated_date": "2025-06-14T21:25:08+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Changhyun Choi",
            "Sungha Kim",
            "H. Jin Kim"
        ],
        "tldr": "This paper investigates inference-time scaling for text-to-image diffusion models by optimizing initial noise without external models, finding performance plateaus after a small number of optimization steps.",
        "tldr_zh": "本文研究了文本到图像扩散模型的推理时缩放，通过优化初始噪声且不依赖外部模型，发现性能在少量优化步骤后达到瓶颈。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]