[
    {
        "title": "RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation",
        "summary": "Subject-driven text-to-image (T2I) generation aims to produce images that\nalign with a given textual description, while preserving the visual identity\nfrom a referenced subject image. Despite its broad downstream applicability --\nranging from enhanced personalization in image generation to consistent\ncharacter representation in video rendering -- progress in this field is\nlimited by the lack of reliable automatic evaluation. Existing methods either\nassess only one aspect of the task (i.e., textual alignment or subject\npreservation), misalign with human judgments, or rely on costly API-based\nevaluation. To address this, we introduce RefVNLI, a cost-effective metric that\nevaluates both textual alignment and subject preservation in a single\nprediction. Trained on a large-scale dataset derived from video-reasoning\nbenchmarks and image perturbations, RefVNLI outperforms or matches existing\nbaselines across multiple benchmarks and subject categories (e.g.,\n\\emph{Animal}, \\emph{Object}), achieving up to 6.4-point gains in textual\nalignment and 8.5-point gains in subject consistency. It also excels with\nlesser-known concepts, aligning with human preferences at over 87\\% accuracy.",
        "url": "http://arxiv.org/abs/2504.17502v1",
        "published_date": "2025-04-24T12:44:51+00:00",
        "updated_date": "2025-04-24T12:44:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aviv Slobodkin",
            "Hagai Taitelbaum",
            "Yonatan Bitton",
            "Brian Gordon",
            "Michal Sokolik",
            "Nitzan Bitton Guetta",
            "Almog Gueta",
            "Royi Rassin",
            "Itay Laish",
            "Dani Lischinski",
            "Idan Szpektor"
        ],
        "tldr": "the paper introduces refvnli, a new cost-effective metric for evaluating subject-driven text-to-image generation that assesses both textual alignment and subject preservation, demonstrating significant improvements over existing methods.",
        "tldr_zh": "该论文介绍了一种新的、具有成本效益的评估指标 refvnli，用于评估主体驱动的文本到图像生成，该指标可以同时评估文本对齐和主体保持，并且比现有方法有了显著改进。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback",
        "summary": "Current text-to-video (T2V) generation models are increasingly popular due to\ntheir ability to produce coherent videos from textual prompts. However, these\nmodels often struggle to generate semantically and temporally consistent videos\nwhen dealing with longer, more complex prompts involving multiple objects or\nsequential events. Additionally, the high computational costs associated with\ntraining or fine-tuning make direct improvements impractical. To overcome these\nlimitations, we introduce NeuS-E, a novel zero-training video refinement\npipeline that leverages neuro-symbolic feedback to automatically enhance video\ngeneration, achieving superior alignment with the prompts. Our approach first\nderives the neuro-symbolic feedback by analyzing a formal video representation\nand pinpoints semantically inconsistent events, objects, and their\ncorresponding frames. This feedback then guides targeted edits to the original\nvideo. Extensive empirical evaluations on both open-source and proprietary T2V\nmodels demonstrate that NeuS-E significantly enhances temporal and logical\nalignment across diverse prompts by almost 40%",
        "url": "http://arxiv.org/abs/2504.17180v2",
        "published_date": "2025-04-24T01:34:12+00:00",
        "updated_date": "2025-04-25T02:41:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Minkyu Choi",
            "S P Sharan",
            "Harsh Goel",
            "Sahil Shah",
            "Sandeep Chinchali"
        ],
        "tldr": "the paper introduces neus-e, a zero-training video refinement pipeline using neuro-symbolic feedback to enhance the temporal and logical consistency of text-to-video generation, improving alignment by almost 40%.",
        "tldr_zh": "该论文介绍了一种名为neus-e的零训练视频优化流程，该流程使用神经符号反馈来增强文本到视频生成的时间和逻辑一致性，并将对齐度提高了近40%。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Token-Shuffle: Towards High-Resolution Image Generation with Autoregressive Models",
        "summary": "Autoregressive (AR) models, long dominant in language generation, are\nincreasingly applied to image synthesis but are often considered less\ncompetitive than Diffusion-based models. A primary limitation is the\nsubstantial number of image tokens required for AR models, which constrains\nboth training and inference efficiency, as well as image resolution. To address\nthis, we present Token-Shuffle, a novel yet simple method that reduces the\nnumber of image tokens in Transformer. Our key insight is the dimensional\nredundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),\nwhere low-dimensional visual codes from visual encoder are directly mapped to\nhigh-dimensional language vocabularies. Leveraging this, we consider two key\noperations: token-shuffle, which merges spatially local tokens along channel\ndimension to decrease the input token number, and token-unshuffle, which\nuntangles the inferred tokens after Transformer blocks to restore the spatial\narrangement for output. Jointly training with textual prompts, our strategy\nrequires no additional pretrained text-encoder and enables MLLMs to support\nextremely high-resolution image synthesis in a unified next-token prediction\nway while maintaining efficient training and inference. For the first time, we\npush the boundary of AR text-to-image generation to a resolution of 2048x2048\nwith gratifying generation performance. In GenAI-benchmark, our 2.7B model\nachieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen\nby 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human\nevaluations also demonstrate our prominent image generation ability in terms of\ntext-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle\ncan serve as a foundational design for efficient high-resolution image\ngeneration within MLLMs.",
        "url": "http://arxiv.org/abs/2504.17789v1",
        "published_date": "2025-04-24T17:59:56+00:00",
        "updated_date": "2025-04-24T17:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xu Ma",
            "Peize Sun",
            "Haoyu Ma",
            "Hao Tang",
            "Chih-Yao Ma",
            "Jialiang Wang",
            "Kunpeng Li",
            "Xiaoliang Dai",
            "Yujun Shi",
            "Xuan Ju",
            "Yushi Hu",
            "Artsiom Sanakoyeu",
            "Felix Juefei-Xu",
            "Ji Hou",
            "Junjiao Tian",
            "Tao Xu",
            "Tingbo Hou",
            "Yen-Cheng Liu",
            "Zecheng He",
            "Zijian He",
            "Matt Feiszli",
            "Peizhao Zhang",
            "Peter Vajda",
            "Sam Tsai",
            "Yun Fu"
        ],
        "tldr": "the paper introduces token-shuffle, a method to reduce the number of image tokens in autoregressive models, enabling high-resolution (2048x2048) text-to-image generation in mllms with competitive performance compared to state-of-the-art ar and diffusion models.",
        "tldr_zh": "该论文提出了一种名为 token-shuffle 的方法，旨在减少自回归模型中的图像 tokens 数量，从而使 mllm 能够在高分辨率 (2048x2048) 下生成文本到图像，并与最先进的 ar 模型和扩散模型相比具有竞争优势。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Step1X-Edit: A Practical Framework for General Image Editing",
        "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
        "url": "http://arxiv.org/abs/2504.17761v1",
        "published_date": "2025-04-24T17:25:12+00:00",
        "updated_date": "2025-04-24T17:25:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shiyu Liu",
            "Yucheng Han",
            "Peng Xing",
            "Fukun Yin",
            "Rui Wang",
            "Wei Cheng",
            "Jiaqi Liao",
            "Yingming Wang",
            "Honghao Fu",
            "Chunrui Han",
            "Guopeng Li",
            "Yuang Peng",
            "Quan Sun",
            "Jingwei Wu",
            "Yan Cai",
            "Zheng Ge",
            "Ranchen Ming",
            "Lei Xia",
            "Xianfang Zeng",
            "Yibo Zhu",
            "Binxing Jiao",
            "Xiangyu Zhang",
            "Gang Yu",
            "Daxin Jiang"
        ],
        "tldr": "the paper introduces step1x-edit, a new open-source image editing model that aims to match the performance of closed-source models like gpt-4o using a multimodal llm and diffusion decoder, evaluated on a new benchmark called gedit-bench.",
        "tldr_zh": "该论文介绍了step1x-edit，一种新型开源图像编辑模型，旨在通过使用多模态llm和扩散解码器来匹配gpt-4o等闭源模型的性能，并在名为gedit-bench的新基准上进行了评估。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generative Fields: Uncovering Hierarchical Feature Control for StyleGAN via Inverted Receptive Fields",
        "summary": "StyleGAN has demonstrated the ability of GANs to synthesize highly-realistic\nfaces of imaginary people from random noise. One limitation of GAN-based image\ngeneration is the difficulty of controlling the features of the generated\nimage, due to the strong entanglement of the low-dimensional latent space.\nPrevious work that aimed to control StyleGAN with image or text prompts\nmodulated sampling in W latent space, which is more expressive than Z latent\nspace. However, W space still has restricted expressivity since it does not\ncontrol the feature synthesis directly; also the feature embedding in W space\nrequires a pre-training process to reconstruct the style signal, limiting its\napplication. This paper introduces the concept of \"generative fields\" to\nexplain the hierarchical feature synthesis in StyleGAN, inspired by the\nreceptive fields of convolution neural networks (CNNs). Additionally, we\npropose a new image editing pipeline for StyleGAN using generative field theory\nand the channel-wise style latent space S, utilizing the intrinsic structural\nfeature of CNNs to achieve disentangled control of feature synthesis at\nsynthesis time.",
        "url": "http://arxiv.org/abs/2504.17712v1",
        "published_date": "2025-04-24T16:15:02+00:00",
        "updated_date": "2025-04-24T16:15:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhuo He",
            "Paul Henderson",
            "Nicolas Pugeault"
        ],
        "tldr": "this paper introduces \"generative fields,\" inspired by cnn receptive fields, to achieve disentangled feature control in stylegan's synthesis process using the channel-wise style latent space s enhancing image editing capabilities.",
        "tldr_zh": "该论文引入了受 cnn 感受野启发的“生成场”概念，旨在通过通道层面的风格潜在空间 s 实现 stylegan 合成过程中解耦的特征控制，从而增强图像编辑能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Labels: Zero-Shot Diabetic Foot Ulcer Wound Segmentation with Self-attention Diffusion Models and the Potential for Text-Guided Customization",
        "summary": "Diabetic foot ulcers (DFUs) pose a significant challenge in healthcare,\nrequiring precise and efficient wound assessment to enhance patient outcomes.\nThis study introduces the Attention Diffusion Zero-shot Unsupervised System\n(ADZUS), a novel text-guided diffusion model that performs wound segmentation\nwithout relying on labeled training data. Unlike conventional deep learning\nmodels, which require extensive annotation, ADZUS leverages zero-shot learning\nto dynamically adapt segmentation based on descriptive prompts, offering\nenhanced flexibility and adaptability in clinical applications. Experimental\nevaluations demonstrate that ADZUS surpasses traditional and state-of-the-art\nsegmentation models, achieving an IoU of 86.68\\% and the highest precision of\n94.69\\% on the chronic wound dataset, outperforming supervised approaches such\nas FUSegNet. Further validation on a custom-curated DFU dataset reinforces its\nrobustness, with ADZUS achieving a median DSC of 75\\%, significantly surpassing\nFUSegNet's 45\\%. The model's text-guided segmentation capability enables\nreal-time customization of segmentation outputs, allowing targeted analysis of\nwound characteristics based on clinical descriptions. Despite its competitive\nperformance, the computational cost of diffusion-based inference and the need\nfor potential fine-tuning remain areas for future improvement. ADZUS represents\na transformative step in wound segmentation, providing a scalable, efficient,\nand adaptable AI-driven solution for medical imaging.",
        "url": "http://arxiv.org/abs/2504.17628v1",
        "published_date": "2025-04-24T14:50:10+00:00",
        "updated_date": "2025-04-24T14:50:10+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Abderrachid Hamrani",
            "Daniela Leizaola",
            "Renato Sousa",
            "Jose P. Ponce",
            "Stanley Mathis",
            "David G. Armstrong",
            "Anuradha Godavarty"
        ],
        "tldr": "the paper introduces adzus, a novel text-guided diffusion model for zero-shot diabetic foot ulcer segmentation, outperforming existing methods with high iou and dsc scores. it allows for text-guided customization, enhancing clinical applicability, though computational cost and fine-tuning needs remain.",
        "tldr_zh": "该论文介绍了一种新型的文本引导扩散模型adzus，用于零样本糖尿病足溃疡分割，其性能优于现有方法，具有较高的iou和dsc分数。它允许文本引导的定制，增强了临床适用性，但计算成本和微调需求仍然存在。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Fast Autoregressive Models for Continuous Latent Generation",
        "summary": "Autoregressive models have demonstrated remarkable success in sequential data\ngeneration, particularly in NLP, but their extension to continuous-domain image\ngeneration presents significant challenges. Recent work, the masked\nautoregressive model (MAR), bypasses quantization by modeling per-token\ndistributions in continuous spaces using a diffusion head but suffers from slow\ninference due to the high computational cost of the iterative denoising\nprocess. To address this, we propose the Fast AutoRegressive model (FAR), a\nnovel framework that replaces MAR's diffusion head with a lightweight shortcut\nhead, enabling efficient few-step sampling while preserving autoregressive\nprinciples. Additionally, FAR seamlessly integrates with causal Transformers,\nextending them from discrete to continuous token generation without requiring\narchitectural modifications. Experiments demonstrate that FAR achieves\n$2.3\\times$ faster inference than MAR while maintaining competitive FID and IS\nscores. This work establishes the first efficient autoregressive paradigm for\nhigh-fidelity continuous-space image generation, bridging the critical gap\nbetween quality and scalability in visual autoregressive modeling.",
        "url": "http://arxiv.org/abs/2504.18391v1",
        "published_date": "2025-04-24T13:57:08+00:00",
        "updated_date": "2025-04-24T13:57:08+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Tiankai Hang",
            "Jianmin Bao",
            "Fangyun Wei",
            "Dong Chen"
        ],
        "tldr": "the paper introduces fast autoregressive model (far) for efficient continuous-space image generation, achieving faster inference than previous methods while maintaining competitive image quality.",
        "tldr_zh": "该论文介绍了快速自回归模型（far），用于有效的连续空间图像生成，与以前的方法相比，实现了更快的推理速度，同时保持了具有竞争力的图像质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Text-to-Image Alignment in Denoising-Based Models through Step Selection",
        "summary": "Visual generative AI models often encounter challenges related to text-image\nalignment and reasoning limitations. This paper presents a novel method for\nselectively enhancing the signal at critical denoising steps, optimizing image\ngeneration based on input semantics. Our approach addresses the shortcomings of\nearly-stage signal modifications, demonstrating that adjustments made at later\nstages yield superior results. We conduct extensive experiments to validate the\neffectiveness of our method in producing semantically aligned images on\nDiffusion and Flow Matching model, achieving state-of-the-art performance. Our\nresults highlight the importance of a judicious choice of sampling stage to\nimprove performance and overall image alignment.",
        "url": "http://arxiv.org/abs/2504.17525v1",
        "published_date": "2025-04-24T13:10:32+00:00",
        "updated_date": "2025-04-24T13:10:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Paul Grimal",
            "Hervé Le Borgne",
            "Olivier Ferret"
        ],
        "tldr": "this paper introduces a novel method for improving text-to-image alignment in denoising-based generative models by selectively enhancing the signal at critical denoising steps, achieving state-of-the-art results.",
        "tldr_zh": "本文介绍了一种新颖的方法，通过在关键去噪步骤中选择性地增强信号，来改进基于去噪的生成模型中的文本到图像对齐，从而实现最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FRAG: Frame Selection Augmented Generation for Long Video and Long Document Understanding",
        "summary": "There has been impressive progress in Large Multimodal Models (LMMs). Recent\nworks extend these models to long inputs, including multi-page documents and\nlong videos. However, the model size and performance of these long context\nmodels are still limited due to the computational cost in both training and\ninference. In this work, we explore an orthogonal direction and process long\ninputs without long context LMMs. We propose Frame Selection Augmented\nGeneration (FRAG), where the model first selects relevant frames within the\ninput, and then only generates the final outputs based on the selected frames.\nThe core of the selection process is done by scoring each frame independently,\nwhich does not require long context processing. The frames with the highest\nscores are then selected by a simple Top-K selection. We show that this\nfrustratingly simple framework is applicable to both long videos and multi-page\ndocuments using existing LMMs without any fine-tuning. We consider two models,\nLLaVA-OneVision and InternVL2, in our experiments and show that FRAG\nconsistently improves the performance and achieves state-of-the-art\nperformances for both long video and long document understanding. For videos,\nFRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on\nVideo-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA\ncompared with recent LMMs specialized in long document understanding. Code is\navailable at: https://github.com/NVlabs/FRAG",
        "url": "http://arxiv.org/abs/2504.17447v1",
        "published_date": "2025-04-24T11:19:18+00:00",
        "updated_date": "2025-04-24T11:19:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "De-An Huang",
            "Subhashree Radhakrishnan",
            "Zhiding Yu",
            "Jan Kautz"
        ],
        "tldr": "the paper introduces frag, a frame selection augmented generation method for long video and document understanding, which selects relevant frames/pages before using existing lmms for output generation, demonstrating improved performance without fine-tuning.",
        "tldr_zh": "该论文介绍了一种名为frag的帧选择增强生成方法，用于理解长视频和文档。该方法首先选择相关的帧/页，然后使用现有的lmm生成输出，无需微调即可提高性能。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models",
        "summary": "Video try-on replaces clothing in videos with target garments. Existing\nmethods struggle to generate high-quality and temporally consistent results\nwhen handling complex clothing patterns and diverse body poses. We present\n3DV-TON, a novel diffusion-based framework for generating high-fidelity and\ntemporally consistent video try-on results. Our approach employs generated\nanimatable textured 3D meshes as explicit frame-level guidance, alleviating the\nissue of models over-focusing on appearance fidelity at the expanse of motion\ncoherence. This is achieved by enabling direct reference to consistent garment\ntexture movements throughout video sequences. The proposed method features an\nadaptive pipeline for generating dynamic 3D guidance: (1) selecting a keyframe\nfor initial 2D image try-on, followed by (2) reconstructing and animating a\ntextured 3D mesh synchronized with original video poses. We further introduce a\nrobust rectangular masking strategy that successfully mitigates artifact\npropagation caused by leaking clothing information during dynamic human and\ngarment movements. To advance video try-on research, we introduce HR-VVT, a\nhigh-resolution benchmark dataset containing 130 videos with diverse clothing\ntypes and scenarios. Quantitative and qualitative results demonstrate our\nsuperior performance over existing methods. The project page is at this link\nhttps://2y7c3.github.io/3DV-TON/",
        "url": "http://arxiv.org/abs/2504.17414v1",
        "published_date": "2025-04-24T10:12:40+00:00",
        "updated_date": "2025-04-24T10:12:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Min Wei",
            "Chaohui Yu",
            "Jingkai Zhou",
            "Fan Wang"
        ],
        "tldr": "the paper introduces 3dv-ton, a diffusion-based video try-on framework using textured 3d meshes for temporal consistency, and presents a new high-resolution video try-on dataset (hr-vvt). it addresses the challenge of generating high-quality and temporally consistent video try-on results with complex clothing patterns and diverse body poses.",
        "tldr_zh": "该论文介绍了3dv-ton，一个基于扩散模型的视频试穿框架，它利用纹理化的3d网格来实现时间一致性， 并提出了一个新的高分辨率视频试穿数据集(hr-vvt)。它解决了在处理复杂服装图案和多样身体姿态时，生成高质量且时间一致的视频试穿结果的挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DRC: Enhancing Personalized Image Generation via Disentangled Representation Composition",
        "summary": "Personalized image generation has emerged as a promising direction in\nmultimodal content creation. It aims to synthesize images tailored to\nindividual style preferences (e.g., color schemes, character appearances,\nlayout) and semantic intentions (e.g., emotion, action, scene contexts) by\nleveraging user-interacted history images and multimodal instructions. Despite\nnotable progress, existing methods -- whether based on diffusion models, large\nlanguage models, or Large Multimodal Models (LMMs) -- struggle to accurately\ncapture and fuse user style preferences and semantic intentions. In particular,\nthe state-of-the-art LMM-based method suffers from the entanglement of visual\nfeatures, leading to Guidance Collapse, where the generated images fail to\npreserve user-preferred styles or reflect the specified semantics.\n  To address these limitations, we introduce DRC, a novel personalized image\ngeneration framework that enhances LMMs through Disentangled Representation\nComposition. DRC explicitly extracts user style preferences and semantic\nintentions from history images and the reference image, respectively, to form\nuser-specific latent instructions that guide image generation within LMMs.\nSpecifically, it involves two critical learning stages: 1) Disentanglement\nlearning, which employs a dual-tower disentangler to explicitly separate style\nand semantic features, optimized via a reconstruction-driven paradigm with\ndifficulty-aware importance sampling; and 2) Personalized modeling, which\napplies semantic-preserving augmentations to effectively adapt the disentangled\nrepresentations for robust personalized generation. Extensive experiments on\ntwo benchmarks demonstrate that DRC shows competitive performance while\neffectively mitigating the guidance collapse issue, underscoring the importance\nof disentangled representation learning for controllable and effective\npersonalized image generation.",
        "url": "http://arxiv.org/abs/2504.17349v1",
        "published_date": "2025-04-24T08:10:10+00:00",
        "updated_date": "2025-04-24T08:10:10+00:00",
        "categories": [
            "cs.CV",
            "cs.IR"
        ],
        "authors": [
            "Yiyan Xu",
            "Wuqiang Zheng",
            "Wenjie Wang",
            "Fengbin Zhu",
            "Xinting Hu",
            "Yang Zhang",
            "Fuli Feng",
            "Tat-Seng Chua"
        ],
        "tldr": "the paper introduces drc, a novel personalized image generation framework that enhances lmms by disentangling style and semantic features to address the guidance collapse issue prevalent in existing methods.",
        "tldr_zh": "该论文介绍了drc，一种新颖的个性化图像生成框架，通过解耦风格和语义特征来增强大型多模态模型（lmm），以解决现有方法中普遍存在的指导崩溃问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Generalized and Training-Free Text-Guided Semantic Manipulation",
        "summary": "Text-guided semantic manipulation refers to semantically editing an image\ngenerated from a source prompt to match a target prompt, enabling the desired\nsemantic changes (e.g., addition, removal, and style transfer) while preserving\nirrelevant contents. With the powerful generative capabilities of the diffusion\nmodel, the task has shown the potential to generate high-fidelity visual\ncontent. Nevertheless, existing methods either typically require time-consuming\nfine-tuning (inefficient), fail to accomplish multiple semantic manipulations\n(poorly extensible), and/or lack support for different modality tasks (limited\ngeneralizability). Upon further investigation, we find that the geometric\nproperties of noises in the diffusion model are strongly correlated with the\nsemantic changes. Motivated by this, we propose a novel $\\textit{GTF}$ for\ntext-guided semantic manipulation, which has the following attractive\ncapabilities: 1) $\\textbf{Generalized}$: our $\\textit{GTF}$ supports multiple\nsemantic manipulations (e.g., addition, removal, and style transfer) and can be\nseamlessly integrated into all diffusion-based methods (i.e., Plug-and-play)\nacross different modalities (i.e., modality-agnostic); and 2)\n$\\textbf{Training-free}$: $\\textit{GTF}$ produces high-fidelity results via\nsimply controlling the geometric relationship between noises without tuning or\noptimization. Our extensive experiments demonstrate the efficacy of our\napproach, highlighting its potential to advance the state-of-the-art in\nsemantics manipulation.",
        "url": "http://arxiv.org/abs/2504.17269v1",
        "published_date": "2025-04-24T05:54:56+00:00",
        "updated_date": "2025-04-24T05:54:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu Hong",
            "Xiao Cai",
            "Pengpeng Zeng",
            "Shuai Zhang",
            "Jingkuan Song",
            "Lianli Gao",
            "Heng Tao Shen"
        ],
        "tldr": "this paper introduces a novel training-free geometric manipulation method (gtf) for text-guided semantic image editing in diffusion models, claiming improved generalization, modality agnosticism, and efficiency compared to existing fine-tuning-based approaches.",
        "tldr_zh": "本文提出了一种新的训练自由的几何操作方法 (gtf)，用于扩散模型中的文本引导的语义图像编辑。该方法声称与现有的基于微调的方法相比，具有更好的泛化性、模态不可知性和效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Generalizable Deepfake Detection with Spatial-Frequency Collaborative Learning and Hierarchical Cross-Modal Fusion",
        "summary": "The rapid evolution of deep generative models poses a critical challenge to\ndeepfake detection, as detectors trained on forgery-specific artifacts often\nsuffer significant performance degradation when encountering unseen forgeries.\nWhile existing methods predominantly rely on spatial domain analysis, frequency\ndomain operations are primarily limited to feature-level augmentation, leaving\nfrequency-native artifacts and spatial-frequency interactions insufficiently\nexploited. To address this limitation, we propose a novel detection framework\nthat integrates multi-scale spatial-frequency analysis for universal deepfake\ndetection. Our framework comprises three key components: (1) a local spectral\nfeature extraction pipeline that combines block-wise discrete cosine transform\nwith cascaded multi-scale convolutions to capture subtle spectral artifacts;\n(2) a global spectral feature extraction pipeline utilizing scale-invariant\ndifferential accumulation to identify holistic forgery distribution patterns;\nand (3) a multi-stage cross-modal fusion mechanism that incorporates\nshallow-layer attention enhancement and deep-layer dynamic modulation to model\nspatial-frequency interactions. Extensive evaluations on widely adopted\nbenchmarks demonstrate that our method outperforms state-of-the-art deepfake\ndetection methods in both accuracy and generalizability.",
        "url": "http://arxiv.org/abs/2504.17223v1",
        "published_date": "2025-04-24T03:23:35+00:00",
        "updated_date": "2025-04-24T03:23:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mengyu Qiao",
            "Runze Tian",
            "Yang Wang"
        ],
        "tldr": "this paper introduces a deepfake detection framework leveraging spatial-frequency analysis and cross-modal fusion, achieving improved accuracy and generalizability compared to existing methods.",
        "tldr_zh": "本文介绍了一种利用空间-频率分析和跨模态融合的深度伪造检测框架，与现有方法相比，该框架在准确性和泛化性方面均有所提高。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FashionM3: Multimodal, Multitask, and Multiround Fashion Assistant based on Unified Vision-Language Model",
        "summary": "Fashion styling and personalized recommendations are pivotal in modern\nretail, contributing substantial economic value in the fashion industry. With\nthe advent of vision-language models (VLM), new opportunities have emerged to\nenhance retailing through natural language and visual interactions. This work\nproposes FashionM3, a multimodal, multitask, and multiround fashion assistant,\nbuilt upon a VLM fine-tuned for fashion-specific tasks. It helps users discover\nsatisfying outfits by offering multiple capabilities including personalized\nrecommendation, alternative suggestion, product image generation, and virtual\ntry-on simulation. Fine-tuned on the novel FashionRec dataset, comprising\n331,124 multimodal dialogue samples across basic, personalized, and alternative\nrecommendation tasks, FashionM3 delivers contextually personalized suggestions\nwith iterative refinement through multiround interactions. Quantitative and\nqualitative evaluations, alongside user studies, demonstrate FashionM3's\nsuperior performance in recommendation effectiveness and practical value as a\nfashion assistant.",
        "url": "http://arxiv.org/abs/2504.17826v1",
        "published_date": "2025-04-24T02:44:22+00:00",
        "updated_date": "2025-04-24T02:44:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kaicheng Pang",
            "Xingxing Zou",
            "Waikeung Wong"
        ],
        "tldr": "the paper introduces fashionm3, a multimodal fashion assistant built on a fine-tuned vision-language model, offering personalized recommendations, alternative suggestions, image generation, and virtual try-on capabilities through multiround dialogue.",
        "tldr_zh": "本文介绍了fashionm3，一个基于微调的视觉-语言模型构建的多模态时尚助手，通过多轮对话提供个性化推荐、替代建议、图像生成和虚拟试穿功能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models",
        "summary": "Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately\ndetect objects and interpret their surroundings. However, even when trained\nusing millions of miles of real-world data, AVs are often unable to detect rare\nfailure modes (RFMs). The problem of RFMs is commonly referred to as the\n\"long-tail challenge\", due to the distribution of data including many instances\nthat are very rarely seen. In this paper, we present a novel approach that\nutilizes advanced generative and explainable AI techniques to aid in\nunderstanding RFMs. Our methods can be used to enhance the robustness and\nreliability of AVs when combined with both downstream model training and\ntesting. We extract segmentation masks for objects of interest (e.g., cars) and\ninvert them to create environmental masks. These masks, combined with carefully\ncrafted text prompts, are fed into a custom diffusion model. We leverage the\nStable Diffusion inpainting model guided by adversarial noise optimization to\ngenerate images containing diverse environments designed to evade object\ndetection models and expose vulnerabilities in AI systems. Finally, we produce\nnatural language descriptions of the generated RFMs that can guide developers\nand policymakers to improve the safety and reliability of AV systems.",
        "url": "http://arxiv.org/abs/2504.17179v1",
        "published_date": "2025-04-24T01:31:13+00:00",
        "updated_date": "2025-04-24T01:31:13+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.RO",
            "68T45, 68T05 68T45, 68T05 68T45, 68T05",
            "I.2.6; I.2.10; I.4.8"
        ],
        "authors": [
            "Mohammad Zarei",
            "Melanie A Jutras",
            "Eliana Evans",
            "Mike Tan",
            "Omid Aaramoon"
        ],
        "tldr": "the paper proposes a novel approach using adversarially guided diffusion models to generate rare failure modes (rfms) in autonomous vehicle perception systems, aiming to enhance their robustness and reliability.",
        "tldr_zh": "该论文提出了一种新颖的方法，使用对抗引导扩散模型来生成自动驾驶感知系统中的罕见故障模式（rfm），旨在增强其鲁棒性和可靠性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Latent Video Dataset Distillation",
        "summary": "Dataset distillation has demonstrated remarkable effectiveness in\nhigh-compression scenarios for image datasets. While video datasets inherently\ncontain greater redundancy, existing video dataset distillation methods\nprimarily focus on compression in the pixel space, overlooking advances in the\nlatent space that have been widely adopted in modern text-to-image and\ntext-to-video models. In this work, we bridge this gap by introducing a novel\nvideo dataset distillation approach that operates in the latent space using a\nstate-of-the-art variational encoder. Furthermore, we employ a diversity-aware\ndata selection strategy to select both representative and diverse samples.\nAdditionally, we introduce a simple, training-free method to further compress\nthe distilled latent dataset. By combining these techniques, our approach\nachieves a new state-of-the-art performance in dataset distillation,\noutperforming prior methods on all datasets, e.g. on HMDB51 IPC 1, we achieve a\n2.6% performance increase; on MiniUCF IPC 5, we achieve a 7.8% performance\nincrease.",
        "url": "http://arxiv.org/abs/2504.17132v1",
        "published_date": "2025-04-23T22:50:39+00:00",
        "updated_date": "2025-04-23T22:50:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ning Li",
            "Antai Andy Liu",
            "Jingran Zhang",
            "Justin Cui"
        ],
        "tldr": "this paper presents a new video dataset distillation technique that operates in the latent space using a variational encoder and a diversity-aware data selection strategy, achieving state-of-the-art performance.",
        "tldr_zh": "本文提出了一种新的视频数据集精馏技术，该技术在潜在空间中使用变分编码器和感知多样性的数据选择策略运行，实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Scene-Aware Location Modeling for Data Augmentation in Automotive Object Detection",
        "summary": "Generative image models are increasingly being used for training data\naugmentation in vision tasks. In the context of automotive object detection,\nmethods usually focus on producing augmented frames that look as realistic as\npossible, for example by replacing real objects with generated ones. Others try\nto maximize the diversity of augmented frames, for example by pasting lots of\ngenerated objects onto existing backgrounds. Both perspectives pay little\nattention to the locations of objects in the scene. Frame layouts are either\nreused with little or no modification, or they are random and disregard realism\nentirely. In this work, we argue that optimal data augmentation should also\ninclude realistic augmentation of layouts. We introduce a scene-aware\nprobabilistic location model that predicts where new objects can realistically\nbe placed in an existing scene. By then inpainting objects in these locations\nwith a generative model, we obtain much stronger augmentation performance than\nexisting approaches. We set a new state of the art for generative data\naugmentation on two automotive object detection tasks, achieving up to\n$2.8\\times$ higher gains than the best competing approach ($+1.4$ vs. $+0.5$\nmAP boost). We also demonstrate significant improvements for instance\nsegmentation.",
        "url": "http://arxiv.org/abs/2504.17076v1",
        "published_date": "2025-04-23T19:52:47+00:00",
        "updated_date": "2025-04-23T19:52:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jens Petersen",
            "Davide Abati",
            "Amirhossein Habibian",
            "Auke Wiggers"
        ],
        "tldr": "this paper introduces a scene-aware probabilistic location model for data augmentation in automotive object detection, demonstrating significant performance improvements compared to existing methods by realistically augmenting object layouts.",
        "tldr_zh": "本文介绍了一种场景感知的概率位置模型，用于汽车目标检测中的数据增强，通过真实地增强目标布局，相比现有方法，性能有了显著提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Distilling semantically aware orders for autoregressive image generation",
        "summary": "Autoregressive patch-based image generation has recently shown competitive\nresults in terms of image quality and scalability. It can also be easily\nintegrated and scaled within Vision-Language models. Nevertheless,\nautoregressive models require a defined order for patch generation. While a\nnatural order based on the dictation of the words makes sense for text\ngeneration, there is no inherent generation order that exists for image\ngeneration. Traditionally, a raster-scan order (from top-left to bottom-right)\nguides autoregressive image generation models. In this paper, we argue that\nthis order is suboptimal, as it fails to respect the causality of the image\ncontent: for instance, when conditioned on a visual description of a sunset, an\nautoregressive model may generate clouds before the sun, even though the color\nof clouds should depend on the color of the sun and not the inverse. In this\nwork, we show that first by training a model to generate patches in\nany-given-order, we can infer both the content and the location (order) of each\npatch during generation. Secondly, we use these extracted orders to finetune\nthe any-given-order model to produce better-quality images. Through our\nexperiments, we show on two datasets that this new generation method produces\nbetter images than the traditional raster-scan approach, with similar training\ncosts and no extra annotations.",
        "url": "http://arxiv.org/abs/2504.17069v1",
        "published_date": "2025-04-23T19:33:58+00:00",
        "updated_date": "2025-04-23T19:33:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Rishav Pramanik",
            "Antoine Poupon",
            "Juan A. Rodriguez",
            "Masih Aminbeidokhti",
            "David Vazquez",
            "Christopher Pal",
            "Zhaozheng Yin",
            "Marco Pedersoli"
        ],
        "tldr": "this paper proposes a novel semantically-aware patch order for autoregressive image generation, improving image quality compared to raster-scan order without additional cost or annotations.",
        "tldr_zh": "本文提出了一种新的语义感知块顺序，用于自回归图像生成，与光栅扫描顺序相比，在不增加成本或注释的情况下提高了图像质量。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ESDiff: Encoding Strategy-inspired Diffusion Model with Few-shot Learning for Color Image Inpainting",
        "summary": "Image inpainting is a technique used to restore missing or damaged regions of\nan image. Traditional methods primarily utilize information from adjacent\npixels for reconstructing missing areas, while they struggle to preserve\ncomplex details and structures. Simultaneously, models based on deep learning\nnecessitate substantial amounts of training data. To address this challenge, an\nencoding strategy-inspired diffusion model with few-shot learning for color\nimage inpainting is proposed in this paper. The main idea of this novel\nencoding strategy is the deployment of a \"virtual mask\" to construct\nhigh-dimensional objects through mutual perturbations between channels. This\napproach enables the diffusion model to capture diverse image representations\nand detailed features from limited training samples. Moreover, the encoding\nstrategy leverages redundancy between channels, integrates with low-rank\nmethods during iterative inpainting, and incorporates the diffusion model to\nachieve accurate information output. Experimental results indicate that our\nmethod exceeds current techniques in quantitative metrics, and the\nreconstructed images quality has been improved in aspects of texture and\nstructural integrity, leading to more precise and coherent results.",
        "url": "http://arxiv.org/abs/2504.17524v1",
        "published_date": "2025-04-24T13:08:36+00:00",
        "updated_date": "2025-04-24T13:08:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junyan Zhang",
            "Yan Li",
            "Mengxiao Geng",
            "Liu Shi",
            "Qiegen Liu"
        ],
        "tldr": "this paper introduces an encoding strategy-inspired diffusion model (esdiff) with few-shot learning for color image inpainting, using virtual masks for channel perturbations to enhance feature extraction from limited data and improve inpainting quality.",
        "tldr_zh": "本文提出了一种编码策略启发的扩散模型（esdiff），结合小样本学习用于彩色图像修复。该模型利用虚拟掩码进行通道扰动，从而在有限数据下增强特征提取并提高修复质量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation",
        "summary": "Soccer is a globally popular sporting event, typically characterized by long\nmatches and distinctive highlight moments. Recent advances in Multimodal Large\nLanguage Models (MLLMs) offer promising capabilities in temporal grounding and\nvideo understanding, soccer commentary generation often requires precise\ntemporal localization and semantically rich descriptions over long-form video.\nHowever, existing soccer MLLMs often rely on the temporal a priori for caption\ngeneration, so they cannot process the soccer video end-to-end. While some\ntraditional approaches follow a two-step paradigm that is complex and fails to\ncapture the global context to achieve suboptimal performance. To solve the\nabove issues, we present TimeSoccer, the first end-to-end soccer MLLM for\nSingle-anchor Dense Video Captioning (SDVC) in full-match soccer videos.\nTimeSoccer jointly predicts timestamps and generates captions in a single pass,\nenabling global context modeling across 45-minute matches. To support long\nvideo understanding of soccer matches, we introduce MoFA-Select, a\ntraining-free, motion-aware frame compression module that adaptively selects\nrepresentative frames via a coarse-to-fine strategy, and incorporates\ncomplementary training paradigms to strengthen the model's ability to handle\nlong temporal sequences. Extensive experiments demonstrate that our TimeSoccer\nachieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end\nform, generating high-quality commentary with accurate temporal alignment and\nstrong semantic relevance.",
        "url": "http://arxiv.org/abs/2504.17365v2",
        "published_date": "2025-04-24T08:27:42+00:00",
        "updated_date": "2025-04-25T05:58:48+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Ling You",
            "Wenxuan Huang",
            "Xinni Xie",
            "Xiangyi Wei",
            "Bangyan Li",
            "Shaohui Lin",
            "Yang Li",
            "Changbo Wang"
        ],
        "tldr": "the paper introduces timesoccer, an end-to-end mllm for generating soccer commentary, which addresses the limitations of existing methods by jointly predicting timestamps and captions for full-match videos, achieving state-of-the-art performance.",
        "tldr_zh": "本文介绍了 timesoccer，一个用于生成足球评论的端到端 mllm。它通过联合预测完整比赛视频的时间戳和标题，解决了现有方法的局限性，并实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Dual Prompting Image Restoration with Diffusion Transformers",
        "summary": "Recent state-of-the-art image restoration methods mostly adopt latent\ndiffusion models with U-Net backbones, yet still facing challenges in achieving\nhigh-quality restoration due to their limited capabilities. Diffusion\ntransformers (DiTs), like SD3, are emerging as a promising alternative because\nof their better quality with scalability. In this paper, we introduce DPIR\n(Dual Prompting Image Restoration), a novel image restoration method that\neffectivly extracts conditional information of low-quality images from multiple\nperspectives. Specifically, DPIR consits of two branches: a low-quality image\nconditioning branch and a dual prompting control branch. The first branch\nutilizes a lightweight module to incorporate image priors into the DiT with\nhigh efficiency. More importantly, we believe that in image restoration,\ntextual description alone cannot fully capture its rich visual characteristics.\nTherefore, a dual prompting module is designed to provide DiT with additional\nvisual cues, capturing both global context and local appearance. The extracted\nglobal-local visual prompts as extra conditional control, alongside textual\nprompts to form dual prompts, greatly enhance the quality of the restoration.\nExtensive experimental results demonstrate that DPIR delivers superior image\nrestoration performance.",
        "url": "http://arxiv.org/abs/2504.17825v1",
        "published_date": "2025-04-24T02:34:44+00:00",
        "updated_date": "2025-04-24T02:34:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Dehong Kong",
            "Fan Li",
            "Zhixin Wang",
            "Jiaqi Xu",
            "Renjing Pei",
            "Wenbo Li",
            "WenQi Ren"
        ],
        "tldr": "the paper introduces dpir, a novel image restoration method using diffusion transformers with a dual prompting mechanism (textual and global-local visual cues) to enhance restoration quality.",
        "tldr_zh": "该论文介绍了dpir，一种新颖的图像修复方法，它使用扩散transformer和双重提示机制（文本提示和全局-局部视觉线索）来提高修复质量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PPS-Ctrl: Controllable Sim-to-Real Translation for Colonoscopy Depth Estimation",
        "summary": "Accurate depth estimation enhances endoscopy navigation and diagnostics, but\nobtaining ground-truth depth in clinical settings is challenging. Synthetic\ndatasets are often used for training, yet the domain gap limits generalization\nto real data. We propose a novel image-to-image translation framework that\npreserves structure while generating realistic textures from clinical data. Our\nkey innovation integrates Stable Diffusion with ControlNet, conditioned on a\nlatent representation extracted from a Per-Pixel Shading (PPS) map. PPS\ncaptures surface lighting effects, providing a stronger structural constraint\nthan depth maps. Experiments show our approach produces more realistic\ntranslations and improves depth estimation over GAN-based MI-CycleGAN. Our code\nis publicly accessible at https://github.com/anaxqx/PPS-Ctrl.",
        "url": "http://arxiv.org/abs/2504.17067v1",
        "published_date": "2025-04-23T19:28:58+00:00",
        "updated_date": "2025-04-23T19:28:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinqi Xiong",
            "Andrea Dunn Beltran",
            "Jun Myeong Choi",
            "Marc Niethammer",
            "Roni Sengupta"
        ],
        "tldr": "this paper proposes a novel image-to-image translation framework leveraging stable diffusion and controlnet for generating realistic colonoscopy images from synthetic data, improving depth estimation by using per-pixel shading (pps) maps for structural constraints.",
        "tldr_zh": "本文提出了一种新颖的图像到图像翻译框架，利用 stable diffusion 和 controlnet 从合成数据生成逼真的结肠镜图像，并通过使用逐像素着色 (pps) 图进行结构约束来提高深度估计。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ePBR: Extended PBR Materials in Image Synthesis",
        "summary": "Realistic indoor or outdoor image synthesis is a core challenge in computer\nvision and graphics. The learning-based approach is easy to use but lacks\nphysical consistency, while traditional Physically Based Rendering (PBR) offers\nhigh realism but is computationally expensive. Intrinsic image representation\noffers a well-balanced trade-off, decomposing images into fundamental\ncomponents (intrinsic channels) such as geometry, materials, and illumination\nfor controllable synthesis. However, existing PBR materials struggle with\ncomplex surface models, particularly high-specular and transparent surfaces. In\nthis work, we extend intrinsic image representations to incorporate both\nreflection and transmission properties, enabling the synthesis of transparent\nmaterials such as glass and windows. We propose an explicit intrinsic\ncompositing framework that provides deterministic, interpretable image\nsynthesis. With the Extended PBR (ePBR) Materials, we can effectively edit the\nmaterials with precise controls.",
        "url": "http://arxiv.org/abs/2504.17062v1",
        "published_date": "2025-04-23T19:15:42+00:00",
        "updated_date": "2025-04-23T19:15:42+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Yu Guo",
            "Zhiqiang Lao",
            "Xiyun Song",
            "Yubin Zhou",
            "Zongfang Lin",
            "Heather Yu"
        ],
        "tldr": "this paper introduces extended pbr (epbr) materials for intrinsic image representation, enabling more realistic synthesis of transparent and high-specular surfaces in computer vision and graphics.",
        "tldr_zh": "本文介绍了扩展的pbr（epbr）材料，用于内在图像表示，从而能够在计算机视觉和图形学中更真实地合成透明和高光表面。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DPMambaIR:All-in-One Image Restoration via Degradation-Aware Prompt State Space Model",
        "summary": "All-in-One image restoration aims to address multiple image degradation\nproblems using a single model, significantly reducing training costs and\ndeployment complexity compared to traditional methods that design dedicated\nmodels for each degradation type. Existing approaches typically rely on\nDegradation-specific models or coarse-grained degradation prompts to guide\nimage restoration. However, they lack fine-grained modeling of degradation\ninformation and face limitations in balancing multi-task conflicts. To overcome\nthese limitations, we propose DPMambaIR, a novel All-in-One image restoration\nframework. By integrating a Degradation-Aware Prompt State Space Model (DP-SSM)\nand a High-Frequency Enhancement Block (HEB), DPMambaIR enables fine-grained\nmodeling of complex degradation information and efficient global integration,\nwhile mitigating the loss of high-frequency details caused by task competition.\nSpecifically, the DP-SSM utilizes a pre-trained degradation extractor to\ncapture fine-grained degradation features and dynamically incorporates them\ninto the state space modeling process, enhancing the model's adaptability to\ndiverse degradation types. Concurrently, the HEB supplements high-frequency\ninformation, effectively addressing the loss of critical details, such as edges\nand textures, in multi-task image restoration scenarios. Extensive experiments\non a mixed dataset containing seven degradation types show that DPMambaIR\nachieves the best performance, with 27.69dB and 0.893 in PSNR and SSIM,\nrespectively. These results highlight the potential and superiority of\nDPMambaIR as a unified solution for All-in-One image restoration.",
        "url": "http://arxiv.org/abs/2504.17732v1",
        "published_date": "2025-04-24T16:46:32+00:00",
        "updated_date": "2025-04-24T16:46:32+00:00",
        "categories": [
            "cs.CV",
            "I.4.4"
        ],
        "authors": [
            "Zhanwen Liu",
            "Sai Zhou",
            "Yuchao Dai",
            "Yang Wang",
            "Yisheng An",
            "Xiangmo Zhao"
        ],
        "tldr": "the paper introduces dpmambair, a novel all-in-one image restoration framework using a degradation-aware prompt state space model and a high-frequency enhancement block to address multiple image degradation problems with improved performance.",
        "tldr_zh": "该论文介绍了 dpmambair，一种新型的 all-in-one 图像恢复框架，它使用了一种降级感知提示状态空间模型和高频增强块，以解决多种图像降级问题，并提高了性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "SDVPT: Semantic-Driven Visual Prompt Tuning for Open-World Object Counting",
        "summary": "Open-world object counting leverages the robust text-image alignment of\npre-trained vision-language models (VLMs) to enable counting of arbitrary\ncategories in images specified by textual queries. However, widely adopted\nnaive fine-tuning strategies concentrate exclusively on text-image consistency\nfor categories contained in training, which leads to limited generalizability\nfor unseen categories. In this work, we propose a plug-and-play Semantic-Driven\nVisual Prompt Tuning framework (SDVPT) that transfers knowledge from the\ntraining set to unseen categories with minimal overhead in parameters and\ninference time. First, we introduce a two-stage visual prompt learning strategy\ncomposed of Category-Specific Prompt Initialization (CSPI) and Topology-Guided\nPrompt Refinement (TGPR). The CSPI generates category-specific visual prompts,\nand then TGPR distills latent structural patterns from the VLM's text encoder\nto refine these prompts. During inference, we dynamically synthesize the visual\nprompts for unseen categories based on the semantic correlation between unseen\nand training categories, facilitating robust text-image alignment for unseen\ncategories. Extensive experiments integrating SDVPT with all available\nopen-world object counting models demonstrate its effectiveness and\nadaptability across three widely used datasets: FSC-147, CARPK, and PUCPR+.",
        "url": "http://arxiv.org/abs/2504.17395v1",
        "published_date": "2025-04-24T09:31:08+00:00",
        "updated_date": "2025-04-24T09:31:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiming Zhao",
            "Guorong Li",
            "Laiyun Qing",
            "Amin Beheshti",
            "Jian Yang",
            "Michael Sheng",
            "Yuankai Qi",
            "Qingming Huang"
        ],
        "tldr": "the paper proposes a semantic-driven visual prompt tuning framework (sdvpt) for open-world object counting, improving generalization to unseen categories by leveraging semantic correlations between seen and unseen categories. sdvpt consists of category-specific prompt initialization (cspi) and topology-guided prompt refinement (tgpr).",
        "tldr_zh": "该论文提出了一种语义驱动的视觉提示调整框架（sdvpt），用于开放世界的物体计数，通过利用已见类别和未见类别之间的语义相关性，提高了对未见类别的泛化能力。sdvpt由类别特定提示初始化（cspi）和拓扑引导提示细化（tgpr）组成。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "I-INR: Iterative Implicit Neural Representations",
        "summary": "Implicit Neural Representations (INRs) have revolutionized signal processing\nand computer vision by modeling signals as continuous, differentiable functions\nparameterized by neural networks. However, their inherent formulation as a\nregression problem makes them prone to regression to the mean, limiting their\nability to capture fine details, retain high-frequency information, and handle\nnoise effectively. To address these challenges, we propose Iterative Implicit\nNeural Representations (I-INRs) a novel plug-and-play framework that enhances\nsignal reconstruction through an iterative refinement process. I-INRs\neffectively recover high-frequency details, improve robustness to noise, and\nachieve superior reconstruction quality. Our framework seamlessly integrates\nwith existing INR architectures, delivering substantial performance gains\nacross various tasks. Extensive experiments show that I-INRs outperform\nbaseline methods, including WIRE, SIREN, and Gauss, in diverse computer vision\napplications such as image restoration, image denoising, and object occupancy\nprediction.",
        "url": "http://arxiv.org/abs/2504.17364v1",
        "published_date": "2025-04-24T08:27:22+00:00",
        "updated_date": "2025-04-24T08:27:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ali Haider",
            "Muhammad Salman Ali",
            "Maryam Qamar",
            "Tahir Khalil",
            "Soo Ye Kim",
            "Jihyong Oh",
            "Enzo Tartaglione",
            "Sung-Ho Bae"
        ],
        "tldr": "the paper introduces iterative implicit neural representations (i-inrs) to improve high-frequency detail capture and noise robustness in implicit neural representations for tasks like image restoration and denoising.",
        "tldr_zh": "该论文介绍了迭代隐式神经表示（i-inrs），以提高隐式神经表示在高频细节捕获和噪声鲁棒性方面的能力，适用于图像修复和去噪等任务。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "DIMT25@ICDAR2025: HW-TSC's End-to-End Document Image Machine Translation System Leveraging Large Vision-Language Model",
        "summary": "This paper presents the technical solution proposed by Huawei Translation\nService Center (HW-TSC) for the \"End-to-End Document Image Machine Translation\nfor Complex Layouts\" competition at the 19th International Conference on\nDocument Analysis and Recognition (DIMT25@ICDAR2025). Leveraging\nstate-of-the-art open-source large vision-language model (LVLM), we introduce a\ntraining framework that combines multi-task learning with perceptual\nchain-of-thought to develop a comprehensive end-to-end document translation\nsystem. During the inference phase, we apply minimum Bayesian decoding and\npost-processing strategies to further enhance the system's translation\ncapabilities. Our solution uniquely addresses both OCR-based and OCR-free\ndocument image translation tasks within a unified framework. This paper\nsystematically details the training methods, inference strategies, LVLM base\nmodels, training data, experimental setups, and results, demonstrating an\neffective approach to document image machine translation.",
        "url": "http://arxiv.org/abs/2504.17315v1",
        "published_date": "2025-04-24T07:17:59+00:00",
        "updated_date": "2025-04-24T07:17:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhanglin Wu",
            "Tengfei Song",
            "Ning Xie",
            "Weidong Zhang",
            "Pengfei Li",
            "Shuang Wu",
            "Chong Li",
            "Junhao Zhu",
            "Hao Yang"
        ],
        "tldr": "this paper presents an end-to-end document image machine translation system using a large vision-language model, incorporating multi-task learning and perceptual chain-of-thought, and achieving state-of-the-art performance in the dimt25 competition.",
        "tldr_zh": "本文介绍了一种端到端的文档图像机器翻译系统，该系统采用大型视觉语言模型，结合了多任务学习和感知链式思考，并在 dimt25 比赛中取得了领先的成绩。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4
    }
]