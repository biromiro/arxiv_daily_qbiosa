[
    {
        "title": "Enhancing Surgical Documentation through Multimodal Visual-Temporal Transformers and Generative AI",
        "summary": "The automatic summarization of surgical videos is essential for enhancing\nprocedural documentation, supporting surgical training, and facilitating\npost-operative analysis. This paper presents a novel method at the intersection\nof artificial intelligence and medicine, aiming to develop machine learning\nmodels with direct real-world applications in surgical contexts. We propose a\nmulti-modal framework that leverages recent advancements in computer vision and\nlarge language models to generate comprehensive video summaries. % The approach\nis structured in three key stages. First, surgical videos are divided into\nclips, and visual features are extracted at the frame level using visual\ntransformers. This step focuses on detecting tools, tissues, organs, and\nsurgical actions. Second, the extracted features are transformed into\nframe-level captions via large language models. These are then combined with\ntemporal features, captured using a ViViT-based encoder, to produce clip-level\nsummaries that reflect the broader context of each video segment. Finally, the\nclip-level descriptions are aggregated into a full surgical report using a\ndedicated LLM tailored for the summarization task. % We evaluate our method on\nthe CholecT50 dataset, using instrument and action annotations from 50\nlaparoscopic videos. The results show strong performance, achieving 96\\%\nprecision in tool detection and a BERT score of 0.74 for temporal context\nsummarization. This work contributes to the advancement of AI-assisted tools\nfor surgical reporting, offering a step toward more intelligent and reliable\nclinical documentation.",
        "url": "http://arxiv.org/abs/2504.19918v1",
        "published_date": "2025-04-28T15:46:02+00:00",
        "updated_date": "2025-04-28T15:46:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hugo Georgenthum",
            "Cristian Cosentino",
            "Fabrizio Marozzo",
            "Pietro Liò"
        ],
        "tldr": "this paper introduces a multimodal ai framework for generating surgical video summaries using visual transformers, vivit, and llms, demonstrating strong performance on the cholect50 dataset for tool detection and temporal context summarization.",
        "tldr_zh": "本文介绍了一个多模态ai框架，利用视觉transformer、vivit和llm生成手术视频摘要，并在cholect50数据集上展示了在工具检测和时间上下文摘要方面的强大性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CineVerse: Consistent Keyframe Synthesis for Cinematic Scene Composition",
        "summary": "We present CineVerse, a novel framework for the task of cinematic scene\ncomposition. Similar to traditional multi-shot generation, our task emphasizes\nthe need for consistency and continuity across frames. However, our task also\nfocuses on addressing challenges inherent to filmmaking, such as multiple\ncharacters, complex interactions, and visual cinematic effects. In order to\nlearn to generate such content, we first create the CineVerse dataset. We use\nthis dataset to train our proposed two-stage approach. First, we prompt a large\nlanguage model (LLM) with task-specific instructions to take in a high-level\nscene description and generate a detailed plan for the overall setting and\ncharacters, as well as the individual shots. Then, we fine-tune a text-to-image\ngeneration model to synthesize high-quality visual keyframes. Experimental\nresults demonstrate that CineVerse yields promising improvements in generating\nvisually coherent and contextually rich movie scenes, paving the way for\nfurther exploration in cinematic video synthesis.",
        "url": "http://arxiv.org/abs/2504.19894v1",
        "published_date": "2025-04-28T15:28:14+00:00",
        "updated_date": "2025-04-28T15:28:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Quynh Phung",
            "Long Mai",
            "Fabian David Caba Heilbron",
            "Feng Liu",
            "Jia-Bin Huang",
            "Cusuh Ham"
        ],
        "tldr": "cineverse introduces a framework and dataset for generating consistent cinematic scene keyframes using llms for scene planning and fine-tuned text-to-image models for synthesis, aiming to address the challenges of multi-character, complex interaction cinematic content creation.",
        "tldr_zh": "cineverse 提出了一个框架和数据集，利用llm进行场景规划，并微调文本到图像模型进行合成，从而生成一致的电影场景关键帧，旨在解决多角色、复杂互动电影内容创作的挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AnimateAnywhere: Rouse the Background in Human Image Animation",
        "summary": "Human image animation aims to generate human videos of given characters and\nbackgrounds that adhere to the desired pose sequence. However, existing methods\nfocus more on human actions while neglecting the generation of background,\nwhich typically leads to static results or inharmonious movements. The\ncommunity has explored camera pose-guided animation tasks, yet preparing the\ncamera trajectory is impractical for most entertainment applications and\nordinary users. As a remedy, we present an AnimateAnywhere framework, rousing\nthe background in human image animation without requirements on camera\ntrajectories. In particular, based on our key insight that the movement of the\nhuman body often reflects the motion of the background, we introduce a\nbackground motion learner (BML) to learn background motions from human pose\nsequences. To encourage the model to learn more accurate cross-frame\ncorrespondences, we further deploy an epipolar constraint on the 3D attention\nmap. Specifically, the mask used to suppress geometrically unreasonable\nattention is carefully constructed by combining an epipolar mask and the\ncurrent 3D attention map. Extensive experiments demonstrate that our\nAnimateAnywhere effectively learns the background motion from human pose\nsequences, achieving state-of-the-art performance in generating human animation\nresults with vivid and realistic backgrounds. The source code and model will be\navailable at https://github.com/liuxiaoyu1104/AnimateAnywhere.",
        "url": "http://arxiv.org/abs/2504.19834v1",
        "published_date": "2025-04-28T14:35:01+00:00",
        "updated_date": "2025-04-28T14:35:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoyu Liu",
            "Mingshuai Yao",
            "Yabo Zhang",
            "Xianhui Lin",
            "Peiran Ren",
            "Xiaoming Li",
            "Ming Liu",
            "Wangmeng Zuo"
        ],
        "tldr": "the paper introduces animateanywhere, a framework for human image animation that focuses on generating realistic background motion based on human pose sequences, using a background motion learner and epipolar constraints on attention maps.",
        "tldr_zh": "本文介绍了animateanywhere，一个基于人体姿势序列生成逼真背景运动的人体图像动画框架，该框架使用背景运动学习器和注意力图上的对极约束。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "RepText: Rendering Visual Text via Replicating",
        "summary": "Although contemporary text-to-image generation models have achieved\nremarkable breakthroughs in producing visually appealing images, their capacity\nto generate precise and flexible typographic elements, especially non-Latin\nalphabets, remains constrained. To address these limitations, we start from an\nnaive assumption that text understanding is only a sufficient condition for\ntext rendering, but not a necessary condition. Based on this, we present\nRepText, which aims to empower pre-trained monolingual text-to-image generation\nmodels with the ability to accurately render, or more precisely, replicate,\nmultilingual visual text in user-specified fonts, without the need to really\nunderstand them. Specifically, we adopt the setting from ControlNet and\nadditionally integrate language agnostic glyph and position of rendered text to\nenable generating harmonized visual text, allowing users to customize text\ncontent, font and position on their needs. To improve accuracy, a text\nperceptual loss is employed along with the diffusion loss. Furthermore, to\nstabilize rendering process, at the inference phase, we directly initialize\nwith noisy glyph latent instead of random initialization, and adopt region\nmasks to restrict the feature injection to only the text region to avoid\ndistortion of the background. We conducted extensive experiments to verify the\neffectiveness of our RepText relative to existing works, our approach\noutperforms existing open-source methods and achieves comparable results to\nnative multi-language closed-source models. To be more fair, we also\nexhaustively discuss its limitations in the end.",
        "url": "http://arxiv.org/abs/2504.19724v1",
        "published_date": "2025-04-28T12:19:53+00:00",
        "updated_date": "2025-04-28T12:19:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haofan Wang",
            "Yujia Xu",
            "Yimeng Li",
            "Junchen Li",
            "Chaowei Zhang",
            "Jing Wang",
            "Kejia Yang",
            "Zhibo Chen"
        ],
        "tldr": "the paper introduces reptext, a method for improving multilingual text rendering in text-to-image generation models by replicating glyphs and positions, achieving comparable results to closed-source models.",
        "tldr_zh": "该论文介绍了reptext，一种通过复制字形和位置来改进文本到图像生成模型中的多语言文本渲染的方法，其结果与闭源模型相当。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer",
        "summary": "Collecting multi-view driving scenario videos to enhance the performance of\n3D visual perception tasks presents significant challenges and incurs\nsubstantial costs, making generative models for realistic data an appealing\nalternative. Yet, the videos generated by recent works suffer from poor quality\nand spatiotemporal consistency, undermining their utility in advancing\nperception tasks under driving scenarios. To address this gap, we propose DiVE,\na diffusion transformer-based generative framework meticulously engineered to\nproduce high-fidelity, temporally coherent, and cross-view consistent\nmulti-view videos, aligning seamlessly with bird's-eye view layouts and textual\ndescriptions. DiVE leverages a unified cross-attention and a SketchFormer to\nexert precise control over multimodal data, while incorporating a view-inflated\nattention mechanism that adds no extra parameters, thereby guaranteeing\nconsistency across views. Despite these advancements, synthesizing\nhigh-resolution videos under multimodal constraints introduces dual challenges:\ninvestigating the optimal classifier-free guidance coniguration under intricate\nmulti-condition inputs and mitigating excessive computational latency in\nhigh-resolution rendering--both of which remain underexplored in prior\nresearches. To resolve these limitations, we introduce two innovations:\nMulti-Control Auxiliary Branch Distillation, which streamlines multi-condition\nCFG selection while circumventing high computational overhead, and Resolution\nProgressive Sampling, a training-free acceleration strategy that staggers\nresolution scaling to reduce high latency due to high resolution. These\ninnovations collectively achieve a 2.62x speedup with minimal quality\ndegradation. Evaluated on the nuScenes dataset, DiVE achieves SOTA performance\nin multi-view video generation, yielding photorealistic outputs with\nexceptional temporal and cross-view coherence.",
        "url": "http://arxiv.org/abs/2504.19614v1",
        "published_date": "2025-04-28T09:20:50+00:00",
        "updated_date": "2025-04-28T09:20:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junpeng Jiang",
            "Gangyi Hong",
            "Miao Zhang",
            "Hengtong Hu",
            "Kun Zhan",
            "Rui Shao",
            "Liqiang Nie"
        ],
        "tldr": "dive is a diffusion transformer-based framework for generating high-fidelity, temporally and cross-view consistent multi-view driving scene videos, featuring innovations for improved efficiency and quality.",
        "tldr_zh": "dive是一个基于扩散transformer的框架，用于生成高保真、时序和跨视角一致的多视角驾驶场景视频，其创新之处在于提高了效率和质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Image Generation Method Based on Heat Diffusion Models",
        "summary": "Denoising Diffusion Probabilistic Models (DDPMs) achieve high-quality image\ngeneration without adversarial training, but they process images as a whole.\nSince adjacent pixels are highly likely to belong to the same object, we\npropose the Heat Diffusion Model (HDM) to further preserve image details and\ngenerate more realistic images. HDM is a model that incorporates pixel-level\noperations while maintaining the same training process as DDPM. In HDM, the\ndiscrete form of the two-dimensional heat equation is integrated into the\ndiffusion and generation formulas of DDPM, enabling the model to compute\nrelationships between neighboring pixels during image processing. Our\nexperiments demonstrate that HDM can generate higher-quality samples compared\nto models such as DDPM, Consistency Diffusion Models (CDM), Latent Diffusion\nModels (LDM), and Vector Quantized Generative Adversarial Networks (VQGAN).",
        "url": "http://arxiv.org/abs/2504.19600v1",
        "published_date": "2025-04-28T09:03:33+00:00",
        "updated_date": "2025-04-28T09:03:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Pengfei Zhang",
            "Shouqing Jia"
        ],
        "tldr": "the paper introduces heat diffusion models (hdm), a novel approach building upon ddpms by incorporating pixel-level relationships derived from the heat equation to enhance image detail and realism in generated samples.",
        "tldr_zh": "该论文介绍了一种热扩散模型（hdm），它在ddpm的基础上，通过结合从热方程导出的像素级关系来增强生成样本中的图像细节和真实感。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SynergyAmodal: Deocclude Anything with Text Control",
        "summary": "Image deocclusion (or amodal completion) aims to recover the invisible\nregions (\\ie, shape and appearance) of occluded instances in images. Despite\nrecent advances, the scarcity of high-quality data that balances diversity,\nplausibility, and fidelity remains a major obstacle. To address this challenge,\nwe identify three critical elements: leveraging in-the-wild image data for\ndiversity, incorporating human expertise for plausibility, and utilizing\ngenerative priors for fidelity. We propose SynergyAmodal, a novel framework for\nco-synthesizing in-the-wild amodal datasets with comprehensive shape and\nappearance annotations, which integrates these elements through a tripartite\ndata-human-model collaboration. First, we design an occlusion-grounded\nself-supervised learning algorithm to harness the diversity of in-the-wild\nimage data, fine-tuning an inpainting diffusion model into a partial completion\ndiffusion model. Second, we establish a co-synthesis pipeline to iteratively\nfilter, refine, select, and annotate the initial deocclusion results of the\npartial completion diffusion model, ensuring plausibility and fidelity through\nhuman expert guidance and prior model constraints. This pipeline generates a\nhigh-quality paired amodal dataset with extensive category and scale diversity,\ncomprising approximately 16K pairs. Finally, we train a full completion\ndiffusion model on the synthesized dataset, incorporating text prompts as\nconditioning signals. Extensive experiments demonstrate the effectiveness of\nour framework in achieving zero-shot generalization and textual\ncontrollability. Our code, dataset, and models will be made publicly available\nat https://github.com/imlixinyang/SynergyAmodal.",
        "url": "http://arxiv.org/abs/2504.19506v1",
        "published_date": "2025-04-28T06:04:17+00:00",
        "updated_date": "2025-04-28T06:04:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinyang Li",
            "Chengjie Yi",
            "Jiawei Lai",
            "Mingbao Lin",
            "Yansong Qu",
            "Shengchuan Zhang",
            "Liujuan Cao"
        ],
        "tldr": "the paper introduces synergyamodal, a framework for generating a high-quality image deocclusion dataset using a data-human-model collaboration approach, enabling zero-shot generalization and text-conditional deocclusion.",
        "tldr_zh": "该论文介绍了 synergyamodal，一个利用数据-人类-模型协同方法生成高质量图像去遮挡数据集的框架，从而实现零样本泛化和文本条件去遮挡。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EarthMapper: Visual Autoregressive Models for Controllable Bidirectional Satellite-Map Translation",
        "summary": "Satellite imagery and maps, as two fundamental data modalities in remote\nsensing, offer direct observations of the Earth's surface and\nhuman-interpretable geographic abstractions, respectively. The task of\nbidirectional translation between satellite images and maps (BSMT) holds\nsignificant potential for applications in urban planning and disaster response.\nHowever, this task presents two major challenges: first, the absence of precise\npixel-wise alignment between the two modalities substantially complicates the\ntranslation process; second, it requires achieving both high-level abstraction\nof geographic features and high-quality visual synthesis, which further\nelevates the technical complexity. To address these limitations, we introduce\nEarthMapper, a novel autoregressive framework for controllable bidirectional\nsatellite-map translation. EarthMapper employs geographic coordinate embeddings\nto anchor generation, ensuring region-specific adaptability, and leverages\nmulti-scale feature alignment within a geo-conditioned joint scale\nautoregression (GJSA) process to unify bidirectional translation in a single\ntraining cycle. A semantic infusion (SI) mechanism is introduced to enhance\nfeature-level consistency, while a key point adaptive guidance (KPAG) mechanism\nis proposed to dynamically balance diversity and precision during inference. We\nfurther contribute CNSatMap, a large-scale dataset comprising 302,132 precisely\naligned satellite-map pairs across 38 Chinese cities, enabling robust\nbenchmarking. Extensive experiments on CNSatMap and the New York dataset\ndemonstrate EarthMapper's superior performance, achieving significant\nimprovements in visual realism, semantic consistency, and structural fidelity\nover state-of-the-art methods. Additionally, EarthMapper excels in zero-shot\ntasks like in-painting, out-painting and coordinate-conditional generation,\nunderscoring its versatility.",
        "url": "http://arxiv.org/abs/2504.19432v1",
        "published_date": "2025-04-28T02:41:12+00:00",
        "updated_date": "2025-04-28T02:41:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhe Dong",
            "Yuzhe Sun",
            "Tianzhu Liu",
            "Wangmeng Zuo",
            "Yanfeng Gu"
        ],
        "tldr": "the paper introduces earthmapper, an autoregressive framework for bidirectional satellite-map translation, addressing pixel misalignment and the need for high-level abstraction and visual synthesis. it includes a new large-scale dataset, cnsatmap, and demonstrates superior performance in various tasks.",
        "tldr_zh": "该论文介绍了 earthmapper，一个用于双向卫星地图转换的自回归框架，解决了像素未对齐以及对高级抽象和视觉合成的需求。它还包含一个新的大规模数据集 cnsatmap，并在各种任务中展示了卓越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CompleteMe: Reference-based Human Image Completion",
        "summary": "Recent methods for human image completion can reconstruct plausible body\nshapes but often fail to preserve unique details, such as specific clothing\npatterns or distinctive accessories, without explicit reference images. Even\nstate-of-the-art reference-based inpainting approaches struggle to accurately\ncapture and integrate fine-grained details from reference images. To address\nthis limitation, we propose CompleteMe, a novel reference-based human image\ncompletion framework. CompleteMe employs a dual U-Net architecture combined\nwith a Region-focused Attention (RFA) Block, which explicitly guides the\nmodel's attention toward relevant regions in reference images. This approach\neffectively captures fine details and ensures accurate semantic correspondence,\nsignificantly improving the fidelity and consistency of completed images.\nAdditionally, we introduce a challenging benchmark specifically designed for\nevaluating reference-based human image completion tasks. Extensive experiments\ndemonstrate that our proposed method achieves superior visual quality and\nsemantic consistency compared to existing techniques. Project page:\nhttps://liagm.github.io/CompleteMe/",
        "url": "http://arxiv.org/abs/2504.20042v1",
        "published_date": "2025-04-28T17:59:56+00:00",
        "updated_date": "2025-04-28T17:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu-Ju Tsai",
            "Brian Price",
            "Qing Liu",
            "Luis Figueroa",
            "Daniil Pakhomov",
            "Zhihong Ding",
            "Scott Cohen",
            "Ming-Hsuan Yang"
        ],
        "tldr": "completeme is a reference-based human image completion framework using a dual u-net architecture and region-focused attention to improve fidelity and consistency by transferring fine-grained details from reference images. they also introduce a new benchmark for this task.",
        "tldr_zh": "completeme是一个基于参考的人体图像补全框架，它使用双u-net架构和区域聚焦注意力机制，通过从参考图像中传输细粒度的细节来提高保真度和一致性。他们还为此任务引入了一个新的基准。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Learning Brenier Potentials with Convex Generative Adversarial Neural Networks",
        "summary": "Brenier proved that under certain conditions on a source and a target\nprobability measure there exists a strictly convex function such that its\ngradient is a transport map from the source to the target distribution. This\nfunction is called the Brenier potential. Furthermore, detailed information on\nthe H\\\"older regularity of the Brenier potential is available. In this work we\ndevelop the statistical learning theory of generative adversarial neural\nnetworks that learn the Brenier potential. As by the transformation of\ndensities formula, the density of the generated measure depends on the second\nderivative of the Brenier potential, we develop the universal approximation\ntheory of ReCU networks with cubic activation $\\mathtt{ReCU}(x)=\\max\\{0,x\\}^3$\nthat combines the favorable approximation properties of H\\\"older functions with\na Lipschitz continuous density. In order to assure the convexity of such\ngeneral networks, we introduce an adversarial training procedure for a\npotential function represented by the ReCU networks that combines the classical\ndiscriminator cross entropy loss with a penalty term that enforces (strict)\nconvexity. We give a detailed decomposition of learning errors and show that\nfor a suitable high penalty parameter all networks chosen in the adversarial\nmin-max optimization problem are strictly convex. This is further exploited to\nprove the consistency of the learning procedure for (slowly) expanding network\ncapacity. We also implement the described learning algorithm and apply it to a\nnumber of standard test cases from Gaussian mixture to image data as target\ndistributions. As predicted in theory, we observe that the convexity loss\nbecomes inactive during the training process and the potentials represented by\nthe neural networks have learned convexity.",
        "url": "http://arxiv.org/abs/2504.19779v1",
        "published_date": "2025-04-28T13:24:52+00:00",
        "updated_date": "2025-04-28T13:24:52+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Claudia Drygala",
            "Hanno Gottschalk",
            "Thomas Kruse",
            "Ségolène Martin",
            "Annika Mütze"
        ],
        "tldr": "this paper presents a novel generative adversarial network (gan) training procedure using recu networks and adversarial training to learn brenier potentials, ensuring convexity for improved generative modeling. the method is validated on gaussian mixture and image data.",
        "tldr_zh": "本文提出了一种新颖的生成对抗网络（gan）训练方法，使用recu网络和对抗训练来学习brenier势，从而确保凸性以改进生成模型。该方法在gaussian混合模型和图像数据上得到了验证。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel View Synthesis in Autonomous Driving Scenes",
        "summary": "Current point-based approaches encounter limitations in scalability and\nrendering quality when using large 3D point cloud maps because using them\ndirectly for novel view synthesis (NVS) leads to degraded visualizations. We\nidentify the primary issue behind these low-quality renderings as a visibility\nmismatch between geometry and appearance, stemming from using these two\nmodalities together. To address this problem, we present CE-NPBG, a new\napproach for novel view synthesis (NVS) in large-scale autonomous driving\nscenes. Our method is a neural point-based technique that leverages two\nmodalities: posed images (cameras) and synchronized raw 3D point clouds\n(LiDAR). We first employ a connectivity relationship graph between appearance\nand geometry, which retrieves points from a large 3D point cloud map observed\nfrom the current camera perspective and uses them for rendering. By leveraging\nthis connectivity, our method significantly improves rendering quality and\nenhances run-time and scalability by using only a small subset of points from\nthe large 3D point cloud map. Our approach associates neural descriptors with\nthe points and uses them to synthesize views. To enhance the encoding of these\ndescriptors and elevate rendering quality, we propose a joint adversarial and\npoint rasterization training. During training, we pair an image-synthesizer\nnetwork with a multi-resolution discriminator. At inference, we decouple them\nand use the image-synthesizer to generate novel views. We also integrate our\nproposal into the recent 3D Gaussian Splatting work to highlight its benefits\nfor improved rendering and scalability.",
        "url": "http://arxiv.org/abs/2504.19557v1",
        "published_date": "2025-04-28T08:02:02+00:00",
        "updated_date": "2025-04-28T08:02:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohammad Altillawi",
            "Fengyi Shen",
            "Liudi Yang",
            "Sai Manoj Prakhya",
            "Ziyuan Liu"
        ],
        "tldr": "this paper introduces ce-npbg, a novel view synthesis method for autonomous driving scenes that enhances rendering quality and scalability by leveraging connectivity between posed images and lidar point clouds using a neural point-based technique.",
        "tldr_zh": "本文介绍了一种名为 ce-npbg 的新视角合成方法，用于自动驾驶场景。该方法利用神经点云技术，通过连接姿态图像和激光雷达点云之间的关系，提高了渲染质量和可扩展性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Masked Language Prompting for Generative Data Augmentation in Few-shot Fashion Style Recognition",
        "summary": "Constructing dataset for fashion style recognition is challenging due to the\ninherent subjectivity and ambiguity of style concepts. Recent advances in\ntext-to-image models have facilitated generative data augmentation by\nsynthesizing images from labeled data, yet existing methods based solely on\nclass names or reference captions often fail to balance visual diversity and\nstyle consistency. In this work, we propose \\textbf{Masked Language Prompting\n(MLP)}, a novel prompting strategy that masks selected words in a reference\ncaption and leverages large language models to generate diverse yet\nsemantically coherent completions. This approach preserves the structural\nsemantics of the original caption while introducing attribute-level variations\naligned with the intended style, enabling style-consistent and diverse image\ngeneration without fine-tuning. Experimental results on the FashionStyle14\ndataset demonstrate that our MLP-based augmentation consistently outperforms\nclass-name and caption-based baselines, validating its effectiveness for\nfashion style recognition under limited supervision.",
        "url": "http://arxiv.org/abs/2504.19455v1",
        "published_date": "2025-04-28T03:42:42+00:00",
        "updated_date": "2025-04-28T03:42:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuki Hirakawa",
            "Ryotaro Shimizu"
        ],
        "tldr": "the paper introduces masked language prompting (mlp) for generative data augmentation in few-shot fashion style recognition. mlp leverages large language models to generate diverse and style-consistent images from masked reference captions, outperforming existing augmentation methods.",
        "tldr_zh": "该论文介绍了用于少样本时尚风格识别的生成数据增强的掩码语言提示（mlp）。 mlp利用大型语言模型从掩码参考字幕中生成多样且风格一致的图像，优于现有的增强方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "UNet with Axial Transformer : A Neural Weather Model for Precipitation Nowcasting",
        "summary": "Making accurate weather predictions can be particularly challenging for\nlocalized storms or events that evolve on hourly timescales, such as\nthunderstorms. Hence, our goal for the project was to model Weather Nowcasting\nfor making highly localized and accurate predictions that apply to the\nimmediate future replacing the current numerical weather models and data\nassimilation systems with Deep Learning approaches. A significant advantage of\nmachine learning is that inference is computationally cheap given an\nalready-trained model, allowing forecasts that are nearly instantaneous and in\nthe native high resolution of the input data. In this work we developed a novel\nmethod that employs Transformer-based machine learning models to forecast\nprecipitation. This approach works by leveraging axial attention mechanisms to\nlearn complex patterns and dynamics from time series frames. Moreover, it is a\ngeneric framework and can be applied to univariate and multivariate time series\ndata, as well as time series embeddings data. This paper represents an initial\nresearch on the dataset used in the domain of next frame prediciton, and hence,\nwe demonstrate state-of-the-art results in terms of metrices (PSNR = 47.67,\nSSIM = 0.9943) used for the given dataset using UNet with Axial Transformer.",
        "url": "http://arxiv.org/abs/2504.19408v1",
        "published_date": "2025-04-28T01:20:30+00:00",
        "updated_date": "2025-04-28T01:20:30+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "eess.SP"
        ],
        "authors": [
            "Maitreya Sonawane",
            "Sumit Mamtani"
        ],
        "tldr": "this paper introduces a unet with axial transformer model for precipitation nowcasting, aiming to replace traditional numerical weather models with deep learning for highly localized and accurate predictions, achieving state-of-the-art results on a specific dataset.",
        "tldr_zh": "该论文介绍了一种用于降水临近预报的带有轴向transformer的unet模型，旨在用深度学习取代传统的数值天气模型，以实现高度局部化和精确的预测，并在特定数据集上取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Boosting 3D Liver Shape Datasets with Diffusion Models and Implicit Neural Representations",
        "summary": "While the availability of open 3D medical shape datasets is increasing,\noffering substantial benefits to the research community, we have found that\nmany of these datasets are, unfortunately, disorganized and contain artifacts.\nThese issues limit the development and training of robust models, particularly\nfor accurate 3D reconstruction tasks. In this paper, we examine the current\nstate of available 3D liver shape datasets and propose a solution using\ndiffusion models combined with implicit neural representations (INRs) to\naugment and expand existing datasets. Our approach utilizes the generative\ncapabilities of diffusion models to create realistic, diverse 3D liver shapes,\ncapturing a wide range of anatomical variations and addressing the problem of\ndata scarcity. Experimental results indicate that our method enhances dataset\ndiversity, providing a scalable solution to improve the accuracy and\nreliability of 3D liver reconstruction and generation in medical applications.\nFinally, we suggest that diffusion models can also be applied to other\ndownstream tasks in 3D medical imaging.",
        "url": "http://arxiv.org/abs/2504.19402v1",
        "published_date": "2025-04-28T00:56:18+00:00",
        "updated_date": "2025-04-28T00:56:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Khoa Tuan Nguyen",
            "Francesca Tozzi",
            "Wouter Willaert",
            "Joris Vankerschaver",
            "Nikdokht Rashidian",
            "Wesley De Neve"
        ],
        "tldr": "this paper proposes using diffusion models and implicit neural representations to augment 3d liver shape datasets, addressing issues of data scarcity and dataset quality to improve 3d liver reconstruction.",
        "tldr_zh": "该论文提出了一种使用扩散模型和隐式神经表示来扩充3d肝脏形状数据集的方法，解决了数据稀缺和数据集质量问题，从而提高3d肝脏重建的准确性。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "HumMorph: Generalized Dynamic Human Neural Fields from Few Views",
        "summary": "We introduce HumMorph, a novel generalized approach to free-viewpoint\nrendering of dynamic human bodies with explicit pose control. HumMorph renders\na human actor in any specified pose given a few observed views (starting from\njust one) in arbitrary poses. Our method enables fast inference as it relies\nonly on feed-forward passes through the model. We first construct a coarse\nrepresentation of the actor in the canonical T-pose, which combines visual\nfeatures from individual partial observations and fills missing information\nusing learned prior knowledge. The coarse representation is complemented by\nfine-grained pixel-aligned features extracted directly from the observed views,\nwhich provide high-resolution appearance information. We show that HumMorph is\ncompetitive with the state-of-the-art when only a single input view is\navailable, however, we achieve results with significantly better visual quality\ngiven just 2 monocular observations. Moreover, previous generalized methods\nassume access to accurate body shape and pose parameters obtained using\nsynchronized multi-camera setups. In contrast, we consider a more practical\nscenario where these body parameters are noisily estimated directly from the\nobserved views. Our experimental results demonstrate that our architecture is\nmore robust to errors in the noisy parameters and clearly outperforms the state\nof the art in this setting.",
        "url": "http://arxiv.org/abs/2504.19390v1",
        "published_date": "2025-04-27T23:35:54+00:00",
        "updated_date": "2025-04-27T23:35:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jakub Zadrożny",
            "Hakan Bilen"
        ],
        "tldr": "hummorph introduces a novel neural field approach for rendering dynamic human bodies from few views with explicit pose control, demonstrating robustness to noisy pose parameters and achieving state-of-the-art performance with limited input views.",
        "tldr_zh": "hummorph 提出了一种新颖的神经场方法，用于从少量视图渲染具有显式姿势控制的动态人体，展示了对噪声姿势参数的鲁棒性，并在有限的输入视图下实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Mitigating Catastrophic Forgetting in the Incremental Learning of Medical Images",
        "summary": "This paper proposes an Incremental Learning (IL) approach to enhance the\naccuracy and efficiency of deep learning models in analyzing T2-weighted (T2w)\nMRI medical images prostate cancer detection using the PI-CAI dataset. We used\nmultiple health centers' artificial intelligence and radiology data, focused on\ndifferent tasks that looked at prostate cancer detection using MRI (PI-CAI). We\nutilized Knowledge Distillation (KD), as it employs generated images from past\ntasks to guide the training of models for subsequent tasks. The approach\nyielded improved performance and faster convergence of the models. To\ndemonstrate the versatility and robustness of our approach, we evaluated it on\nthe PI-CAI dataset, a diverse set of medical imaging modalities including OCT\nand PathMNIST, and the benchmark continual learning dataset CIFAR-10. Our\nresults indicate that KD can be a promising technique for IL in medical image\nanalysis in which data is sourced from individual health centers and the\nstorage of large datasets is not feasible. By using generated images from prior\ntasks, our method enables the model to retain and apply previously acquired\nknowledge without direct access to the original data.",
        "url": "http://arxiv.org/abs/2504.20033v1",
        "published_date": "2025-04-28T17:56:04+00:00",
        "updated_date": "2025-04-28T17:56:04+00:00",
        "categories": [
            "cs.CV",
            "I.2.6; I.2.10"
        ],
        "authors": [
            "Sara Yavari",
            "Jacob Furst"
        ],
        "tldr": "the paper introduces an incremental learning (il) approach using knowledge distillation (kd) with generated images to mitigate catastrophic forgetting in medical image analysis, particularly for prostate cancer detection in mri. it demonstrates the method's versatility on pi-cai, oct, pathmnist, and cifar-10 datasets.",
        "tldr_zh": "该论文提出了一种增量学习（il）方法，使用知识蒸馏（kd）与生成的图像，以减轻医学图像分析中的灾难性遗忘，特别是前列腺癌的mri检测。 它在pi-cai，oct，pathmnist和cifar-10数据集上展示了该方法的多功能性。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "CoDEx: Combining Domain Expertise for Spatial Generalization in Satellite Image Analysis",
        "summary": "Global variations in terrain appearance raise a major challenge for satellite\nimage analysis, leading to poor model performance when training on locations\nthat differ from those encountered at test time. This remains true even with\nrecent large global datasets. To address this challenge, we propose a novel\ndomain-generalization framework for satellite images. Instead of trying to\nlearn a single generalizable model, we train one expert model per training\ndomain, while learning experts' similarity and encouraging similar experts to\nbe consistent. A model selection module then identifies the most suitable\nexperts for a given test sample and aggregates their predictions. Experiments\non four datasets (DynamicEarthNet, MUDS, OSCD, and FMoW) demonstrate consistent\ngains over existing domain generalization and adaptation methods. Our code is\npublicly available at https://github.com/Abhishek19009/CoDEx.",
        "url": "http://arxiv.org/abs/2504.19737v1",
        "published_date": "2025-04-28T12:33:39+00:00",
        "updated_date": "2025-04-28T12:33:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Abhishek Kuriyal",
            "Elliot Vincent",
            "Mathieu Aubry",
            "Loic Landrieu"
        ],
        "tldr": "the paper introduces codex, a domain-generalization framework that trains separate expert models for each training domain in satellite imagery and then selects and aggregates predictions from the most suitable experts for new test samples, demonstrating improved performance across multiple datasets.",
        "tldr_zh": "该论文提出了codex，一个领域泛化框架，它为卫星图像中的每个训练领域训练单独的专家模型，然后选择并聚合来自最适合新测试样本的专家的预测，并在多个数据集上展示了改进的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "Neural network task specialization via domain constraining",
        "summary": "This paper introduces a concept of neural network specialization via\ntask-specific domain constraining, aimed at enhancing network performance on\ndata subspace in which the network operates. The study presents experiments on\ntraining specialists for image classification and object detection tasks. The\nresults demonstrate that specialization can enhance a generalist's accuracy\neven without additional data or changing training regimes: solely by\nconstraining class label space in which the network performs. Theoretical and\nexperimental analyses indicate that effective specialization requires modifying\ntraditional fine-tuning methods and constraining data space to semantically\ncoherent subsets. The specialist extraction phase before tuning the network is\nproposed for maximal performance gains. We also provide analysis of the\nevolution of the feature space during specialization. This study paves way to\nfuture research for developing more advanced dynamically configurable image\nanalysis systems, where computations depend on the specific input.\nAdditionally, the proposed methods can help improve system performance in\nscenarios where certain data domains should be excluded from consideration of\nthe generalist network.",
        "url": "http://arxiv.org/abs/2504.19592v1",
        "published_date": "2025-04-28T08:57:01+00:00",
        "updated_date": "2025-04-28T08:57:01+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Roman Malashin",
            "Daniil Ilyukhin"
        ],
        "tldr": "the paper explores neural network specialization by constraining the class label space during fine-tuning, leading to improved performance in image classification and object detection, even without new data. they propose a specialist extraction phase for performance gains.",
        "tldr_zh": "该论文通过在微调期间约束类别标签空间来探索神经网络的专业化，从而提高图像分类和对象检测的性能，即使没有新数据。他们提出一个专家提取阶段以获得性能提升。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 5
    },
    {
        "title": "DEEMO: De-identity Multimodal Emotion Recognition and Reasoning",
        "summary": "Emotion understanding is a critical yet challenging task. Most existing\napproaches rely heavily on identity-sensitive information, such as facial\nexpressions and speech, which raises concerns about personal privacy. To\naddress this, we introduce the De-identity Multimodal Emotion Recognition and\nReasoning (DEEMO), a novel task designed to enable emotion understanding using\nde-identified video and audio inputs. The DEEMO dataset consists of two\nsubsets: DEEMO-NFBL, which includes rich annotations of Non-Facial Body\nLanguage (NFBL), and DEEMO-MER, an instruction dataset for Multimodal Emotion\nRecognition and Reasoning using identity-free cues. This design supports\nemotion understanding without compromising identity privacy. In addition, we\npropose DEEMO-LLaMA, a Multimodal Large Language Model (MLLM) that integrates\nde-identified audio, video, and textual information to enhance both emotion\nrecognition and reasoning. Extensive experiments show that DEEMO-LLaMA achieves\nstate-of-the-art performance on both tasks, outperforming existing MLLMs by a\nsignificant margin, achieving 74.49% accuracy and 74.45% F1-score in\nde-identity emotion recognition, and 6.20 clue overlap and 7.66 label overlap\nin de-identity emotion reasoning. Our work contributes to ethical AI by\nadvancing privacy-preserving emotion understanding and promoting responsible\naffective computing.",
        "url": "http://arxiv.org/abs/2504.19549v1",
        "published_date": "2025-04-28T07:55:11+00:00",
        "updated_date": "2025-04-28T07:55:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Deng Li",
            "Bohao Xing",
            "Xin Liu",
            "Baiqiang Xia",
            "Bihan Wen",
            "Heikki Kälviäinen"
        ],
        "tldr": "the paper introduces deemo, a new task and dataset for de-identified multimodal emotion recognition and reasoning, along with deemo-llama, an mllm that outperforms existing models on this task, contributing to privacy-preserving affective computing.",
        "tldr_zh": "该论文介绍了deemo，一个用于去身份化多模态情感识别和推理的新任务和数据集，以及deemo-llama，一个在此任务上优于现有模型的mllm，为保护隐私的情感计算做出了贡献。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "Adversarial Shallow Watermarking",
        "summary": "Recent advances in digital watermarking make use of deep neural networks for\nmessage embedding and extraction. They typically follow the ``encoder-noise\nlayer-decoder''-based architecture. By deliberately establishing a\ndifferentiable noise layer to simulate the distortion of the watermarked\nsignal, they jointly train the deep encoder and decoder to fit the noise layer\nto guarantee robustness. As a result, they are usually weak against unknown\ndistortions that are not used in their training pipeline. In this paper, we\npropose a novel watermarking framework to resist unknown distortions, namely\nAdversarial Shallow Watermarking (ASW). ASW utilizes only a shallow decoder\nthat is randomly parameterized and designed to be insensitive to distortions\nfor watermarking extraction. During the watermark embedding, ASW freezes the\nshallow decoder and adversarially optimizes a host image until its updated\nversion (i.e., the watermarked image) stably triggers the shallow decoder to\noutput the watermark message. During the watermark extraction, it accurately\nrecovers the message from the watermarked image by leveraging the insensitive\nnature of the shallow decoder against arbitrary distortions. Our ASW is\ntraining-free, encoder-free, and noise layer-free. Experiments indicate that\nthe watermarked images created by ASW have strong robustness against various\nunknown distortions. Compared to the existing ``encoder-noise layer-decoder''\napproaches, ASW achieves comparable results on known distortions and better\nrobustness on unknown distortions.",
        "url": "http://arxiv.org/abs/2504.19529v1",
        "published_date": "2025-04-28T07:12:20+00:00",
        "updated_date": "2025-04-28T07:12:20+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Guobiao Li",
            "Lei Tan",
            "Yuliang Xue",
            "Gaozhi Liu",
            "Zhenxing Qian",
            "Sheng Li",
            "Xinpeng Zhang"
        ],
        "tldr": "the paper introduces adversarial shallow watermarking (asw), a training-free watermarking method that uses an adversarially optimized host image and a shallow decoder to achieve robustness against unknown distortions, outperforming existing methods in this aspect.",
        "tldr_zh": "该论文介绍了一种名为对抗性浅层水印（asw）的免训练水印方法，该方法使用对抗性优化的宿主图像和一个浅层解码器，以实现对未知失真的鲁棒性，并且在这方面优于现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "Enhancing Quality for VVC Compressed Videos with Omniscient Quality Enhancement Model",
        "summary": "The latest video coding standard H.266/VVC has shown its great improvement in\nterms of compression performance when compared to its predecessor HEVC\nstandard. Though VVC was implemented with many advanced techniques, it still\nmet the same challenges as its predecessor due to the need for even higher\nperceptual quality demand at the decoder side as well as the compression\nperformance at the encoder side. The advancement of Artificial Intelligence\n(AI) technology, notably the deep learning-based video quality enhancement\nmethods, was shown to be a promising approach to improving the perceptual\nquality experience. In this paper, we propose a novel Omniscient video quality\nenhancement Network for VVC compressed Videos. The Omniscient Network for\ncompressed video quality enhancement was originally designed for HEVC\ncompressed videos in which not only the spatial-temporal features but also\ncross-frequencies information were employed to augment the visual quality.\nInspired by this work, we propose a modification of the OVQE model and\nintegrate it into the lasted STD-VVC (Standard Versatile Video Coding) decoder\narchitecture. As assessed in a rich set of test conditions, the proposed\nOVQE-VVC solution is able to achieve significant PSNR improvement, notably\naround 0.74 dB and up to 1.2 dB with respect to the original STD-VVC codec.\nThis also corresponds to around 19.6% of bitrate saving while keeping a similar\nquality observation.",
        "url": "http://arxiv.org/abs/2504.19935v1",
        "published_date": "2025-04-28T16:08:49+00:00",
        "updated_date": "2025-04-28T16:08:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiem HoangVan",
            "Hieu Bui Minh",
            "Sang NguyenQuang",
            "Wen-Hsiao Peng"
        ],
        "tldr": "this paper proposes a modified omniscient video quality enhancement (ovqe) model integrated into the vvc decoder to improve the perceptual quality of compressed videos, achieving significant psnr improvements and bitrate savings.",
        "tldr_zh": "本文提出了一种改进的全知视频质量增强（ovqe）模型，并将其集成到vvc解码器中，以提高压缩视频的感知质量，从而显著提高psnr并节省比特率。",
        "relevance_score": 2,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    },
    {
        "title": "Federated Out-of-Distribution Generalization: A Causal Augmentation View",
        "summary": "Federated learning aims to collaboratively model by integrating multi-source\ninformation to obtain a model that can generalize across all client data.\nExisting methods often leverage knowledge distillation or data augmentation to\nmitigate the negative impact of data bias across clients. However, the limited\nperformance of teacher models on out-of-distribution samples and the inherent\nquality gap between augmented and original data hinder their effectiveness and\nthey typically fail to leverage the advantages of incorporating rich contextual\ninformation. To address these limitations, this paper proposes a Federated\nCausal Augmentation method, termed FedCAug, which employs causality-inspired\ndata augmentation to break the spurious correlation between attributes and\ncategories. Specifically, it designs a causal region localization module to\naccurately identify and decouple the background and objects in the image,\nproviding rich contextual information for causal data augmentation.\nAdditionally, it designs a causality-inspired data augmentation module that\nintegrates causal features and within-client context to generate counterfactual\nsamples. This significantly enhances data diversity, and the entire process\ndoes not require any information sharing between clients, thereby contributing\nto the protection of data privacy. Extensive experiments conducted on three\ndatasets reveal that FedCAug markedly reduces the model's reliance on\nbackground to predict sample labels, achieving superior performance compared to\nstate-of-the-art methods.",
        "url": "http://arxiv.org/abs/2504.19882v1",
        "published_date": "2025-04-28T15:13:48+00:00",
        "updated_date": "2025-04-28T15:13:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Runhui Zhang",
            "Sijin Zhou",
            "Zhuang Qi"
        ],
        "tldr": "the paper introduces fedcaug, a federated learning method that uses causality-inspired data augmentation to improve out-of-distribution generalization by breaking spurious correlations between attributes and categories without inter-client information sharing.",
        "tldr_zh": "该论文介绍了fedcaug，一种联邦学习方法，它使用因果关系驱动的数据增强来提高分布外泛化能力，通过打破属性和类别之间的虚假相关性，且无需客户端间的信息共享。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    },
    {
        "title": "NSegment : Noisy Segment Improves Remote Sensing Image Segmentation",
        "summary": "Labeling errors in remote sensing (RS) image segmentation datasets often\nremain implicit and subtle due to ambiguous class boundaries, mixed pixels,\nshadows, complex terrain features, and subjective annotator bias. Furthermore,\nthe scarcity of annotated RS data due to high image acquisition and labeling\ncosts complicates training noise-robust models. While sophisticated mechanisms\nsuch as label selection or noise correction might address this issue, they tend\nto increase training time and add implementation complexity. In this letter, we\npropose NSegment-a simple yet effective data augmentation solution to mitigate\nthis issue. Unlike traditional methods, it applies elastic transformations only\nto segmentation labels, varying deformation intensity per sample in each\ntraining epoch to address annotation inconsistencies. Experimental results\ndemonstrate that our approach improves the performance of RS image segmentation\non various state-of-the-art models.",
        "url": "http://arxiv.org/abs/2504.19634v1",
        "published_date": "2025-04-28T09:49:35+00:00",
        "updated_date": "2025-04-28T09:49:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yechan Kim",
            "DongHo Yoon",
            "SooYeon Kim",
            "Moongu Jeon"
        ],
        "tldr": "the paper introduces nsegment, a data augmentation technique that applies elastic transformations to noisy remote sensing image segmentation labels, improving model performance without increasing training complexity.",
        "tldr_zh": "本文介绍了一种名为nsegment的数据增强技术，该技术对有噪声的遥感图像分割标签应用弹性变换，从而在不增加训练复杂性的前提下提高模型性能。",
        "relevance_score": 2,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4
    },
    {
        "title": "Lightweight Adapter Learning for More Generalized Remote Sensing Change Detection",
        "summary": "Deep learning methods have shown promising performances in remote sensing\nimage change detection (CD). However, existing methods usually train a\ndataset-specific deep network for each dataset. Due to the significant\ndifferences in the data distribution and labeling between various datasets, the\ntrained dataset-specific deep network has poor generalization performances on\nother datasets. To solve this problem, this paper proposes a change adapter\nnetwork (CANet) for a more universal and generalized CD. CANet contains\ndataset-shared and dataset-specific learning modules. The former explores the\ndiscriminative features of images, and the latter designs a lightweight adapter\nmodel, to deal with the characteristics of different datasets in data\ndistribution and labeling. The lightweight adapter can quickly generalize the\ndeep network for new CD tasks with a small computation cost. Specifically, this\npaper proposes an interesting change region mask (ICM) in the adapter, which\ncan adaptively focus on interested change objects and decrease the influence of\nlabeling differences in various datasets. Moreover, CANet adopts a unique batch\nnormalization layer for each dataset to deal with data distribution\ndifferences. Compared with existing deep learning methods, CANet can achieve\nsatisfactory CD performances on various datasets simultaneously. Experimental\nresults on several public datasets have verified the effectiveness and\nadvantages of the proposed CANet on CD. CANet has a stronger generalization\nability, smaller training costs (merely updating 4.1%-7.7% parameters), and\nbetter performances under limited training datasets than other deep learning\nmethods, which also can be flexibly inserted with existing deep models.",
        "url": "http://arxiv.org/abs/2504.19598v1",
        "published_date": "2025-04-28T09:01:56+00:00",
        "updated_date": "2025-04-28T09:01:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Dou Quan",
            "Rufan Zhou",
            "Shuang Wang",
            "Ning Huyan",
            "Dong Zhao",
            "Yunan Li",
            "Licheng Jiao"
        ],
        "tldr": "the paper introduces a change adapter network (canet) for remote sensing change detection that uses lightweight adapters and dataset-specific normalization to improve generalization across different datasets while minimizing computational costs.",
        "tldr_zh": "该论文介绍了一种用于遥感变化检测的变化适配器网络（canet），它使用轻量级适配器和数据集特定的归一化来提高不同数据集之间的泛化能力，同时最大限度地降低计算成本。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    }
]