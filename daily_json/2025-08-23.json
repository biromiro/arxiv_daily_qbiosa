[
    {
        "title": "Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation",
        "summary": "Recent advancements in video generation have substantially improved visual\nquality and temporal coherence, making these models increasingly appealing for\napplications such as autonomous driving, particularly in the context of driving\nsimulation and so-called \"world models\". In this work, we investigate the\neffects of existing fine-tuning video generation approaches on structured\ndriving datasets and uncover a potential trade-off: although visual fidelity\nimproves, spatial accuracy in modeling dynamic elements may degrade. We\nattribute this degradation to a shift in the alignment between visual quality\nand dynamic understanding objectives. In datasets with diverse scene structures\nwithin temporal space, where objects or perspective shift in varied ways, these\nobjectives tend to highly correlated. However, the very regular and repetitive\nnature of driving scenes allows visual quality to improve by modeling dominant\nscene motion patterns, without necessarily preserving fine-grained dynamic\nbehavior. As a result, fine-tuning encourages the model to prioritize\nsurface-level realism over dynamic accuracy. To further examine this\nphenomenon, we show that simple continual learning strategies, such as replay\nfrom diverse domains, can offer a balanced alternative by preserving spatial\naccuracy while maintaining strong visual quality.",
        "url": "http://arxiv.org/abs/2508.16512v1",
        "published_date": "2025-08-22T16:35:19+00:00",
        "updated_date": "2025-08-22T16:35:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chun-Peng Chang",
            "Chen-Yu Wang",
            "Julian Schmidt",
            "Holger Caesar",
            "Alain Pagani"
        ],
        "tldr": "The paper identifies a trade-off in fine-tuning video generators for driving simulation: improved visual fidelity can degrade spatial accuracy in dynamic elements. They propose continual learning strategies for a better balance.",
        "tldr_zh": "该论文指出，在微调用于驾驶模拟的视频生成器时存在一个权衡：提高视觉保真度可能会降低动态元素中的空间准确性。他们提出了持续学习策略，以实现更好的平衡。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models",
        "summary": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure.In addition, during cache reuse, we dynamically estimate the\ncorresponding noise and filter it out to reduce its impact on the sampling\ndirection.Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models.",
        "url": "http://arxiv.org/abs/2508.16212v1",
        "published_date": "2025-08-22T08:36:58+00:00",
        "updated_date": "2025-08-22T08:36:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Huanpeng Chu",
            "Wei Wu",
            "Guanyu Fen",
            "Yutao Zhang"
        ],
        "tldr": "The paper introduces OmniCache, a training-free caching method for accelerating diffusion transformer models by strategically reusing computations across the entire sampling trajectory, considering a global perspective.",
        "tldr_zh": "该论文介绍了OmniCache，一种无需训练的缓存方法，通过策略性地重用整个采样轨迹中的计算，并从全局角度考虑，从而加速扩散Transformer模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion Transformers",
        "summary": "Diffusion Transformers (DiTs) have demonstrated exceptional performance in\nhigh-fidelity image and video generation. To reduce their substantial\ncomputational costs, feature caching techniques have been proposed to\naccelerate inference by reusing hidden representations from previous timesteps.\nHowever, current methods often struggle to maintain generation quality at high\nacceleration ratios, where prediction errors increase sharply due to the\ninherent instability of long-step forecasting. In this work, we adopt an\nordinary differential equation (ODE) perspective on the hidden-feature\nsequence, modeling layer representations along the trajectory as a feature-ODE.\nWe attribute the degradation of existing caching strategies to their inability\nto robustly integrate historical features under large skipping intervals. To\naddress this, we propose FoCa (Forecast-then-Calibrate), which treats feature\ncaching as a feature-ODE solving problem. Extensive experiments on image\nsynthesis, video generation, and super-resolution tasks demonstrate the\neffectiveness of FoCa, especially under aggressive acceleration. Without\nadditional training, FoCa achieves near-lossless speedups of 5.50 times on\nFLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high\nquality with a 4.53 times speedup on DiT.",
        "url": "http://arxiv.org/abs/2508.16211v1",
        "published_date": "2025-08-22T08:34:03+00:00",
        "updated_date": "2025-08-22T08:34:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shikang Zheng",
            "Liang Feng",
            "Xinyu Wang",
            "Qinming Zhou",
            "Peiliang Cai",
            "Chang Zou",
            "Jiacheng Liu",
            "Yuqi Lin",
            "Junjie Chen",
            "Yue Ma",
            "Linfeng Zhang"
        ],
        "tldr": "The paper introduces FoCa, a novel feature caching technique for Diffusion Transformers that uses an ODE perspective to improve inference speed while maintaining generation quality, particularly at high acceleration ratios.",
        "tldr_zh": "该论文介绍了FoCa，一种新颖的Diffusion Transformer特征缓存技术，它使用ODE视角来提高推理速度，同时保持生成质量，尤其是在高加速比下。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Two-flow Feedback Multi-scale Progressive Generative Adversarial Network",
        "summary": "Although diffusion model has made good progress in the field of image\ngeneration, GAN\\cite{huang2023adaptive} still has a large development space due\nto its unique advantages, such as WGAN\\cite{liu2021comparing},\nSSGAN\\cite{guibas2021adaptive} \\cite{zhang2022vsa} \\cite{zhou2024adapt} and so\non. In this paper, we propose a novel two-flow feedback multi-scale progressive\ngenerative adversarial network (MSPG-SEN) for GAN models. This paper has four\ncontributions: 1) : We propose a two-flow feedback multi-scale progressive\nGenerative Adversarial network (MSPG-SEN), which not only improves image\nquality and human visual perception on the basis of retaining the advantages of\nthe existing GAN model, but also simplifies the training process and reduces\nthe training cost of GAN networks. Our experimental results show that, MSPG-SEN\nhas achieved state-of-the-art generation results on the following five\ndatasets,INKK The dataset is 89.7\\%,AWUN The dataset is 78.3\\%,IONJ The dataset\nis 85.5\\%,POKL The dataset is 88.7\\%,OPIN The dataset is 96.4\\%. 2) : We\npropose an adaptive perception-behavioral feedback loop (APFL), which\neffectively improves the robustness and training stability of the model and\nreduces the training cost. 3) : We propose a globally connected two-flow\ndynamic residual network(). After ablation experiments, it can effectively\nimprove the training efficiency and greatly improve the generalization ability,\nwith stronger flexibility. 4) : We propose a new dynamic embedded attention\nmechanism (DEMA). After experiments, the attention can be extended to a variety\nof image processing tasks, which can effectively capture global-local\ninformation, improve feature separation capability and feature expression\ncapabilities, and requires minimal computing resources only 88.7\\% with INJK\nWith strong cross-task capability.",
        "url": "http://arxiv.org/abs/2508.16089v1",
        "published_date": "2025-08-22T04:59:08+00:00",
        "updated_date": "2025-08-22T04:59:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sun Weikai",
            "Song Shijie",
            "Chi Wenjie"
        ],
        "tldr": "This paper introduces a novel two-flow feedback multi-scale progressive GAN (MSPG-SEN) with adaptive perception-behavioral feedback, a globally connected two-flow dynamic residual network, and a dynamic embedded attention mechanism (DEMA), claiming state-of-the-art results on several image datasets. The method purports to improve image quality, training stability, and generalization ability while reducing training cost.",
        "tldr_zh": "本文提出了一种新型的双流反馈多尺度渐进式GAN (MSPG-SEN)，它具有自适应感知-行为反馈、全局连接的双流动态残差网络和动态嵌入注意力机制 (DEMA)。该方法声称在多个图像数据集上取得了最先进的结果，旨在提高图像质量、训练稳定性和泛化能力，同时降低训练成本。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "UniEM-3M: A Universal Electron Micrograph Dataset for Microstructural Segmentation and Generation",
        "summary": "Quantitative microstructural characterization is fundamental to materials\nscience, where electron micrograph (EM) provides indispensable high-resolution\ninsights. However, progress in deep learning-based EM characterization has been\nhampered by the scarcity of large-scale, diverse, and expert-annotated\ndatasets, due to acquisition costs, privacy concerns, and annotation\ncomplexity. To address this issue, we introduce UniEM-3M, the first large-scale\nand multimodal EM dataset for instance-level understanding. It comprises 5,091\nhigh-resolution EMs, about 3 million instance segmentation labels, and\nimage-level attribute-disentangled textual descriptions, a subset of which will\nbe made publicly available. Furthermore, we are also releasing a text-to-image\ndiffusion model trained on the entire collection to serve as both a powerful\ndata augmentation tool and a proxy for the complete data distribution. To\nestablish a rigorous benchmark, we evaluate various representative instance\nsegmentation methods on the complete UniEM-3M and present UniEM-Net as a strong\nbaseline model. Quantitative experiments demonstrate that this flow-based model\noutperforms other advanced methods on this challenging benchmark. Our\nmultifaceted release of a partial dataset, a generative model, and a\ncomprehensive benchmark -- available at huggingface -- will significantly\naccelerate progress in automated materials analysis.",
        "url": "http://arxiv.org/abs/2508.16239v1",
        "published_date": "2025-08-22T09:20:00+00:00",
        "updated_date": "2025-08-22T09:20:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nan wang",
            "Zhiyi Xia",
            "Yiming Li",
            "Shi Tang",
            "Zuxin Fan",
            "Xi Fang",
            "Haoyi Tao",
            "Xiaochen Cai",
            "Guolin Ke",
            "Linfeng Zhang",
            "Yanhui Hong"
        ],
        "tldr": "The paper introduces UniEM-3M, a large-scale electron micrograph dataset with instance segmentation labels and textual descriptions, along with a text-to-image diffusion model for data augmentation and a benchmark for instance segmentation methods, aiming to accelerate automated materials analysis.",
        "tldr_zh": "该论文介绍了UniEM-3M，一个大规模的电子显微镜图像数据集，包含实例分割标签和文本描述，以及一个用于数据增强的文本到图像扩散模型和一个实例分割方法的基准，旨在加速自动化材料分析。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FlexMUSE: Multimodal Unification and Semantics Enhancement Framework with Flexible interaction for Creative Writing",
        "summary": "Multi-modal creative writing (MMCW) aims to produce illustrated articles.\nUnlike common multi-modal generative (MMG) tasks such as storytelling or\ncaption generation, MMCW is an entirely new and more abstract challenge where\ntextual and visual contexts are not strictly related to each other. Existing\nmethods for related tasks can be forcibly migrated to this track, but they\nrequire specific modality inputs or costly training, and often suffer from\nsemantic inconsistencies between modalities. Therefore, the main challenge lies\nin economically performing MMCW with flexible interactive patterns, where the\nsemantics between the modalities of the output are more aligned. In this work,\nwe propose FlexMUSE with a T2I module to enable optional visual input. FlexMUSE\npromotes creativity and emphasizes the unification between modalities by\nproposing the modality semantic alignment gating (msaGate) to restrict the\ntextual input. Besides, an attention-based cross-modality fusion is proposed to\naugment the input features for semantic enhancement. The modality semantic\ncreative direct preference optimization (mscDPO) within FlexMUSE is designed by\nextending the rejected samples to facilitate the writing creativity. Moreover,\nto advance the MMCW, we expose a dataset called ArtMUSE which contains with\naround 3k calibrated text-image pairs. FlexMUSE achieves promising results,\ndemonstrating its consistency, creativity and coherence.",
        "url": "http://arxiv.org/abs/2508.16230v1",
        "published_date": "2025-08-22T09:01:48+00:00",
        "updated_date": "2025-08-22T09:01:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jiahao Chen",
            "Zhiyong Ma",
            "Wenbiao Du",
            "Qingyuan Chuai"
        ],
        "tldr": "The paper introduces FlexMUSE, a framework for multi-modal creative writing (text and image generation) with flexible interaction, addressing semantic inconsistencies and costly training issues in existing methods. It also includes a new dataset, ArtMUSE.",
        "tldr_zh": "该论文介绍了一种用于多模态创意写作（文本和图像生成）的框架FlexMUSE，它具有灵活的交互方式，旨在解决现有方法中存在的语义不一致和训练成本高昂的问题。论文还提供了一个新的数据集ArtMUSE。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PromptFlare: Prompt-Generalized Defense via Cross-Attention Decoy in Diffusion-Based Inpainting",
        "summary": "The success of diffusion models has enabled effortless, high-quality image\nmodifications that precisely align with users' intentions, thereby raising\nconcerns about their potential misuse by malicious actors. Previous studies\nhave attempted to mitigate such misuse through adversarial attacks. However,\nthese approaches heavily rely on image-level inconsistencies, which pose\nfundamental limitations in addressing the influence of textual prompts. In this\npaper, we propose PromptFlare, a novel adversarial protection method designed\nto protect images from malicious modifications facilitated by diffusion-based\ninpainting models. Our approach leverages the cross-attention mechanism to\nexploit the intrinsic properties of prompt embeddings. Specifically, we\nidentify and target shared token of prompts that is invariant and semantically\nuninformative, injecting adversarial noise to suppress the sampling process.\nThe injected noise acts as a cross-attention decoy, diverting the model's focus\naway from meaningful prompt-image alignments and thereby neutralizing the\neffect of prompt. Extensive experiments on the EditBench dataset demonstrate\nthat our method achieves state-of-the-art performance across various metrics\nwhile significantly reducing computational overhead and GPU memory usage. These\nfindings highlight PromptFlare as a robust and efficient protection against\nunauthorized image manipulations. The code is available at\nhttps://github.com/NAHOHYUN-SKKU/PromptFlare.",
        "url": "http://arxiv.org/abs/2508.16217v1",
        "published_date": "2025-08-22T08:42:46+00:00",
        "updated_date": "2025-08-22T08:42:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hohyun Na",
            "Seunghoo Hong",
            "Simon S. Woo"
        ],
        "tldr": "The paper introduces PromptFlare, a novel adversarial defense method against malicious image modifications in diffusion-based inpainting models by injecting adversarial noise targeting uninformative prompt tokens, achieving state-of-the-art performance with reduced computational cost.",
        "tldr_zh": "该论文介绍了一种名为PromptFlare的新型对抗防御方法，通过注入对抗噪声来针对基于扩散的图像修复模型中的恶意图像修改，该噪声针对的是无信息的提示标记，从而以更低的计算成本实现了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Text-Driven 3D Hand Motion Generation from Sign Language Data",
        "summary": "Our goal is to train a generative model of 3D hand motions, conditioned on\nnatural language descriptions specifying motion characteristics such as\nhandshapes, locations, finger/hand/arm movements. To this end, we automatically\nbuild pairs of 3D hand motions and their associated textual labels with\nunprecedented scale. Specifically, we leverage a large-scale sign language\nvideo dataset, along with noisy pseudo-annotated sign categories, which we\ntranslate into hand motion descriptions via an LLM that utilizes a dictionary\nof sign attributes, as well as our complementary motion-script cues. This data\nenables training a text-conditioned hand motion diffusion model HandMDM, that\nis robust across domains such as unseen sign categories from the same sign\nlanguage, but also signs from another sign language and non-sign hand\nmovements. We contribute extensive experimental investigation of these\nscenarios and will make our trained models and data publicly available to\nsupport future research in this relatively new field.",
        "url": "http://arxiv.org/abs/2508.15902v1",
        "published_date": "2025-08-21T18:02:47+00:00",
        "updated_date": "2025-08-21T18:02:47+00:00",
        "categories": [
            "cs.CV",
            "65-XX",
            "I.4.9; I.5.1"
        ],
        "authors": [
            "Léore Bensabath",
            "Mathis Petrovich",
            "Gül Varol"
        ],
        "tldr": "The paper introduces HandMDM, a text-conditioned diffusion model for generating 3D hand motions from natural language descriptions, trained on a large-scale sign language dataset with LLM-generated labels, and demonstrates its robustness across different sign languages and non-sign hand movements.",
        "tldr_zh": "本文介绍了一种名为HandMDM的文本条件扩散模型，用于从自然语言描述生成3D手部动作。该模型使用大规模手语数据集进行训练，并利用LLM生成标签，证明了其在不同手语和非手语手部运动中的鲁棒性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]