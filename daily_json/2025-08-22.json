[
    {
        "title": "CineScale: Free Lunch in High-Resolution Cinematic Visual Generation",
        "summary": "Visual diffusion models achieve remarkable progress, yet they are typically\ntrained at limited resolutions due to the lack of high-resolution data and\nconstrained computation resources, hampering their ability to generate\nhigh-fidelity images or videos at higher resolutions. Recent efforts have\nexplored tuning-free strategies to exhibit the untapped potential\nhigher-resolution visual generation of pre-trained models. However, these\nmethods are still prone to producing low-quality visual content with repetitive\npatterns. The key obstacle lies in the inevitable increase in high-frequency\ninformation when the model generates visual content exceeding its training\nresolution, leading to undesirable repetitive patterns deriving from the\naccumulated errors. In this work, we propose CineScale, a novel inference\nparadigm to enable higher-resolution visual generation. To tackle the various\nissues introduced by the two types of video generation architectures, we\npropose dedicated variants tailored to each. Unlike existing baseline methods\nthat are confined to high-resolution T2I and T2V generation, CineScale broadens\nthe scope by enabling high-resolution I2V and V2V synthesis, built atop\nstate-of-the-art open-source video generation frameworks. Extensive experiments\nvalidate the superiority of our paradigm in extending the capabilities of\nhigher-resolution visual generation for both image and video models.\nRemarkably, our approach enables 8k image generation without any fine-tuning,\nand achieves 4k video generation with only minimal LoRA fine-tuning. Generated\nvideo samples are available at our website:\nhttps://eyeline-labs.github.io/CineScale/.",
        "url": "http://arxiv.org/abs/2508.15774v1",
        "published_date": "2025-08-21T17:59:57+00:00",
        "updated_date": "2025-08-21T17:59:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haonan Qiu",
            "Ning Yu",
            "Ziqi Huang",
            "Paul Debevec",
            "Ziwei Liu"
        ],
        "tldr": "The paper introduces CineScale, a novel inference paradigm that enables high-resolution (up to 8k images and 4k videos) visual generation from pre-trained diffusion models with minimal or no fine-tuning, addressing the issue of repetitive patterns in existing methods.",
        "tldr_zh": "该论文介绍了CineScale，一种新颖的推理范式，它能够从预训练的扩散模型中生成高分辨率（高达8k图像和4k视频）的视觉内容，只需极少或无需微调，从而解决了现有方法中重复模式的问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Waver: Wave Your Way to Lifelike Video Generation",
        "summary": "We present Waver, a high-performance foundation model for unified image and\nvideo generation. Waver can directly generate videos with durations ranging\nfrom 5 to 10 seconds at a native resolution of 720p, which are subsequently\nupscaled to 1080p. The model simultaneously supports text-to-video (T2V),\nimage-to-video (I2V), and text-to-image (T2I) generation within a single,\nintegrated framework. We introduce a Hybrid Stream DiT architecture to enhance\nmodality alignment and accelerate training convergence. To ensure training data\nquality, we establish a comprehensive data curation pipeline and manually\nannotate and train an MLLM-based video quality model to filter for the\nhighest-quality samples. Furthermore, we provide detailed training and\ninference recipes to facilitate the generation of high-quality videos. Building\non these contributions, Waver excels at capturing complex motion, achieving\nsuperior motion amplitude and temporal consistency in video synthesis. Notably,\nit ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial\nAnalysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming\nexisting open-source models and matching or surpassing state-of-the-art\ncommercial solutions. We hope this technical report will help the community\nmore efficiently train high-quality video generation models and accelerate\nprogress in video generation technologies. Official page:\nhttps://github.com/FoundationVision/Waver.",
        "url": "http://arxiv.org/abs/2508.15761v1",
        "published_date": "2025-08-21T17:56:10+00:00",
        "updated_date": "2025-08-21T17:56:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yifu Zhang",
            "Hao Yang",
            "Yuqi Zhang",
            "Yifei Hu",
            "Fengda Zhu",
            "Chuang Lin",
            "Xiaofeng Mei",
            "Yi Jiang",
            "Zehuan Yuan",
            "Bingyue Peng"
        ],
        "tldr": "Waver is a new foundation model for unified image and video generation that achieves state-of-the-art results in text-to-video, image-to-video, and text-to-image tasks, outperforming open-source models and matching commercial solutions.",
        "tldr_zh": "Waver 是一个新的统一图像和视频生成的基础模型，在文本到视频、图像到视频和文本到图像任务中取得了最先进的结果，优于开源模型并与商业解决方案相匹配。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception",
        "summary": "Generative video modeling has made significant strides, yet ensuring\nstructural and temporal consistency over long sequences remains a challenge.\nCurrent methods predominantly rely on RGB signals, leading to accumulated\nerrors in object structure and motion over extended durations. To address these\nissues, we introduce WorldWeaver, a robust framework for long video generation\nthat jointly models RGB frames and perceptual conditions within a unified\nlong-horizon modeling scheme. Our training framework offers three key\nadvantages. First, by jointly predicting perceptual conditions and color\ninformation from a unified representation, it significantly enhances temporal\nconsistency and motion dynamics. Second, by leveraging depth cues, which we\nobserve to be more resistant to drift than RGB, we construct a memory bank that\npreserves clearer contextual information, improving quality in long-horizon\nvideo generation. Third, we employ segmented noise scheduling for training\nprediction groups, which further mitigates drift and reduces computational\ncost. Extensive experiments on both diffusion- and rectified flow-based models\ndemonstrate the effectiveness of WorldWeaver in reducing temporal drift and\nimproving the fidelity of generated videos.",
        "url": "http://arxiv.org/abs/2508.15720v1",
        "published_date": "2025-08-21T16:57:33+00:00",
        "updated_date": "2025-08-21T16:57:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiheng Liu",
            "Xueqing Deng",
            "Shoufa Chen",
            "Angtian Wang",
            "Qiushan Guo",
            "Mingfei Han",
            "Zeyue Xue",
            "Mengzhao Chen",
            "Ping Luo",
            "Linjie Yang"
        ],
        "tldr": "WorldWeaver is a framework for long-horizon video generation that uses joint modeling of RGB and perceptual conditions, depth cues, and segmented noise scheduling to improve temporal consistency and reduce drift.",
        "tldr_zh": "WorldWeaver是一个长时程视频生成框架，它通过联合建模RGB和感知条件、深度线索以及分段噪声调度来提高时间一致性并减少漂移。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Pretrained Diffusion Models Are Inherently Skipped-Step Samplers",
        "summary": "Diffusion models have been achieving state-of-the-art results across various\ngeneration tasks. However, a notable drawback is their sequential generation\nprocess, requiring long-sequence step-by-step generation. Existing methods,\nsuch as DDIM, attempt to reduce sampling steps by constructing a class of\nnon-Markovian diffusion processes that maintain the same training objective.\nHowever, there remains a gap in understanding whether the original diffusion\nprocess can achieve the same efficiency without resorting to non-Markovian\nprocesses. In this paper, we provide a confirmative answer and introduce\nskipped-step sampling, a mechanism that bypasses multiple intermediate\ndenoising steps in the iterative generation process, in contrast with the\ntraditional step-by-step refinement of standard diffusion inference. Crucially,\nwe demonstrate that this skipped-step sampling mechanism is derived from the\nsame training objective as the standard diffusion model, indicating that\naccelerated sampling via skipped-step sampling via a Markovian way is an\nintrinsic property of pretrained diffusion models. Additionally, we propose an\nenhanced generation method by integrating our accelerated sampling technique\nwith DDIM. Extensive experiments on popular pretrained diffusion models,\nincluding the OpenAI ADM, Stable Diffusion, and Open Sora models, show that our\nmethod achieves high-quality generation with significantly reduced sampling\nsteps.",
        "url": "http://arxiv.org/abs/2508.15233v1",
        "published_date": "2025-08-21T04:45:13+00:00",
        "updated_date": "2025-08-21T04:45:13+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Wenju Xu"
        ],
        "tldr": "The paper introduces a skipped-step sampling method for diffusion models that accelerates generation without sacrificing quality, demonstrating it's an inherent property of pretrained diffusion models and integrating it with DDIM for further speedup.",
        "tldr_zh": "该论文提出了一种用于扩散模型的跳步采样方法，该方法可以在不牺牲质量的前提下加速生成过程。研究表明这种方法是预训练扩散模型固有的属性，并将其与DDIM集成以进一步提高速度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Collaborative Multi-Modal Coding for High-Quality 3D Generation",
        "summary": "3D content inherently encompasses multi-modal characteristics and can be\nprojected into different modalities (e.g., RGB images, RGBD, and point clouds).\nEach modality exhibits distinct advantages in 3D asset modeling: RGB images\ncontain vivid 3D textures, whereas point clouds define fine-grained 3D\ngeometries. However, most existing 3D-native generative architectures either\noperate predominantly within single-modality paradigms-thus overlooking the\ncomplementary benefits of multi-modality data-or restrict themselves to 3D\nstructures, thereby limiting the scope of available training datasets. To\nholistically harness multi-modalities for 3D modeling, we present TriMM, the\nfirst feed-forward 3D-native generative model that learns from basic\nmulti-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMM\nfirst introduces collaborative multi-modal coding, which integrates\nmodality-specific features while preserving their unique representational\nstrengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced to\nraise the robustness and performance of multi-modal coding. 3) Based on the\nembedded multi-modal code, TriMM employs a triplane latent diffusion model to\ngenerate 3D assets of superior quality, enhancing both the texture and the\ngeometric detail. Extensive experiments on multiple well-known datasets\ndemonstrate that TriMM, by effectively leveraging multi-modality, achieves\ncompetitive performance with models trained on large-scale datasets, despite\nutilizing a small amount of training data. Furthermore, we conduct additional\nexperiments on recent RGB-D datasets, verifying the feasibility of\nincorporating other multi-modal datasets into 3D generation.",
        "url": "http://arxiv.org/abs/2508.15228v1",
        "published_date": "2025-08-21T04:31:14+00:00",
        "updated_date": "2025-08-21T04:31:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziang Cao",
            "Zhaoxi Chen",
            "Liang Pan",
            "Ziwei Liu"
        ],
        "tldr": "The paper introduces TriMM, a novel feed-forward 3D-native generative model that leverages collaborative multi-modal coding (RGB, RGBD, point clouds) and auxiliary 2D/3D supervision to generate high-quality 3D assets with enhanced texture and geometric detail, even with limited training data.",
        "tldr_zh": "该论文介绍了TriMM，一种新型前馈3D原生生成模型，它利用协作式多模态编码（RGB、RGBD、点云）和辅助2D/3D监督，生成具有增强纹理和几何细节的高质量3D资产，即使在训练数据有限的情况下也是如此。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Scaling Group Inference for Diverse and High-Quality Generation",
        "summary": "Generative models typically sample outputs independently, and recent\ninference-time guidance and scaling algorithms focus on improving the quality\nof individual samples. However, in real-world applications, users are often\npresented with a set of multiple images (e.g., 4-8) for each prompt, where\nindependent sampling tends to lead to redundant results, limiting user choices\nand hindering idea exploration. In this work, we introduce a scalable group\ninference method that improves both the diversity and quality of a group of\nsamples. We formulate group inference as a quadratic integer assignment\nproblem: candidate outputs are modeled as graph nodes, and a subset is selected\nto optimize sample quality (unary term) while maximizing group diversity\n(binary term). To substantially improve runtime efficiency, we progressively\nprune the candidate set using intermediate predictions, allowing our method to\nscale up to large candidate sets. Extensive experiments show that our method\nsignificantly improves group diversity and quality compared to independent\nsampling baselines and recent inference algorithms. Our framework generalizes\nacross a wide range of tasks, including text-to-image, image-to-image, image\nprompting, and video generation, enabling generative models to treat multiple\noutputs as cohesive groups rather than independent samples.",
        "url": "http://arxiv.org/abs/2508.15773v1",
        "published_date": "2025-08-21T17:59:57+00:00",
        "updated_date": "2025-08-21T17:59:57+00:00",
        "categories": [
            "cs.CV",
            "cs.GR",
            "cs.LG"
        ],
        "authors": [
            "Gaurav Parmar",
            "Or Patashnik",
            "Daniil Ostashev",
            "Kuan-Chieh Wang",
            "Kfir Aberman",
            "Srinivasa Narasimhan",
            "Jun-Yan Zhu"
        ],
        "tldr": "The paper introduces a scalable group inference method for generative models that improves both diversity and quality of multiple generated samples by formulating it as a quadratic integer assignment problem and progressively pruning candidates.",
        "tldr_zh": "该论文介绍了一种可扩展的生成模型分组推理方法，通过将其构建为二次整数分配问题并逐步修剪候选样本，从而提高多个生成样本的多样性和质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Visual Autoregressive Modeling for Instruction-Guided Image Editing",
        "summary": "Recent advances in diffusion models have brought remarkable visual fidelity\nto instruction-guided image editing. However, their global denoising process\ninherently entangles the edited region with the entire image context, leading\nto unintended spurious modifications and compromised adherence to editing\ninstructions. In contrast, autoregressive models offer a distinct paradigm by\nformulating image synthesis as a sequential process over discrete visual\ntokens. Their causal and compositional mechanism naturally circumvents the\nadherence challenges of diffusion-based methods. In this paper, we present\nVAREdit, a visual autoregressive (VAR) framework that reframes image editing as\na next-scale prediction problem. Conditioned on source image features and text\ninstructions, VAREdit generates multi-scale target features to achieve precise\nedits. A core challenge in this paradigm is how to effectively condition the\nsource image tokens. We observe that finest-scale source features cannot\neffectively guide the prediction of coarser target features. To bridge this\ngap, we introduce a Scale-Aligned Reference (SAR) module, which injects\nscale-matched conditioning information into the first self-attention layer.\nVAREdit demonstrates significant advancements in both editing adherence and\nefficiency. On standard benchmarks, it outperforms leading diffusion-based\nmethods by 30\\%+ higher GPT-Balance score. Moreover, it completes a\n$512\\times512$ editing in 1.2 seconds, making it 2.2$\\times$ faster than the\nsimilarly sized UltraEdit. The models are available at\nhttps://github.com/HiDream-ai/VAREdit.",
        "url": "http://arxiv.org/abs/2508.15772v1",
        "published_date": "2025-08-21T17:59:32+00:00",
        "updated_date": "2025-08-21T17:59:32+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Qingyang Mao",
            "Qi Cai",
            "Yehao Li",
            "Yingwei Pan",
            "Mingyue Cheng",
            "Ting Yao",
            "Qi Liu",
            "Tao Mei"
        ],
        "tldr": "The paper introduces VAREdit, a visual autoregressive framework for instruction-guided image editing that addresses limitations of diffusion models by using a next-scale prediction approach conditioned on source image features and text instructions, achieving improved editing adherence and efficiency.",
        "tldr_zh": "该论文介绍了VAREdit，一个用于指令引导图像编辑的可视化自回归框架。它通过使用基于源图像特征和文本指令的下一尺度预测方法，解决了扩散模型的局限性，实现了更好的编辑准确性和效率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Intern-S1: A Scientific Multimodal Foundation Model",
        "summary": "In recent years, a plethora of open-source foundation models have emerged,\nachieving remarkable progress in some widely attended fields, with performance\nbeing quite close to that of closed-source models. However, in high-value but\nmore challenging scientific professional fields, either the fields still rely\non expert models, or the progress of general foundation models lags\nsignificantly compared to those in popular areas, far from sufficient for\ntransforming scientific research and leaving substantial gap between\nopen-source models and closed-source models in these scientific domains. To\nmitigate this gap and explore a step further toward Artificial General\nIntelligence (AGI), we introduce Intern-S1, a specialized generalist equipped\nwith general understanding and reasoning capabilities with expertise to analyze\nmultiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE)\nmodel with 28 billion activated parameters and 241 billion total parameters,\ncontinually pre-trained on 5T tokens, including over 2.5T tokens from\nscientific domains. In the post-training stage, Intern-S1 undergoes offline and\nthen online reinforcement learning (RL) in InternBootCamp, where we propose\nMixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks\nsimultaneously. Through integrated innovations in algorithms, data, and\ntraining systems, Intern-S1 achieved top-tier performance in online RL\ntraining.On comprehensive evaluation benchmarks, Intern-S1 demonstrates\ncompetitive performance on general reasoning tasks among open-source models and\nsignificantly outperforms open-source models in scientific domains, surpassing\nclosed-source state-of-the-art models in professional tasks, such as molecular\nsynthesis planning, reaction condition prediction, predicting thermodynamic\nstabilities for crystals. Our models are available at\nhttps://huggingface.co/internlm/Intern-S1.",
        "url": "http://arxiv.org/abs/2508.15763v1",
        "published_date": "2025-08-21T17:58:00+00:00",
        "updated_date": "2025-08-21T17:58:00+00:00",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Lei Bai",
            "Zhongrui Cai",
            "Maosong Cao",
            "Weihan Cao",
            "Chiyu Chen",
            "Haojiong Chen",
            "Kai Chen",
            "Pengcheng Chen",
            "Ying Chen",
            "Yongkang Chen",
            "Yu Cheng",
            "Yu Cheng",
            "Pei Chu",
            "Tao Chu",
            "Erfei Cui",
            "Ganqu Cui",
            "Long Cui",
            "Ziyun Cui",
            "Nianchen Deng",
            "Ning Ding",
            "Nanqin Dong",
            "Peijie Dong",
            "Shihan Dou",
            "Sinan Du",
            "Haodong Duan",
            "Caihua Fan",
            "Ben Gao",
            "Changjiang Gao",
            "Jianfei Gao",
            "Songyang Gao",
            "Yang Gao",
            "Zhangwei Gao",
            "Jiaye Ge",
            "Qiming Ge",
            "Lixin Gu",
            "Yuzhe Gu",
            "Aijia Guo",
            "Qipeng Guo",
            "Xu Guo",
            "Conghui He",
            "Junjun He",
            "Yili Hong",
            "Siyuan Hou",
            "Caiyu Hu",
            "Hanglei Hu",
            "Jucheng Hu",
            "Ming Hu",
            "Zhouqi Hua",
            "Haian Huang",
            "Junhao Huang",
            "Xu Huang",
            "Zixian Huang",
            "Zhe Jiang",
            "Lingkai Kong",
            "Linyang Li",
            "Peiji Li",
            "Pengze Li",
            "Shuaibin Li",
            "Tianbin Li",
            "Wei Li",
            "Yuqiang Li",
            "Dahua Lin",
            "Junyao Lin",
            "Tianyi Lin",
            "Zhishan Lin",
            "Hongwei Liu",
            "Jiangning Liu",
            "Jiyao Liu",
            "Junnan Liu",
            "Kai Liu",
            "Kaiwen Liu",
            "Kuikun Liu",
            "Shichun Liu",
            "Shudong Liu",
            "Wei Liu",
            "Xinyao Liu",
            "Yuhong Liu",
            "Zhan Liu",
            "Yinquan Lu",
            "Haijun Lv",
            "Hongxia Lv",
            "Huijie Lv",
            "Qidang Lv",
            "Ying Lv",
            "Chengqi Lyu",
            "Chenglong Ma",
            "Jianpeng Ma",
            "Ren Ma",
            "Runmin Ma",
            "Runyuan Ma",
            "Xinzhu Ma",
            "Yichuan Ma",
            "Zihan Ma",
            "Sixuan Mi",
            "Junzhi Ning",
            "Wenchang Ning",
            "Xinle Pang",
            "Jiahui Peng",
            "Runyu Peng",
            "Yu Qiao",
            "Jiantao Qiu",
            "Xiaoye Qu",
            "Yuan Qu",
            "Yuchen Ren",
            "Fukai Shang",
            "Wenqi Shao",
            "Junhao Shen",
            "Shuaike Shen",
            "Chunfeng Song",
            "Demin Song",
            "Diping Song",
            "Chenlin Su",
            "Weijie Su",
            "Weigao Sun",
            "Yu Sun",
            "Qian Tan",
            "Cheng Tang",
            "Huanze Tang",
            "Kexian Tang",
            "Shixiang Tang",
            "Jian Tong",
            "Aoran Wang",
            "Bin Wang",
            "Dong Wang",
            "Lintao Wang",
            "Rui Wang",
            "Weiyun Wang",
            "Wenhai Wang",
            "Yi Wang",
            "Ziyi Wang",
            "Ling-I Wu",
            "Wen Wu",
            "Yue Wu",
            "Zijian Wu",
            "Linchen Xiao",
            "Shuhao Xing",
            "Chao Xu",
            "Huihui Xu",
            "Jun Xu",
            "Ruiliang Xu",
            "Wanghan Xu",
            "GanLin Yang",
            "Yuming Yang",
            "Haochen Ye",
            "Jin Ye",
            "Shenglong Ye",
            "Jia Yu",
            "Jiashuo Yu",
            "Jing Yu",
            "Fei Yuan",
            "Bo Zhang",
            "Chao Zhang",
            "Chen Zhang",
            "Hongjie Zhang",
            "Jin Zhang",
            "Qiaosheng Zhang",
            "Qiuyinzhe Zhang",
            "Songyang Zhang",
            "Taolin Zhang",
            "Wenlong Zhang",
            "Wenwei Zhang",
            "Yechen Zhang",
            "Ziyang Zhang",
            "Haiteng Zhao",
            "Qian Zhao",
            "Xiangyu Zhao",
            "Xiangyu Zhao",
            "Bowen Zhou",
            "Dongzhan Zhou",
            "Peiheng Zhou",
            "Yuhao Zhou",
            "Yunhua Zhou",
            "Dongsheng Zhu",
            "Lin Zhu",
            "Yicheng Zou"
        ],
        "tldr": "The paper introduces Intern-S1, a scientific multimodal foundation model with 28B activated parameters, pre-trained on 5T tokens, demonstrating state-of-the-art performance in scientific tasks, surpassing even closed-source models in certain areas.",
        "tldr_zh": "该论文介绍了 Intern-S1，一个科学多模态基础模型，具有 280 亿个激活参数，经过 5T tokens的预训练，在科学任务中表现出最先进的性能，并在某些领域超过了闭源模型。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion",
        "summary": "Mesh models have become increasingly accessible for numerous cities; however,\nthe lack of realistic textures restricts their application in virtual urban\nnavigation and autonomous driving. To address this, this paper proposes MeSS\n(Meshbased Scene Synthesis) for generating high-quality, styleconsistent\noutdoor scenes with city mesh models serving as the geometric prior. While\nimage and video diffusion models can leverage spatial layouts (such as depth\nmaps or HD maps) as control conditions to generate street-level perspective\nviews, they are not directly applicable to 3D scene generation. Video diffusion\nmodels excel at synthesizing consistent view sequences that depict scenes but\noften struggle to adhere to predefined camera paths or align accurately with\nrendered control videos. In contrast, image diffusion models, though unable to\nguarantee cross-view visual consistency, can produce more geometry-aligned\nresults when combined with ControlNet. Building on this insight, our approach\nenhances image diffusion models by improving cross-view consistency. The\npipeline comprises three key stages: first, we generate geometrically\nconsistent sparse views using Cascaded Outpainting ControlNets; second, we\npropagate denser intermediate views via a component dubbed AGInpaint; and\nthird, we globally eliminate visual inconsistencies (e.g., varying exposure)\nusing the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting\n(3DGS) scene is reconstructed by initializing Gaussian balls on the mesh\nsurface. Our method outperforms existing approaches in both geometric alignment\nand generation quality. Once synthesized, the scene can be rendered in diverse\nstyles through relighting and style transfer techniques.",
        "url": "http://arxiv.org/abs/2508.15169v1",
        "published_date": "2025-08-21T02:16:15+00:00",
        "updated_date": "2025-08-21T02:16:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuyang Chen",
            "Zhijun Zhai",
            "Kaixuan Zhou",
            "Zengmao Wang",
            "Jianan He",
            "Dong Wang",
            "Yanfeng Zhang",
            "mingwei Sun",
            "Rüdiger Westermann",
            "Konrad Schindler",
            "Liqiu Meng"
        ],
        "tldr": "The paper introduces MeSS, a novel pipeline that leverages city mesh models and cascaded diffusion models to generate high-quality, cross-view consistent outdoor scenes, reconstructible as 3D Gaussian splatting scenes.",
        "tldr_zh": "该论文介绍了MeSS，一种利用城市网格模型和级联扩散模型生成高质量、视角一致的室外场景的新流程，可以重建为3D高斯溅射场景。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CurveFlow: Curvature-Guided Flow Matching for Image Generation",
        "summary": "Existing rectified flow models are based on linear trajectories between data\nand noise distributions. This linearity enforces zero curvature, which can\ninadvertently force the image generation process through low-probability\nregions of the data manifold. A key question remains underexplored: how does\nthe curvature of these trajectories correlate with the semantic alignment\nbetween generated images and their corresponding captions, i.e., instructional\ncompliance? To address this, we introduce CurveFlow, a novel flow matching\nframework designed to learn smooth, non-linear trajectories by directly\nincorporating curvature guidance into the flow path. Our method features a\nrobust curvature regularization technique that penalizes abrupt changes in the\ntrajectory's intrinsic dynamics.Extensive experiments on MS COCO 2014 and 2017\ndemonstrate that CurveFlow achieves state-of-the-art performance in\ntext-to-image generation, significantly outperforming both standard rectified\nflow variants and other non-linear baselines like Rectified Diffusion. The\nimprovements are especially evident in semantic consistency metrics such as\nBLEU, METEOR, ROUGE, and CLAIR. This confirms that our curvature-aware modeling\nsubstantially enhances the model's ability to faithfully follow complex\ninstructions while simultaneously maintaining high image quality. The code is\nmade publicly available at\nhttps://github.com/Harvard-AI-and-Robotics-Lab/CurveFlow.",
        "url": "http://arxiv.org/abs/2508.15093v1",
        "published_date": "2025-08-20T22:06:13+00:00",
        "updated_date": "2025-08-20T22:06:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yan Luo",
            "Drake Du",
            "Hao Huang",
            "Yi Fang",
            "Mengyu Wang"
        ],
        "tldr": "CurveFlow introduces curvature guidance into rectified flow models for improved text-to-image generation, demonstrating state-of-the-art performance and enhanced semantic consistency.",
        "tldr_zh": "CurveFlow将曲率引导引入到修正流模型中，以改进文本到图像的生成，展示了最先进的性能并增强了语义一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass",
        "summary": "3D content generation has recently attracted significant research interest\ndue to its applications in VR/AR and embodied AI. In this work, we address the\nchallenging task of synthesizing multiple 3D assets within a single scene\nimage. Concretely, our contributions are fourfold: (i) we present SceneGen, a\nnovel framework that takes a scene image and corresponding object masks as\ninput, simultaneously producing multiple 3D assets with geometry and texture.\nNotably, SceneGen operates with no need for optimization or asset retrieval;\n(ii) we introduce a novel feature aggregation module that integrates local and\nglobal scene information from visual and geometric encoders within the feature\nextraction module. Coupled with a position head, this enables the generation of\n3D assets and their relative spatial positions in a single feedforward pass;\n(iii) we demonstrate SceneGen's direct extensibility to multi-image input\nscenarios. Despite being trained solely on single-image inputs, our\narchitectural design enables improved generation performance with multi-image\ninputs; and (iv) extensive quantitative and qualitative evaluations confirm the\nefficiency and robust generation abilities of our approach. We believe this\nparadigm offers a novel solution for high-quality 3D content generation,\npotentially advancing its practical applications in downstream tasks. The code\nand model will be publicly available at: https://mengmouxu.github.io/SceneGen.",
        "url": "http://arxiv.org/abs/2508.15769v1",
        "published_date": "2025-08-21T17:59:16+00:00",
        "updated_date": "2025-08-21T17:59:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yanxu Meng",
            "Haoning Wu",
            "Ya Zhang",
            "Weidi Xie"
        ],
        "tldr": "SceneGen is a novel framework for generating multiple 3D assets from a single scene image in a single feedforward pass, using a new feature aggregation module to integrate local and global scene information.",
        "tldr_zh": "SceneGen是一个新颖的框架，它可以通过单次前馈传递从单个场景图像生成多个3D资产，并使用新的特征聚合模块来整合局部和全局场景信息。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Are Virtual DES Images a Valid Alternative to the Real Ones?",
        "summary": "Contrast-enhanced spectral mammography (CESM) is an imaging modality that\nprovides two types of images, commonly known as low-energy (LE) and dual-energy\nsubtracted (DES) images. In many domains, particularly in medicine, the\nemergence of image-to-image translation techniques has enabled the artificial\ngeneration of images using other images as input. Within CESM, applying such\ntechniques to generate DES images from LE images could be highly beneficial,\npotentially reducing patient exposure to radiation associated with high-energy\nimage acquisition. In this study, we investigated three models for the\nartificial generation of DES images (virtual DES): a pre-trained U-Net model, a\nU-Net trained end-to-end model, and a CycleGAN model. We also performed a\nseries of experiments to assess the impact of using virtual DES images on the\nclassification of CESM examinations into malignant and non-malignant\ncategories. To our knowledge, this is the first study to evaluate the impact of\nvirtual DES images on CESM lesion classification. The results demonstrate that\nthe best performance was achieved with the pre-trained U-Net model, yielding an\nF1 score of 85.59% when using the virtual DES images, compared to 90.35% with\nthe real DES images. This discrepancy likely results from the additional\ndiagnostic information in real DES images, which contributes to a higher\nclassification accuracy. Nevertheless, the potential for virtual DES image\ngeneration is considerable and future advancements may narrow this performance\ngap to a level where exclusive reliance on virtual DES images becomes\nclinically viable.",
        "url": "http://arxiv.org/abs/2508.15594v1",
        "published_date": "2025-08-21T14:07:42+00:00",
        "updated_date": "2025-08-21T14:07:42+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Ana C. Perre",
            "Luís A. Alexandre",
            "Luís C. Freire"
        ],
        "tldr": "This paper explores generating virtual dual-energy subtracted (DES) images from low-energy (LE) images in contrast-enhanced spectral mammography (CESM) using image-to-image translation techniques, achieving promising but slightly lower performance compared to real DES images for lesion classification.",
        "tldr_zh": "本文探讨了使用图像到图像转换技术，从对比增强光谱乳腺摄影 (CESM) 中的低能量 (LE) 图像生成虚拟双能量减影 (DES) 图像，在病灶分类方面取得了有希望但略低于真实 DES 图像的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]