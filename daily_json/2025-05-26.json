[
    {
        "title": "Plug-and-Play Context Feature Reuse for Efficient Masked Generation",
        "summary": "Masked generative models (MGMs) have emerged as a powerful framework for\nimage synthesis, combining parallel decoding with strong bidirectional context\nmodeling. However, generating high-quality samples typically requires many\niterative decoding steps, resulting in high inference costs. A straightforward\nway to speed up generation is by decoding more tokens in each step, thereby\nreducing the total number of steps. However, when many tokens are decoded\nsimultaneously, the model can only estimate the univariate marginal\ndistributions independently, failing to capture the dependency among them. As a\nresult, reducing the number of steps significantly compromises generation\nfidelity. In this work, we introduce ReCAP (Reused Context-Aware Prediction), a\nplug-and-play module that accelerates inference in MGMs by constructing\nlow-cost steps via reusing feature embeddings from previously decoded context\ntokens. ReCAP interleaves standard full evaluations with lightweight steps that\ncache and reuse context features, substantially reducing computation while\npreserving the benefits of fine-grained, iterative generation. We demonstrate\nits effectiveness on top of three representative MGMs (MaskGIT, MAGE, and MAR),\nincluding both discrete and continuous token spaces and covering diverse\narchitectural designs. In particular, on ImageNet256 class-conditional\ngeneration, ReCAP achieves up to 2.4x faster inference than the base model with\nminimal performance drop, and consistently delivers better efficiency-fidelity\ntrade-offs under various generation settings.",
        "url": "http://arxiv.org/abs/2505.19089v1",
        "published_date": "2025-05-25T10:57:35+00:00",
        "updated_date": "2025-05-25T10:57:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuejie Liu",
            "Anji Liu",
            "Guy Van den Broeck",
            "Yitao Liang"
        ],
        "tldr": "The paper introduces ReCAP, a plug-and-play module for masked generative models (MGMs) that accelerates inference by reusing feature embeddings from previously decoded context tokens, achieving faster inference with minimal performance drop.",
        "tldr_zh": "该论文介绍了一种用于掩码生成模型（MGMs）的即插即用模块ReCAP，它通过重用先前解码的上下文标记中的特征嵌入来加速推理，从而以最小的性能下降实现更快的推理。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Jodi: Unification of Visual Generation and Understanding via Joint Modeling",
        "summary": "Visual generation and understanding are two deeply interconnected aspects of\nhuman intelligence, yet they have been traditionally treated as separate tasks\nin machine learning. In this paper, we propose Jodi, a diffusion framework that\nunifies visual generation and understanding by jointly modeling the image\ndomain and multiple label domains. Specifically, Jodi is built upon a linear\ndiffusion transformer along with a role switch mechanism, which enables it to\nperform three particular types of tasks: (1) joint generation, where the model\nsimultaneously generates images and multiple labels; (2) controllable\ngeneration, where images are generated conditioned on any combination of\nlabels; and (3) image perception, where multiple labels can be predicted at\nonce from a given image. Furthermore, we present the Joint-1.6M dataset, which\ncontains 200,000 high-quality images collected from public sources, automatic\nlabels for 7 visual domains, and LLM-generated captions. Extensive experiments\ndemonstrate that Jodi excels in both generation and understanding tasks and\nexhibits strong extensibility to a wider range of visual domains. Code is\navailable at https://github.com/VIPL-GENUN/Jodi.",
        "url": "http://arxiv.org/abs/2505.19084v1",
        "published_date": "2025-05-25T10:40:52+00:00",
        "updated_date": "2025-05-25T10:40:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Yifeng Xu",
            "Zhenliang He",
            "Meina Kan",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "tldr": "The paper introduces Jodi, a diffusion-based framework that unifies visual generation and understanding by jointly modeling images and multiple labels, along with a new dataset, Joint-1.6M.",
        "tldr_zh": "该论文介绍了一种名为Jodi的扩散框架，它通过联合建模图像和多个标签来统一视觉生成和理解，同时还提出了一个新的数据集Joint-1.6M。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation",
        "summary": "Diffusion Transformers (DiTs) are essential for video generation but suffer\nfrom significant latency due to the quadratic complexity of attention. By\ncomputing only critical tokens, sparse attention reduces computational costs\nand offers a promising acceleration approach. However, we identify that\nexisting methods fail to approach optimal generation quality under the same\ncomputation budget for two reasons: (1) Inaccurate critical token\nidentification: current methods cluster tokens based on position rather than\nsemantics, leading to imprecise aggregated representations. (2) Excessive\ncomputation waste: critical tokens are scattered among non-critical ones,\nleading to wasted computation on GPUs, which are optimized for processing\ncontiguous tokens. In this paper, we propose SVG2, a training-free framework\nthat maximizes identification accuracy and minimizes computation waste,\nachieving a Pareto frontier trade-off between generation quality and\nefficiency. The core of SVG2 is semantic-aware permutation, which clusters and\nreorders tokens based on semantic similarity using k-means. This approach\nensures both a precise cluster representation, improving identification\naccuracy, and a densified layout of critical tokens, enabling efficient\ncomputation without padding. Additionally, SVG2 integrates top-p dynamic budget\ncontrol and customized kernel implementations, achieving up to 2.30x and 1.89x\nspeedup while maintaining a PSNR of up to 30 and 26 on HunyuanVideo and Wan\n2.1, respectively.",
        "url": "http://arxiv.org/abs/2505.18875v1",
        "published_date": "2025-05-24T21:30:29+00:00",
        "updated_date": "2025-05-24T21:30:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuo Yang",
            "Haocheng Xi",
            "Yilong Zhao",
            "Muyang Li",
            "Jintao Zhang",
            "Han Cai",
            "Yujun Lin",
            "Xiuyu Li",
            "Chenfeng Xu",
            "Kelly Peng",
            "Jianfei Chen",
            "Song Han",
            "Kurt Keutzer",
            "Ion Stoica"
        ],
        "tldr": "The paper introduces Sparse VideoGen2 (SVG2), a training-free framework for accelerating video generation by using semantic-aware permutation to improve sparse attention, leading to significant speedups with maintained quality.",
        "tldr_zh": "该论文介绍了一种名为Sparse VideoGen2 (SVG2)的免训练框架，通过使用语义感知排列来改进稀疏注意力，从而加速视频生成，并在保持质量的同时显著提高速度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "How to build a consistency model: Learning flow maps via self-distillation",
        "summary": "Building on the framework proposed in Boffi et al. (2024), we present a\nsystematic approach for learning flow maps associated with flow and diffusion\nmodels. Flow map-based models, commonly known as consistency models, encompass\nrecent efforts to improve the efficiency of generative models based on\nsolutions to differential equations. By exploiting a relationship between the\nvelocity field underlying a continuous-time flow and the instantaneous rate of\nchange of the flow map, we show how to convert existing distillation schemes\ninto direct training algorithms via self-distillation, eliminating the need for\npre-trained models. We empirically evaluate several instantiations of our\nframework, finding that high-dimensional tasks like image synthesis benefit\nfrom objective functions that avoid temporal and spatial derivatives of the\nflow map, while lower-dimensional tasks can benefit from objectives\nincorporating higher-order derivatives to capture sharp features.",
        "url": "http://arxiv.org/abs/2505.18825v1",
        "published_date": "2025-05-24T18:50:50+00:00",
        "updated_date": "2025-05-24T18:50:50+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Nicholas M. Boffi",
            "Michael S. Albergo",
            "Eric Vanden-Eijnden"
        ],
        "tldr": "This paper introduces a self-distillation approach for training consistency models (flow map-based generative models) without pre-trained models, showing improved efficiency in image synthesis and other generative tasks.",
        "tldr_zh": "本文提出了一种自蒸馏方法，用于训练一致性模型（基于流图的生成模型），无需预训练模型，从而提高了图像合成和其他生成任务的效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]