[
    {
        "title": "Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation",
        "summary": "Generative models have revolutionized Artificial Intelligence (AI),\nparticularly in multimodal applications. However, adapting these models to the\nmedical domain poses unique challenges due to the complexity of medical data\nand the stringent need for clinical accuracy. In this work, we introduce a\nframework specifically designed for multimodal medical data generation. By\nenabling the generation of multi-view chest X-rays and their associated\nclinical report, it bridges the gap between general-purpose vision-language\nmodels and the specialized requirements of healthcare. Leveraging the MIMIC-CXR\ndataset, the proposed framework shows superior performance in generating\nhigh-fidelity images and semantically coherent reports. Our quantitative\nevaluation reveals significant results in terms of FID and BLEU scores,\nshowcasing the quality of the generated data. Notably, our framework achieves\ncomparable or even superior performance compared to real data on downstream\ndisease classification tasks, underlining its potential as a tool for medical\nresearch and diagnostics. This study highlights the importance of\ndomain-specific adaptations in enhancing the relevance and utility of\ngenerative models for clinical applications, paving the way for future\nadvancements in synthetic multimodal medical data generation.",
        "url": "http://arxiv.org/abs/2505.01091v1",
        "published_date": "2025-05-02T08:07:24+00:00",
        "updated_date": "2025-05-02T08:07:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Daniele Molino",
            "Francesco di Feola",
            "Linlin Shen",
            "Paolo Soda",
            "Valerio Guarrasi"
        ],
        "tldr": "this paper introduces a vision-language model tailored for generating multi-view chest x-rays and corresponding clinical reports using the mimic-cxr dataset, demonstrating promising results in image fidelity and report coherence, even outperforming real data in disease classification.",
        "tldr_zh": "本文介绍了一种视觉-语言模型，专门用于生成多角度胸部x光片和相应的临床报告，该模型使用mimic-cxr数据集，在图像保真度和报告连贯性方面表现出良好的效果，甚至在疾病分类方面优于真实数据。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "GENMO: A GENeralist Model for Human MOtion",
        "summary": "Human motion modeling traditionally separates motion generation and\nestimation into distinct tasks with specialized models. Motion generation\nmodels focus on creating diverse, realistic motions from inputs like text,\naudio, or keyframes, while motion estimation models aim to reconstruct accurate\nmotion trajectories from observations like videos. Despite sharing underlying\nrepresentations of temporal dynamics and kinematics, this separation limits\nknowledge transfer between tasks and requires maintaining separate models. We\npresent GENMO, a unified Generalist Model for Human Motion that bridges motion\nestimation and generation in a single framework. Our key insight is to\nreformulate motion estimation as constrained motion generation, where the\noutput motion must precisely satisfy observed conditioning signals. Leveraging\nthe synergy between regression and diffusion, GENMO achieves accurate global\nmotion estimation while enabling diverse motion generation. We also introduce\nan estimation-guided training objective that exploits in-the-wild videos with\n2D annotations and text descriptions to enhance generative diversity.\nFurthermore, our novel architecture handles variable-length motions and mixed\nmultimodal conditions (text, audio, video) at different time intervals,\noffering flexible control. This unified approach creates synergistic benefits:\ngenerative priors improve estimated motions under challenging conditions like\nocclusions, while diverse video data enhances generation capabilities.\nExtensive experiments demonstrate GENMO's effectiveness as a generalist\nframework that successfully handles multiple human motion tasks within a single\nmodel.",
        "url": "http://arxiv.org/abs/2505.01425v1",
        "published_date": "2025-05-02T17:59:55+00:00",
        "updated_date": "2025-05-02T17:59:55+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Jiefeng Li",
            "Jinkun Cao",
            "Haotian Zhang",
            "Davis Rempe",
            "Jan Kautz",
            "Umar Iqbal",
            "Ye Yuan"
        ],
        "tldr": "the paper introduces genmo, a generalist model for human motion that unifies motion generation and estimation tasks within a single framework, achieving state-of-the-art results by reformulating motion estimation as constrained generation and using a novel architecture for handling multimodal conditions.",
        "tldr_zh": "该论文介绍了genmo，一个用于人体运动的通用模型，它将运动生成和估计任务统一在一个框架中，通过将运动估计重新表述为约束生成，并使用一种新颖的架构来处理多模态条件，从而实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TSTMotion: Training-free Scene-awarenText-to-motion Generation",
        "summary": "Text-to-motion generation has recently garnered significant research\ninterest, primarily focusing on generating human motion sequences in blank\nbackgrounds. However, human motions commonly occur within diverse 3D scenes,\nwhich has prompted exploration into scene-aware text-to-motion generation\nmethods. Yet, existing scene-aware methods often rely on large-scale\nground-truth motion sequences in diverse 3D scenes, which poses practical\nchallenges due to the expensive cost. To mitigate this challenge, we are the\nfirst to propose a \\textbf{T}raining-free \\textbf{S}cene-aware\n\\textbf{T}ext-to-\\textbf{Motion} framework, dubbed as \\textbf{TSTMotion}, that\nefficiently empowers pre-trained blank-background motion generators with the\nscene-aware capability. Specifically, conditioned on the given 3D scene and\ntext description, we adopt foundation models together to reason, predict and\nvalidate a scene-aware motion guidance. Then, the motion guidance is\nincorporated into the blank-background motion generators with two\nmodifications, resulting in scene-aware text-driven motion sequences. Extensive\nexperiments demonstrate the efficacy and generalizability of our proposed\nframework. We release our code in \\href{https://tstmotion.github.io/}{Project\nPage}.",
        "url": "http://arxiv.org/abs/2505.01182v1",
        "published_date": "2025-05-02T10:50:04+00:00",
        "updated_date": "2025-05-02T10:50:04+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ziyan Guo",
            "Haoxuan Qu",
            "Hossein Rahmani",
            "Dewen Soh",
            "Ping Hu",
            "Qiuhong Ke",
            "Jun Liu"
        ],
        "tldr": "the paper introduces tstmotion, a training-free framework for generating scene-aware human motions from text descriptions by leveraging foundation models to guide pre-trained blank-background motion generators.",
        "tldr_zh": "该论文提出了tstmotion，一个无需训练的框架，通过利用基础模型来引导预训练的空白背景运动生成器，从而从文本描述生成场景感知的人体运动。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis",
        "summary": "Long video generation involves generating extended videos using models\ntrained on short videos, suffering from distribution shifts due to varying\nframe counts. It necessitates the use of local information from the original\nshort frames to enhance visual and motion quality, and global information from\nthe entire long frames to ensure appearance consistency. Existing training-free\nmethods struggle to effectively integrate the benefits of both, as appearance\nand motion in videos are closely coupled, leading to motion inconsistency and\nvisual quality. In this paper, we reveal that global and local information can\nbe precisely decoupled into consistent appearance and motion intensity\ninformation by applying Principal Component Analysis (PCA), allowing for\nrefined complementary integration of global consistency and local quality. With\nthis insight, we propose FreePCA, a training-free long video generation\nparadigm based on PCA that simultaneously achieves high consistency and\nquality. Concretely, we decouple consistent appearance and motion intensity\nfeatures by measuring cosine similarity in the principal component space.\nCritically, we progressively integrate these features to preserve original\nquality and ensure smooth transitions, while further enhancing consistency by\nreusing the mean statistics of the initial noise. Experiments demonstrate that\nFreePCA can be applied to various video diffusion models without requiring\ntraining, leading to substantial improvements. Code is available at\nhttps://github.com/JosephTiTan/FreePCA.",
        "url": "http://arxiv.org/abs/2505.01172v1",
        "published_date": "2025-05-02T10:27:58+00:00",
        "updated_date": "2025-05-02T10:27:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiangtong Tan",
            "Hu Yu",
            "Jie Huang",
            "Jie Xiao",
            "Feng Zhao"
        ],
        "tldr": "freepca addresses the distribution shift problem in training-free long video generation by using principal component analysis (pca) to decouple and integrate appearance and motion information, leading to improved consistency and quality without retraining.",
        "tldr_zh": "freepca通过主成分分析(pca)解耦并整合外观和运动信息，解决了免训练长视频生成中的分布偏移问题，从而在无需重新训练的情况下提高了视频的连贯性和质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VSC: Visual Search Compositional Text-to-Image Diffusion Model",
        "summary": "Text-to-image diffusion models have shown impressive capabilities in\ngenerating realistic visuals from natural-language prompts, yet they often\nstruggle with accurately binding attributes to corresponding objects,\nespecially in prompts containing multiple attribute-object pairs. This\nchallenge primarily arises from the limitations of commonly used text encoders,\nsuch as CLIP, which can fail to encode complex linguistic relationships and\nmodifiers effectively. Existing approaches have attempted to mitigate these\nissues through attention map control during inference and the use of layout\ninformation or fine-tuning during training, yet they face performance drops\nwith increased prompt complexity. In this work, we introduce a novel\ncompositional generation method that leverages pairwise image embeddings to\nimprove attribute-object binding. Our approach decomposes complex prompts into\nsub-prompts, generates corresponding images, and computes visual prototypes\nthat fuse with text embeddings to enhance representation. By applying\nsegmentation-based localization training, we address cross-attention\nmisalignment, achieving improved accuracy in binding multiple attributes to\nobjects. Our approaches outperform existing compositional text-to-image\ndiffusion models on the benchmark T2I CompBench, achieving better image\nquality, evaluated by humans, and emerging robustness under scaling number of\nbinding pairs in the prompt.",
        "url": "http://arxiv.org/abs/2505.01104v1",
        "published_date": "2025-05-02T08:31:43+00:00",
        "updated_date": "2025-05-02T08:31:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Do Huu Dat",
            "Nam Hyeonu",
            "Po-Yuan Mao",
            "Tae-Hyun Oh"
        ],
        "tldr": "the paper introduces a new compositional text-to-image diffusion model (vsc) that improves attribute-object binding in complex prompts by decomposing prompts, generating visual prototypes, and using segmentation-based localization training. it outperforms existing models on t2i compbench.",
        "tldr_zh": "该论文提出了一种新的组合文本到图像扩散模型（vsc），通过分解提示、生成视觉原型和使用基于分割的定位训练，提高了复杂提示中属性-对象绑定的准确性。 该模型在t2i compbench上优于现有模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generating Animated Layouts as Structured Text Representations",
        "summary": "Despite the remarkable progress in text-to-video models, achieving precise\ncontrol over text elements and animated graphics remains a significant\nchallenge, especially in applications such as video advertisements. To address\nthis limitation, we introduce Animated Layout Generation, a novel approach to\nextend static graphic layouts with temporal dynamics. We propose a Structured\nText Representation for fine-grained video control through hierarchical visual\nelements. To demonstrate the effectiveness of our approach, we present VAKER\n(Video Ad maKER), a text-to-video advertisement generation pipeline that\ncombines a three-stage generation process with Unstructured Text Reasoning for\nseamless integration with LLMs. VAKER fully automates video advertisement\ngeneration by incorporating dynamic layout trajectories for objects and\ngraphics across specific video frames. Through extensive evaluations, we\ndemonstrate that VAKER significantly outperforms existing methods in generating\nvideo advertisements. Project Page:\nhttps://yeonsangshin.github.io/projects/Vaker",
        "url": "http://arxiv.org/abs/2505.00975v1",
        "published_date": "2025-05-02T03:37:09+00:00",
        "updated_date": "2025-05-02T03:37:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yeonsang Shin",
            "Jihwan Kim",
            "Yumin Song",
            "Kyungseung Lee",
            "Hyunhee Chung",
            "Taeyoung Na"
        ],
        "tldr": "the paper introduces animated layout generation and vaker, a text-to-video advertisement generation pipeline, which uses structured text representations to control dynamic layouts and outperforms existing methods.",
        "tldr_zh": "该论文介绍了动画布局生成和 vaker (一个文本到视频广告生成流程)，它使用结构化的文本表示来控制动态布局，并且超越了现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improving Editability in Image Generation with Layer-wise Memory",
        "summary": "Most real-world image editing tasks require multiple sequential edits to\nachieve desired results. Current editing approaches, primarily designed for\nsingle-object modifications, struggle with sequential editing: especially with\nmaintaining previous edits along with adapting new objects naturally into the\nexisting content. These limitations significantly hinder complex editing\nscenarios where multiple objects need to be modified while preserving their\ncontextual relationships. We address this fundamental challenge through two key\nproposals: enabling rough mask inputs that preserve existing content while\nnaturally integrating new elements and supporting consistent editing across\nmultiple modifications. Our framework achieves this through layer-wise memory,\nwhich stores latent representations and prompt embeddings from previous edits.\nWe propose Background Consistency Guidance that leverages memorized latents to\nmaintain scene coherence and Multi-Query Disentanglement in cross-attention\nthat ensures natural adaptation to existing content. To evaluate our method, we\npresent a new benchmark dataset incorporating semantic alignment metrics and\ninteractive editing scenarios. Through comprehensive experiments, we\ndemonstrate superior performance in iterative image editing tasks with minimal\nuser effort, requiring only rough masks while maintaining high-quality results\nthroughout multiple editing steps.",
        "url": "http://arxiv.org/abs/2505.01079v1",
        "published_date": "2025-05-02T07:36:49+00:00",
        "updated_date": "2025-05-02T07:36:49+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Daneul Kim",
            "Jaeah Lee",
            "Jaesik Park"
        ],
        "tldr": "this paper proposes a layer-wise memory framework with background consistency guidance and multi-query disentanglement for improving the editability of image generation, especially in sequential editing tasks with multiple objects.",
        "tldr_zh": "本文提出了一种分层记忆框架，包含背景一致性引导和多查询解耦，以提高图像生成的可编辑性，尤其是在涉及多个对象的顺序编辑任务中。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models",
        "summary": "Humans naturally understand 3D spatial relationships, enabling complex\nreasoning like predicting collisions of vehicles from different directions.\nCurrent large multimodal models (LMMs), however, lack of this capability of 3D\nspatial reasoning. This limitation stems from the scarcity of 3D training data\nand the bias in current model designs toward 2D data. In this paper, we\nsystematically study the impact of 3D-informed data, architecture, and training\nsetups, introducing SpatialLLM, a large multi-modal model with advanced 3D\nspatial reasoning abilities. To address data limitations, we develop two types\nof 3D-informed training datasets: (1) 3D-informed probing data focused on\nobject's 3D location and orientation, and (2) 3D-informed conversation data for\ncomplex spatial relationships. Notably, we are the first to curate VQA data\nthat incorporate 3D orientation relationships on real images. Furthermore, we\nsystematically integrate these two types of training data with the\narchitectural and training designs of LMMs, providing a roadmap for optimal\ndesign aimed at achieving superior 3D reasoning capabilities. Our SpatialLLM\nadvances machines toward highly capable 3D-informed reasoning, surpassing\nGPT-4o performance by 8.7%. Our systematic empirical design and the resulting\nfindings offer valuable insights for future research in this direction.",
        "url": "http://arxiv.org/abs/2505.00788v1",
        "published_date": "2025-05-01T18:36:17+00:00",
        "updated_date": "2025-05-01T18:36:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wufei Ma",
            "Luoxin Ye",
            "Nessa McWeeney",
            "Celso M de Melo",
            "Alan Yuille",
            "Jieneng Chen"
        ],
        "tldr": "the paper introduces spatialllm, a large multimodal model designed to enhance 3d spatial reasoning by using 3d-informed datasets and architectural/training designs, outperforming gpt-4o in certain benchmarks.",
        "tldr_zh": "该论文介绍了spatialllm，一个大型多模态模型，通过使用3d信息数据集和架构/训练设计来增强3d空间推理能力，在特定基准测试中优于gpt-4o。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Evaluating Vision Language Model Adaptations for Radiology Report Generation in Low-Resource Languages",
        "summary": "The integration of artificial intelligence in healthcare has opened new\nhorizons for improving medical diagnostics and patient care. However,\nchallenges persist in developing systems capable of generating accurate and\ncontextually relevant radiology reports, particularly in low-resource\nlanguages. In this study, we present a comprehensive benchmark to evaluate the\nperformance of instruction-tuned Vision-Language Models (VLMs) in the\nspecialized task of radiology report generation across three low-resource\nlanguages: Italian, German, and Spanish. Employing the LLaVA architectural\nframework, we conducted a systematic evaluation of pre-trained models utilizing\ngeneral datasets, domain-specific datasets, and low-resource language-specific\ndatasets. In light of the unavailability of models that possess prior knowledge\nof both the medical domain and low-resource languages, we analyzed various\nadaptations to determine the most effective approach for these contexts. The\nresults revealed that language-specific models substantially outperformed both\ngeneral and domain-specific models in generating radiology reports, emphasizing\nthe critical role of linguistic adaptation. Additionally, models fine-tuned\nwith medical terminology exhibited enhanced performance across all languages\ncompared to models with generic knowledge, highlighting the importance of\ndomain-specific training. We also explored the influence of the temperature\nparameter on the coherence of report generation, providing insights for optimal\nmodel settings. Our findings highlight the importance of tailored language and\ndomain-specific training for improving the quality and accuracy of radiological\nreports in multilingual settings. This research not only advances our\nunderstanding of VLMs adaptability in healthcare but also points to significant\navenues for future investigations into model tuning and language-specific\nadaptations.",
        "url": "http://arxiv.org/abs/2505.01096v1",
        "published_date": "2025-05-02T08:14:03+00:00",
        "updated_date": "2025-05-02T08:14:03+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Marco Salmè",
            "Rosa Sicilia",
            "Paolo Soda",
            "Valerio Guarrasi"
        ],
        "tldr": "this paper benchmarks instruction-tuned vision-language models (vlms) using the llava framework for radiology report generation in low-resource languages, finding that language-specific and domain-specific fine-tuning significantly improves performance.",
        "tldr_zh": "本文使用 llava 框架，针对低资源语言的放射学报告生成，对指令微调的视觉-语言模型 (vlm) 进行了基准测试，发现特定语言和特定领域的微调显着提高了性能。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer",
        "summary": "This study proposes a novel approach combining Multimodal Deep Learning with\nintrinsic eXplainable Artificial Intelligence techniques to predict\npathological response in non-small cell lung cancer patients undergoing\nneoadjuvant therapy. Due to the limitations of existing radiomics and unimodal\ndeep learning approaches, we introduce an intermediate fusion strategy that\nintegrates imaging and clinical data, enabling efficient interaction between\ndata modalities. The proposed Multimodal Doctor-in-the-Loop method further\nenhances clinical relevance by embedding clinicians' domain knowledge directly\ninto the training process, guiding the model's focus gradually from broader\nlung regions to specific lesions. Results demonstrate improved predictive\naccuracy and explainability, providing insights into optimal data integration\nstrategies for clinical applications.",
        "url": "http://arxiv.org/abs/2505.01390v1",
        "published_date": "2025-05-02T16:57:37+00:00",
        "updated_date": "2025-05-02T16:57:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Alice Natalina Caragliano",
            "Claudia Tacconi",
            "Carlo Greco",
            "Lorenzo Nibid",
            "Edy Ippolito",
            "Michele Fiore",
            "Giuseppe Perrone",
            "Sara Ramella",
            "Paolo Soda",
            "Valerio Guarrasi"
        ],
        "tldr": "this paper presents a multimodal deep learning framework with doctor-in-the-loop explainability for predicting pathological response in non-small cell lung cancer, integrating imaging and clinical data.",
        "tldr_zh": "本文提出了一种多模态深度学习框架，通过医生参与的解释性方法来预测非小细胞肺癌的病理反应，集成了影像和临床数据。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 4
    },
    {
        "title": "Edge Detection based on Channel Attention and Inter-region Independence Test",
        "summary": "Existing edge detection methods often suffer from noise amplification and\nexcessive retention of non-salient details, limiting their applicability in\nhigh-precision industrial scenarios. To address these challenges, we propose\nCAM-EDIT, a novel framework that integrates Channel Attention Mechanism (CAM)\nand Edge Detection via Independence Testing (EDIT). The CAM module adaptively\nenhances discriminative edge features through multi-channel fusion, while the\nEDIT module employs region-wise statistical independence analysis (using\nFisher's exact test and chi-square test) to suppress uncorrelated\nnoise.Extensive experiments on BSDS500 and NYUDv2 datasets demonstrate\nstate-of-the-art performance. Among the nine comparison algorithms, the\nF-measure scores of CAM-EDIT are 0.635 and 0.460, representing improvements of\n19.2\\% to 26.5\\% over traditional methods (Canny, CannySR), and better than the\nlatest learning based methods (TIP2020, MSCNGP). Noise robustness evaluations\nfurther reveal a 2.2\\% PSNR improvement under Gaussian noise compared to\nbaseline methods. Qualitative results exhibit cleaner edge maps with reduced\nartifacts, demonstrating its potential for high-precision industrial\napplications.",
        "url": "http://arxiv.org/abs/2505.01040v1",
        "published_date": "2025-05-02T06:30:21+00:00",
        "updated_date": "2025-05-02T06:30:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ru-yu Yan",
            "Da-Qing Zhang"
        ],
        "tldr": "the paper introduces cam-edit, a novel edge detection framework using channel attention and inter-region independence testing, achieving state-of-the-art performance on bsds500 and nyudv2 datasets, particularly robust to noise and suited for high-precision industrial applications.",
        "tldr_zh": "该论文介绍了cam-edit，一种新颖的边缘检测框架，它使用通道注意力和区域间独立性测试，在bsds500和nyudv2数据集上实现了最先进的性能，尤其是在噪声方面表现出强大的鲁棒性，适用于高精度工业应用。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    }
]