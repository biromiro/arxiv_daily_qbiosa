[
    {
        "title": "ContentV: Efficient Training of Video Generation Models with Limited Compute",
        "summary": "Recent advances in video generation demand increasingly efficient training\nrecipes to mitigate escalating computational costs. In this report, we present\nContentV, an 8B-parameter text-to-video model that achieves state-of-the-art\nperformance (85.14 on VBench) after training on 256 x 64GB Neural Processing\nUnits (NPUs) for merely four weeks. ContentV generates diverse, high-quality\nvideos across multiple resolutions and durations from text prompts, enabled by\nthree key innovations: (1) A minimalist architecture that maximizes reuse of\npre-trained image generation models for video generation; (2) A systematic\nmulti-stage training strategy leveraging flow matching for enhanced efficiency;\nand (3) A cost-effective reinforcement learning with human feedback framework\nthat improves generation quality without requiring additional human\nannotations. All the code and models are available at:\nhttps://contentv.github.io.",
        "url": "http://arxiv.org/abs/2506.05343v1",
        "published_date": "2025-06-05T17:59:54+00:00",
        "updated_date": "2025-06-05T17:59:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenfeng Lin",
            "Renjie Chen",
            "Boyuan Liu",
            "Shiyue Yan",
            "Ruoyu Feng",
            "Jiangchuan Wei",
            "Yichen Zhang",
            "Yimeng Zhou",
            "Chao Feng",
            "Jiao Ran",
            "Qi Wu",
            "Zuotao Liu",
            "Mingyu Guo"
        ],
        "tldr": "ContentV is an 8B-parameter text-to-video model achieving state-of-the-art performance on VBench through architecture reuse, multi-stage training with flow matching, and cost-effective RLHF, trained efficiently on NPUs.",
        "tldr_zh": "ContentV是一个80亿参数的文本到视频模型，通过架构重用、结合流匹配的多阶段训练和经济高效的RLHF，在VBench上实现了最先进的性能，并在NPU上进行了高效训练。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Contrastive Flow Matching",
        "summary": "Unconditional flow-matching trains diffusion models to transport samples from\na source distribution to a target distribution by enforcing that the flows\nbetween sample pairs are unique. However, in conditional settings (e.g.,\nclass-conditioned models), this uniqueness is no longer guaranteed--flows from\ndifferent conditions may overlap, leading to more ambiguous generations. We\nintroduce Contrastive Flow Matching, an extension to the flow matching\nobjective that explicitly enforces uniqueness across all conditional flows,\nenhancing condition separation. Our approach adds a contrastive objective that\nmaximizes dissimilarities between predicted flows from arbitrary sample pairs.\nWe validate Contrastive Flow Matching by conducting extensive experiments\nacross varying model architectures on both class-conditioned (ImageNet-1k) and\ntext-to-image (CC3M) benchmarks. Notably, we find that training models with\nContrastive Flow Matching (1) improves training speed by a factor of up to 9x,\n(2) requires up to 5x fewer de-noising steps and (3) lowers FID by up to 8.9\ncompared to training the same models with flow matching. We release our code\nat: https://github.com/gstoica27/DeltaFM.git.",
        "url": "http://arxiv.org/abs/2506.05350v1",
        "published_date": "2025-06-05T17:59:58+00:00",
        "updated_date": "2025-06-05T17:59:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "George Stoica",
            "Vivek Ramanujan",
            "Xiang Fan",
            "Ali Farhadi",
            "Ranjay Krishna",
            "Judy Hoffman"
        ],
        "tldr": "The paper introduces Contrastive Flow Matching, an improved flow matching objective for conditional diffusion models that enforces uniqueness across conditional flows, leading to faster training, fewer denoising steps, and improved FID scores.",
        "tldr_zh": "该论文介绍了对比流匹配（Contrastive Flow Matching），这是一种改进的条件扩散模型流匹配目标，它强制执行跨条件流的唯一性，从而加快训练速度，减少去噪步骤并提高FID分数。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AliTok: Towards Sequence Modeling Alignment between Tokenizer and Autoregressive Model",
        "summary": "Autoregressive image generation aims to predict the next token based on\nprevious ones. However, existing image tokenizers encode tokens with\nbidirectional dependencies during the compression process, which hinders the\neffective modeling by autoregressive models. In this paper, we propose a novel\nAligned Tokenizer (AliTok), which utilizes a causal decoder to establish\nunidirectional dependencies among encoded tokens, thereby aligning the token\nmodeling approach between the tokenizer and autoregressive model. Furthermore,\nby incorporating prefix tokens and employing two-stage tokenizer training to\nenhance reconstruction consistency, AliTok achieves great reconstruction\nperformance while being generation-friendly. On ImageNet-256 benchmark, using a\nstandard decoder-only autoregressive model as the generator with only 177M\nparameters, AliTok achieves a gFID score of 1.50 and an IS of 305.9. When the\nparameter count is increased to 662M, AliTok achieves a gFID score of 1.35,\nsurpassing the state-of-the-art diffusion method with 10x faster sampling\nspeed. The code and weights are available at\nhttps://github.com/ali-vilab/alitok.",
        "url": "http://arxiv.org/abs/2506.05289v1",
        "published_date": "2025-06-05T17:45:10+00:00",
        "updated_date": "2025-06-05T17:45:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pingyu Wu",
            "Kai Zhu",
            "Yu Liu",
            "Longxiang Tang",
            "Jian Yang",
            "Yansong Peng",
            "Wei Zhai",
            "Yang Cao",
            "Zheng-Jun Zha"
        ],
        "tldr": "The paper introduces AliTok, a novel aligned tokenizer for autoregressive image generation that establishes unidirectional dependencies among tokens, achieving state-of-the-art gFID scores with faster sampling speeds compared to diffusion methods.",
        "tldr_zh": "该论文介绍了一种名为AliTok的新型对齐分词器，用于自回归图像生成，它在token之间建立单向依赖关系，与扩散方法相比，实现了最先进的gFID分数和更快的采样速度。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Video World Models with Long-term Spatial Memory",
        "summary": "Emerging world models autoregressively generate video frames in response to\nactions, such as camera movements and text prompts, among other control\nsignals. Due to limited temporal context window sizes, these models often\nstruggle to maintain scene consistency during revisits, leading to severe\nforgetting of previously generated environments. Inspired by the mechanisms of\nhuman memory, we introduce a novel framework to enhancing long-term consistency\nof video world models through a geometry-grounded long-term spatial memory. Our\nframework includes mechanisms to store and retrieve information from the\nlong-term spatial memory and we curate custom datasets to train and evaluate\nworld models with explicitly stored 3D memory mechanisms. Our evaluations show\nimproved quality, consistency, and context length compared to relevant\nbaselines, paving the way towards long-term consistent world generation.",
        "url": "http://arxiv.org/abs/2506.05284v1",
        "published_date": "2025-06-05T17:42:34+00:00",
        "updated_date": "2025-06-05T17:42:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tong Wu",
            "Shuai Yang",
            "Ryan Po",
            "Yinghao Xu",
            "Ziwei Liu",
            "Dahua Lin",
            "Gordon Wetzstein"
        ],
        "tldr": "This paper introduces a novel framework for video world models that uses a geometry-grounded long-term spatial memory to improve long-term consistency and reduce forgetting during revisits, leading to improved generation quality and context length.",
        "tldr_zh": "该论文介绍了一种新的视频世界模型框架，它使用基于几何的长期空间记忆来提高长期一致性，减少重访期间的遗忘，从而提高生成质量和上下文长度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Aligning Latent Spaces with Flow Priors",
        "summary": "This paper presents a novel framework for aligning learnable latent spaces to\narbitrary target distributions by leveraging flow-based generative models as\npriors. Our method first pretrains a flow model on the target features to\ncapture the underlying distribution. This fixed flow model subsequently\nregularizes the latent space via an alignment loss, which reformulates the flow\nmatching objective to treat the latents as optimization targets. We formally\nprove that minimizing this alignment loss establishes a computationally\ntractable surrogate objective for maximizing a variational lower bound on the\nlog-likelihood of latents under the target distribution. Notably, the proposed\nmethod eliminates computationally expensive likelihood evaluations and avoids\nODE solving during optimization. As a proof of concept, we demonstrate in a\ncontrolled setting that the alignment loss landscape closely approximates the\nnegative log-likelihood of the target distribution. We further validate the\neffectiveness of our approach through large-scale image generation experiments\non ImageNet with diverse target distributions, accompanied by detailed\ndiscussions and ablation studies. With both theoretical and empirical\nvalidation, our framework paves a new way for latent space alignment.",
        "url": "http://arxiv.org/abs/2506.05240v1",
        "published_date": "2025-06-05T16:59:53+00:00",
        "updated_date": "2025-06-05T16:59:53+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Yizhuo Li",
            "Yuying Ge",
            "Yixiao Ge",
            "Ying Shan",
            "Ping Luo"
        ],
        "tldr": "The paper introduces a novel method for aligning latent spaces to target distributions using flow-based generative models as priors, optimizing an alignment loss without likelihood evaluations or ODE solving. They validate their approach with image generation on ImageNet.",
        "tldr_zh": "本文提出了一种新颖的方法，通过利用基于流的生成模型作为先验，将潜在空间与目标分布对齐。该方法优化一个alignment loss，无需进行可能性评估或解ODE。通过 ImageNet 上的图像生成实验验证了该方法的有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "HMAR: Efficient Hierarchical Masked Auto-Regressive Image Generation",
        "summary": "Visual Auto-Regressive modeling (VAR) has shown promise in bridging the speed\nand quality gap between autoregressive image models and diffusion models. VAR\nreformulates autoregressive modeling by decomposing an image into successive\nresolution scales. During inference, an image is generated by predicting all\nthe tokens in the next (higher-resolution) scale, conditioned on all tokens in\nall previous (lower-resolution) scales. However, this formulation suffers from\nreduced image quality due to the parallel generation of all tokens in a\nresolution scale; has sequence lengths scaling superlinearly in image\nresolution; and requires retraining to change the sampling schedule.\n  We introduce Hierarchical Masked Auto-Regressive modeling (HMAR), a new image\ngeneration algorithm that alleviates these issues using next-scale prediction\nand masked prediction to generate high-quality images with fast sampling. HMAR\nreformulates next-scale prediction as a Markovian process, wherein the\nprediction of each resolution scale is conditioned only on tokens in its\nimmediate predecessor instead of the tokens in all predecessor resolutions.\nWhen predicting a resolution scale, HMAR uses a controllable multi-step masked\ngeneration procedure to generate a subset of the tokens in each step. On\nImageNet 256x256 and 512x512 benchmarks, HMAR models match or outperform\nparameter-matched VAR, diffusion, and autoregressive baselines. We develop\nefficient IO-aware block-sparse attention kernels that allow HMAR to achieve\nfaster training and inference times over VAR by over 2.5x and 1.75x\nrespectively, as well as over 3x lower inference memory footprint. Finally,\nHMAR yields additional flexibility over VAR; its sampling schedule can be\nchanged without further training, and it can be applied to image editing tasks\nin a zero-shot manner.",
        "url": "http://arxiv.org/abs/2506.04421v1",
        "published_date": "2025-06-04T20:08:07+00:00",
        "updated_date": "2025-06-04T20:08:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Hermann Kumbong",
            "Xian Liu",
            "Tsung-Yi Lin",
            "Ming-Yu Liu",
            "Xihui Liu",
            "Ziwei Liu",
            "Daniel Y. Fu",
            "Christopher Ré",
            "David W. Romero"
        ],
        "tldr": "The paper introduces HMAR, a novel hierarchical masked auto-regressive image generation algorithm that improves upon VAR models by achieving faster training and inference, lower memory footprint, and increased flexibility without sacrificing image quality. HMAR uses a next-scale prediction Markovian process and a controllable multi-step masked generation procedure.",
        "tldr_zh": "该论文介绍了一种名为HMAR的新型分层掩码自回归图像生成算法，通过实现更快的训练和推理速度、更低的内存占用和更高的灵活性，同时不牺牲图像质量，改进了VAR模型。 HMAR使用下一尺度预测马尔可夫过程和可控的多步掩码生成过程。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]