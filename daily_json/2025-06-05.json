[
    {
        "title": "FullDiT2: Efficient In-Context Conditioning for Video Diffusion Transformers",
        "summary": "Fine-grained and efficient controllability on video diffusion transformers\nhas raised increasing desires for the applicability. Recently, In-context\nConditioning emerged as a powerful paradigm for unified conditional video\ngeneration, which enables diverse controls by concatenating varying context\nconditioning signals with noisy video latents into a long unified token\nsequence and jointly processing them via full-attention, e.g., FullDiT. Despite\ntheir effectiveness, these methods face quadratic computation overhead as task\ncomplexity increases, hindering practical deployment. In this paper, we study\nthe efficiency bottleneck neglected in original in-context conditioning video\ngeneration framework. We begin with systematic analysis to identify two key\nsources of the computation inefficiencies: the inherent redundancy within\ncontext condition tokens and the computational redundancy in context-latent\ninteractions throughout the diffusion process. Based on these insights, we\npropose FullDiT2, an efficient in-context conditioning framework for general\ncontrollability in both video generation and editing tasks, which innovates\nfrom two key perspectives. Firstly, to address the token redundancy, FullDiT2\nleverages a dynamic token selection mechanism to adaptively identify important\ncontext tokens, reducing the sequence length for unified full-attention.\nAdditionally, a selective context caching mechanism is devised to minimize\nredundant interactions between condition tokens and video latents. Extensive\nexperiments on six diverse conditional video editing and generation tasks\ndemonstrate that FullDiT2 achieves significant computation reduction and 2-3\ntimes speedup in averaged time cost per diffusion step, with minimal\ndegradation or even higher performance in video generation quality. The project\npage is at \\href{https://fulldit2.github.io/}{https://fulldit2.github.io/}.",
        "url": "http://arxiv.org/abs/2506.04213v1",
        "published_date": "2025-06-04T17:57:09+00:00",
        "updated_date": "2025-06-04T17:57:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuanhua He",
            "Quande Liu",
            "Zixuan Ye",
            "Wecai Ye",
            "Qiulin Wang",
            "Xintao Wang",
            "Qifeng Chen",
            "Pengfei Wan",
            "Di Zhang",
            "Kun Gai"
        ],
        "tldr": "The paper introduces FullDiT2, an efficient in-context conditioning framework for video diffusion transformers, which reduces computation overhead by addressing token and interaction redundancies, achieving significant speedups in video generation and editing tasks.",
        "tldr_zh": "本文介绍了FullDiT2，一种用于视频扩散transformer的高效上下文条件框架，通过解决token和交互冗余来降低计算开销，从而在视频生成和编辑任务中实现了显著的加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ControlThinker: Unveiling Latent Semantics for Controllable Image Generation through Visual Reasoning",
        "summary": "The field of controllable image generation has seen significant advancements,\nwith various architectures improving generation layout consistency with control\nsignals. However, contemporary methods still face challenges in bridging the\nsemantic gap between input text prompts with sparse semantics and the target\nimages, often over-relying on low-level control signals to infer regional\ndetails. To address this challenge, we propose ControlThinker, a novel\nframework that employs a \"comprehend-then-generate\" paradigm. Firstly, by\nincentivizing the visual reasoning capability of a MLLM, latent semantics from\ncontrol images are mined to enrich text prompts. This enriched semantic\nunderstanding then seamlessly aids in image generation without the need for\nadditional complex modifications. To further tackle the uncertainty arising\nfrom the ambiguity of control images, we encourage broader exploration of\nreasoning trajectories and select the optimal one using a metric-based output\nreward model (ORM). Extensive experimental results demonstrate that\nControlThinker effectively mitigates the semantic gap between raw text prompts\nand target images, resulting in improved visual quality and semantic\nconsistency across a wide range of benchmarks. The code and models are\navailable at https://github.com/Maplebb/ControlThinker.",
        "url": "http://arxiv.org/abs/2506.03596v1",
        "published_date": "2025-06-04T05:56:19+00:00",
        "updated_date": "2025-06-04T05:56:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Feng Han",
            "Yang Jiao",
            "Shaoxiang Chen",
            "Junhao Xu",
            "Jingjing Chen",
            "Yu-Gang Jiang"
        ],
        "tldr": "ControlThinker enhances controllable image generation by using a MLLM to extract latent semantics from control images and enrich text prompts, thereby improving visual quality and semantic consistency.",
        "tldr_zh": "ControlThinker 通过使用多模态大语言模型 (MLLM) 从控制图像中提取潜在语义并丰富文本提示，从而增强可控图像生成，进而提高视觉质量和语义一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Resolving Task Objective Conflicts in Unified Multimodal Understanding and Generation via Task-Aware Mixture-of-Experts",
        "summary": "Unified multimodal large language models (MLLMs) based on end-to-end\nautoregressive (AR) transformers effectively integrate both understanding and\ngeneration tasks within a single framework. However, intrinsic Task Objective\nConflicts between high-level semantic abstraction in understanding and\nfine-grained detail preservation in generation pose significant challenges,\noften leading to suboptimal trade-offs and task interference. Existing\nsolutions, such as decoupling shared visual encoders, fall short of\nfundamentally resolving these conflicts due to inherent AR architecture. In\nthis paper, we propose a novel approach that decouples internal components of\nAR to resolve task objective conflicts. Specifically, we design UTAMoE, a\nUnified Task-Aware Mixture-of-Experts (MoE) framework that decouples internal\nAR modules via a Task-Aware MoE Layer to create task-specific optimization\nsubpaths. To enhance task differentiation while maintaining overall\ncoordination, we introduce a novel Two-Stage Training Strategy. Extensive\nexperiments on multimodal benchmarks demonstrate that UTAMoE mitigates task\nobjective conflicts, achieving state-of-the-art performance across various\ntasks. Visualizations and ablation studies further validate the effectiveness\nof our approach.",
        "url": "http://arxiv.org/abs/2506.03591v1",
        "published_date": "2025-06-04T05:44:21+00:00",
        "updated_date": "2025-06-04T05:44:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaxing Zhang",
            "Xinyi Zeng",
            "Hao Tang"
        ],
        "tldr": "This paper introduces UTAMoE, a Task-Aware Mixture-of-Experts framework, to resolve task objective conflicts in unified multimodal understanding and generation within autoregressive models, achieving state-of-the-art performance.",
        "tldr_zh": "本文提出了一种任务感知的混合专家模型UTAMoE，旨在解决自回归模型中统一多模态理解和生成任务之间的目标冲突，并取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]