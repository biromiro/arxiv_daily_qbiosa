[
    {
        "title": "Transition Models: Rethinking the Generative Learning Objective",
        "summary": "A fundamental dilemma in generative modeling persists: iterative diffusion\nmodels achieve outstanding fidelity, but at a significant computational cost,\nwhile efficient few-step alternatives are constrained by a hard quality\nceiling. This conflict between generation steps and output quality arises from\nrestrictive training objectives that focus exclusively on either infinitesimal\ndynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by\nintroducing an exact, continuous-time dynamics equation that analytically\ndefines state transitions across any finite time interval. This leads to a\nnovel generative paradigm, Transition Models (TiM), which adapt to\narbitrary-step transitions, seamlessly traversing the generative trajectory\nfrom single leaps to fine-grained refinement with more steps. Despite having\nonly 865M parameters, TiM achieves state-of-the-art performance, surpassing\nleading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across\nall evaluated step counts. Importantly, unlike previous few-step generators,\nTiM demonstrates monotonic quality improvement as the sampling budget\nincreases. Additionally, when employing our native-resolution strategy, TiM\ndelivers exceptional fidelity at resolutions up to 4096x4096.",
        "url": "http://arxiv.org/abs/2509.04394v1",
        "published_date": "2025-09-04T17:05:59+00:00",
        "updated_date": "2025-09-04T17:05:59+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Zidong Wang",
            "Yiyuan Zhang",
            "Xiaoyu Yue",
            "Xiangyu Yue",
            "Yangguang Li",
            "Wanli Ouyang",
            "Lei Bai"
        ],
        "tldr": "The paper introduces Transition Models (TiM), a novel generative model that addresses the trade-off between fidelity and computational cost in generative modeling by using an analytically defined continuous-time dynamics equation, outperforming existing models with fewer parameters.",
        "tldr_zh": "该论文介绍了Transition Models (TiM)，一种新型生成模型，通过使用解析定义的连续时间动力学方程解决生成建模中保真度和计算成本之间的权衡，以更少的参数优于现有模型。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "A Generative Foundation Model for Chest Radiography",
        "summary": "The scarcity of well-annotated diverse medical images is a major hurdle for\ndeveloping reliable AI models in healthcare. Substantial technical advances\nhave been made in generative foundation models for natural images. Here we\ndevelop `ChexGen', a generative vision-language foundation model that\nintroduces a unified framework for text-, mask-, and bounding box-guided\nsynthesis of chest radiographs. Built upon the latent diffusion transformer\narchitecture, ChexGen was pretrained on the largest curated chest X-ray dataset\nto date, consisting of 960,000 radiograph-report pairs. ChexGen achieves\naccurate synthesis of radiographs through expert evaluations and quantitative\nmetrics. We demonstrate the utility of ChexGen for training data augmentation\nand supervised pretraining, which led to performance improvements across\ndisease classification, detection, and segmentation tasks using a small\nfraction of training data. Further, our model enables the creation of diverse\npatient cohorts that enhance model fairness by detecting and mitigating\ndemographic biases. Our study supports the transformative role of generative\nfoundation models in building more accurate, data-efficient, and equitable\nmedical AI systems.",
        "url": "http://arxiv.org/abs/2509.03903v1",
        "published_date": "2025-09-04T05:53:58+00:00",
        "updated_date": "2025-09-04T05:53:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanfeng Ji",
            "Dan Lin",
            "Xiyue Wang",
            "Lu Zhang",
            "Wenhui Zhou",
            "Chongjian Ge",
            "Ruihang Chu",
            "Xiaoli Yang",
            "Junhan Zhao",
            "Junsong Chen",
            "Xiangde Luo",
            "Sen Yang",
            "Jin Fang",
            "Ping Luo",
            "Ruijiang Li"
        ],
        "tldr": "The paper introduces ChexGen, a generative vision-language foundation model for chest radiographs, demonstrating its utility in data augmentation, supervised pretraining, and bias mitigation, leading to improved performance across several downstream tasks.",
        "tldr_zh": "该论文介绍了ChexGen，一个用于胸部X光片的生成式视觉-语言基础模型，展示了其在数据增强、有监督预训练和偏差缓解方面的效用，从而提高了多个下游任务的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image -- Technical Preview",
        "summary": "We introduce the Virtual Fitting Room (VFR), a novel video generative model\nthat produces arbitrarily long virtual try-on videos. Our VFR models long video\ngeneration tasks as an auto-regressive, segment-by-segment generation process,\neliminating the need for resource-intensive generation and lengthy video data,\nwhile providing the flexibility to generate videos of arbitrary length. The key\nchallenges of this task are twofold: ensuring local smoothness between adjacent\nsegments and maintaining global temporal consistency across different segments.\nTo address these challenges, we propose our VFR framework, which ensures\nsmoothness through a prefix video condition and enforces consistency with the\nanchor video -- a 360-degree video that comprehensively captures the human's\nwholebody appearance. Our VFR generates minute-scale virtual try-on videos with\nboth local smoothness and global temporal consistency under various motions,\nmaking it a pioneering work in long virtual try-on video generation.",
        "url": "http://arxiv.org/abs/2509.04450v1",
        "published_date": "2025-09-04T17:59:55+00:00",
        "updated_date": "2025-09-04T17:59:55+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Jun-Kun Chen",
            "Aayush Bansal",
            "Minh Phuoc Vo",
            "Yu-Xiong Wang"
        ],
        "tldr": "The paper introduces Virtual Fitting Room (VFR), a novel video generative model for creating arbitrarily long and consistent virtual try-on videos from a single image, addressing smoothness and consistency challenges through a segment-by-segment generation process conditioned on prefix and anchor videos.",
        "tldr_zh": "该论文介绍了虚拟试衣间（VFR），一种新颖的视频生成模型，用于从单张图像创建任意长度且一致的虚拟试穿视频。它通过基于前缀和锚定视频的分段生成过程，解决了视频的平滑性和一致性问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models",
        "summary": "Text-to-image diffusion models have demonstrated significant capabilities to\ngenerate diverse and detailed visuals in various domains, and story\nvisualization is emerging as a particularly promising application. However, as\ntheir use in real-world creative domains increases, the need for providing\nenhanced control, refinement, and the ability to modify images post-generation\nin a consistent manner becomes an important challenge. Existing methods often\nlack the flexibility to apply fine or coarse edits while maintaining visual and\nnarrative consistency across multiple frames, preventing creators from\nseamlessly crafting and refining their visual stories. To address these\nchallenges, we introduce Plot'n Polish, a zero-shot framework that enables\nconsistent story generation and provides fine-grained control over story\nvisualizations at various levels of detail.",
        "url": "http://arxiv.org/abs/2509.04446v1",
        "published_date": "2025-09-04T17:59:34+00:00",
        "updated_date": "2025-09-04T17:59:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kiymet Akdemir",
            "Jing Shi",
            "Kushal Kafle",
            "Brian Price",
            "Pinar Yanardag"
        ],
        "tldr": "The paper introduces Plot'n Polish, a zero-shot framework for consistent story visualization using text-to-image diffusion models, offering enhanced control and editing capabilities for creators to refine visual narratives.",
        "tldr_zh": "该论文介绍了 Plot'n Polish，一个零样本框架，使用文本到图像扩散模型实现一致的故事可视化，为创作者提供增强的控制和编辑能力，以改进视觉叙事。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network Weight Space Diffusion",
        "summary": "Creating human avatars is a highly desirable yet challenging task. Recent\nadvancements in radiance field rendering have achieved unprecedented\nphotorealism and real-time performance for personalized dynamic human avatars.\nHowever, these approaches are typically limited to person-specific rendering\nmodels trained on multi-view video data for a single individual, limiting their\nability to generalize across different identities. On the other hand,\ngenerative approaches leveraging prior knowledge from pre-trained 2D diffusion\nmodels can produce cartoonish, static human avatars, which are animated through\nsimple skeleton-based articulation. Therefore, the avatars generated by these\nmethods suffer from lower rendering quality compared to person-specific\nrendering methods and fail to capture pose-dependent deformations such as cloth\nwrinkles. In this paper, we propose a novel approach that unites the strengths\nof person-specific rendering and diffusion-based generative modeling to enable\ndynamic human avatar generation with both high photorealism and realistic\npose-dependent deformations. Our method follows a two-stage pipeline: first, we\noptimize a set of person-specific UNets, with each network representing a\ndynamic human avatar that captures intricate pose-dependent deformations. In\nthe second stage, we train a hyper diffusion model over the optimized network\nweights. During inference, our method generates network weights for real-time,\ncontrollable rendering of dynamic human avatars. Using a large-scale,\ncross-identity, multi-view video dataset, we demonstrate that our approach\noutperforms state-of-the-art human avatar generation methods.",
        "url": "http://arxiv.org/abs/2509.04145v1",
        "published_date": "2025-09-04T12:15:55+00:00",
        "updated_date": "2025-09-04T12:15:55+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Dongliang Cao",
            "Guoxing Sun",
            "Marc Habermann",
            "Florian Bernard"
        ],
        "tldr": "The paper introduces a novel method for generating dynamic human avatars with high photorealism and realistic pose-dependent deformations by combining person-specific rendering with a hyper diffusion model trained on network weights.",
        "tldr_zh": "该论文提出了一种新方法，通过结合个性化渲染和在网络权重上训练的超扩散模型，生成具有高度真实感和逼真姿势相关变形的动态人体化身。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image Generation",
        "summary": "Text-to-image diffusion models have achieved remarkable image quality, but\nthey still struggle with complex, multiele ment prompts, and limited stylistic\ndiversity. To address these limitations, we propose a Multi-Expert Planning and\nGen eration Framework (MEPG) that synergistically integrates position- and\nstyle-aware large language models (LLMs) with spatial-semantic expert modules.\nThe framework comprises two core components: (1) a Position-Style-Aware (PSA)\nmodule that utilizes a supervised fine-tuned LLM to decom pose input prompts\ninto precise spatial coordinates and style encoded semantic instructions; and\n(2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera\ntion through dynamic expert routing across both local regions and global areas.\nDuring the generation process for each lo cal region, specialized models (e.g.,\nrealism experts, styliza tion specialists) are selectively activated for each\nspatial par tition via attention-based gating mechanisms. The architec ture\nsupports lightweight integration and replacement of ex pert models, providing\nstrong extensibility. Additionally, an interactive interface enables real-time\nspatial layout editing and per-region style selection from a portfolio of\nexperts. Ex periments show that MEPG significantly outperforms base line models\nwith the same backbone in both image quality\n  and style diversity.",
        "url": "http://arxiv.org/abs/2509.04126v1",
        "published_date": "2025-09-04T11:44:28+00:00",
        "updated_date": "2025-09-04T11:44:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuan Zhao",
            "Liu Lin"
        ],
        "tldr": "The paper introduces MEPG, a framework that enhances text-to-image generation by using LLMs for prompt decomposition and a Multi-Expert Diffusion module for cross-region generation with dynamic expert routing, demonstrably improving image quality and style diversity.",
        "tldr_zh": "该论文介绍了一种名为MEPG的框架，通过使用LLM进行提示分解和一个多专家扩散模块进行跨区域生成，实现了增强文本到图像的生成，并显著提高了图像质量和风格多样性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Human Motion Video Generation: A Survey",
        "summary": "Human motion video generation has garnered significant research interest due\nto its broad applications, enabling innovations such as photorealistic singing\nheads or dynamic avatars that seamlessly dance to music. However, existing\nsurveys in this field focus on individual methods, lacking a comprehensive\noverview of the entire generative process. This paper addresses this gap by\nproviding an in-depth survey of human motion video generation, encompassing\nover ten sub-tasks, and detailing the five key phases of the generation\nprocess: input, motion planning, motion video generation, refinement, and\noutput. Notably, this is the first survey that discusses the potential of large\nlanguage models in enhancing human motion video generation. Our survey reviews\nthe latest developments and technological trends in human motion video\ngeneration across three primary modalities: vision, text, and audio. By\ncovering over two hundred papers, we offer a thorough overview of the field and\nhighlight milestone works that have driven significant technological\nbreakthroughs. Our goal for this survey is to unveil the prospects of human\nmotion video generation and serve as a valuable resource for advancing the\ncomprehensive applications of digital humans. A complete list of the models\nexamined in this survey is available in Our Repository\nhttps://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.",
        "url": "http://arxiv.org/abs/2509.03883v1",
        "published_date": "2025-09-04T04:39:21+00:00",
        "updated_date": "2025-09-04T04:39:21+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Haiwei Xue",
            "Xiangyang Luo",
            "Zhanghao Hu",
            "Xin Zhang",
            "Xunzhi Xiang",
            "Yuqin Dai",
            "Jianzhuang Liu",
            "Zhensong Zhang",
            "Minglei Li",
            "Jian Yang",
            "Fei Ma",
            "Zhiyong Wu",
            "Changpeng Yang",
            "Zonghong Dai",
            "Fei Richard Yu"
        ],
        "tldr": "This paper presents a comprehensive survey of human motion video generation, covering sub-tasks, key phases, modalities (vision, text, audio), and the potential of large language models in this domain, reviewing over 200 papers.",
        "tldr_zh": "本文全面调研了人体运动视频生成领域，涵盖子任务、关键阶段、模态（视觉、文本、音频），以及大型语言模型在该领域的潜力，并回顾了200多篇论文。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Fitting Image Diffusion Models on Video Datasets",
        "summary": "Image diffusion models are trained on independently sampled static images.\nWhile this is the bedrock task protocol in generative modeling, capturing the\ntemporal world through the lens of static snapshots is information-deficient by\ndesign. This limitation leads to slower convergence, limited distributional\ncoverage, and reduced generalization. In this work, we propose a simple and\neffective training strategy that leverages the temporal inductive bias present\nin continuous video frames to improve diffusion training. Notably, the proposed\nmethod requires no architectural modification and can be seamlessly integrated\ninto standard diffusion training pipelines. We evaluate our method on the\nHandCo dataset, where hand-object interactions exhibit dense temporal coherence\nand subtle variations in finger articulation often result in semantically\ndistinct motions. Empirically, our method accelerates convergence by over\n2$\\text{x}$ faster and achieves lower FID on both training and validation\ndistributions. It also improves generative diversity by encouraging the model\nto capture meaningful temporal variations. We further provide an optimization\nanalysis showing that our regularization reduces the gradient variance, which\ncontributes to faster convergence.",
        "url": "http://arxiv.org/abs/2509.03794v1",
        "published_date": "2025-09-04T01:04:54+00:00",
        "updated_date": "2025-09-04T01:04:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Juhun Lee",
            "Simon S. Woo"
        ],
        "tldr": "This paper proposes a training strategy that leverages temporal inductive bias in video frames to improve image diffusion model training, leading to faster convergence, better generalization, and improved generative diversity without architectural modifications.",
        "tldr_zh": "该论文提出了一种利用视频帧中的时间归纳偏置来改进图像扩散模型训练的训练策略，从而在不进行架构修改的情况下，实现了更快的收敛速度、更好的泛化能力和改进的生成多样性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Few-step Flow for 3D Generation via Marginal-Data Transport Distillation",
        "summary": "Flow-based 3D generation models typically require dozens of sampling steps\nduring inference. Though few-step distillation methods, particularly\nConsistency Models (CMs), have achieved substantial advancements in\naccelerating 2D diffusion models, they remain under-explored for more complex\n3D generation tasks. In this study, we propose a novel framework, MDT-dist, for\nfew-step 3D flow distillation. Our approach is built upon a primary objective:\ndistilling the pretrained model to learn the Marginal-Data Transport. Directly\nlearning this objective needs to integrate the velocity fields, while this\nintegral is intractable to be implemented. Therefore, we propose two\noptimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD),\nto equivalently convert the optimization target from the transport level to the\nvelocity and the distribution level respectively. Velocity Matching (VM) learns\nto stably match the velocity fields between the student and the teacher, but\ninevitably provides biased gradient estimates. Velocity Distillation (VD)\nfurther enhances the optimization process by leveraging the learned velocity\nfields to perform probability density distillation. When evaluated on the\npioneer 3D generation framework TRELLIS, our method reduces sampling steps of\neach flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s\n(2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high\nvisual and geometric fidelity. Extensive experiments demonstrate that our\nmethod significantly outperforms existing CM distillation methods, and enables\nTRELLIS to achieve superior performance in few-step 3D generation.",
        "url": "http://arxiv.org/abs/2509.04406v1",
        "published_date": "2025-09-04T17:24:31+00:00",
        "updated_date": "2025-09-04T17:24:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zanwei Zhou",
            "Taoran Yi",
            "Jiemin Fang",
            "Chen Yang",
            "Lingxi Xie",
            "Xinggang Wang",
            "Wei Shen",
            "Qi Tian"
        ],
        "tldr": "The paper introduces MDT-dist, a novel framework for few-step 3D flow distillation leveraging Velocity Matching and Velocity Distillation objectives, achieving significant speedups in 3D generation while maintaining fidelity.",
        "tldr_zh": "该论文介绍了一种名为MDT-dist的新框架，用于少步数的3D流蒸馏，利用速度匹配和速度蒸馏目标函数，在保持精度的同时，显著提升了3D生成的速度。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction",
        "summary": "In this paper, we propose OccTENS, a generative occupancy world model that\nenables controllable, high-fidelity long-term occupancy generation while\nmaintaining computational efficiency. Different from visual generation, the\noccupancy world model must capture the fine-grained 3D geometry and dynamic\nevolution of the 3D scenes, posing great challenges for the generative models.\nRecent approaches based on autoregression (AR) have demonstrated the potential\nto predict vehicle movement and future occupancy scenes simultaneously from\nhistorical observations, but they typically suffer from \\textbf{inefficiency},\n\\textbf{temporal degradation} in long-term generation and \\textbf{lack of\ncontrollability}. To holistically address these issues, we reformulate the\noccupancy world model as a temporal next-scale prediction (TENS) task, which\ndecomposes the temporal sequence modeling problem into the modeling of spatial\nscale-by-scale generation and temporal scene-by-scene prediction. With a\n\\textbf{TensFormer}, OccTENS can effectively manage the temporal causality and\nspatial relationships of occupancy sequences in a flexible and scalable way. To\nenhance the pose controllability, we further propose a holistic pose\naggregation strategy, which features a unified sequence modeling for occupancy\nand ego-motion. Experiments show that OccTENS outperforms the state-of-the-art\nmethod with both higher occupancy quality and faster inference time.",
        "url": "http://arxiv.org/abs/2509.03887v1",
        "published_date": "2025-09-04T05:06:47+00:00",
        "updated_date": "2025-09-04T05:06:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bu Jin",
            "Songen Gu",
            "Xiaotao Hu",
            "Yupeng Zheng",
            "Xiaoyang Guo",
            "Qian Zhang",
            "Xiaoxiao Long",
            "Wei Yin"
        ],
        "tldr": "The paper introduces OccTENS, a generative occupancy world model that predicts future 3D scene occupancy with better efficiency, temporal stability, and controllability by framing it as a temporal next-scale prediction task. It uses a TensFormer architecture and pose aggregation strategy to achieve state-of-the-art performance.",
        "tldr_zh": "该论文提出了一种生成式占用世界模型OccTENS，通过将问题转化为时间下一尺度预测任务，能够更高效、更稳定、更可控地预测未来三维场景的占用情况。它使用TensFormer架构和姿态聚合策略，实现了最先进的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "From Editor to Dense Geometry Estimator",
        "summary": "Leveraging visual priors from pre-trained text-to-image (T2I) generative\nmodels has shown success in dense prediction. However, dense prediction is\ninherently an image-to-image task, suggesting that image editing models, rather\nthan T2I generative models, may be a more suitable foundation for fine-tuning.\n  Motivated by this, we conduct a systematic analysis of the fine-tuning\nbehaviors of both editors and generators for dense geometry estimation. Our\nfindings show that editing models possess inherent structural priors, which\nenable them to converge more stably by ``refining\" their innate features, and\nultimately achieve higher performance than their generative counterparts.\n  Based on these findings, we introduce \\textbf{FE2E}, a framework that\npioneeringly adapts an advanced editing model based on Diffusion Transformer\n(DiT) architecture for dense geometry prediction. Specifically, to tailor the\neditor for this deterministic task, we reformulate the editor's original flow\nmatching loss into the ``consistent velocity\" training objective. And we use\nlogarithmic quantization to resolve the precision conflict between the editor's\nnative BFloat16 format and the high precision demand of our tasks.\nAdditionally, we leverage the DiT's global attention for a cost-free joint\nestimation of depth and normals in a single forward pass, enabling their\nsupervisory signals to mutually enhance each other.\n  Without scaling up the training data, FE2E achieves impressive performance\nimprovements in zero-shot monocular depth and normal estimation across multiple\ndatasets. Notably, it achieves over 35\\% performance gains on the ETH3D dataset\nand outperforms the DepthAnything series, which is trained on 100$\\times$ data.\nThe project page can be accessed \\href{https://amap-ml.github.io/FE2E/}{here}.",
        "url": "http://arxiv.org/abs/2509.04338v1",
        "published_date": "2025-09-04T15:58:50+00:00",
        "updated_date": "2025-09-04T15:58:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "JiYuan Wang",
            "Chunyu Lin",
            "Lei Sun",
            "Rongying Liu",
            "Lang Nie",
            "Mingxing Li",
            "Kang Liao",
            "Xiangxiang Chu",
            "Yao Zhao"
        ],
        "tldr": "The paper introduces FE2E, a framework that adapts image editing models (specifically a Diffusion Transformer) for dense geometry estimation tasks, achieving significant performance gains over text-to-image models and existing methods, even with less training data.",
        "tldr_zh": "该论文介绍了FE2E，一个将图像编辑模型（特别是Diffusion Transformer）应用于稠密几何估计任务的框架。相比于文本到图像模型和现有方法，FE2E取得了显著的性能提升，即使使用更少的训练数据。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]