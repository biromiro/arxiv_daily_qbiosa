[
    {
        "title": "UniVerse-1: Unified Audio-Video Generation via Stitching of Experts",
        "summary": "We introduce UniVerse-1, a unified, Veo-3-like model capable of\nsimultaneously generating coordinated audio and video. To enhance training\nefficiency, we bypass training from scratch and instead employ a stitching of\nexperts (SoE) technique. This approach deeply fuses the corresponding blocks of\npre-trained video and music generation experts models, thereby fully leveraging\ntheir foundational capabilities. To ensure accurate annotations and temporal\nalignment for both ambient sounds and speech with video content, we developed\nan online annotation pipeline that processes the required training data and\ngenerates labels during training process. This strategy circumvents the\nperformance degradation often caused by misalignment text-based annotations.\nThrough the synergy of these techniques, our model, after being finetuned on\napproximately 7,600 hours of audio-video data, produces results with\nwell-coordinated audio-visuals for ambient sounds generation and strong\nalignment for speech generation. To systematically evaluate our proposed\nmethod, we introduce Verse-Bench, a new benchmark dataset. In an effort to\nadvance research in audio-video generation and to close the performance gap\nwith state-of-the-art models such as Veo3, we make our model and code publicly\navailable. We hope this contribution will benefit the broader research\ncommunity. Project page: https://dorniwang.github.io/UniVerse-1/.",
        "url": "http://arxiv.org/abs/2509.06155v1",
        "published_date": "2025-09-07T17:55:03+00:00",
        "updated_date": "2025-09-07T17:55:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Duomin Wang",
            "Wei Zuo",
            "Aojie Li",
            "Ling-Hao Chen",
            "Xinyao Liao",
            "Deyu Zhou",
            "Zixin Yin",
            "Xili Dai",
            "Daxin Jiang",
            "Gang Yu"
        ],
        "tldr": "UniVerse-1 is a novel audio-video generation model that leverages a stitching of experts (SoE) technique to fuse pre-trained video and music models, achieving coordinated audio-visual generation and strong speech alignment after finetuning, along with a new benchmark dataset, Verse-Bench.",
        "tldr_zh": "UniVerse-1是一个新型的音视频生成模型，它利用专家缝合（SoE）技术融合预训练的视频和音乐模型，在微调后实现了协调的视听生成和强大的语音对齐，并提出了一个新的基准数据集Verse-Bench。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Home-made Diffusion Model from Scratch to Hatch",
        "summary": "We introduce Home-made Diffusion Model (HDM), an efficient yet powerful\ntext-to-image diffusion model optimized for training (and inferring) on\nconsumer-grade hardware. HDM achieves competitive 1024x1024 generation quality\nwhile maintaining a remarkably low training cost of $535-620 using four RTX5090\nGPUs, representing a significant reduction in computational requirements\ncompared to traditional approaches. Our key contributions include: (1)\nCross-U-Transformer (XUT), a novel U-shape transformer, Cross-U-Transformer\n(XUT), that employs cross-attention for skip connections, providing superior\nfeature integration that leads to remarkable compositional consistency; (2) a\ncomprehensive training recipe that incorporates TREAD acceleration, a novel\nshifted square crop strategy for efficient arbitrary aspect-ratio training, and\nprogressive resolution scaling; and (3) an empirical demonstration that smaller\nmodels (343M parameters) with carefully crafted architectures can achieve\nhigh-quality results and emergent capabilities, such as intuitive camera\ncontrol. Our work provides an alternative paradigm of scaling, demonstrating a\nviable path toward democratizing high-quality text-to-image generation for\nindividual researchers and smaller organizations with limited computational\nresources.",
        "url": "http://arxiv.org/abs/2509.06068v1",
        "published_date": "2025-09-07T14:21:57+00:00",
        "updated_date": "2025-09-07T14:21:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shih-Ying Yeh"
        ],
        "tldr": "The paper introduces Home-made Diffusion Model (HDM), a text-to-image diffusion model optimized for training on consumer-grade hardware, achieving competitive image generation quality with significantly reduced computational costs. It also introduces the Cross-U-Transformer (XUT) architecture and a comprehensive training recipe.",
        "tldr_zh": "该论文介绍了 Home-made Diffusion Model (HDM)，一种针对在消费级硬件上训练进行优化的文本到图像扩散模型，以显著降低的计算成本实现了具有竞争力的图像生成质量。它还引入了 Cross-U-Transformer (XUT) 架构和一个全面的训练方案。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models",
        "summary": "Recent advancements in aligning image and video generative models via GRPO\nhave achieved remarkable gains in enhancing human preference alignment.\nHowever, these methods still face high computational costs from on-policy\nrollouts and excessive SDE sampling steps, as well as training instability due\nto sparse rewards. In this paper, we propose BranchGRPO, a novel method that\nintroduces a branch sampling policy updating the SDE sampling process. By\nsharing computation across common prefixes and pruning low-reward paths and\nredundant depths, BranchGRPO substantially lowers the per-update compute cost\nwhile maintaining or improving exploration diversity. This work makes three\nmain contributions: (1) a branch sampling scheme that reduces rollout and\ntraining cost; (2) a tree-based advantage estimator incorporating dense\nprocess-level rewards; and (3) pruning strategies exploiting path and depth\nredundancy to accelerate convergence and boost performance. Experiments on\nimage and video preference alignment show that BranchGRPO improves alignment\nscores by 16% over strong baselines, while cutting training time by 50%.",
        "url": "http://arxiv.org/abs/2509.06040v1",
        "published_date": "2025-09-07T12:53:06+00:00",
        "updated_date": "2025-09-07T12:53:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Yuming Li",
            "Yikai Wang",
            "Yuying Zhu",
            "Zhongyu Zhao",
            "Ming Lu",
            "Qi She",
            "Shanghang Zhang"
        ],
        "tldr": "BranchGRPO introduces a branched sampling approach to GRPO for more efficient and stable alignment of image and video generative models with human preferences, achieving significant speedups and alignment improvements.",
        "tldr_zh": "BranchGRPO 提出了一种分支采样方法来改进 GRPO，以实现图像和视频生成模型与人类偏好更高效和稳定的对齐，从而显著提高速度和对齐效果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance",
        "summary": "Vision-language models have demonstrated impressive capabilities in\ngenerating 2D images under various conditions; however the impressive\nperformance of these models in 2D is largely enabled by extensive, readily\navailable pretrained foundation models. Critically, comparable pretrained\nfoundation models do not exist for 3D, significantly limiting progress in this\ndomain. As a result, the potential of vision-language models to produce\nhigh-resolution 3D counterfactual medical images conditioned solely on natural\nlanguage descriptions remains completely unexplored. Addressing this gap would\nenable powerful clinical and research applications, such as personalized\ncounterfactual explanations, simulation of disease progression scenarios, and\nenhanced medical training by visualizing hypothetical medical conditions in\nrealistic detail. Our work takes a meaningful step toward addressing this\nchallenge by introducing a framework capable of generating high-resolution 3D\ncounterfactual medical images of synthesized patients guided by free-form\nlanguage prompts. We adapt state-of-the-art 3D diffusion models with\nenhancements from Simple Diffusion and incorporate augmented conditioning to\nimprove text alignment and image quality. To our knowledge, this represents the\nfirst demonstration of a language-guided native-3D diffusion model applied\nspecifically to neurological imaging data, where faithful three-dimensional\nmodeling is essential to represent the brain's three-dimensional structure.\nThrough results on two distinct neurological MRI datasets, our framework\nsuccessfully simulates varying counterfactual lesion loads in Multiple\nSclerosis (MS), and cognitive states in Alzheimer's disease, generating\nhigh-quality images while preserving subject fidelity in synthetically\ngenerated medical images. Our results lay the groundwork for prompt-driven\ndisease progression analysis within 3D medical imaging.",
        "url": "http://arxiv.org/abs/2509.05978v1",
        "published_date": "2025-09-07T08:52:18+00:00",
        "updated_date": "2025-09-07T08:52:18+00:00",
        "categories": [
            "eess.IV",
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Mohamed Mohamed",
            "Brennan Nichyporuk",
            "Douglas L. Arnold",
            "Tal Arbel"
        ],
        "tldr": "This paper introduces a framework for generating high-resolution 3D counterfactual medical images from language prompts, specifically applied to neurological imaging and demonstrates its use in simulating disease progression in MS and Alzheimer's. It's the first language-guided native-3D diffusion model for neurological imaging.",
        "tldr_zh": "本文介绍了一种框架，用于从语言提示生成高分辨率3D反事实医学图像，专门应用于神经影像，并演示了其在模拟MS和阿尔茨海默病疾病进展中的应用。 这是首个用于神经影像的语言引导的native-3D扩散模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Coefficients-Preserving Sampling for Reinforcement Learning with Flow Matching",
        "summary": "Reinforcement Learning (RL) has recently emerged as a powerful technique for\nimproving image and video generation in Diffusion and Flow Matching models,\nspecifically for enhancing output quality and alignment with prompts. A\ncritical step for applying online RL methods on Flow Matching is the\nintroduction of stochasticity into the deterministic framework, commonly\nrealized by Stochastic Differential Equation (SDE). Our investigation reveals a\nsignificant drawback to this approach: SDE-based sampling introduces pronounced\nnoise artifacts in the generated images, which we found to be detrimental to\nthe reward learning process. A rigorous theoretical analysis traces the origin\nof this noise to an excess of stochasticity injected during inference. To\naddress this, we draw inspiration from Denoising Diffusion Implicit Models\n(DDIM) to reformulate the sampling process. Our proposed method,\nCoefficients-Preserving Sampling (CPS), eliminates these noise artifacts. This\nleads to more accurate reward modeling, ultimately enabling faster and more\nstable convergence for reinforcement learning-based optimizers like Flow-GRPO\nand Dance-GRPO. Code will be released at https://github.com/IamCreateAI/FlowCPS",
        "url": "http://arxiv.org/abs/2509.05952v1",
        "published_date": "2025-09-07T07:25:00+00:00",
        "updated_date": "2025-09-07T07:25:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Feng Wang",
            "Zihao Yu"
        ],
        "tldr": "This paper proposes a new sampling method, Coefficients-Preserving Sampling (CPS), for reinforcement learning with Flow Matching to reduce noise artifacts in generated images, leading to faster and more stable convergence for RL-based optimizers.",
        "tldr_zh": "本文提出了一种新的采样方法，即系数保持采样 (CPS)，用于 Flow Matching 的强化学习，以减少生成图像中的噪声伪影，从而更快、更稳定地收敛 RL 优化器。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]