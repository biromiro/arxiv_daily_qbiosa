[
    {
        "title": "Native-Resolution Image Synthesis",
        "summary": "We introduce native-resolution image synthesis, a novel generative modeling\nparadigm that enables the synthesis of images at arbitrary resolutions and\naspect ratios. This approach overcomes the limitations of conventional\nfixed-resolution, square-image methods by natively handling variable-length\nvisual tokens, a core challenge for traditional techniques. To this end, we\nintroduce the Native-resolution diffusion Transformer (NiT), an architecture\ndesigned to explicitly model varying resolutions and aspect ratios within its\ndenoising process. Free from the constraints of fixed formats, NiT learns\nintrinsic visual distributions from images spanning a broad range of\nresolutions and aspect ratios. Notably, a single NiT model simultaneously\nachieves the state-of-the-art performance on both ImageNet-256x256 and 512x512\nbenchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in\nadvanced large language models, NiT, trained solely on ImageNet, demonstrates\nexcellent zero-shot generalization performance. It successfully generates\nhigh-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)\nand diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These\nfindings indicate the significant potential of native-resolution modeling as a\nbridge between visual generative modeling and advanced LLM methodologies.",
        "url": "http://arxiv.org/abs/2506.03131v1",
        "published_date": "2025-06-03T17:57:33+00:00",
        "updated_date": "2025-06-03T17:57:33+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zidong Wang",
            "Lei Bai",
            "Xiangyu Yue",
            "Wanli Ouyang",
            "Yiyuan Zhang"
        ],
        "tldr": "This paper introduces Native-resolution diffusion Transformer (NiT), a novel architecture for generating images at arbitrary resolutions and aspect ratios, achieving state-of-the-art performance and zero-shot generalization.",
        "tldr_zh": "本文介绍了一种新颖的 Native-resolution diffusion Transformer (NiT) 架构，用于生成任意分辨率和宽高比的图像，实现了最先进的性能和零样本泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation",
        "summary": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets.",
        "url": "http://arxiv.org/abs/2506.03147v1",
        "published_date": "2025-06-03T17:59:33+00:00",
        "updated_date": "2025-06-03T17:59:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Bin Lin",
            "Zongjian Li",
            "Xinhua Cheng",
            "Yuwei Niu",
            "Yang Ye",
            "Xianyi He",
            "Shenghai Yuan",
            "Wangbo Yu",
            "Shaodong Wang",
            "Yunyang Ge",
            "Yatian Pang",
            "Li Yuan"
        ],
        "tldr": "The paper introduces UniWorld, a unified generative framework leveraging semantic encodings from visual-language models to achieve strong performance in image editing and understanding tasks with significantly less data than existing methods like BAGEL. They open-source all resources.",
        "tldr_zh": "本文介绍UniWorld，一个统一的生成框架，利用视觉-语言模型的语义编码，以远少于现有方法（如BAGEL）的数据量，在图像编辑和理解任务中实现了强大的性能。他们开源了所有资源。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video Generation",
        "summary": "Diffusion Models have achieved remarkable results in video synthesis but\nrequire iterative denoising steps, leading to substantial computational\noverhead. Consistency Models have made significant progress in accelerating\ndiffusion models. However, directly applying them to video diffusion models\noften results in severe degradation of temporal consistency and appearance\ndetails. In this paper, by analyzing the training dynamics of Consistency\nModels, we identify a key conflicting learning dynamics during the distillation\nprocess: there is a significant discrepancy in the optimization gradients and\nloss contributions across different timesteps. This discrepancy prevents the\ndistilled student model from achieving an optimal state, leading to compromised\ntemporal consistency and degraded appearance details. To address this issue, we\npropose a parameter-efficient \\textbf{Dual-Expert Consistency Model~(DCM)},\nwhere a semantic expert focuses on learning semantic layout and motion, while a\ndetail expert specializes in fine detail refinement. Furthermore, we introduce\nTemporal Coherence Loss to improve motion consistency for the semantic expert\nand apply GAN and Feature Matching Loss to enhance the synthesis quality of the\ndetail expert.Our approach achieves state-of-the-art visual quality with\nsignificantly reduced sampling steps, demonstrating the effectiveness of expert\nspecialization in video diffusion model distillation. Our code and models are\navailable at\n\\href{https://github.com/Vchitect/DCM}{https://github.com/Vchitect/DCM}.",
        "url": "http://arxiv.org/abs/2506.03123v1",
        "published_date": "2025-06-03T17:55:04+00:00",
        "updated_date": "2025-06-03T17:55:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhengyao Lv",
            "Chenyang Si",
            "Tianlin Pan",
            "Zhaoxi Chen",
            "Kwan-Yee K. Wong",
            "Yu Qiao",
            "Ziwei Liu"
        ],
        "tldr": "The paper introduces a Dual-Expert Consistency Model (DCM) to improve the efficiency and quality of video generation by addressing conflicting learning dynamics in Consistency Models, leading to better temporal consistency and detail.",
        "tldr_zh": "该论文提出了一种双专家一致性模型（DCM），通过解决一致性模型中冲突的学习动态，从而提高视频生成的效率和质量，进而获得更好的时间和细节一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PartComposer: Learning and Composing Part-Level Concepts from Single-Image Examples",
        "summary": "We present PartComposer: a framework for part-level concept learning from\nsingle-image examples that enables text-to-image diffusion models to compose\nnovel objects from meaningful components. Existing methods either struggle with\neffectively learning fine-grained concepts or require a large dataset as input.\nWe propose a dynamic data synthesis pipeline generating diverse part\ncompositions to address one-shot data scarcity. Most importantly, we propose to\nmaximize the mutual information between denoised latents and structured concept\ncodes via a concept predictor, enabling direct regulation on concept\ndisentanglement and re-composition supervision. Our method achieves strong\ndisentanglement and controllable composition, outperforming subject and\npart-level baselines when mixing concepts from the same, or different, object\ncategories.",
        "url": "http://arxiv.org/abs/2506.03004v1",
        "published_date": "2025-06-03T15:43:28+00:00",
        "updated_date": "2025-06-03T15:43:28+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Junyu Liu",
            "R. Kenny Jones",
            "Daniel Ritchie"
        ],
        "tldr": "PartComposer learns part-level concepts from single images by dynamic data synthesis and mutual information maximization between denoised latents and concept codes, enabling controllable object composition in diffusion models.",
        "tldr_zh": "PartComposer 通过动态数据合成和去噪潜在空间与概念代码之间的互信息最大化，从单张图像中学习部件级别的概念，从而在扩散模型中实现可控的对象合成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation",
        "summary": "With the advancement of language models, unified multimodal understanding and\ngeneration have made significant strides, with model architectures evolving\nfrom separated components to unified single-model frameworks. This paper\nexplores an efficient training paradigm to build a single transformer for\nunified multimodal understanding and generation. Specifically, we propose a\nmultimodal warmup strategy utilizing prior knowledge to extend capabilities. To\naddress cross-modal compatibility challenges, we introduce feature pre-scaling\nand multimodal AdaLN techniques. Integrating the proposed technologies, we\npresent the HaploOmni, a new single multimodal transformer. With limited\ntraining costs, HaploOmni achieves competitive performance across multiple\nimage and video understanding and generation benchmarks over advanced unified\nmodels. All codes will be made public at https://github.com/Tencent/HaploVLM.",
        "url": "http://arxiv.org/abs/2506.02975v1",
        "published_date": "2025-06-03T15:14:00+00:00",
        "updated_date": "2025-06-03T15:14:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yicheng Xiao",
            "Lin Song",
            "Rui Yang",
            "Cheng Cheng",
            "Zunnan Xu",
            "Zhaoyang Zhang",
            "Yixiao Ge",
            "Xiu Li",
            "Ying Shan"
        ],
        "tldr": "This paper introduces HaploOmni, a single transformer model for unified multimodal video understanding and generation, trained with a multimodal warmup strategy and feature pre-scaling & AdaLN techniques, achieving competitive performance with limited training costs.",
        "tldr_zh": "本文介绍HaploOmni，一个用于统一多模态视频理解和生成的单Transformer模型，通过多模态warmup和特征预缩放与AdaLN技术进行训练，并以有限的训练成本实现了具有竞争力的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LumosFlow: Motion-Guided Long Video Generation",
        "summary": "Long video generation has gained increasing attention due to its widespread\napplications in fields such as entertainment and simulation. Despite advances,\nsynthesizing temporally coherent and visually compelling long sequences remains\na formidable challenge. Conventional approaches often synthesize long videos by\nsequentially generating and concatenating short clips, or generating key frames\nand then interpolate the intermediate frames in a hierarchical manner. However,\nboth of them still remain significant challenges, leading to issues such as\ntemporal repetition or unnatural transitions. In this paper, we revisit the\nhierarchical long video generation pipeline and introduce LumosFlow, a\nframework introduce motion guidance explicitly. Specifically, we first employ\nthe Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames\nwith larger motion intervals, thereby ensuring content diversity in the\ngenerated long videos. Given the complexity of interpolating contextual\ntransitions between key frames, we further decompose the intermediate frame\ninterpolation into motion generation and post-hoc refinement. For each pair of\nkey frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes\ncomplex and large-motion optical flows, while MotionControlNet subsequently\nrefines the warped results to enhance quality and guide intermediate frame\ngeneration. Compared with traditional video frame interpolation, we achieve 15x\ninterpolation, ensuring reasonable and continuous motion between adjacent\nframes. Experiments show that our method can generate long videos with\nconsistent motion and appearance. Code and models will be made publicly\navailable upon acceptance. Our project page:\nhttps://jiahaochen1.github.io/LumosFlow/",
        "url": "http://arxiv.org/abs/2506.02497v1",
        "published_date": "2025-06-03T06:25:00+00:00",
        "updated_date": "2025-06-03T06:25:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahao Chen",
            "Hangjie Yuan",
            "Yichen Qian",
            "Jingyun Liang",
            "Jiazheng Xing",
            "Pengwei Liu",
            "Weihua Chen",
            "Fan Wang",
            "Bing Su"
        ],
        "tldr": "LumosFlow addresses long video generation by using a motion-guided hierarchical approach, employing Large Motion Text-to-Video Diffusion Models for keyframes and Latent Optical Flow Diffusion Models for intermediate frame interpolation, achieving 15x interpolation and improved motion coherence.",
        "tldr_zh": "LumosFlow 专注于长视频生成，采用运动引导的分层方法，使用大运动文本到视频扩散模型生成关键帧，并使用潜在光流扩散模型进行中间帧插值，实现了 15 倍插值并提高了运动连贯性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Motion aware video generative model",
        "summary": "Recent advances in diffusion-based video generation have yielded\nunprecedented quality in visual content and semantic coherence. However,\ncurrent approaches predominantly rely on statistical learning from vast\ndatasets without explicitly modeling the underlying physics of motion,\nresulting in subtle yet perceptible non-physical artifacts that diminish the\nrealism of generated videos. This paper introduces a physics-informed frequency\ndomain approach to enhance the physical plausibility of generated videos. We\nfirst conduct a systematic analysis of the frequency-domain characteristics of\ndiverse physical motions (translation, rotation, scaling), revealing that each\nmotion type exhibits distinctive and identifiable spectral signatures. Building\non this theoretical foundation, we propose two complementary components: (1) a\nphysical motion loss function that quantifies and optimizes the conformity of\ngenerated videos to ideal frequency-domain motion patterns, and (2) a frequency\ndomain enhancement module that progressively learns to adjust video features to\nconform to physical motion constraints while preserving original network\nfunctionality through a zero-initialization strategy. Experiments across\nmultiple video diffusion architectures demonstrate that our approach\nsignificantly enhances motion quality and physical plausibility without\ncompromising visual quality or semantic alignment. Our frequency-domain\nphysical motion framework generalizes effectively across different video\ngeneration architectures, offering a principled approach to incorporating\nphysical constraints into deep learning-based video synthesis pipelines. This\nwork seeks to establish connections between data-driven models and\nphysics-based motion models.",
        "url": "http://arxiv.org/abs/2506.02244v1",
        "published_date": "2025-06-02T20:42:54+00:00",
        "updated_date": "2025-06-02T20:42:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Bowen Xue",
            "Giuseppe Claudio Guarnera",
            "Shuang Zhao",
            "Zahra Montazeri"
        ],
        "tldr": "This paper introduces a frequency-domain approach to enhance the physical plausibility of diffusion-based video generation by incorporating physics-informed motion priors, resulting in improved motion quality and realism.",
        "tldr_zh": "本文提出了一种频域方法，通过结合物理信息运动先验来提高基于扩散的视频生成的物理合理性，从而提高运动质量和真实感。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Smoothed Preference Optimization via ReNoise Inversion for Aligning Diffusion Models with Varied Human Preferences",
        "summary": "Direct Preference Optimization (DPO) aligns text-to-image (T2I) generation\nmodels with human preferences using pairwise preference data. Although\nsubstantial resources are expended in collecting and labeling datasets, a\ncritical aspect is often neglected: \\textit{preferences vary across individuals\nand should be represented with more granularity.} To address this, we propose\nSmPO-Diffusion, a novel method for modeling preference distributions to improve\nthe DPO objective, along with a numerical upper bound estimation for the\ndiffusion optimization objective. First, we introduce a smoothed preference\ndistribution to replace the original binary distribution. We employ a reward\nmodel to simulate human preferences and apply preference likelihood averaging\nto improve the DPO loss, such that the loss function approaches zero when\npreferences are similar. Furthermore, we utilize an inversion technique to\nsimulate the trajectory preference distribution of the diffusion model,\nenabling more accurate alignment with the optimization objective. Our approach\neffectively mitigates issues of excessive optimization and objective\nmisalignment present in existing methods through straightforward modifications.\nOur SmPO-Diffusion achieves state-of-the-art performance in preference\nevaluation, outperforming baselines across metrics with lower training costs.\nThe project page is https://jaydenlyh.github.io/SmPO-project-page/.",
        "url": "http://arxiv.org/abs/2506.02698v1",
        "published_date": "2025-06-03T09:47:22+00:00",
        "updated_date": "2025-06-03T09:47:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunhong Lu",
            "Qichao Wang",
            "Hengyuan Cao",
            "Xiaoyin Xu",
            "Min Zhang"
        ],
        "tldr": "The paper introduces SmPO-Diffusion, a novel method for aligning text-to-image diffusion models with diverse human preferences by modeling preference distributions and improving the DPO objective with a smoothed preference distribution and renoise inversion. It claims state-of-the-art performance with lower training costs.",
        "tldr_zh": "本文介绍了一种名为SmPO-Diffusion的新方法，通过对偏好分布进行建模，并利用平滑的偏好分布和重噪声反演来改进DPO目标，从而使文本到图像的扩散模型与不同的人类偏好对齐。 它声称以更低的训练成本实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]