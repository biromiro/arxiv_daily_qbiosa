[
    {
        "title": "Policy Optimized Text-to-Image Pipeline Design",
        "summary": "Text-to-image generation has evolved beyond single monolithic models to\ncomplex multi-component pipelines. These combine fine-tuned generators,\nadapters, upscaling blocks and even editing steps, leading to significant\nimprovements in image quality. However, their effective design requires\nsubstantial expertise. Recent approaches have shown promise in automating this\nprocess through large language models (LLMs), but they suffer from two critical\nlimitations: extensive computational requirements from generating images with\nhundreds of predefined pipelines, and poor generalization beyond memorized\ntraining examples. We introduce a novel reinforcement learning-based framework\nthat addresses these inefficiencies. Our approach first trains an ensemble of\nreward models capable of predicting image quality scores directly from\nprompt-workflow combinations, eliminating the need for costly image generation\nduring training. We then implement a two-phase training strategy: initial\nworkflow vocabulary training followed by GRPO-based optimization that guides\nthe model toward higher-performing regions of the workflow space. Additionally,\nwe incorporate a classifier-free guidance based enhancement technique that\nextrapolates along the path between the initial and GRPO-tuned models, further\nimproving output quality. We validate our approach through a set of\ncomparisons, showing that it can successfully create new flows with greater\ndiversity and lead to superior image quality compared to existing baselines.",
        "url": "http://arxiv.org/abs/2505.21478v1",
        "published_date": "2025-05-27T17:50:47+00:00",
        "updated_date": "2025-05-27T17:50:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Uri Gadot",
            "Rinon Gal",
            "Yftah Ziser",
            "Gal Chechik",
            "Shie Mannor"
        ],
        "tldr": "This paper introduces a reinforcement learning framework to optimize text-to-image pipeline design by training reward models to predict image quality, leading to better image quality and diversity compared to existing methods.",
        "tldr_zh": "本文提出了一种基于强化学习的框架，通过训练奖励模型预测图像质量来优化文本到图像的管道设计，从而获得比现有方法更好的图像质量和多样性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via Next-Detail Prediction",
        "summary": "This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image\ngeneration method that models images through a novel next-detail prediction\nstrategy. By learning a resolution-aware token sequence supervised with\nprogressively degraded images, DetailFlow enables the generation process to\nstart from the global structure and incrementally refine details. This\ncoarse-to-fine 1D token sequence aligns well with the autoregressive inference\nmechanism, providing a more natural and efficient way for the AR model to\ngenerate complex visual content. Our compact 1D AR model achieves high-quality\nimage synthesis with significantly fewer tokens than previous approaches, i.e.\nVAR/VQGAN. We further propose a parallel inference mechanism with\nself-correction that accelerates generation speed by approximately 8x while\nreducing accumulation sampling error inherent in teacher-forcing supervision.\nOn the ImageNet 256x256 benchmark, our method achieves 2.96 gFID with 128\ntokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require\n680 tokens in their AR models. Moreover, due to the significantly reduced token\ncount and parallel inference mechanism, our method runs nearly 2x faster\ninference speed compared to VAR and FlexVAR. Extensive experimental results\ndemonstrate DetailFlow's superior generation quality and efficiency compared to\nexisting state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2505.21473v1",
        "published_date": "2025-05-27T17:45:21+00:00",
        "updated_date": "2025-05-27T17:45:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiheng Liu",
            "Liao Qu",
            "Huichao Zhang",
            "Xu Wang",
            "Yi Jiang",
            "Yiming Gao",
            "Hu Ye",
            "Xian Li",
            "Shuai Wang",
            "Daniel K. Du",
            "Shu Cheng",
            "Zehuan Yuan",
            "Xinglong Wu"
        ],
        "tldr": "DetailFlow introduces a novel coarse-to-fine 1D autoregressive image generation method with next-detail prediction, achieving better quality and efficiency with fewer tokens and a parallel inference mechanism compared to VAR/VQGAN.",
        "tldr_zh": "DetailFlow提出了一种新颖的由粗到精的1D自回归图像生成方法，通过预测下一个细节来实现。与VAR/VQGAN相比，该方法以更少的tokens和并行推理机制实现了更好的质量和效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Normalized Attention Guidance: Universal Negative Guidance for Diffusion Model",
        "summary": "Negative guidance -- explicitly suppressing unwanted attributes -- remains a\nfundamental challenge in diffusion models, particularly in few-step sampling\nregimes. While Classifier-Free Guidance (CFG) works well in standard settings,\nit fails under aggressive sampling step compression due to divergent\npredictions between positive and negative branches. We present Normalized\nAttention Guidance (NAG), an efficient, training-free mechanism that applies\nextrapolation in attention space with L1-based normalization and refinement.\nNAG restores effective negative guidance where CFG collapses while maintaining\nfidelity. Unlike existing approaches, NAG generalizes across architectures\n(UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image,\nvideo), functioning as a \\textit{universal} plug-in with minimal computational\noverhead. Through extensive experimentation, we demonstrate consistent\nimprovements in text alignment (CLIP Score), fidelity (FID, PFID), and\nhuman-perceived quality (ImageReward). Our ablation studies validate each\ndesign component, while user studies confirm significant preference for\nNAG-guided outputs. As a model-agnostic inference-time approach requiring no\nretraining, NAG provides effortless negative guidance for all modern diffusion\nframeworks -- pseudocode in the Appendix!",
        "url": "http://arxiv.org/abs/2505.21179v1",
        "published_date": "2025-05-27T13:30:46+00:00",
        "updated_date": "2025-05-27T13:30:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dar-Yen Chen",
            "Hmrishav Bandyopadhyay",
            "Kai Zou",
            "Yi-Zhe Song"
        ],
        "tldr": "This paper introduces Normalized Attention Guidance (NAG), a training-free, universal negative guidance method for diffusion models that improves text alignment, fidelity, and perceived quality, especially in few-step sampling.",
        "tldr_zh": "该论文介绍了一种名为归一化注意力引导（NAG）的无需训练的通用负引导方法，用于扩散模型，该方法可以提高文本对齐、保真度和感知质量，尤其是在少步采样中。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Differentiable Solver Search for Fast Diffusion Sampling",
        "summary": "Diffusion models have demonstrated remarkable generation quality but at the\ncost of numerous function evaluations. Recently, advanced ODE-based solvers\nhave been developed to mitigate the substantial computational demands of\nreverse-diffusion solving under limited sampling steps. However, these solvers,\nheavily inspired by Adams-like multistep methods, rely solely on t-related\nLagrange interpolation. We show that t-related Lagrange interpolation is\nsuboptimal for diffusion model and reveal a compact search space comprised of\ntime steps and solver coefficients. Building on our analysis, we propose a\nnovel differentiable solver search algorithm to identify more optimal solver.\nEquipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and\nFlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256\nwith only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of\n2.33 with only 10 steps. Notably, our searched solver outperforms traditional\nsolvers by a significant margin. Moreover, our searched solver demonstrates\ngenerality across various model architectures, resolutions, and model sizes.",
        "url": "http://arxiv.org/abs/2505.21114v1",
        "published_date": "2025-05-27T12:33:43+00:00",
        "updated_date": "2025-05-27T12:33:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuai Wang",
            "Zexian Li",
            "Qipeng zhang",
            "Tianhui Song",
            "Xubin Li",
            "Tiezheng Ge",
            "Bo Zheng",
            "Limin Wang"
        ],
        "tldr": "The paper introduces a differentiable solver search algorithm to identify more optimal solvers for diffusion models, achieving state-of-the-art FID scores on ImageNet256 with only 10 steps, outperforming traditional solvers.",
        "tldr_zh": "该论文介绍了一种可微求解器搜索算法，用于识别更优化的扩散模型求解器，仅用 10 步在 ImageNet256 上实现了最先进的 FID 分数，优于传统求解器。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Frame-Level Captions for Long Video Generation with Complex Multi Scenes",
        "summary": "Generating long videos that can show complex stories, like movie scenes from\nscripts, has great promise and offers much more than short clips. However,\ncurrent methods that use autoregression with diffusion models often struggle\nbecause their step-by-step process naturally leads to a serious error\naccumulation (drift). Also, many existing ways to make long videos focus on\nsingle, continuous scenes, making them less useful for stories with many events\nand changes. This paper introduces a new approach to solve these problems.\nFirst, we propose a novel way to annotate datasets at the frame-level,\nproviding detailed text guidance needed for making complex, multi-scene long\nvideos. This detailed guidance works with a Frame-Level Attention Mechanism to\nmake sure text and video match precisely. A key feature is that each part\n(frame) within these windows can be guided by its own distinct text prompt. Our\ntraining uses Diffusion Forcing to provide the model with the ability to handle\ntime flexibly. We tested our approach on difficult VBench 2.0 benchmarks\n(\"Complex Plots\" and \"Complex Landscapes\") based on the WanX2.1-T2V-1.3B model.\nThe results show our method is better at following instructions in complex,\nchanging scenes and creates high-quality long videos. We plan to share our\ndataset annotation methods and trained models with the research community.\nProject page: https://zgctroy.github.io/frame-level-captions .",
        "url": "http://arxiv.org/abs/2505.20827v1",
        "published_date": "2025-05-27T07:39:43+00:00",
        "updated_date": "2025-05-27T07:39:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guangcong Zheng",
            "Jianlong Yuan",
            "Bo Wang",
            "Haoyang Huang",
            "Guoqing Ma",
            "Nan Duan"
        ],
        "tldr": "This paper introduces a frame-level captioned dataset and a diffusion-based video generation method with frame-level attention to generate long, complex, multi-scene videos, addressing error accumulation and lack of scene transitions in existing methods.",
        "tldr_zh": "本文介绍了一个帧级别字幕数据集和一个基于扩散的视频生成方法，该方法具有帧级别注意力机制，用于生成长的、复杂的、多场景的视频，解决了现有方法中的误差累积和场景转换不足的问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction",
        "summary": "In this paper, we unify more than 10 existing one-step diffusion distillation\napproaches, such as Diff-Instruct, DMD, SIM, SiD, $f$-distill, etc, inside a\ntheory-driven framework which we name the \\textbf{\\emph{Uni-Instruct}}.\nUni-Instruct is motivated by our proposed diffusion expansion theory of the\n$f$-divergence family. Then we introduce key theories that overcome the\nintractability issue of the original expanded $f$-divergence, resulting in an\nequivalent yet tractable loss that effectively trains one-step diffusion models\nby minimizing the expanded $f$-divergence family. The novel unification\nintroduced by Uni-Instruct not only offers new theoretical contributions that\nhelp understand existing approaches from a high-level perspective but also\nleads to state-of-the-art one-step diffusion generation performances. On the\nCIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet\nInception Distance (FID) values of \\textbf{\\emph{1.46}} for unconditional\ngeneration and \\textbf{\\emph{1.38}} for conditional generation. On the\nImageNet-$64\\times 64$ generation benchmark, Uni-Instruct achieves a new SoTA\none-step generation FID of \\textbf{\\emph{1.02}}, which outperforms its 79-step\nteacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35).\nWe also apply Uni-Instruct on broader tasks like text-to-3D generation. For\ntext-to-3D generation, Uni-Instruct gives decent results, which slightly\noutperforms previous methods, such as SDS and VSD, in terms of both generation\nquality and diversity. Both the solid theoretical and empirical contributions\nof Uni-Instruct will potentially help future studies on one-step diffusion\ndistillation and knowledge transferring of diffusion models.",
        "url": "http://arxiv.org/abs/2505.20755v1",
        "published_date": "2025-05-27T05:55:45+00:00",
        "updated_date": "2025-05-27T05:55:45+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Yifei Wang",
            "Weimin Bai",
            "Colin Zhang",
            "Debing Zhang",
            "Weijian Luo",
            "He Sun"
        ],
        "tldr": "The paper introduces Uni-Instruct, a theoretical framework that unifies existing one-step diffusion distillation approaches, achieving state-of-the-art generation performance on image benchmarks and showing promising results on text-to-3D generation.",
        "tldr_zh": "本文介绍了Uni-Instruct，一个统一现有单步扩散蒸馏方法的理论框架，在图像基准测试上实现了最先进的生成性能，并在文本到3D生成方面显示出前景。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LeDiFlow: Learned Distribution-guided Flow Matching to Accelerate Image Generation",
        "summary": "Enhancing the efficiency of high-quality image generation using Diffusion\nModels (DMs) is a significant challenge due to the iterative nature of the\nprocess. Flow Matching (FM) is emerging as a powerful generative modeling\nparadigm based on a simulation-free training objective instead of a score-based\none used in DMs. Typical FM approaches rely on a Gaussian distribution prior,\nwhich induces curved, conditional probability paths between the prior and\ntarget data distribution. These curved paths pose a challenge for the Ordinary\nDifferential Equation (ODE) solver, requiring a large number of inference calls\nto the flow prediction network. To address this issue, we present Learned\nDistribution-guided Flow Matching (LeDiFlow), a novel scalable method for\ntraining FM-based image generation models using a better-suited prior\ndistribution learned via a regression-based auxiliary model. By initializing\nthe ODE solver with a prior closer to the target data distribution, LeDiFlow\nenables the learning of more computationally tractable probability paths. These\npaths directly translate to fewer solver steps needed for high-quality image\ngeneration at inference time. Our method utilizes a State-Of-The-Art (SOTA)\ntransformer architecture combined with latent space sampling and can be trained\non a consumer workstation. We empirically demonstrate that LeDiFlow remarkably\noutperforms the respective FM baselines. For instance, when operating directly\non pixels, our model accelerates inference by up to 3.75x compared to the\ncorresponding pixel-space baseline. Simultaneously, our latent FM model\nenhances image quality on average by 1.32x in CLIP Maximum Mean Discrepancy\n(CMMD) metric against its respective baseline.",
        "url": "http://arxiv.org/abs/2505.20723v1",
        "published_date": "2025-05-27T05:07:37+00:00",
        "updated_date": "2025-05-27T05:07:37+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "I.4; I.2"
        ],
        "authors": [
            "Pascal Zwick",
            "Nils Friederich",
            "Maximilian Beichter",
            "Lennart Hilbert",
            "Ralf Mikut",
            "Oliver Bringmann"
        ],
        "tldr": "LeDiFlow improves Flow Matching for image generation by learning a better prior distribution, leading to faster inference and enhanced image quality compared to baseline FM methods.",
        "tldr_zh": "LeDiFlow通过学习一个更好的先验分布改进了图像生成的Flow Matching方法，与基线FM方法相比，实现了更快的推理速度和更高的图像质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generative Image Compression by Estimating Gradients of the Rate-variable Feature Distribution",
        "summary": "While learned image compression (LIC) focuses on efficient data transmission,\ngenerative image compression (GIC) extends this framework by integrating\ngenerative modeling to produce photo-realistic reconstructed images. In this\npaper, we propose a novel diffusion-based generative modeling framework\ntailored for generative image compression. Unlike prior diffusion-based\napproaches that indirectly exploit diffusion modeling, we reinterpret the\ncompression process itself as a forward diffusion path governed by stochastic\ndifferential equations (SDEs). A reverse neural network is trained to\nreconstruct images by reversing the compression process directly, without\nrequiring Gaussian noise initialization. This approach achieves smooth rate\nadjustment and photo-realistic reconstructions with only a minimal number of\nsampling steps. Extensive experiments on benchmark datasets demonstrate that\nour method outperforms existing generative image compression approaches across\na range of metrics, including perceptual distortion, statistical fidelity, and\nno-reference quality assessments.",
        "url": "http://arxiv.org/abs/2505.20984v1",
        "published_date": "2025-05-27T10:18:24+00:00",
        "updated_date": "2025-05-27T10:18:24+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Minghao Han",
            "Weiyi You",
            "Jinhua Zhang",
            "Leheng Zhang",
            "Ce Zhu",
            "Shuhang Gu"
        ],
        "tldr": "This paper introduces a novel diffusion-based generative image compression framework that directly reverses the compression process using stochastic differential equations (SDEs), achieving realistic reconstructions with efficient rate adjustment.",
        "tldr_zh": "本文提出了一种新的基于扩散的生成图像压缩框架，该框架使用随机微分方程（SDEs）直接反转压缩过程，从而实现逼真的重建和高效的速率调整。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]