[
    {
        "title": "OmniGenBench: A Benchmark for Omnipotent Multimodal Generation across 50+ Tasks",
        "summary": "Recent breakthroughs in large multimodal models (LMMs), such as the\nimpressive GPT-4o-Native, have demonstrated remarkable proficiency in following\ngeneral-purpose instructions for image generation. However, current benchmarks\noften lack the necessary breadth and depth to fully evaluate the diverse\ncapabilities of these models. To overcome this limitation, we introduce\nOmniGenBench, a novel and comprehensive benchmark meticulously designed to\nassess the instruction-following abilities of state-of-the-art LMMs across both\nperception-centric and cognition-centric dimensions. Our OmniGenBench includes\n57 diverse sub-tasks grounded in real-world scenarios, systematically\ncategorized according to the specific model capabilities they demand. For\nrigorous evaluation, we further employ a dual-mode protocol. This protocol\nutilizes off-the-shelf visual parsing tools for perception-centric tasks and a\npowerful LLM-based judger for cognition-centric tasks to assess the alignment\nbetween generated images and user instructions. Using OmniGenBench, we evaluate\nmainstream generative models, including prevalent models like GPT-4o,\nGemini-2.0-Flash, and Seedream, and provide in-depth comparisons and analyses\nof their performance.Code and data are available at\nhttps://github.com/emilia113/OmniGenBench.",
        "url": "http://arxiv.org/abs/2505.18775v1",
        "published_date": "2025-05-24T16:29:34+00:00",
        "updated_date": "2025-05-24T16:29:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jiayu Wang",
            "Yang Jiao",
            "Yue Yu",
            "Tianwen Qian",
            "Shaoxiang Chen",
            "Jingjing Chen",
            "Yu-Gang Jiang"
        ],
        "tldr": "The paper introduces OmniGenBench, a new comprehensive benchmark for evaluating the instruction-following abilities of large multimodal models across a diverse set of 57 image generation tasks, utilizing both perception-centric and cognition-centric evaluation methods.",
        "tldr_zh": "该论文介绍了 OmniGenBench，一个全新的综合基准，用于评估大型多模态模型在包含57个图像生成任务的集合中，遵循指令的能力。该基准采用了感知中心和认知中心的评估方法。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
        "summary": "Video Diffusion Transformers (VDiTs) have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nattention acceleration methods leverage the sparsity of attention patterns to\nimprove efficiency; however, they often overlook inefficiencies of redundant\nlong-range interactions. To address this problem, we propose \\textbf{VORTA}, an\nacceleration framework with two novel components: 1) a sparse attention\nmechanism that efficiently captures long-range dependencies, and 2) a routing\nstrategy that adaptively replaces full 3D attention with specialized sparse\nattention variants throughout the sampling process. It achieves a $1.76\\times$\nend-to-end speedup without quality loss on VBench. Furthermore, VORTA can\nseamlessly integrate with various other acceleration methods, such as caching\nand step distillation, reaching up to $14.41\\times$ speedup with negligible\nperformance degradation. VORTA demonstrates its efficiency and enhances the\npracticality of VDiTs in real-world settings.",
        "url": "http://arxiv.org/abs/2505.18809v1",
        "published_date": "2025-05-24T17:46:47+00:00",
        "updated_date": "2025-05-24T17:46:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenhao Sun",
            "Rong-Cheng Tu",
            "Yifu Ding",
            "Zhao Jin",
            "Jingyi Liao",
            "Shunyu Liu",
            "Dacheng Tao"
        ],
        "tldr": "The paper introduces VORTA, an acceleration framework for Video Diffusion Transformers using sparse attention and adaptive routing, achieving significant speedups with minimal quality loss.",
        "tldr_zh": "该论文介绍了VORTA，一个通过稀疏注意力机制和自适应路由加速视频扩散Transformer的框架，在几乎不损失质量的前提下实现了显著的速度提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Rethinking Direct Preference Optimization in Diffusion Models",
        "summary": "Aligning text-to-image (T2I) diffusion models with human preferences has\nemerged as a critical research challenge. While recent advances in this area\nhave extended preference optimization techniques from large language models\n(LLMs) to the diffusion setting, they often struggle with limited exploration.\nIn this work, we propose a novel and orthogonal approach to enhancing\ndiffusion-based preference optimization. First, we introduce a stable reference\nmodel update strategy that relaxes the frozen reference model, encouraging\nexploration while maintaining a stable optimization anchor through reference\nmodel regularization. Second, we present a timestep-aware training strategy\nthat mitigates the reward scale imbalance problem across timesteps. Our method\ncan be integrated into various preference optimization algorithms. Experimental\nresults show that our approach improves the performance of state-of-the-art\nmethods on human preference evaluation benchmarks.",
        "url": "http://arxiv.org/abs/2505.18736v1",
        "published_date": "2025-05-24T15:14:45+00:00",
        "updated_date": "2025-05-24T15:14:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junyong Kang",
            "Seohyun Lim",
            "Kyungjune Baek",
            "Hyunjung Shim"
        ],
        "tldr": "This paper introduces a novel approach to improve preference optimization in text-to-image diffusion models by enhancing exploration and addressing reward scale imbalance. The method integrates into existing algorithms and shows improved performance on human preference benchmarks.",
        "tldr_zh": "本文提出了一种新颖的方法，通过增强探索和解决奖励规模不平衡问题，来改进文本到图像扩散模型中的偏好优化。该方法可以集成到现有算法中，并在人类偏好基准测试中表现出更高的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improved Immiscible Diffusion: Accelerate Diffusion Training by Reducing Its Miscibility",
        "summary": "The substantial training cost of diffusion models hinders their deployment.\nImmiscible Diffusion recently showed that reducing diffusion trajectory mixing\nin the noise space via linear assignment accelerates training by simplifying\ndenoising. To extend immiscible diffusion beyond the inefficient linear\nassignment under high batch sizes and high dimensions, we refine this concept\nto a broader miscibility reduction at any layer and by any implementation.\nSpecifically, we empirically demonstrate the bijective nature of the denoising\nprocess with respect to immiscible diffusion, ensuring its preservation of\ngenerative diversity. Moreover, we provide thorough analysis and show\nstep-by-step how immiscibility eases denoising and improves efficiency.\nExtending beyond linear assignment, we propose a family of implementations\nincluding K-nearest neighbor (KNN) noise selection and image scaling to reduce\nmiscibility, achieving up to >4x faster training across diverse models and\ntasks including unconditional/conditional generation, image editing, and\nrobotics planning. Furthermore, our analysis of immiscibility offers a novel\nperspective on how optimal transport (OT) enhances diffusion training. By\nidentifying trajectory miscibility as a fundamental bottleneck, we believe this\nwork establishes a potentially new direction for future research into\nhigh-efficiency diffusion training. The code is available at\nhttps://github.com/yhli123/Immiscible-Diffusion.",
        "url": "http://arxiv.org/abs/2505.18521v1",
        "published_date": "2025-05-24T05:38:35+00:00",
        "updated_date": "2025-05-24T05:38:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiheng Li",
            "Feng Liang",
            "Dan Kondratyuk",
            "Masayoshi Tomizuka",
            "Kurt Keutzer",
            "Chenfeng Xu"
        ],
        "tldr": "This paper introduces methods to reduce 'miscibility' in diffusion models, leading to faster training across various applications like image generation and robotics planning, with >4x speedups reported.",
        "tldr_zh": "本文介绍了一种减少扩散模型中“混溶性” (miscibility) 的方法，从而加快了在图像生成和机器人规划等各种应用中的训练速度，据报道速度提高了 4 倍以上。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]