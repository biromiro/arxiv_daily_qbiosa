[
    {
        "title": "HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation",
        "summary": "Customized video generation aims to produce videos featuring specific\nsubjects under flexible user-defined conditions, yet existing methods often\nstruggle with identity consistency and limited input modalities. In this paper,\nwe propose HunyuanCustom, a multi-modal customized video generation framework\nthat emphasizes subject consistency while supporting image, audio, video, and\ntext conditions. Built upon HunyuanVideo, our model first addresses the\nimage-text conditioned generation task by introducing a text-image fusion\nmodule based on LLaVA for enhanced multi-modal understanding, along with an\nimage ID enhancement module that leverages temporal concatenation to reinforce\nidentity features across frames. To enable audio- and video-conditioned\ngeneration, we further propose modality-specific condition injection\nmechanisms: an AudioNet module that achieves hierarchical alignment via spatial\ncross-attention, and a video-driven injection module that integrates\nlatent-compressed conditional video through a patchify-based feature-alignment\nnetwork. Extensive experiments on single- and multi-subject scenarios\ndemonstrate that HunyuanCustom significantly outperforms state-of-the-art open-\nand closed-source methods in terms of ID consistency, realism, and text-video\nalignment. Moreover, we validate its robustness across downstream tasks,\nincluding audio and video-driven customized video generation. Our results\nhighlight the effectiveness of multi-modal conditioning and identity-preserving\nstrategies in advancing controllable video generation. All the code and models\nare available at https://hunyuancustom.github.io.",
        "url": "http://arxiv.org/abs/2505.04512v1",
        "published_date": "2025-05-07T15:33:18+00:00",
        "updated_date": "2025-05-07T15:33:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Teng Hu",
            "Zhentao Yu",
            "Zhengguang Zhou",
            "Sen Liang",
            "Yuan Zhou",
            "Qin Lin",
            "Qinglin Lu"
        ],
        "tldr": "hunyuancustom is a multimodal video generation framework enhancing identity consistency and supporting image, audio, video, and text conditions, outperforming existing methods in multiple aspects.",
        "tldr_zh": "hunyuancustom是一个多模态视频生成框架，它增强了身份一致性，并支持图像、音频、视频和文本条件，在多个方面优于现有方法。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "On Path to Multimodal Generalist: General-Level and General-Bench",
        "summary": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/",
        "url": "http://arxiv.org/abs/2505.04620v1",
        "published_date": "2025-05-07T17:59:32+00:00",
        "updated_date": "2025-05-07T17:59:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Fei",
            "Yuan Zhou",
            "Juncheng Li",
            "Xiangtai Li",
            "Qingshan Xu",
            "Bobo Li",
            "Shengqiong Wu",
            "Yaoting Wang",
            "Junbao Zhou",
            "Jiahao Meng",
            "Qingyu Shi",
            "Zhiyuan Zhou",
            "Liangtao Shi",
            "Minghe Gao",
            "Daoan Zhang",
            "Zhiqi Ge",
            "Weiming Wu",
            "Siliang Tang",
            "Kaihang Pan",
            "Yaobo Ye",
            "Haobo Yuan",
            "Tao Zhang",
            "Tianjie Ju",
            "Zixiang Meng",
            "Shilin Xu",
            "Liyu Jia",
            "Wentao Hu",
            "Meng Luo",
            "Jiebo Luo",
            "Tat-Seng Chua",
            "Shuicheng Yan",
            "Hanwang Zhang"
        ],
        "tldr": "this paper introduces general-level, a 5-scale evaluation framework, and general-bench, a benchmark with over 700 tasks, to assess the progress of multimodal large language models (mllms) toward agi by measuring synergy across modalities and comprehension/generation capabilities. they evaluate 100 existing mllms, revealing capability rankings and challenges in achieving agi.",
        "tldr_zh": "本文提出了 general-level, 一个五尺度评估框架，以及 general-bench，一个包含超过700个任务的基准，旨在通过测量跨模态及理解/生成能力的协同作用，评估多模态大型语言模型 (mllm) 在实现 agi 方面的进展。他们评估了 100 个现有的 mllm，揭示了能力排名以及实现 agi 所面临的挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions Using Diffusion Model",
        "summary": "Generating 3D CT volumes from descriptive free-text inputs presents a\ntransformative opportunity in diagnostics and research. In this paper, we\nintroduce Text2CT, a novel approach for synthesizing 3D CT volumes from textual\ndescriptions using the diffusion model. Unlike previous methods that rely on\nfixed-format text input, Text2CT employs a novel prompt formulation that\nenables generation from diverse, free-text descriptions. The proposed framework\nencodes medical text into latent representations and decodes them into\nhigh-resolution 3D CT scans, effectively bridging the gap between semantic text\ninputs and detailed volumetric representations in a unified 3D framework. Our\nmethod demonstrates superior performance in preserving anatomical fidelity and\ncapturing intricate structures as described in the input text. Extensive\nevaluations show that our approach achieves state-of-the-art results, offering\npromising potential applications in diagnostics, and data augmentation.",
        "url": "http://arxiv.org/abs/2505.04522v1",
        "published_date": "2025-05-07T15:53:56+00:00",
        "updated_date": "2025-05-07T15:53:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pengfei Guo",
            "Can Zhao",
            "Dong Yang",
            "Yufan He",
            "Vishwesh Nath",
            "Ziyue Xu",
            "Pedro R. A. S. Bassi",
            "Zongwei Zhou",
            "Benjamin D. Simon",
            "Stephanie Anne Harmon",
            "Baris Turkbey",
            "Daguang Xu"
        ],
        "tldr": "the paper introduces text2ct, a diffusion model-based approach for generating 3d ct volumes directly from free-text medical descriptions, demonstrating state-of-the-art performance in anatomical fidelity and structural detail.",
        "tldr_zh": "该论文介绍了一种名为text2ct的方法，它使用扩散模型直接从自由文本医学描述生成三维ct体数据，并在解剖保真度和结构细节方面表现出最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Defining and Quantifying Creative Behavior in Popular Image Generators",
        "summary": "Creativity of generative AI models has been a subject of scientific debate in\nthe last years, without a conclusive answer. In this paper, we study creativity\nfrom a practical perspective and introduce quantitative measures that help the\nuser to choose a suitable AI model for a given task. We evaluated our measures\non a number of popular image-to-image generation models, and the results of\nthis suggest that our measures conform to human intuition.",
        "url": "http://arxiv.org/abs/2505.04497v1",
        "published_date": "2025-05-07T15:20:17+00:00",
        "updated_date": "2025-05-07T15:20:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.4.m; I.2.m"
        ],
        "authors": [
            "Aditi Ramaswamy"
        ],
        "tldr": "this paper introduces quantitative measures for assessing the creativity of image generation models and evaluates several popular models, finding the measures align with human perception.",
        "tldr_zh": "本文介绍了一种量化图像生成模型创造力的方法，并评估了几种流行的模型，发现该方法与人类感知相符。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Flow Matching using Latent Variables",
        "summary": "Flow matching models have shown great potential in image generation tasks\namong probabilistic generative models. Building upon the ideas of continuous\nnormalizing flows, flow matching models generalize the transport path of the\ndiffusion models from a simple prior distribution to the data. Most flow\nmatching models in the literature do not explicitly model the underlying\nstructure/manifold in the target data when learning the flow from a simple\nsource distribution like the standard Gaussian. This leads to inefficient\nlearning, especially for many high-dimensional real-world datasets, which often\nreside in a low-dimensional manifold. Existing strategies of incorporating\nmanifolds, including data with underlying multi-modal distribution, often\nrequire expensive training and hence frequently lead to suboptimal performance.\nTo this end, we present \\texttt{Latent-CFM}, which provides simplified\ntraining/inference strategies to incorporate multi-modal data structures using\npretrained deep latent variable models. Through experiments on multi-modal\nsynthetic data and widely used image benchmark datasets, we show that\n\\texttt{Latent-CFM} exhibits improved generation quality with significantly\nless training ($\\sim 50\\%$ less in some cases) and computation than\nstate-of-the-art flow matching models. Using a 2d Darcy flow dataset, we\ndemonstrate that our approach generates more physically accurate samples than\ncompetitive approaches. In addition, through latent space analysis, we\ndemonstrate that our approach can be used for conditional image generation\nconditioned on latent features.",
        "url": "http://arxiv.org/abs/2505.04486v1",
        "published_date": "2025-05-07T14:59:23+00:00",
        "updated_date": "2025-05-07T14:59:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Anirban Samaddar",
            "Yixuan Sun",
            "Viktor Nilsson",
            "Sandeep Madireddy"
        ],
        "tldr": "the paper introduces latent-cfm, a novel flow matching model that leverages latent variable models to improve generation quality and training efficiency for multi-modal data, achieving state-of-the-art results with significantly less computation.",
        "tldr_zh": "该论文介绍了latent-cfm，一种新颖的流匹配模型，它利用潜在变量模型来提高多模态数据的生成质量和训练效率，以显著降低的计算量实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RLMiniStyler: Light-weight RL Style Agent for Arbitrary Sequential Neural Style Generation",
        "summary": "Arbitrary style transfer aims to apply the style of any given artistic image\nto another content image. Still, existing deep learning-based methods often\nrequire significant computational costs to generate diverse stylized results.\nMotivated by this, we propose a novel reinforcement learning-based framework\nfor arbitrary style transfer RLMiniStyler. This framework leverages a unified\nreinforcement learning policy to iteratively guide the style transfer process\nby exploring and exploiting stylization feedback, generating smooth sequences\nof stylized results while achieving model lightweight. Furthermore, we\nintroduce an uncertainty-aware multi-task learning strategy that automatically\nadjusts loss weights to adapt to the content and style balance requirements at\ndifferent training stages, thereby accelerating model convergence. Through a\nseries of experiments across image various resolutions, we have validated the\nadvantages of RLMiniStyler over other state-of-the-art methods in generating\nhigh-quality, diverse artistic image sequences at a lower cost. Codes are\navailable at https://github.com/fengxiaoming520/RLMiniStyler.",
        "url": "http://arxiv.org/abs/2505.04424v1",
        "published_date": "2025-05-07T13:57:42+00:00",
        "updated_date": "2025-05-07T13:57:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jing Hu",
            "Chengming Feng",
            "Shu Hu",
            "Ming-Ching Chang",
            "Xin Li",
            "Xi Wu",
            "Xin Wang"
        ],
        "tldr": "the paper introduces rlministyler, a lightweight reinforcement learning framework for arbitrary style transfer that generates diverse, high-quality stylized image sequences at a lower computational cost.",
        "tldr_zh": "该论文介绍了 rlministyler，一种轻量级的强化学习框架，用于任意风格迁移，以较低的计算成本生成多样化、高质量的风格化图像序列。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "CountDiffusion: Text-to-Image Synthesis with Training-Free Counting-Guidance Diffusion",
        "summary": "Stable Diffusion has advanced text-to-image synthesis, but training models to\ngenerate images with accurate object quantity is still difficult due to the\nhigh computational cost and the challenge of teaching models the abstract\nconcept of quantity. In this paper, we propose CountDiffusion, a training-free\nframework aiming at generating images with correct object quantity from textual\ndescriptions. CountDiffusion consists of two stages. In the first stage, an\nintermediate denoising result is generated by the diffusion model to predict\nthe final synthesized image with one-step denoising, and a counting model is\nused to count the number of objects in this image. In the second stage, a\ncorrection module is used to correct the object quantity by changing the\nattention map of the object with universal guidance. The proposed\nCountDiffusion can be plugged into any diffusion-based text-to-image (T2I)\ngeneration models without further training. Experiment results demonstrate the\nsuperiority of our proposed CountDiffusion, which improves the accurate object\nquantity generation ability of T2I models by a large margin.",
        "url": "http://arxiv.org/abs/2505.04347v1",
        "published_date": "2025-05-07T11:47:35+00:00",
        "updated_date": "2025-05-07T11:47:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanyu Li",
            "Pencheng Wan",
            "Liang Han",
            "Yaowei Wang",
            "Liqiang Nie",
            "Min Zhang"
        ],
        "tldr": "the paper introduces countdiffusion, a training-free method for improving object quantity accuracy in text-to-image synthesis using diffusion models by incorporating a counting model and correction module.",
        "tldr_zh": "该论文介绍了countdiffusion，一种无需训练的方法，通过结合计数模型和校正模块，提高文本到图像合成中目标数量的准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-turn Consistent Image Editing",
        "summary": "Many real-world applications, such as interactive photo retouching, artistic\ncontent creation, and product design, require flexible and iterative image\nediting. However, existing image editing methods primarily focus on achieving\nthe desired modifications in a single step, which often struggles with\nambiguous user intent, complex transformations, or the need for progressive\nrefinements. As a result, these methods frequently produce inconsistent\noutcomes or fail to meet user expectations. To address these challenges, we\npropose a multi-turn image editing framework that enables users to iteratively\nrefine their edits, progressively achieving more satisfactory results. Our\napproach leverages flow matching for accurate image inversion and a\ndual-objective Linear Quadratic Regulators (LQR) for stable sampling,\neffectively mitigating error accumulation. Additionally, by analyzing the\nlayer-wise roles of transformers, we introduce a adaptive attention\nhighlighting method that enhances editability while preserving multi-turn\ncoherence. Extensive experiments demonstrate that our framework significantly\nimproves edit success rates and visual fidelity compared to existing methods.",
        "url": "http://arxiv.org/abs/2505.04320v1",
        "published_date": "2025-05-07T11:11:23+00:00",
        "updated_date": "2025-05-07T11:11:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zijun Zhou",
            "Yingying Deng",
            "Xiangyu He",
            "Weiming Dong",
            "Fan Tang"
        ],
        "tldr": "the paper introduces a multi-turn image editing framework that allows users to iteratively refine edits, leveraging flow matching, dual-objective lqr, and adaptive attention highlighting to improve edit success rates and visual fidelity.",
        "tldr_zh": "该论文介绍了一个多轮图像编辑框架，允许用户迭代地改进编辑，利用流动匹配、双目标lqr和自适应注意力突出显示来提高编辑成功率和视觉保真度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Person-In-Situ: Scene-Consistent Human Image Insertion with Occlusion-Aware Pose Control",
        "summary": "Compositing human figures into scene images has broad applications in areas\nsuch as entertainment and advertising. However, existing methods often cannot\nhandle occlusion of the inserted person by foreground objects and unnaturally\nplace the person in the frontmost layer. Moreover, they offer limited control\nover the inserted person's pose. To address these challenges, we propose two\nmethods. Both allow explicit pose control via a 3D body model and leverage\nlatent diffusion models to synthesize the person at a contextually appropriate\ndepth, naturally handling occlusions without requiring occlusion masks. The\nfirst is a two-stage approach: the model first learns a depth map of the scene\nwith the person through supervised learning, and then synthesizes the person\naccordingly. The second method learns occlusion implicitly and synthesizes the\nperson directly from input data without explicit depth supervision.\nQuantitative and qualitative evaluations show that both methods outperform\nexisting approaches by better preserving scene consistency while accurately\nreflecting occlusions and user-specified poses.",
        "url": "http://arxiv.org/abs/2505.04052v1",
        "published_date": "2025-05-07T01:47:15+00:00",
        "updated_date": "2025-05-07T01:47:15+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Shun Masuda",
            "Yuki Endo",
            "Yoshihiro Kanamori"
        ],
        "tldr": "this paper introduces two methods for realistically inserting human figures into scene images with occlusion handling and pose control using latent diffusion models, one with explicit depth learning and the other with implicit occlusion learning.",
        "tldr_zh": "本文介绍了两种方法，利用潜在扩散模型将人物图像逼真地插入到场景图像中，同时处理遮挡和姿势控制。一种方法采用显式深度学习，另一种方法采用隐式遮挡学习。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TerraFusion: Joint Generation of Terrain Geometry and Texture Using Latent Diffusion Models",
        "summary": "3D terrain models are essential in fields such as video game development and\nfilm production. Since surface color often correlates with terrain geometry,\ncapturing this relationship is crucial to achieving realism. However, most\nexisting methods generate either a heightmap or a texture, without sufficiently\naccounting for the inherent correlation. In this paper, we propose a method\nthat jointly generates terrain heightmaps and textures using a latent diffusion\nmodel. First, we train the model in an unsupervised manner to randomly generate\npaired heightmaps and textures. Then, we perform supervised learning of an\nexternal adapter to enable user control via hand-drawn sketches. Experiments\nshow that our approach allows intuitive terrain generation while preserving the\ncorrelation between heightmaps and textures.",
        "url": "http://arxiv.org/abs/2505.04050v1",
        "published_date": "2025-05-07T01:41:12+00:00",
        "updated_date": "2025-05-07T01:41:12+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Kazuki Higo",
            "Toshiki Kanai",
            "Yuki Endo",
            "Yoshihiro Kanamori"
        ],
        "tldr": "this paper introduces terrafusion, a latent diffusion model for jointly generating terrain heightmaps and textures, enabling user control through sketch-based input while preserving correlation between geometry and appearance.",
        "tldr_zh": "本文介绍 terrafusion，一种潜在扩散模型，用于联合生成地形高度图和纹理，通过基于草图的输入实现用户控制，同时保持几何形状和外观之间的相关性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning",
        "summary": "We present VideoPath-LLaVA, the first large multimodal model (LMM) in\ncomputational pathology that integrates three distinct image scenarios, single\npatch images, automatically keyframe-extracted clips, and manually segmented\nvideo pathology images, to mimic the natural diagnostic process of\npathologists. By generating detailed histological descriptions and culminating\nin a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives\nwith diagnostic reasoning.\n  Central to our approach is the VideoPath-Instruct dataset, comprising 4278\nvideo and diagnosis-specific chain-of-thought instructional pairs sourced from\neducational histopathology videos on YouTube. Although high-quality data is\ncritical for enhancing diagnostic reasoning, its creation is time-intensive and\nlimited in volume. To overcome this challenge, we transfer knowledge from\nexisting single-image instruction datasets to train on weakly annotated,\nkeyframe-extracted clips, followed by fine-tuning on manually segmented videos.\nVideoPath-LLaVA establishes a new benchmark in pathology video analysis and\noffers a promising foundation for future AI systems that support clinical\ndecision-making through integrated visual and diagnostic reasoning. Our code,\ndata, and model are publicly available at\nhttps://github.com/trinhvg/VideoPath-LLaVA.",
        "url": "http://arxiv.org/abs/2505.04192v1",
        "published_date": "2025-05-07T07:41:19+00:00",
        "updated_date": "2025-05-07T07:41:19+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Trinh T. L. Vuong",
            "Jin Tae Kwak"
        ],
        "tldr": "videopath-llava is a new large multimodal model for pathology diagnostic reasoning, trained on a novel dataset of histopathology videos and images, and offers a benchmark for ai in clinical decision-making.",
        "tldr_zh": "videopath-llava 是一个新的大型多模态模型，用于病理诊断推理，它使用一个新颖的组织病理学视频和图像数据集进行训练，并为临床决策中ai的应用提供了一个基准。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction",
        "summary": "Patient motion during medical image acquisition causes blurring, ghosting,\nand distorts organs, which makes image interpretation challenging.Current\nstate-of-the-art algorithms using Generative Adversarial Network (GAN)-based\nmethods with their ability to learn the mappings between corrupted images and\ntheir ground truth via Structural Similarity Index Measure (SSIM) loss\neffectively generate motion-free images. However, we identified the following\nlimitations: (i) they mainly focus on global structural characteristics and\ntherefore overlook localized features that often carry critical pathological\ninformation, and (ii) the SSIM loss function struggles to handle images with\nvarying pixel intensities, luminance factors, and variance. In this study, we\npropose Motion-Aware Image SYnthesis (MAISY) which initially characterize\nmotion and then uses it for correction by: (a) leveraging the foundation model\nSegment Anything Model (SAM), to dynamically learn spatial patterns along\nanatomical boundaries where motion artifacts are most pronounced and, (b)\nintroducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively\nemphasizes spatial regions with high pixel variance to preserve essential\nanatomical details during artifact correction. Experiments on chest and head CT\ndatasets demonstrate that our model outperformed the state-of-the-art\ncounterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by\n10%, and Dice by 16%.",
        "url": "http://arxiv.org/abs/2505.04105v1",
        "published_date": "2025-05-07T03:44:28+00:00",
        "updated_date": "2025-05-07T03:44:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Andrew Zhang",
            "Hao Wang",
            "Shuchang Ye",
            "Michael Fulham",
            "Jinman Kim"
        ],
        "tldr": "the paper introduces maisy, a gan-based method for medical image motion correction that leverages sam for spatial pattern learning and a variance-selective ssim loss for preserving anatomical details, outperforming existing methods on ct datasets.",
        "tldr_zh": "该论文介绍了一种名为maisy 的基于 gan 的医学图像运动校正方法，该方法利用 sam 进行空间模式学习，并使用方差选择性 ssim 损失来保留解剖细节，在 ct 数据集上优于现有方法。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Deep residual learning with product units",
        "summary": "We propose a deep product-unit residual neural network (PURe) that integrates\nproduct units into residual blocks to improve the expressiveness and parameter\nefficiency of deep convolutional networks. Unlike standard summation neurons,\nproduct units enable multiplicative feature interactions, potentially offering\na more powerful representation of complex patterns. PURe replaces conventional\nconvolutional layers with 2D product units in the second layer of each residual\nblock, eliminating nonlinear activation functions to preserve structural\ninformation. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS,\nPURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper\nResNet152, while converging nearly five times faster and demonstrating strong\nrobustness to Poisson noise. On ImageNet, PURe architectures outperform\nstandard ResNet models at similar depths, with PURe34 achieving a top-1\naccuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet\nvariants (ResNet50, ResNet101) while utilizing significantly fewer parameters\nand computational resources. On CIFAR-10, PURe consistently outperforms ResNet\nvariants across varying depths, with PURe272 reaching 95.01% test accuracy,\ncomparable to ResNet1001 but at less than half the model size. These results\ndemonstrate that PURe achieves a favorable balance between accuracy,\nefficiency, and robustness. Compared to traditional residual networks, PURe not\nonly achieves competitive classification performance with faster convergence\nand fewer parameters, but also demonstrates greater robustness to noise. Its\neffectiveness across diverse datasets highlights the potential of\nproduct-unit-based architectures for scalable and reliable deep learning in\ncomputer vision.",
        "url": "http://arxiv.org/abs/2505.04397v1",
        "published_date": "2025-05-07T13:21:25+00:00",
        "updated_date": "2025-05-07T13:21:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Ziyuan Li",
            "Uwe Jaekel",
            "Babette Dellen"
        ],
        "tldr": "the paper introduces pure, a deep residual network integrating product units for improved expressiveness and efficiency, validated on image classification benchmarks with promising results regarding accuracy, speed, and robustness.",
        "tldr_zh": "该论文介绍了pure，一种集成乘积单元的深度残差网络，旨在提高表达性和效率。在图像分类基准测试中验证了其在准确性、速度和鲁棒性方面的良好结果。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "A Weak Supervision Learning Approach Towards an Equitable Parking Lot Occupancy Estimation",
        "summary": "The scarcity and high cost of labeled high-resolution imagery have long\nchallenged remote sensing applications, particularly in low-income regions\nwhere high-resolution data are scarce. In this study, we propose a weak\nsupervision framework that estimates parking lot occupancy using 3m resolution\nsatellite imagery. By leveraging coarse temporal labels -- based on the\nassumption that parking lots of major supermarkets and hardware stores in\nGermany are typically full on Saturdays and empty on Sundays -- we train a\npairwise comparison model that achieves an AUC of 0.92 on large parking lots.\nThe proposed approach minimizes the reliance on expensive high-resolution\nimages and holds promise for scalable urban mobility analysis. Moreover, the\nmethod can be adapted to assess transit patterns and resource allocation in\nvulnerable communities, providing a data-driven basis to improve the well-being\nof those most in need.",
        "url": "http://arxiv.org/abs/2505.04229v1",
        "published_date": "2025-05-07T08:27:18+00:00",
        "updated_date": "2025-05-07T08:27:18+00:00",
        "categories": [
            "cs.CV",
            "cs.CY"
        ],
        "authors": [
            "Theophilus Aidoo",
            "Till Koebe",
            "Akansh Maurya",
            "Hewan Shrestha",
            "Ingmar Weber"
        ],
        "tldr": "this paper uses weak supervision (temporal assumptions about parking lot occupancy) to estimate parking lot occupancy from 3m satellite imagery, addressing data scarcity in remote sensing particularly relevant to low-income regions.",
        "tldr_zh": "该论文利用弱监督（关于停车场占用情况的时间假设）从3米卫星图像中估计停车场占用率，解决了遥感数据稀缺问题，尤其与低收入地区相关。",
        "relevance_score": 1,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 3
    }
]