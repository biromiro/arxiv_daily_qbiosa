[
    {
        "title": "$\\nabla$NABLA: Neighborhood Adaptive Block-Level Attention",
        "summary": "Recent progress in transformer-based architectures has demonstrated\nremarkable success in video generation tasks. However, the quadratic complexity\nof full attention mechanisms remains a critical bottleneck, particularly for\nhigh-resolution and long-duration video sequences. In this paper, we propose\nNABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that\ndynamically adapts to sparsity patterns in video diffusion transformers (DiTs).\nBy leveraging block-wise attention with adaptive sparsity-driven threshold,\nNABLA reduces computational overhead while preserving generative quality. Our\nmethod does not require custom low-level operator design and can be seamlessly\nintegrated with PyTorch's Flex Attention operator. Experiments demonstrate that\nNABLA achieves up to 2.7x faster training and inference compared to baseline\nalmost without compromising quantitative metrics (CLIP score, VBench score,\nhuman evaluation score) and visual quality drop. The code and model weights are\navailable here: https://github.com/gen-ai-team/Wan2.1-NABLA",
        "url": "http://arxiv.org/abs/2507.13546v1",
        "published_date": "2025-07-17T21:36:36+00:00",
        "updated_date": "2025-07-17T21:36:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dmitrii Mikhailov",
            "Aleksey Letunovskiy",
            "Maria Kovaleva",
            "Vladimir Arkhipkin",
            "Vladimir Korviakov",
            "Vladimir Polovnikov",
            "Viacheslav Vasilev",
            "Evelina Sidorova",
            "Denis Dimitrov"
        ],
        "tldr": "The paper introduces NABLA, a novel attention mechanism for video diffusion transformers that uses adaptive block-level sparsity to reduce computational cost while maintaining generative quality, achieving up to 2.7x speedup.",
        "tldr_zh": "该论文介绍了NABLA，一种用于视频扩散Transformer的新型注意力机制，它采用自适应块级稀疏性来降低计算成本，同时保持生成质量，速度提升高达2.7倍。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]