[
    {
        "title": "Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective",
        "summary": "Autoregressive large language models (LLMs) have unified a vast range of\nlanguage tasks, inspiring preliminary efforts in autoregressive video\ngeneration. Existing autoregressive video generators either diverge from\nstandard LLM architectures, depend on bulky external text encoders, or incur\nprohibitive latency due to next-token decoding. In this paper, we introduce\nLumos-1, an autoregressive video generator that retains the LLM architecture\nwith minimal architectural modifications. To inject spatiotemporal correlations\nin LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its\nimbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE\nscheme that preserves the original textual RoPE while providing comprehensive\nfrequency spectra and scaled 3D positions for modeling multimodal\nspatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy\nthat obeys intra-frame bidirectionality and inter-frame temporal causality.\nBased on this dependency strategy, we identify the issue of frame-wise loss\nimbalance caused by spatial information redundancy and solve it by proposing\nAutoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal\ntube masking during training with a compatible inference-time masking policy to\navoid quality degradation. By using memory-efficient training techniques, we\npre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on\nGenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code\nand models are available at https://github.com/alibaba-damo-academy/Lumos.",
        "url": "http://arxiv.org/abs/2507.08801v1",
        "published_date": "2025-07-11T17:59:42+00:00",
        "updated_date": "2025-07-11T17:59:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Hangjie Yuan",
            "Weihua Chen",
            "Jun Cen",
            "Hu Yu",
            "Jingyun Liang",
            "Shuning Chang",
            "Zhihui Lin",
            "Tao Feng",
            "Pengwei Liu",
            "Jiazheng Xing",
            "Hao Luo",
            "Jiasheng Tang",
            "Fan Wang",
            "Yi Yang"
        ],
        "tldr": "Lumos-1 is an autoregressive video generator built upon LLM architecture with modifications like MM-RoPE and AR-DF to improve spatiotemporal modeling and training efficiency, achieving competitive performance with fewer resources.",
        "tldr_zh": "Lumos-1是一个基于LLM架构的自回归视频生成器，通过MM-RoPE和AR-DF等改进来提高时空建模和训练效率，并以更少的资源实现了有竞争力的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Vision Foundation Models as Effective Visual Tokenizers for Autoregressive Image Generation",
        "summary": "Leveraging the powerful representations of pre-trained vision foundation\nmodels -- traditionally used for visual comprehension -- we explore a novel\ndirection: building an image tokenizer directly atop such models, a largely\nunderexplored area. Specifically, we employ a frozen vision foundation model as\nthe encoder of our tokenizer. To enhance its effectiveness, we introduce two\nkey components: (1) a region-adaptive quantization framework that reduces\nredundancy in the pre-trained features on regular 2D grids, and (2) a semantic\nreconstruction objective that aligns the tokenizer's outputs with the\nfoundation model's representations to preserve semantic fidelity. Based on\nthese designs, our proposed image tokenizer, VFMTok, achieves substantial\nimprovements in image reconstruction and generation quality, while also\nenhancing token efficiency. It further boosts autoregressive (AR) generation --\nachieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model\nconvergence by three times, and enabling high-fidelity class-conditional\nsynthesis without the need for classifier-free guidance (CFG). The code will be\nreleased publicly to benefit the community.",
        "url": "http://arxiv.org/abs/2507.08441v1",
        "published_date": "2025-07-11T09:32:45+00:00",
        "updated_date": "2025-07-11T09:32:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Anlin Zheng",
            "Xin Wen",
            "Xuanyang Zhang",
            "Chuofan Ma",
            "Tiancai Wang",
            "Gang Yu",
            "Xiangyu Zhang",
            "Xiaojuan Qi"
        ],
        "tldr": "The paper introduces VFMTok, a novel image tokenizer built upon pre-trained vision foundation models, achieving substantial improvements in image generation quality and token efficiency, especially in autoregressive generation.",
        "tldr_zh": "该论文介绍了VFMTok，一种基于预训练视觉基础模型的新型图像标记器，在图像生成质量和标记效率方面取得了显著提升，尤其是在自回归生成方面。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers",
        "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
        "url": "http://arxiv.org/abs/2507.08422v1",
        "published_date": "2025-07-11T09:07:43+00:00",
        "updated_date": "2025-07-11T09:07:43+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Wongi Jeong",
            "Kyungryeol Lee",
            "Hoigi Seo",
            "Se Young Chun"
        ],
        "tldr": "This paper introduces Region-Adaptive Latent Upsampling (RALU), a training-free method to accelerate diffusion transformer inference by performing mixed-resolution sampling to reduce computational cost with minimal image quality degradation. It shows significant speed-ups on FLUX and Stable Diffusion 3 and is compatible with existing temporal acceleration techniques.",
        "tldr_zh": "本文介绍了一种名为区域自适应潜在上采样（RALU）的免训练方法，通过执行混合分辨率采样来加速扩散 Transformer 的推理，从而在最小图像质量损失的情况下降低计算成本。它在 FLUX 和 Stable Diffusion 3 上显示出显著的加速效果，并且与现有的时间加速技术兼容。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]