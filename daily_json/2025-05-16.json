[
    {
        "title": "End-to-End Vision Tokenizer Tuning",
        "summary": "Existing vision tokenization isolates the optimization of vision tokenizers\nfrom downstream training, implicitly assuming the visual tokens can generalize\nwell across various tasks, e.g., image generation and visual question\nanswering. The vision tokenizer optimized for low-level reconstruction is\nagnostic to downstream tasks requiring varied representations and semantics.\nThis decoupled paradigm introduces a critical misalignment: The loss of the\nvision tokenization can be the representation bottleneck for target tasks. For\nexample, errors in tokenizing text in a given image lead to poor results when\nrecognizing or generating them. To address this, we propose ETT, an end-to-end\nvision tokenizer tuning approach that enables joint optimization between vision\ntokenization and target autoregressive tasks. Unlike prior autoregressive\nmodels that use only discrete indices from a frozen vision tokenizer, ETT\nleverages the visual embeddings of the tokenizer codebook, and optimizes the\nvision tokenizers end-to-end with both reconstruction and caption objectives.\nETT can be seamlessly integrated into existing training pipelines with minimal\narchitecture modifications. Our ETT is simple to implement and integrate,\nwithout the need to adjust the original codebooks or architectures of the\nemployed large language models. Extensive experiments demonstrate that our\nproposed end-to-end vision tokenizer tuning unlocks significant performance\ngains, i.e., 2-6% for multimodal understanding and visual generation tasks\ncompared to frozen tokenizer baselines, while preserving the original\nreconstruction capability. We hope this very simple and strong method can\nempower multimodal foundation models besides image generation and\nunderstanding.",
        "url": "http://arxiv.org/abs/2505.10562v1",
        "published_date": "2025-05-15T17:59:39+00:00",
        "updated_date": "2025-05-15T17:59:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenxuan Wang",
            "Fan Zhang",
            "Yufeng Cui",
            "Haiwen Diao",
            "Zhuoyan Luo",
            "Huchuan Lu",
            "Jing Liu",
            "Xinlong Wang"
        ],
        "tldr": "The paper presents ETT, a method for end-to-end tuning of vision tokenizers jointly with downstream autoregressive tasks, improving performance in multimodal understanding and visual generation by addressing the misalignment between tokenizer optimization and task-specific requirements.",
        "tldr_zh": "该论文提出了一种名为ETT的端到端视觉令牌器调整方法，该方法将视觉令牌器的优化与下游自回归任务联合进行，通过解决令牌器优化和任务特定需求之间的不匹配，从而提高多模态理解和视觉生成方面的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis",
        "summary": "This paper does not describe a new method; instead, it provides a thorough\nexploration of an important yet understudied design space related to recent\nadvances in text-to-image synthesis -- specifically, the deep fusion of large\nlanguage models (LLMs) and diffusion transformers (DiTs) for multi-modal\ngeneration. Previous studies mainly focused on overall system performance\nrather than detailed comparisons with alternative methods, and key design\ndetails and training recipes were often left undisclosed. These gaps create\nuncertainty about the real potential of this approach. To fill these gaps, we\nconduct an empirical study on text-to-image generation, performing controlled\ncomparisons with established baselines, analyzing important design choices, and\nproviding a clear, reproducible recipe for training at scale. We hope this work\noffers meaningful data points and practical guidelines for future research in\nmulti-modal generation.",
        "url": "http://arxiv.org/abs/2505.10046v1",
        "published_date": "2025-05-15T07:43:23+00:00",
        "updated_date": "2025-05-15T07:43:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bingda Tang",
            "Boyang Zheng",
            "Xichen Pan",
            "Sayak Paul",
            "Saining Xie"
        ],
        "tldr": "This paper explores the design space of deeply fusing LLMs and Diffusion Transformers for Text-to-Image synthesis, providing controlled comparisons, design analysis, and a reproducible training recipe to address gaps in previous research.",
        "tldr_zh": "本文探索了深度融合大型语言模型和扩散Transformer用于文本到图像合成的设计空间，通过受控比较、设计分析和可重复的训练配方，旨在填补以往研究中的空白。",
        "relevance_score": 9,
        "novelty_claim_score": 5,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]