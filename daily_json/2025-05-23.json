[
    {
        "title": "LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning",
        "summary": "In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large\nLanguage Model (MLLM) that integrates visual instruction tuning with masked\ndiffusion models, representing a departure from the autoregressive paradigms\ndominant in current multimodal approaches. Built upon LLaDA, a representative\nlarge language diffusion model, LLaDA-V incorporates a vision encoder and MLP\nconnector that projects visual features into the language embedding space,\nenabling effective multimodal alignment. Our empirical investigation reveals\nseveral intriguing results: First, LLaDA-V demonstrates promising multimodal\nperformance despite its language model being weaker on purely textual tasks\nthan counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same\ninstruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal\ntasks with better data scalability. It also narrows the performance gap to\nQwen2-VL, suggesting the effectiveness of its architecture for multimodal\ntasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal\nunderstanding compared to existing hybrid autoregressive-diffusion and purely\ndiffusion-based MLLMs. Our findings suggest that large language diffusion\nmodels show promise in multimodal contexts and warrant further investigation in\nfuture research. Project page and codes:\nhttps://ml-gsai.github.io/LLaDA-V-demo/.",
        "url": "http://arxiv.org/abs/2505.16933v1",
        "published_date": "2025-05-22T17:23:26+00:00",
        "updated_date": "2025-05-22T17:23:26+00:00",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Zebin You",
            "Shen Nie",
            "Xiaolu Zhang",
            "Jun Hu",
            "Jun Zhou",
            "Zhiwu Lu",
            "Ji-Rong Wen",
            "Chongxuan Li"
        ],
        "tldr": "LLaDA-V is a novel, purely diffusion-based multimodal large language model that achieves competitive or state-of-the-art performance on various multimodal tasks, demonstrating the potential of diffusion models in this domain. It outperforms other diffusion-based and hybrid approaches.",
        "tldr_zh": "LLaDA-V是一种新颖的、纯粹的基于扩散的多模态大型语言模型，在各种多模态任务上实现了有竞争力甚至是最先进的性能，展示了扩散模型在该领域的潜力。 它优于其他基于扩散和混合的方法。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Training-Free Efficient Video Generation via Dynamic Token Carving",
        "summary": "Despite the remarkable generation quality of video Diffusion Transformer\n(DiT) models, their practical deployment is severely hindered by extensive\ncomputational requirements. This inefficiency stems from two key challenges:\nthe quadratic complexity of self-attention with respect to token length and the\nmulti-step nature of diffusion models. To address these limitations, we present\nJenga, a novel inference pipeline that combines dynamic attention carving with\nprogressive resolution generation. Our approach leverages two key insights: (1)\nearly denoising steps do not require high-resolution latents, and (2) later\nsteps do not require dense attention. Jenga introduces a block-wise attention\nmechanism that dynamically selects relevant token interactions using 3D\nspace-filling curves, alongside a progressive resolution strategy that\ngradually increases latent resolution during generation. Experimental results\ndemonstrate that Jenga achieves substantial speedups across multiple\nstate-of-the-art video diffusion models while maintaining comparable generation\nquality (8.83$\\times$ speedup with 0.01\\% performance drop on VBench). As a\nplug-and-play solution, Jenga enables practical, high-quality video generation\non modern hardware by reducing inference time from minutes to seconds --\nwithout requiring model retraining. Code:\nhttps://github.com/dvlab-research/Jenga",
        "url": "http://arxiv.org/abs/2505.16864v1",
        "published_date": "2025-05-22T16:21:32+00:00",
        "updated_date": "2025-05-22T16:21:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuechen Zhang",
            "Jinbo Xing",
            "Bin Xia",
            "Shaoteng Liu",
            "Bohao Peng",
            "Xin Tao",
            "Pengfei Wan",
            "Eric Lo",
            "Jiaya Jia"
        ],
        "tldr": "The paper introduces Jenga, a training-free inference pipeline that accelerates video diffusion models by dynamically carving attention and progressively increasing resolution, achieving significant speedups with minimal quality loss.",
        "tldr_zh": "该论文介绍了一种名为Jenga的无需训练的推理流程，通过动态地雕刻注意力并逐步提高分辨率来加速视频扩散模型，在仅损失极少质量的情况下实现了显著的加速。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "TensorAR: Refinement is All You Need in Autoregressive Image Generation",
        "summary": "Autoregressive (AR) image generators offer a language-model-friendly approach\nto image generation by predicting discrete image tokens in a causal sequence.\nHowever, unlike diffusion models, AR models lack a mechanism to refine previous\npredictions, limiting their generation quality. In this paper, we introduce\nTensorAR, a new AR paradigm that reformulates image generation from next-token\nprediction to next-tensor prediction. By generating overlapping windows of\nimage patches (tensors) in a sliding fashion, TensorAR enables iterative\nrefinement of previously generated content. To prevent information leakage\nduring training, we propose a discrete tensor noising scheme, which perturbs\ninput tokens via codebook-indexed noise. TensorAR is implemented as a\nplug-and-play module compatible with existing AR models. Extensive experiments\non LlamaGEN, Open-MAGVIT2, and RAR demonstrate that TensorAR significantly\nimproves the generation performance of autoregressive models.",
        "url": "http://arxiv.org/abs/2505.16324v1",
        "published_date": "2025-05-22T07:27:25+00:00",
        "updated_date": "2025-05-22T07:27:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Cheng Cheng",
            "Lin Song",
            "Yicheng Xiao",
            "Yuxin Chen",
            "Xuchong Zhang",
            "Hongbin Sun",
            "Ying Shan"
        ],
        "tldr": "The paper introduces TensorAR, a novel autoregressive image generation paradigm that uses overlapping tensors and a discrete noising scheme to enable iterative refinement, significantly improving performance compared to existing AR models.",
        "tldr_zh": "该论文介绍了TensorAR，一种新的自回归图像生成范式，它使用重叠张量和离散噪声方案来实现迭代细化，与现有的AR模型相比，显著提高了性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning",
        "summary": "Visual generation models have made remarkable progress in creating realistic\nimages from text prompts, yet struggle with complex prompts that specify\nmultiple objects with precise spatial relationships and attributes. Effective\nhandling of such prompts requires explicit reasoning about the semantic content\nand spatial layout. We present GoT-R1, a framework that applies reinforcement\nlearning to enhance semantic-spatial reasoning in visual generation. Building\nupon the Generation Chain-of-Thought approach, GoT-R1 enables models to\nautonomously discover effective reasoning strategies beyond predefined\ntemplates through carefully designed reinforcement learning. To achieve this,\nwe propose a dual-stage multi-dimensional reward framework that leverages MLLMs\nto evaluate both the reasoning process and final output, enabling effective\nsupervision across the entire generation pipeline. The reward system assesses\nsemantic alignment, spatial accuracy, and visual quality in a unified approach.\nExperimental results demonstrate significant improvements on T2I-CompBench\nbenchmark, particularly in compositional tasks involving precise spatial\nrelationships and attribute binding. GoT-R1 advances the state-of-the-art in\nimage generation by successfully transferring sophisticated reasoning\ncapabilities to the visual generation domain. To facilitate future research, we\nmake our code and pretrained models publicly available at\nhttps://github.com/gogoduan/GoT-R1.",
        "url": "http://arxiv.org/abs/2505.17022v1",
        "published_date": "2025-05-22T17:59:58+00:00",
        "updated_date": "2025-05-22T17:59:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG",
            "cs.MM"
        ],
        "authors": [
            "Chengqi Duan",
            "Rongyao Fang",
            "Yuqing Wang",
            "Kun Wang",
            "Linjiang Huang",
            "Xingyu Zeng",
            "Hongsheng Li",
            "Xihui Liu"
        ],
        "tldr": "The paper introduces GoT-R1, a reinforcement learning framework that enhances semantic-spatial reasoning in visual generation models, using MLLMs for reward evaluation and achieving significant improvements in compositional image generation tasks.",
        "tldr_zh": "该论文介绍了GoT-R1，一个强化学习框架，通过使用MLLM进行奖励评估，增强视觉生成模型中的语义-空间推理能力，并在组合图像生成任务中取得了显著改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training",
        "summary": "Diffusion Transformers (DiTs) deliver state-of-the-art image quality, yet\ntheir training remains notoriously slow. A recent remedy -- representation\nalignment (REPA) that matches DiT hidden features to those of a non-generative\nteacher (e.g. DINO) -- dramatically accelerates the early epochs but plateaus\nor even degrades performance later. We trace this failure to a capacity\nmismatch: once the generative student begins modelling the joint data\ndistribution, the teacher's lower-dimensional embeddings and attention patterns\nbecome a straitjacket rather than a guide. We then introduce HASTE (Holistic\nAlignment with Stage-wise Termination for Efficient training), a two-phase\nschedule that keeps the help and drops the hindrance. Phase I applies a\nholistic alignment loss that simultaneously distills attention maps (relational\npriors) and feature projections (semantic anchors) from the teacher into\nmid-level layers of the DiT, yielding rapid convergence. Phase II then performs\none-shot termination that deactivates the alignment loss, once a simple trigger\nsuch as a fixed iteration is hit, freeing the DiT to focus on denoising and\nexploit its generative capacity. HASTE speeds up training of diverse DiTs\nwithout architecture changes. On ImageNet 256X256, it reaches the vanilla\nSiT-XL/2 baseline FID in 50 epochs and matches REPA's best FID in 500 epochs,\namounting to a 28X reduction in optimization steps. HASTE also improves\ntext-to-image DiTs on MS-COCO, demonstrating to be a simple yet principled\nrecipe for efficient diffusion training across various tasks. Our code is\navailable at https://github.com/NUS-HPC-AI-Lab/HASTE .",
        "url": "http://arxiv.org/abs/2505.16792v1",
        "published_date": "2025-05-22T15:34:33+00:00",
        "updated_date": "2025-05-22T15:34:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ziqiao Wang",
            "Wangbo Zhao",
            "Yuhao Zhou",
            "Zekai Li",
            "Zhiyuan Liang",
            "Mingjia Shi",
            "Xuanlei Zhao",
            "Pengfei Zhou",
            "Kaipeng Zhang",
            "Zhangyang Wang",
            "Kai Wang",
            "Yang You"
        ],
        "tldr": "The paper introduces HASTE, a two-phase training schedule for Diffusion Transformers that significantly accelerates training by using representation alignment with staged termination, addressing the limitations of existing methods like REPA.",
        "tldr_zh": "该论文介绍了一种名为HASTE的两阶段训练方案，用于扩散Transformer模型，通过分阶段终止的表示对齐显著加速训练，解决了现有方法（如REPA）的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]