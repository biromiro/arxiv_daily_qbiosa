[
    {
        "title": "Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization",
        "summary": "We introduce Tinker, a versatile framework for high-fidelity 3D editing that\noperates in both one-shot and few-shot regimes without any per-scene\nfinetuning. Unlike prior techniques that demand extensive per-scene\noptimization to ensure multi-view consistency or to produce dozens of\nconsistent edited input views, Tinker delivers robust, multi-view consistent\nedits from as few as one or two images. This capability stems from repurposing\npretrained diffusion models, which unlocks their latent 3D awareness. To drive\nresearch in this space, we curate the first large-scale multi-view editing\ndataset and data pipeline, spanning diverse scenes and styles. Building on this\ndataset, we develop our framework capable of generating multi-view consistent\nedited views without per-scene training, which consists of two novel\ncomponents: (1) Referring multi-view editor: Enables precise, reference-driven\nedits that remain coherent across all viewpoints. (2) Any-view-to-video\nsynthesizer: Leverages spatial-temporal priors from video diffusion to perform\nhigh-quality scene completion and novel-view generation even from sparse\ninputs. Through extensive experiments, Tinker significantly reduces the barrier\nto generalizable 3D content creation, achieving state-of-the-art performance on\nediting, novel-view synthesis, and rendering enhancement tasks. We believe that\nTinker represents a key step towards truly scalable, zero-shot 3D editing.\nProject webpage: https://aim-uofa.github.io/Tinker",
        "url": "http://arxiv.org/abs/2508.14811v1",
        "published_date": "2025-08-20T16:02:59+00:00",
        "updated_date": "2025-08-20T16:02:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Canyu Zhao",
            "Xiaoman Li",
            "Tianjian Feng",
            "Zhiyue Zhao",
            "Hao Chen",
            "Chunhua Shen"
        ],
        "tldr": "Tinker is a novel framework for multi-view consistent 3D editing from sparse inputs using pretrained diffusion models, achieving state-of-the-art performance without per-scene optimization, and they provide a new dataset for this task.",
        "tldr_zh": "Tinker是一个新颖的框架，它利用预训练的扩散模型，能够从稀疏输入中实现多视角一致的3D编辑，无需对每个场景进行优化，并达到了最先进的性能。他们为此任务提供了一个新的数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Squeezed Diffusion Models",
        "summary": "Diffusion models typically inject isotropic Gaussian noise, disregarding\nstructure in the data. Motivated by the way quantum squeezed states\nredistribute uncertainty according to the Heisenberg uncertainty principle, we\nintroduce Squeezed Diffusion Models (SDM), which scale noise anisotropically\nalong the principal component of the training distribution. As squeezing\nenhances the signal-to-noise ratio in physics, we hypothesize that scaling\nnoise in a data-dependent manner can better assist diffusion models in learning\nimportant data features. We study two configurations: (i) a Heisenberg\ndiffusion model that compensates the scaling on the principal axis with inverse\nscaling on orthogonal directions and (ii) a standard SDM variant that scales\nonly the principal axis. Counterintuitively, on CIFAR-10/100 and CelebA-64,\nmild antisqueezing - i.e. increasing variance on the principal axis -\nconsistently improves FID by up to 15% and shifts the precision-recall frontier\ntoward higher recall. Our results demonstrate that simple, data-aware noise\nshaping can deliver robust generative gains without architectural changes.",
        "url": "http://arxiv.org/abs/2508.14871v1",
        "published_date": "2025-08-20T17:37:53+00:00",
        "updated_date": "2025-08-20T17:37:53+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Jyotirmai Singh",
            "Samar Khanna",
            "James Burgess"
        ],
        "tldr": "The paper introduces Squeezed Diffusion Models (SDM), which anisotropically scale noise along the principal component of the training distribution. Counterintuitively, increasing variance on the principal axis improves generative performance.",
        "tldr_zh": "该论文介绍了挤压扩散模型（SDM），它能够沿训练分布的主成分各向异性地缩放噪声。与直觉相反，增加主轴上的方差可以提高生成性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "TransLight: Image-Guided Customized Lighting Control with Generative Decoupling",
        "summary": "Most existing illumination-editing approaches fail to simultaneously provide\ncustomized control of light effects and preserve content integrity. This makes\nthem less effective for practical lighting stylization requirements, especially\nin the challenging task of transferring complex light effects from a reference\nimage to a user-specified target image. To address this problem, we propose\nTransLight, a novel framework that enables high-fidelity and high-freedom\ntransfer of light effects. Extracting the light effect from the reference image\nis the most critical and challenging step in our method. The difficulty lies in\nthe complex geometric structure features embedded in light effects that are\nhighly coupled with content in real-world scenarios. To achieve this, we first\npresent Generative Decoupling, where two fine-tuned diffusion models are used\nto accurately separate image content and light effects, generating a newly\ncurated, million-scale dataset of image-content-light triplets. Then, we employ\nIC-Light as the generative model and train our model with our triplets,\ninjecting the reference lighting image as an additional conditioning signal.\nThe resulting TransLight model enables customized and natural transfer of\ndiverse light effects. Notably, by thoroughly disentangling light effects from\nreference images, our generative decoupling strategy endows TransLight with\nhighly flexible illumination control. Experimental results establish TransLight\nas the first method to successfully transfer light effects across disparate\nimages, delivering more customized illumination control than existing\ntechniques and charting new directions for research in illumination\nharmonization and editing.",
        "url": "http://arxiv.org/abs/2508.14814v1",
        "published_date": "2025-08-20T16:05:12+00:00",
        "updated_date": "2025-08-20T16:05:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zongming Li",
            "Lianghui Zhu",
            "Haocheng Shen",
            "Longjin Ran",
            "Wenyu Liu",
            "Xinggang Wang"
        ],
        "tldr": "The paper introduces TransLight, a framework for transferring light effects between images with customized control, achieved by decoupling content and lighting using fine-tuned diffusion models and a curated dataset.",
        "tldr_zh": "该论文介绍了TransLight，一个通过使用微调的扩散模型和一个精心策划的数据集来解耦内容和光照，从而实现图像之间光照效果转移并进行定制控制的框架。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AnchorSync: Global Consistency Optimization for Long Video Editing",
        "summary": "Editing long videos remains a challenging task due to the need for\nmaintaining both global consistency and temporal coherence across thousands of\nframes. Existing methods often suffer from structural drift or temporal\nartifacts, particularly in minute-long sequences. We introduce AnchorSync, a\nnovel diffusion-based framework that enables high-quality, long-term video\nediting by decoupling the task into sparse anchor frame editing and smooth\nintermediate frame interpolation. Our approach enforces structural consistency\nthrough a progressive denoising process and preserves temporal dynamics via\nmultimodal guidance. Extensive experiments show that AnchorSync produces\ncoherent, high-fidelity edits, surpassing prior methods in visual quality and\ntemporal stability.",
        "url": "http://arxiv.org/abs/2508.14609v1",
        "published_date": "2025-08-20T10:51:24+00:00",
        "updated_date": "2025-08-20T10:51:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zichi Liu",
            "Yinggui Wang",
            "Tao Wei",
            "Chao Ma"
        ],
        "tldr": "AnchorSync is a diffusion-based video editing framework that improves long video editing by decoupling it into sparse anchor frame editing and intermediate frame interpolation, enhancing visual quality and temporal stability.",
        "tldr_zh": "AnchorSync是一个基于扩散模型的视频编辑框架，通过将视频编辑任务分解为稀疏锚帧编辑和中间帧插值，从而改善长视频编辑效果，提高视觉质量和时间稳定性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SATURN: Autoregressive Image Generation Guided by Scene Graphs",
        "summary": "State-of-the-art text-to-image models excel at photorealistic rendering but\noften struggle to capture the layout and object relationships implied by\ncomplex prompts. Scene graphs provide a natural structural prior, yet previous\ngraph-guided approaches have typically relied on heavy GAN or diffusion\npipelines, which lag behind modern autoregressive architectures in both speed\nand fidelity. We introduce SATURN (Structured Arrangement of Triplets for\nUnified Rendering Networks), a lightweight extension to VAR-CLIP that\ntranslates a scene graph into a salience-ordered token sequence, enabling a\nfrozen CLIP-VQ-VAE backbone to interpret graph structure while fine-tuning only\nthe VAR transformer. On the Visual Genome dataset, SATURN reduces FID from\n56.45% to 21.62% and increases the Inception Score from 16.03 to 24.78,\noutperforming prior methods such as SG2IM and SGDiff without requiring extra\nmodules or multi-stage training. Qualitative results further confirm\nimprovements in object count fidelity and spatial relation accuracy, showing\nthat SATURN effectively combines structural awareness with state-of-the-art\nautoregressive fidelity.",
        "url": "http://arxiv.org/abs/2508.14502v1",
        "published_date": "2025-08-20T07:45:08+00:00",
        "updated_date": "2025-08-20T07:45:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Thanh-Nhan Vo",
            "Trong-Thuan Nguyen",
            "Tam V. Nguyen",
            "Minh-Triet Tran"
        ],
        "tldr": "SATURN is a lightweight extension to VAR-CLIP that uses scene graphs to guide autoregressive image generation, achieving improved FID and Inception Score on Visual Genome compared to previous scene graph-guided methods.",
        "tldr_zh": "SATURN是一种VAR-CLIP的轻量级扩展，它使用场景图来引导自回归图像生成，与之前的场景图引导方法相比，在Visual Genome上实现了改进的FID和Inception Score。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MUSE: Multi-Subject Unified Synthesis via Explicit Layout Semantic Expansion",
        "summary": "Existing text-to-image diffusion models have demonstrated remarkable\ncapabilities in generating high-quality images guided by textual prompts.\nHowever, achieving multi-subject compositional synthesis with precise spatial\ncontrol remains a significant challenge. In this work, we address the task of\nlayout-controllable multi-subject synthesis (LMS), which requires both faithful\nreconstruction of reference subjects and their accurate placement in specified\nregions within a unified image. While recent advancements have separately\nimproved layout control and subject synthesis, existing approaches struggle to\nsimultaneously satisfy the dual requirements of spatial precision and identity\npreservation in this composite task. To bridge this gap, we propose MUSE, a\nunified synthesis framework that employs concatenated cross-attention (CCA) to\nseamlessly integrate layout specifications with textual guidance through\nexplicit semantic space expansion. The proposed CCA mechanism enables\nbidirectional modality alignment between spatial constraints and textual\ndescriptions without interference. Furthermore, we design a progressive\ntwo-stage training strategy that decomposes the LMS task into learnable\nsub-objectives for effective optimization. Extensive experiments demonstrate\nthat MUSE achieves zero-shot end-to-end generation with superior spatial\naccuracy and identity consistency compared to existing solutions, advancing the\nfrontier of controllable image synthesis. Our code and model are available at\nhttps://github.com/pf0607/MUSE.",
        "url": "http://arxiv.org/abs/2508.14440v1",
        "published_date": "2025-08-20T05:52:26+00:00",
        "updated_date": "2025-08-20T05:52:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fei Peng",
            "Junqiang Wu",
            "Yan Li",
            "Tingting Gao",
            "Di Zhang",
            "Huiyuan Fu"
        ],
        "tldr": "The paper introduces MUSE, a framework for layout-controllable multi-subject image synthesis that uses concatenated cross-attention to better integrate layout and text, achieving improved spatial accuracy and subject identity preservation.",
        "tldr_zh": "该论文介绍了MUSE，一个用于可控布局的多主体图像合成框架，它使用串联交叉注意力来更好地集成布局和文本，从而提高空间精度和主体身份保持。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Disentanglement in T-space for Faster and Distributed Training of Diffusion Models with Fewer Latent-states",
        "summary": "We challenge a fundamental assumption of diffusion models, namely, that a\nlarge number of latent-states or time-steps is required for training so that\nthe reverse generative process is close to a Gaussian. We first show that with\ncareful selection of a noise schedule, diffusion models trained over a small\nnumber of latent states (i.e. $T \\sim 32$) match the performance of models\ntrained over a much large number of latent states ($T \\sim 1,000$). Second, we\npush this limit (on the minimum number of latent states required) to a single\nlatent-state, which we refer to as complete disentanglement in T-space. We show\nthat high quality samples can be easily generated by the disentangled model\nobtained by combining several independently trained single latent-state models.\nWe provide extensive experiments to show that the proposed disentangled model\nprovides 4-6$\\times$ faster convergence measured across a variety of metrics on\ntwo different datasets.",
        "url": "http://arxiv.org/abs/2508.14413v1",
        "published_date": "2025-08-20T04:21:26+00:00",
        "updated_date": "2025-08-20T04:21:26+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Samarth Gupta",
            "Raghudeep Gadde",
            "Rui Chen",
            "Aleix M. Martinez"
        ],
        "tldr": "This paper introduces a method to train diffusion models with significantly fewer latent states (even down to a single state) while maintaining performance and improving training speed through disentanglement in T-space.",
        "tldr_zh": "该论文介绍了一种使用明显更少的潜在状态（甚至单个状态）来训练扩散模型的方法，同时通过 T 空间中的解缠来保持性能并提高训练速度。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Taming Transformer for Emotion-Controllable Talking Face Generation",
        "summary": "Talking face generation is a novel and challenging generation task, aiming at\nsynthesizing a vivid speaking-face video given a specific audio. To fulfill\nemotion-controllable talking face generation, current methods need to overcome\ntwo challenges: One is how to effectively model the multimodal relationship\nrelated to the specific emotion, and the other is how to leverage this\nrelationship to synthesize identity preserving emotional videos. In this paper,\nwe propose a novel method to tackle the emotion-controllable talking face\ngeneration task discretely. Specifically, we employ two pre-training strategies\nto disentangle audio into independent components and quantize videos into\ncombinations of visual tokens. Subsequently, we propose the emotion-anchor (EA)\nrepresentation that integrates the emotional information into visual tokens.\nFinally, we introduce an autoregressive transformer to model the global\ndistribution of the visual tokens under the given conditions and further\npredict the index sequence for synthesizing the manipulated videos. We conduct\nexperiments on the MEAD dataset that controls the emotion of videos conditioned\non multiple emotional audios. Extensive experiments demonstrate the\nsuperiorities of our method both qualitatively and quantitatively.",
        "url": "http://arxiv.org/abs/2508.14359v1",
        "published_date": "2025-08-20T02:16:52+00:00",
        "updated_date": "2025-08-20T02:16:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziqi Zhang",
            "Cheng Deng"
        ],
        "tldr": "This paper introduces a transformer-based method for emotion-controllable talking face generation by disentangling audio, quantizing video into visual tokens, and using an emotion-anchor representation to integrate emotional information.",
        "tldr_zh": "本文介绍了一种基于Transformer的情感可控说话人脸生成方法，该方法通过解耦音频、将视频量化为视觉标记，并使用情感锚定表示来整合情感信息。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation",
        "summary": "Video generation has recently shown superiority in urban scene synthesis for\nautonomous driving. Existing video generation approaches to autonomous driving\nprimarily focus on RGB video generation and lack the ability to support\nmulti-modal video generation. However, multi-modal data, such as depth maps and\nsemantic maps, are crucial for holistic urban scene understanding in autonomous\ndriving. Although it is feasible to use multiple models to generate different\nmodalities, this increases the difficulty of model deployment and does not\nleverage complementary cues for multi-modal data generation. To address this\nproblem, in this work, we propose a novel multi-modal multi-view video\ngeneration approach to autonomous driving. Specifically, we construct a unified\ndiffusion transformer model composed of modal-shared components and\nmodal-specific components. Then, we leverage diverse conditioning inputs to\nencode controllable scene structure and content cues into the unified diffusion\nmodel for multi-modal multi-view video generation. In this way, our approach is\ncapable of generating multi-modal multi-view driving scene videos in a unified\nframework. Our experiments on the challenging real-world autonomous driving\ndataset, nuScenes, show that our approach can generate multi-modal multi-view\nurban scene videos with high fidelity and controllability, surpassing the\nstate-of-the-art methods.",
        "url": "http://arxiv.org/abs/2508.14327v1",
        "published_date": "2025-08-20T00:51:36+00:00",
        "updated_date": "2025-08-20T00:51:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guile Wu",
            "David Huang",
            "Dongfeng Bai",
            "Bingbing Liu"
        ],
        "tldr": "The paper proposes a unified diffusion transformer model for generating multi-modal (RGB, depth, semantic) and multi-view driving scene videos, demonstrating improved fidelity and controllability compared to existing methods.",
        "tldr_zh": "该论文提出了一种统一的扩散Transformer模型，用于生成多模态（RGB，深度，语义）和多视角的驾驶场景视频，与现有方法相比，展示了更高的保真度和可控性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting",
        "summary": "Recent developments in 3D Gaussian Splatting have significantly enhanced\nnovel view synthesis, yet generating high-quality renderings from extreme novel\nviewpoints or partially observed regions remains challenging. Meanwhile,\ndiffusion models exhibit strong generative capabilities, but their reliance on\ntext prompts and lack of awareness of specific scene information hinder\naccurate 3D reconstruction tasks. To address these limitations, we introduce\nGSFix3D, a novel framework that improves the visual fidelity in\nunder-constrained regions by distilling prior knowledge from diffusion models\ninto 3D representations, while preserving consistency with observed scene\ndetails. At its core is GSFixer, a latent diffusion model obtained via our\ncustomized fine-tuning protocol that can leverage both mesh and 3D Gaussians to\nadapt pretrained generative models to a variety of environments and artifact\ntypes from different reconstruction methods, enabling robust novel view repair\nfor unseen camera poses. Moreover, we propose a random mask augmentation\nstrategy that empowers GSFixer to plausibly inpaint missing regions.\nExperiments on challenging benchmarks demonstrate that our GSFix3D and GSFixer\nachieve state-of-the-art performance, requiring only minimal scene-specific\nfine-tuning on captured data. Real-world test further confirms its resilience\nto potential pose errors. Our code and data will be made publicly available.\nProject page: https://gsfix3d.github.io.",
        "url": "http://arxiv.org/abs/2508.14717v1",
        "published_date": "2025-08-20T13:49:53+00:00",
        "updated_date": "2025-08-20T13:49:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaxin Wei",
            "Stefan Leutenegger",
            "Simon Schaefer"
        ],
        "tldr": "GSFix3D improves novel view synthesis in Gaussian Splatting by distilling knowledge from diffusion models into 3D representations, enabling robust repair of under-constrained regions with minimal scene-specific fine-tuning.",
        "tldr_zh": "GSFix3D通过将扩散模型的知识提炼到3D表示中，改进了高斯溅射中的新颖视图合成，从而能够以最少的场景特定微调来稳健地修复约束不足的区域。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality English Text-to-Image Communities",
        "summary": "We proposed the Chinese Text Adapter-Flux (CTA-Flux). An adaptation method\nfits the Chinese text inputs to Flux, a powerful text-to-image (TTI) generative\nmodel initially trained on the English corpus. Despite the notable image\ngeneration ability conditioned on English text inputs, Flux performs poorly\nwhen processing non-English prompts, particularly due to linguistic and\ncultural biases inherent in predominantly English-centric training datasets.\nExisting approaches, such as translating non-English prompts into English or\nfinetuning models for bilingual mappings, inadequately address culturally\nspecific semantics, compromising image authenticity and quality. To address\nthis issue, we introduce a novel method to bridge Chinese semantic\nunderstanding with compatibility in English-centric TTI model communities.\nExisting approaches relying on ControlNet-like architectures typically require\na massive parameter scale and lack direct control over Chinese semantics. In\ncomparison, CTA-flux leverages MultiModal Diffusion Transformer (MMDiT) to\ncontrol the Flux backbone directly, significantly reducing the number of\nparameters while enhancing the model's understanding of Chinese semantics. This\nintegration significantly improves the generation quality and cultural\nauthenticity without extensive retraining of the entire model, thus maintaining\ncompatibility with existing text-to-image plugins such as LoRA, IP-Adapter, and\nControlNet. Empirical evaluations demonstrate that CTA-flux supports Chinese\nand English prompts and achieves superior image generation quality, visual\nrealism, and faithful depiction of Chinese semantics.",
        "url": "http://arxiv.org/abs/2508.14405v1",
        "published_date": "2025-08-20T04:03:54+00:00",
        "updated_date": "2025-08-20T04:03:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yue Gong",
            "Shanyuan Liu",
            "Liuzhuozheng Li",
            "Jian Zhu",
            "Bo Cheng",
            "Liebucha Wu",
            "Xiaoyu Wu",
            "Yuhang Ma",
            "Dawei Leng",
            "Yuhui Yin"
        ],
        "tldr": "The paper introduces CTA-Flux, a method to adapt Chinese text inputs to the English-centric TTI model Flux using a MultiModal Diffusion Transformer, improving Chinese semantic understanding and image generation quality while maintaining compatibility with existing plugins.",
        "tldr_zh": "该论文介绍了CTA-Flux，一种使用多模态扩散变换器将中文文本输入适配到以英语为中心的 TTI 模型 Flux 的方法，从而提高中文语义理解和图像生成质量，并保持与现有插件的兼容性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]