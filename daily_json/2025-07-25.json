[
    {
        "title": "Captain Cinema: Towards Short Movie Generation",
        "summary": "We present Captain Cinema, a generation framework for short movie generation.\nGiven a detailed textual description of a movie storyline, our approach firstly\ngenerates a sequence of keyframes that outline the entire narrative, which\nensures long-range coherence in both the storyline and visual appearance (e.g.,\nscenes and characters). We refer to this step as top-down keyframe planning.\nThese keyframes then serve as conditioning signals for a video synthesis model,\nwhich supports long context learning, to produce the spatio-temporal dynamics\nbetween them. This step is referred to as bottom-up video synthesis. To support\nstable and efficient generation of multi-scene long narrative cinematic works,\nwe introduce an interleaved training strategy for Multimodal Diffusion\nTransformers (MM-DiT), specifically adapted for long-context video data. Our\nmodel is trained on a specially curated cinematic dataset consisting of\ninterleaved data pairs. Our experiments demonstrate that Captain Cinema\nperforms favorably in the automated creation of visually coherent and narrative\nconsistent short movies in high quality and efficiency. Project page:\nhttps://thecinema.ai",
        "url": "http://arxiv.org/abs/2507.18634v1",
        "published_date": "2025-07-24T17:59:56+00:00",
        "updated_date": "2025-07-24T17:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junfei Xiao",
            "Ceyuan Yang",
            "Lvmin Zhang",
            "Shengqu Cai",
            "Yang Zhao",
            "Yuwei Guo",
            "Gordon Wetzstein",
            "Maneesh Agrawala",
            "Alan Yuille",
            "Lu Jiang"
        ],
        "tldr": "Captain Cinema is a framework for generating short movies from textual descriptions using a two-stage approach: top-down keyframe planning followed by bottom-up video synthesis with Multimodal Diffusion Transformers.",
        "tldr_zh": "Captain Cinema是一个通过文本描述生成短电影的框架，它使用两阶段方法：首先进行自上而下的关键帧规划，然后使用多模态扩散Transformer进行自下而上的视频合成。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis",
        "summary": "Distribution Matching Distillation (DMD) is a promising score distillation\ntechnique that compresses pre-trained teacher diffusion models into efficient\none-step or multi-step student generators. Nevertheless, its reliance on the\nreverse Kullback-Leibler (KL) divergence minimization potentially induces mode\ncollapse (or mode-seeking) in certain applications. To circumvent this inherent\ndrawback, we propose Adversarial Distribution Matching (ADM), a novel framework\nthat leverages diffusion-based discriminators to align the latent predictions\nbetween real and fake score estimators for score distillation in an adversarial\nmanner. In the context of extremely challenging one-step distillation, we\nfurther improve the pre-trained generator by adversarial distillation with\nhybrid discriminators in both latent and pixel spaces. Different from the mean\nsquared error used in DMD2 pre-training, our method incorporates the\ndistributional loss on ODE pairs collected from the teacher model, and thus\nproviding a better initialization for score distillation fine-tuning in the\nnext stage. By combining the adversarial distillation pre-training with ADM\nfine-tuning into a unified pipeline termed DMDX, our proposed method achieves\nsuperior one-step performance on SDXL compared to DMD2 while consuming less GPU\ntime. Additional experiments that apply multi-step ADM distillation on\nSD3-Medium, SD3.5-Large, and CogVideoX set a new benchmark towards efficient\nimage and video synthesis.",
        "url": "http://arxiv.org/abs/2507.18569v1",
        "published_date": "2025-07-24T16:45:05+00:00",
        "updated_date": "2025-07-24T16:45:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanzuo Lu",
            "Yuxi Ren",
            "Xin Xia",
            "Shanchuan Lin",
            "Xing Wang",
            "Xuefeng Xiao",
            "Andy J. Ma",
            "Xiaohua Xie",
            "Jian-Huang Lai"
        ],
        "tldr": "The paper introduces Adversarial Distribution Matching (ADM), a method for distilling diffusion models that uses adversarial training to address mode collapse issues in Distribution Matching Distillation (DMD), achieving better one-step image and video synthesis performance with less GPU time.",
        "tldr_zh": "该论文介绍了对抗分布匹配（ADM），一种用于蒸馏扩散模型的方法，它使用对抗训练来解决分布匹配蒸馏（DMD）中的模式崩溃问题，从而以更少的GPU时间实现更好的一步式图像和视频合成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]