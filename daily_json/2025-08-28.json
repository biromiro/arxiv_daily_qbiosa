[
    {
        "title": "Ego-centric Predictive Model Conditioned on Hand Trajectories",
        "summary": "In egocentric scenarios, anticipating both the next action and its visual\noutcome is essential for understanding human-object interactions and for\nenabling robotic planning. However, existing paradigms fall short of jointly\nmodeling these aspects. Vision-Language-Action (VLA) models focus on action\nprediction but lack explicit modeling of how actions influence the visual\nscene, while video prediction models generate future frames without\nconditioning on specific actions, often resulting in implausible or\ncontextually inconsistent outcomes. To bridge this gap, we propose a unified\ntwo-stage predictive framework that jointly models action and visual future in\negocentric scenarios, conditioned on hand trajectories. In the first stage, we\nperform consecutive state modeling to process heterogeneous inputs (visual\nobservations, language, and action history) and explicitly predict future hand\ntrajectories. In the second stage, we introduce causal cross-attention to fuse\nmulti-modal cues, leveraging inferred action signals to guide an image-based\nLatent Diffusion Model (LDM) for frame-by-frame future video generation. Our\napproach is the first unified model designed to handle both egocentric human\nactivity understanding and robotic manipulation tasks, providing explicit\npredictions of both upcoming actions and their visual consequences. Extensive\nexperiments on Ego4D, BridgeData, and RLBench demonstrate that our method\noutperforms state-of-the-art baselines in both action prediction and future\nvideo synthesis.",
        "url": "http://arxiv.org/abs/2508.19852v1",
        "published_date": "2025-08-27T13:09:55+00:00",
        "updated_date": "2025-08-27T13:09:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Binjie Zhang",
            "Mike Zheng Shou"
        ],
        "tldr": "The paper introduces a novel two-stage framework for egocentric video prediction, which jointly models action and visual future conditioned on hand trajectories, outperforming SOTA methods on multiple datasets.",
        "tldr_zh": "该论文介绍了一种新颖的两阶段框架，用于以自我为中心的视频预测，该框架联合模拟了以手部轨迹为条件的动作和视觉未来，并在多个数据集上优于SOTA方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment",
        "summary": "Motion generation is essential for animating virtual characters and embodied\nagents. While recent text-driven methods have made significant strides, they\noften struggle with achieving precise alignment between linguistic descriptions\nand motion semantics, as well as with the inefficiencies of slow, multi-step\ninference. To address these issues, we introduce TMR++ Aligned Preference\nOptimization (TAPO), an innovative framework that aligns subtle motion\nvariations with textual modifiers and incorporates iterative adjustments to\nreinforce semantic grounding. To further enable real-time synthesis, we propose\nMotionFLUX, a high-speed generation framework based on deterministic rectified\nflow matching. Unlike traditional diffusion models, which require hundreds of\ndenoising steps, MotionFLUX constructs optimal transport paths between noise\ndistributions and motion spaces, facilitating real-time synthesis. The\nlinearized probability paths reduce the need for multi-step sampling typical of\nsequential methods, significantly accelerating inference time without\nsacrificing motion quality. Experimental results demonstrate that, together,\nTAPO and MotionFLUX form a unified system that outperforms state-of-the-art\napproaches in both semantic consistency and motion quality, while also\naccelerating generation speed. The code and pretrained models will be released.",
        "url": "http://arxiv.org/abs/2508.19527v1",
        "published_date": "2025-08-27T02:45:09+00:00",
        "updated_date": "2025-08-27T02:45:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiting Gao",
            "Dan Song",
            "Diqiong Jiang",
            "Chao Xue",
            "An-An Liu"
        ],
        "tldr": "This paper introduces MotionFLUX, a real-time text-guided motion generation framework that uses rectified flow matching and preference alignment (TAPO) to achieve faster inference and improved semantic consistency compared to diffusion-based methods.",
        "tldr_zh": "本文介绍了MotionFLUX，一种实时的文本引导运动生成框架，该框架使用校正流匹配和偏好对齐（TAPO）来实现比基于扩散的方法更快的推理和更高的语义一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model",
        "summary": "Skin images from real-world clinical practice are often limited, resulting in\na shortage of training data for deep-learning models. While many studies have\nexplored skin image synthesis, existing methods often generate low-quality\nimages and lack control over the lesion's location and type. To address these\nlimitations, we present LF-VAR, a model leveraging quantified lesion\nmeasurement scores and lesion type labels to guide the clinically relevant and\ncontrollable synthesis of skin images. It enables controlled skin synthesis\nwith specific lesion characteristics based on language prompts. We train a\nmultiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to\nencode images into discrete latent representations for structured tokenization.\nThen, a Visual AutoRegressive (VAR) Transformer trained on tokenized\nrepresentations facilitates image synthesis. Lesion measurement from the lesion\nregion and types as conditional embeddings are integrated to enhance synthesis\nfidelity. Our method achieves the best overall FID score (average 0.74) among\nseven lesion types, improving upon the previous state-of-the-art (SOTA) by\n6.3%. The study highlights our controllable skin synthesis model's\neffectiveness in generating high-fidelity, clinically relevant synthetic skin\nimages. Our framework code is available at\nhttps://github.com/echosun1996/LF-VAR.",
        "url": "http://arxiv.org/abs/2508.19626v1",
        "published_date": "2025-08-27T07:04:58+00:00",
        "updated_date": "2025-08-27T07:04:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiajun Sun",
            "Zhen Yu",
            "Siyuan Yan",
            "Jason J. Ong",
            "Zongyuan Ge",
            "Lei Zhang"
        ],
        "tldr": "The paper introduces LF-VAR, a controllable skin image synthesis model using a VQVAE and VAR Transformer, guided by lesion measurement scores and types, achieving state-of-the-art FID scores.",
        "tldr_zh": "该论文介绍了 LF-VAR，一种可控的皮肤图像合成模型，使用 VQVAE 和 VAR Transformer，并由病灶测量分数和类型引导，实现了最先进的 FID 分数。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]