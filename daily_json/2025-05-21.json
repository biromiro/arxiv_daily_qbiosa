[
    {
        "title": "Emerging Properties in Unified Multimodal Pretraining",
        "summary": "Unifying multimodal understanding and generation has shown impressive\ncapabilities in cutting-edge proprietary systems. In this work, we introduce\nBAGEL, an open0source foundational model that natively supports multimodal\nunderstanding and generation. BAGEL is a unified, decoder0only model pretrained\non trillions of tokens curated from large0scale interleaved text, image, video,\nand web data. When scaled with such diverse multimodal interleaved data, BAGEL\nexhibits emerging capabilities in complex multimodal reasoning. As a result, it\nsignificantly outperforms open-source unified models in both multimodal\ngeneration and understanding across standard benchmarks, while exhibiting\nadvanced multimodal reasoning abilities such as free-form image manipulation,\nfuture frame prediction, 3D manipulation, and world navigation. In the hope of\nfacilitating further opportunities for multimodal research, we share the key\nfindings, pretraining details, data creation protocal, and release our code and\ncheckpoints to the community. The project page is at https://bagel-ai.org/",
        "url": "http://arxiv.org/abs/2505.14683v1",
        "published_date": "2025-05-20T17:59:30+00:00",
        "updated_date": "2025-05-20T17:59:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chaorui Deng",
            "Deyao Zhu",
            "Kunchang Li",
            "Chenhui Gou",
            "Feng Li",
            "Zeyu Wang",
            "Shu Zhong",
            "Weihao Yu",
            "Xiaonan Nie",
            "Ziang Song",
            "Guang Shi",
            "Haoqi Fan"
        ],
        "tldr": "The paper introduces BAGEL, an open-source unified multimodal model pretrained on a massive dataset, demonstrating strong performance in multimodal understanding and generation with emerging reasoning abilities.",
        "tldr_zh": "该论文介绍了BAGEL，一个开源的统一多模态模型，它在一个庞大的数据集上进行了预训练，展示了在多模态理解和生成方面的强大性能，并具有新兴的推理能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal Understanding and Generation",
        "summary": "We introduce UniGen, a unified multimodal large language model (MLLM) capable\nof image understanding and generation. We study the full training pipeline of\nUniGen from a data-centric perspective, including multi-stage pre-training,\nsupervised fine-tuning, and direct preference optimization. More importantly,\nwe propose a new Chain-of-Thought Verification (CoT-V) strategy for test-time\nscaling, which significantly boosts UniGen's image generation quality using a\nsimple Best-of-N test-time strategy. Specifically, CoT-V enables UniGen to act\nas both image generator and verifier at test time, assessing the semantic\nalignment between a text prompt and its generated image in a step-by-step CoT\nmanner. Trained entirely on open-source datasets across all stages, UniGen\nachieves state-of-the-art performance on a range of image understanding and\ngeneration benchmarks, with a final score of 0.78 on GenEval and 85.19 on\nDPG-Bench. Through extensive ablation studies, our work provides actionable\ninsights and addresses key challenges in the full life cycle of building\nunified MLLMs, contributing meaningful directions to the future research.",
        "url": "http://arxiv.org/abs/2505.14682v1",
        "published_date": "2025-05-20T17:59:26+00:00",
        "updated_date": "2025-05-20T17:59:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rui Tian",
            "Mingfei Gao",
            "Mingze Xu",
            "Jiaming Hu",
            "Jiasen Lu",
            "Zuxuan Wu",
            "Yinfei Yang",
            "Afshin Dehghan"
        ],
        "tldr": "The paper introduces UniGen, a unified MLLM for image understanding and generation, highlighting a data-centric training approach and a novel Chain-of-Thought Verification (CoT-V) strategy that improves image generation quality. It achieves SOTA results on several benchmarks using only open-source data.",
        "tldr_zh": "本文介绍了UniGen，一个用于图像理解和生成的统一多模态大型语言模型（MLLM），重点介绍了以数据为中心的训练方法和一种新型的思维链验证（CoT-V）策略，该策略提高了图像生成质量。它仅使用开源数据，在多个基准测试中取得了SOTA效果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Instructing Text-to-Image Diffusion Models via Classifier-Guided Semantic Optimization",
        "summary": "Text-to-image diffusion models have emerged as powerful tools for\nhigh-quality image generation and editing. Many existing approaches rely on\ntext prompts as editing guidance. However, these methods are constrained by the\nneed for manual prompt crafting, which can be time-consuming, introduce\nirrelevant details, and significantly limit editing performance. In this work,\nwe propose optimizing semantic embeddings guided by attribute classifiers to\nsteer text-to-image models toward desired edits, without relying on text\nprompts or requiring any training or fine-tuning of the diffusion model. We\nutilize classifiers to learn precise semantic embeddings at the dataset level.\nThe learned embeddings are theoretically justified as the optimal\nrepresentation of attribute semantics, enabling disentangled and accurate\nedits. Experiments further demonstrate that our method achieves high levels of\ndisentanglement and strong generalization across different domains of data.",
        "url": "http://arxiv.org/abs/2505.14254v1",
        "published_date": "2025-05-20T12:07:01+00:00",
        "updated_date": "2025-05-20T12:07:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanyuan Chang",
            "Yinghua Yao",
            "Tao Qin",
            "Mengmeng Wang",
            "Ivor Tsang",
            "Guang Dai"
        ],
        "tldr": "The paper presents a method for editing images generated by text-to-image diffusion models by optimizing semantic embeddings guided by attribute classifiers, removing the need for manual prompt engineering and diffusion model training.",
        "tldr_zh": "该论文提出了一种通过优化由属性分类器引导的语义嵌入来编辑文本到图像扩散模型生成的图像的方法，无需手动提示工程和扩散模型训练。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LMP: Leveraging Motion Prior in Zero-Shot Video Generation with Diffusion Transformer",
        "summary": "In recent years, large-scale pre-trained diffusion transformer models have\nmade significant progress in video generation. While current DiT models can\nproduce high-definition, high-frame-rate, and highly diverse videos, there is a\nlack of fine-grained control over the video content. Controlling the motion of\nsubjects in videos using only prompts is challenging, especially when it comes\nto describing complex movements. Further, existing methods fail to control the\nmotion in image-to-video generation, as the subject in the reference image\noften differs from the subject in the reference video in terms of initial\nposition, size, and shape. To address this, we propose the Leveraging Motion\nPrior (LMP) framework for zero-shot video generation. Our framework harnesses\nthe powerful generative capabilities of pre-trained diffusion transformers to\nenable motion in the generated videos to reference user-provided motion videos\nin both text-to-video and image-to-video generation. To this end, we first\nintroduce a foreground-background disentangle module to distinguish between\nmoving subjects and backgrounds in the reference video, preventing interference\nin the target video generation. A reweighted motion transfer module is designed\nto allow the target video to reference the motion from the reference video. To\navoid interference from the subject in the reference video, we propose an\nappearance separation module to suppress the appearance of the reference\nsubject in the target video. We annotate the DAVIS dataset with detailed\nprompts for our experiments and design evaluation metrics to validate the\neffectiveness of our method. Extensive experiments demonstrate that our\napproach achieves state-of-the-art performance in generation quality,\nprompt-video consistency, and control capability. Our homepage is available at\nhttps://vpx-ecnu.github.io/LMP-Website/",
        "url": "http://arxiv.org/abs/2505.14167v1",
        "published_date": "2025-05-20T10:18:29+00:00",
        "updated_date": "2025-05-20T10:18:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Changgu Chen",
            "Xiaoyan Yang",
            "Junwei Shu",
            "Changbo Wang",
            "Yang Li"
        ],
        "tldr": "The paper introduces Leveraging Motion Prior (LMP), a framework for zero-shot video generation that allows motion in generated videos to be controlled by user-provided motion videos using pre-trained diffusion transformers. It addresses the lack of fine-grained motion control in existing video generation models.",
        "tldr_zh": "该论文提出了Leveraging Motion Prior (LMP)框架，用于零样本视频生成，该框架利用预训练的扩散Transformer，允许通过用户提供的运动视频来控制生成视频中的运动。它解决了现有视频生成模型缺乏对细粒度运动控制的问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improving Compositional Generation with Diffusion Models Using Lift Scores",
        "summary": "We introduce a novel resampling criterion using lift scores, for improving\ncompositional generation in diffusion models. By leveraging the lift scores, we\nevaluate whether generated samples align with each single condition and then\ncompose the results to determine whether the composed prompt is satisfied. Our\nkey insight is that lift scores can be efficiently approximated using only the\noriginal diffusion model, requiring no additional training or external modules.\nWe develop an optimized variant that achieves relatively lower computational\noverhead during inference while maintaining effectiveness. Through extensive\nexperiments, we demonstrate that lift scores significantly improved the\ncondition alignment for compositional generation across 2D synthetic data,\nCLEVR position tasks, and text-to-image synthesis. Our code is available at\nhttp://github.com/rainorangelemon/complift.",
        "url": "http://arxiv.org/abs/2505.13740v1",
        "published_date": "2025-05-19T21:34:42+00:00",
        "updated_date": "2025-05-19T21:34:42+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Chenning Yu",
            "Sicun Gao"
        ],
        "tldr": "This paper introduces a novel resampling criterion using lift scores within diffusion models to improve compositional generation without requiring additional training, demonstrating improved condition alignment across various tasks.",
        "tldr_zh": "本文提出了一种使用提升分数的 novel 重新采样标准，用于改进扩散模型中的组合生成，无需额外训练，并在各种任务中展示了改进的条件对齐。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]