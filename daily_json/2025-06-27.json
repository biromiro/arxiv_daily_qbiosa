[
    {
        "title": "Instella-T2I: Pushing the Limits of 1D Discrete Latent Space Image Generation",
        "summary": "Image tokenization plays a critical role in reducing the computational\ndemands of modeling high-resolution images, significantly improving the\nefficiency of image and multimodal understanding and generation. Recent\nadvances in 1D latent spaces have reduced the number of tokens required by\neliminating the need for a 2D grid structure. In this paper, we further advance\ncompact discrete image representation by introducing 1D binary image latents.\nBy representing each image as a sequence of binary vectors, rather than using\ntraditional one-hot codebook tokens, our approach preserves high-resolution\ndetails while maintaining the compactness of 1D latents. To the best of our\nknowledge, our text-to-image models are the first to achieve competitive\nperformance in both diffusion and auto-regressive generation using just 128\ndiscrete tokens for images up to 1024x1024, demonstrating up to a 32-fold\nreduction in token numbers compared to standard VQ-VAEs. The proposed 1D binary\nlatent space, coupled with simple model architectures, achieves marked\nimprovements in speed training and inference speed. Our text-to-image models\nallow for a global batch size of 4096 on a single GPU node with 8 AMD MI300X\nGPUs, and the training can be completed within 200 GPU days. Our models achieve\ncompetitive performance compared to modern image generation models without any\nin-house private training data or post-training refinements, offering a\nscalable and efficient alternative to conventional tokenization methods.",
        "url": "http://arxiv.org/abs/2506.21022v1",
        "published_date": "2025-06-26T05:48:36+00:00",
        "updated_date": "2025-06-26T05:48:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ze Wang",
            "Hao Chen",
            "Benran Hu",
            "Jiang Liu",
            "Ximeng Sun",
            "Jialian Wu",
            "Yusheng Su",
            "Xiaodong Yu",
            "Emad Barsoum",
            "Zicheng Liu"
        ],
        "tldr": "The paper introduces a novel 1D binary image latent representation (Instella-T2I) for text-to-image generation, achieving competitive performance with significantly fewer tokens and improved training/inference speed compared to VQ-VAE baselines.",
        "tldr_zh": "该论文提出了一种新的用于文本生成图像的1D二进制图像潜在表示（Instella-T2I），相比于 VQ-VAE 基线，它以更少的 tokens 数实现了具有竞争力的性能，并提升了训练和推理速度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "XVerse: Consistent Multi-Subject Control of Identity and Semantic Attributes via DiT Modulation",
        "summary": "Achieving fine-grained control over subject identity and semantic attributes\n(pose, style, lighting) in text-to-image generation, particularly for multiple\nsubjects, often undermines the editability and coherence of Diffusion\nTransformers (DiTs). Many approaches introduce artifacts or suffer from\nattribute entanglement. To overcome these challenges, we propose a novel\nmulti-subject controlled generation model XVerse. By transforming reference\nimages into offsets for token-specific text-stream modulation, XVerse allows\nfor precise and independent control for specific subject without disrupting\nimage latents or features. Consequently, XVerse offers high-fidelity, editable\nmulti-subject image synthesis with robust control over individual subject\ncharacteristics and semantic attributes. This advancement significantly\nimproves personalized and complex scene generation capabilities.",
        "url": "http://arxiv.org/abs/2506.21416v1",
        "published_date": "2025-06-26T16:04:16+00:00",
        "updated_date": "2025-06-26T16:04:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bowen Chen",
            "Mengyi Zhao",
            "Haomiao Sun",
            "Li Chen",
            "Xu Wang",
            "Kang Du",
            "Xinglong Wu"
        ],
        "tldr": "XVerse presents a novel Diffusion Transformer (DiT) modulation technique for fine-grained control of subject identity and semantic attributes in multi-subject text-to-image generation, offering improved editability and coherence.",
        "tldr_zh": "XVerse 提出了一种新颖的扩散Transformer (DiT)调制技术，用于在多主体文本到图像生成中精确控制主体身份和语义属性，从而提高可编辑性和连贯性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GenFlow: Interactive Modular System for Image Generation",
        "summary": "Generative art unlocks boundless creative possibilities, yet its full\npotential remains untapped due to the technical expertise required for advanced\narchitectural concepts and computational workflows. To bridge this gap, we\npresent GenFlow, a novel modular framework that empowers users of all skill\nlevels to generate images with precision and ease. Featuring a node-based\neditor for seamless customization and an intelligent assistant powered by\nnatural language processing, GenFlow transforms the complexity of workflow\ncreation into an intuitive and accessible experience. By automating deployment\nprocesses and minimizing technical barriers, our framework makes cutting-edge\ngenerative art tools available to everyone. A user study demonstrated GenFlow's\nability to optimize workflows, reduce task completion times, and enhance user\nunderstanding through its intuitive interface and adaptive features. These\nresults position GenFlow as a groundbreaking solution that redefines\naccessibility and efficiency in the realm of generative art.",
        "url": "http://arxiv.org/abs/2506.21369v1",
        "published_date": "2025-06-26T15:18:00+00:00",
        "updated_date": "2025-06-26T15:18:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Duc-Hung Nguyen",
            "Huu-Phuc Huynh",
            "Minh-Triet Tran",
            "Trung-Nghia Le"
        ],
        "tldr": "GenFlow is a modular, node-based framework designed to make image generation more accessible to users of all skill levels through an intuitive interface and NLP-powered assistant.",
        "tldr_zh": "GenFlow是一个模块化的、基于节点的框架，旨在通过直观的界面和NLP驱动的助手，使各种技能水平的用户更容易进行图像生成。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]