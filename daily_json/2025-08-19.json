[
    {
        "title": "Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation",
        "summary": "The computational demands of self-attention mechanisms pose a critical\nchallenge for transformer-based video generation, particularly in synthesizing\nultra-long sequences. Current approaches, such as factorized attention and\nfixed sparse patterns, fail to fully exploit the inherent spatio-temporal\nredundancies in video data. Through systematic analysis of video diffusion\ntransformers (DiT), we uncover a key insight: Attention matrices exhibit\nstructured, yet heterogeneous sparsity patterns, where specialized heads\ndynamically attend to distinct spatiotemporal regions (e.g., local pattern,\ncross-shaped pattern, or global pattern). Existing sparse attention methods\neither impose rigid constraints or introduce significant overhead, limiting\ntheir effectiveness. To address this, we propose Compact Attention, a\nhardware-aware acceleration framework featuring three innovations: 1) Adaptive\ntiling strategies that approximate diverse spatial interaction patterns via\ndynamic tile grouping, 2) Temporally varying windows that adjust sparsity\nlevels based on frame proximity, and 3) An automated configuration search\nalgorithm that optimizes sparse patterns while preserving critical attention\npathways. Our method achieves 1.6~2.5x acceleration in attention computation on\nsingle-GPU setups while maintaining comparable visual quality with\nfull-attention baselines. This work provides a principled approach to unlocking\nefficient long-form video generation through structured sparsity exploitation.\nProject Page: https://yo-ava.github.io/Compact-Attention.github.io/",
        "url": "http://arxiv.org/abs/2508.12969v1",
        "published_date": "2025-08-18T14:45:42+00:00",
        "updated_date": "2025-08-18T14:45:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qirui Li",
            "Guangcong Zheng",
            "Qi Zhao",
            "Jie Li",
            "Bin Dong",
            "Yiwu Yao",
            "Xi Li"
        ],
        "tldr": "This paper introduces Compact Attention, a hardware-aware acceleration framework that leverages structured spatio-temporal sparsity in attention matrices to improve the efficiency of transformer-based video generation, achieving 1.6-2.5x speedup with comparable visual quality.",
        "tldr_zh": "本文介绍了 Compact Attention，这是一个硬件感知的加速框架，它利用注意力矩阵中结构化的时空稀疏性来提高基于 transformer 的视频生成的效率，在保持可比视觉质量的同时实现了 1.6-2.5 倍的加速。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Next Visual Granularity Generation",
        "summary": "We propose a novel approach to image generation by decomposing an image into\na structured sequence, where each element in the sequence shares the same\nspatial resolution but differs in the number of unique tokens used, capturing\ndifferent level of visual granularity. Image generation is carried out through\nour newly introduced Next Visual Granularity (NVG) generation framework, which\ngenerates a visual granularity sequence beginning from an empty image and\nprogressively refines it, from global layout to fine details, in a structured\nmanner. This iterative process encodes a hierarchical, layered representation\nthat offers fine-grained control over the generation process across multiple\ngranularity levels. We train a series of NVG models for class-conditional image\ngeneration on the ImageNet dataset and observe clear scaling behavior. Compared\nto the VAR series, NVG consistently outperforms it in terms of FID scores (3.30\n-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to\nshowcase the capability and potential of the NVG framework. Our code and models\nwill be released.",
        "url": "http://arxiv.org/abs/2508.12811v1",
        "published_date": "2025-08-18T10:47:37+00:00",
        "updated_date": "2025-08-18T10:47:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Yikai Wang",
            "Zhouxia Wang",
            "Zhonghua Wu",
            "Qingyi Tao",
            "Kang Liao",
            "Chen Change Loy"
        ],
        "tldr": "The paper introduces a Next Visual Granularity (NVG) generation framework that decomposes image generation into progressively refined visual granularity sequences, outperforming VAR series on ImageNet.",
        "tldr_zh": "本文提出了一种下一代视觉粒度（NVG）生成框架，该框架将图像生成分解为逐步细化的视觉粒度序列，在ImageNet上优于VAR系列。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]