[
    {
        "title": "ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback",
        "summary": "With the rapid advancement of generative models, general-purpose generation\nhas gained increasing attention as a promising approach to unify diverse tasks\nacross modalities within a single system. Despite this progress, existing\nopen-source frameworks often remain fragile and struggle to support complex\nreal-world applications due to the lack of structured workflow planning and\nexecution-level feedback. To address these limitations, we present ComfyMind, a\ncollaborative AI system designed to enable robust and scalable general-purpose\ngeneration, built on the ComfyUI platform. ComfyMind introduces two core\ninnovations: Semantic Workflow Interface (SWI) that abstracts low-level node\ngraphs into callable functional modules described in natural language, enabling\nhigh-level composition and reducing structural errors; Search Tree Planning\nmechanism with localized feedback execution, which models generation as a\nhierarchical decision process and allows adaptive correction at each stage.\nTogether, these components improve the stability and flexibility of complex\ngenerative workflows. We evaluate ComfyMind on three public benchmarks:\nComfyBench, GenEval, and Reason-Edit, which span generation, editing, and\nreasoning tasks. Results show that ComfyMind consistently outperforms existing\nopen-source baselines and achieves performance comparable to GPT-Image-1.\nComfyMind paves a promising path for the development of open-source\ngeneral-purpose generative AI systems. Project page:\nhttps://github.com/LitaoGuo/ComfyMind",
        "url": "http://arxiv.org/abs/2505.17908v1",
        "published_date": "2025-05-23T13:53:03+00:00",
        "updated_date": "2025-05-23T13:53:03+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Litao Guo",
            "Xinli Xu",
            "Luozhou Wang",
            "Jiantao Lin",
            "Jinsong Zhou",
            "Zixin Zhang",
            "Bolan Su",
            "Ying-Cong Chen"
        ],
        "tldr": "ComfyMind is a collaborative AI system built on ComfyUI that introduces a Semantic Workflow Interface (SWI) and Search Tree Planning to improve the stability and flexibility of complex generative workflows, achieving performance comparable to GPT-Image-1 on several benchmarks.",
        "tldr_zh": "ComfyMind是一个基于ComfyUI的协作AI系统，它引入了语义工作流界面（SWI）和搜索树规划，以提高复杂生成工作流的稳定性和灵活性，并在多个基准测试中实现了与GPT-Image-1相当的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Slot-MLLM: Object-Centric Visual Tokenization for Multimodal LLM",
        "summary": "Recently, multimodal large language models (MLLMs) have emerged as a key\napproach in achieving artificial general intelligence. In particular,\nvision-language MLLMs have been developed to generate not only text but also\nvisual outputs from multimodal inputs. This advancement requires efficient\nimage tokens that LLMs can process effectively both in input and output.\nHowever, existing image tokenization methods for MLLMs typically capture only\nglobal abstract concepts or uniformly segmented image patches, restricting\nMLLMs' capability to effectively understand or generate detailed visual\ncontent, particularly at the object level. To address this limitation, we\npropose an object-centric visual tokenizer based on Slot Attention specifically\nfor MLLMs. In particular, based on the Q-Former encoder, diffusion decoder, and\nresidual vector quantization, our proposed discretized slot tokens can encode\nlocal visual details while maintaining high-level semantics, and also align\nwith textual data to be integrated seamlessly within a unified next-token\nprediction framework of LLMs. The resulting Slot-MLLM demonstrates significant\nperformance improvements over baselines with previous visual tokenizers across\nvarious vision-language tasks that entail local detailed comprehension and\ngeneration. Notably, this work is the first demonstration of the feasibility of\nobject-centric slot attention performed with MLLMs and in-the-wild natural\nimages.",
        "url": "http://arxiv.org/abs/2505.17726v1",
        "published_date": "2025-05-23T10:43:45+00:00",
        "updated_date": "2025-05-23T10:43:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Donghwan Chi",
            "Hyomin Kim",
            "Yoonjin Oh",
            "Yongjin Kim",
            "Donghoon Lee",
            "Daejin Jo",
            "Jongmin Kim",
            "Junyeob Baek",
            "Sungjin Ahn",
            "Sungwoong Kim"
        ],
        "tldr": "The paper introduces Slot-MLLM, an object-centric visual tokenizer based on Slot Attention, designed to improve MLLMs' ability to process and generate detailed visual content, showing performance gains in vision-language tasks.",
        "tldr_zh": "该论文介绍了Slot-MLLM，一种基于Slot Attention的面向对象中心的可视化分词器，旨在提高MLLM处理和生成详细视觉内容的能力，并在视觉语言任务中表现出性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Scaling Image and Video Generation via Test-Time Evolutionary Search",
        "summary": "As the marginal cost of scaling computation (data and parameters) during\nmodel pre-training continues to increase substantially, test-time scaling (TTS)\nhas emerged as a promising direction for improving generative model performance\nby allocating additional computation at inference time. While TTS has\ndemonstrated significant success across multiple language tasks, there remains\na notable gap in understanding the test-time scaling behaviors of image and\nvideo generative models (diffusion-based or flow-based models). Although recent\nworks have initiated exploration into inference-time strategies for vision\ntasks, these approaches face critical limitations: being constrained to\ntask-specific domains, exhibiting poor scalability, or falling into reward\nover-optimization that sacrifices sample diversity. In this paper, we propose\n\\textbf{Evo}lutionary \\textbf{Search} (EvoSearch), a novel, generalist, and\nefficient TTS method that effectively enhances the scalability of both image\nand video generation across diffusion and flow models, without requiring\nadditional training or model expansion. EvoSearch reformulates test-time\nscaling for diffusion and flow models as an evolutionary search problem,\nleveraging principles from biological evolution to efficiently explore and\nrefine the denoising trajectory. By incorporating carefully designed selection\nand mutation mechanisms tailored to the stochastic differential equation\ndenoising process, EvoSearch iteratively generates higher-quality offspring\nwhile preserving population diversity. Through extensive evaluation across both\ndiffusion and flow architectures for image and video generation tasks, we\ndemonstrate that our method consistently outperforms existing approaches,\nachieves higher diversity, and shows strong generalizability to unseen\nevaluation metrics. Our project is available at the website\nhttps://tinnerhrhe.github.io/evosearch.",
        "url": "http://arxiv.org/abs/2505.17618v1",
        "published_date": "2025-05-23T08:25:46+00:00",
        "updated_date": "2025-05-23T08:25:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Haoran He",
            "Jiajun Liang",
            "Xintao Wang",
            "Pengfei Wan",
            "Di Zhang",
            "Kun Gai",
            "Ling Pan"
        ],
        "tldr": "The paper introduces EvoSearch, a test-time evolutionary search method to improve the scalability, quality, and diversity of image and video generation using diffusion/flow models without additional training.",
        "tldr_zh": "该论文介绍了一种名为EvoSearch的测试时进化搜索方法，用于提升基于扩散模型/流模型的图像和视频生成的可扩展性、质量和多样性，且无需额外训练。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning",
        "summary": "Despite recent progress in text-to-image (T2I) generation, existing models\noften struggle to faithfully capture user intentions from short and\nunder-specified prompts. While prior work has attempted to enhance prompts\nusing large language models (LLMs), these methods frequently generate stylistic\nor unrealistic content due to insufficient grounding in visual semantics and\nreal-world composition. Inspired by recent advances in reasoning for language\nmodel, we propose RePrompt, a novel reprompting framework that introduces\nexplicit reasoning into the prompt enhancement process via reinforcement\nlearning. Instead of relying on handcrafted rules or stylistic rewrites, our\nmethod trains a language model to generate structured, self-reflective prompts\nby optimizing for image-level outcomes. The tailored reward models assesse the\ngenerated images in terms of human preference, semantic alignment, and visual\ncomposition, providing indirect supervision to refine prompt generation. Our\napproach enables end-to-end training without human-annotated data. Experiments\non GenEval and T2I-Compbench show that RePrompt significantly boosts spatial\nlayout fidelity and compositional generalization across diverse T2I backbones,\nestablishing new state-of-the-art results.",
        "url": "http://arxiv.org/abs/2505.17540v1",
        "published_date": "2025-05-23T06:44:26+00:00",
        "updated_date": "2025-05-23T06:44:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mingrui Wu",
            "Lu Wang",
            "Pu Zhao",
            "Fangkai Yang",
            "Jianjin Zhang",
            "Jianfeng Liu",
            "Yuefeng Zhan",
            "Weihao Han",
            "Hao Sun",
            "Jiayi Ji",
            "Xiaoshuai Sun",
            "Qingwei Lin",
            "Weiwei Deng",
            "Dongmei Zhang",
            "Feng Sun",
            "Qi Zhang",
            "Rongrong Ji"
        ],
        "tldr": "The paper introduces RePrompt, a reinforcement learning-based framework for enhancing text-to-image generation by training a language model to generate structured prompts via image-level feedback, achieving state-of-the-art results in spatial layout and compositional generalization.",
        "tldr_zh": "该论文介绍了一种名为RePrompt的框架，它使用强化学习增强文本到图像的生成，通过图像级别的反馈训练语言模型生成结构化提示，并在空间布局和组合泛化方面实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Co-Reinforcement Learning for Unified Multimodal Understanding and Generation",
        "summary": "This paper presents a pioneering exploration of reinforcement learning (RL)\nvia group relative policy optimization for unified multimodal large language\nmodels (ULMs), aimed at simultaneously reinforcing generation and understanding\ncapabilities. Through systematic pilot studies, we uncover the significant\npotential of ULMs to enable the synergistic co-evolution of dual capabilities\nwithin a shared policy optimization framework. Building on this insight, we\nintroduce \\textbf{CoRL}, a co-reinforcement learning framework comprising a\nunified RL stage for joint optimization and a refined RL stage for\ntask-specific enhancement. With the proposed CoRL, our resulting model,\n\\textbf{ULM-R1}, achieves average improvements of \\textbf{7%} on three\ntext-to-image generation datasets and \\textbf{23%} on nine multimodal\nunderstanding benchmarks. These results demonstrate the effectiveness of CoRL\nand highlight the substantial benefit of reinforcement learning in facilitating\ncross-task synergy and optimization for ULMs.",
        "url": "http://arxiv.org/abs/2505.17534v1",
        "published_date": "2025-05-23T06:41:07+00:00",
        "updated_date": "2025-05-23T06:41:07+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.MM"
        ],
        "authors": [
            "Jingjing Jiang",
            "Chongjie Si",
            "Jun Luo",
            "Hanwang Zhang",
            "Chao Ma"
        ],
        "tldr": "This paper introduces CoRL, a co-reinforcement learning framework for unified multimodal large language models (ULMs), achieving improvements in both text-to-image generation and multimodal understanding tasks.",
        "tldr_zh": "本文介绍了一种用于统一多模态大型语言模型（ULM）的协同强化学习框架CoRL，在文本到图像生成和多模态理解任务中均取得了改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving",
        "summary": "Visual language models (VLMs) have attracted increasing interest in\nautonomous driving due to their powerful reasoning capabilities. However,\nexisting VLMs typically utilize discrete text Chain-of-Thought (CoT) tailored\nto the current scenario, which essentially represents highly abstract and\nsymbolic compression of visual information, potentially leading to\nspatio-temporal relationship ambiguity and fine-grained information loss. Is\nautonomous driving better modeled on real-world simulation and imagination than\non pure symbolic logic? In this paper, we propose a spatio-temporal CoT\nreasoning method that enables models to think visually. First, VLM serves as a\nworld model to generate unified image frame for predicting future world states:\nwhere perception results (e.g., lane divider and 3D detection) represent the\nfuture spatial relationships, and ordinary future frame represent the temporal\nevolution relationships. This spatio-temporal CoT then serves as intermediate\nreasoning steps, enabling the VLM to function as an inverse dynamics model for\ntrajectory planning based on current observations and future predictions. To\nimplement visual generation in VLMs, we propose a unified pretraining paradigm\nintegrating visual generation and understanding, along with a progressive\nvisual CoT enhancing autoregressive image generation. Extensive experimental\nresults demonstrate the effectiveness of the proposed method, advancing\nautonomous driving towards visual reasoning.",
        "url": "http://arxiv.org/abs/2505.17685v1",
        "published_date": "2025-05-23T09:55:32+00:00",
        "updated_date": "2025-05-23T09:55:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuang Zeng",
            "Xinyuan Chang",
            "Mengwei Xie",
            "Xinran Liu",
            "Yifan Bai",
            "Zheng Pan",
            "Mu Xu",
            "Xing Wei"
        ],
        "tldr": "The paper proposes a spatio-temporal Chain-of-Thought (CoT) method using VLMs for autonomous driving, generating future world states as intermediate reasoning steps for trajectory planning. It introduces a unified pretraining paradigm for visual generation and understanding.",
        "tldr_zh": "该论文提出了一种基于视觉语言模型(VLMs)的时空链式思考(CoT)方法，用于自动驾驶，该方法生成未来世界状态作为轨迹规划的中间推理步骤。它引入了一种统一的视觉生成和理解的预训练范式。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]