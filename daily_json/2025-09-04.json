[
    {
        "title": "Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?",
        "summary": "Text-to-image (T2I) generation aims to synthesize images from textual\nprompts, which jointly specify what must be shown and imply what can be\ninferred, thereby corresponding to two core capabilities: composition and\nreasoning. However, with the emerging advances of T2I models in reasoning\nbeyond composition, existing benchmarks reveal clear limitations in providing\ncomprehensive evaluations across and within these capabilities. Meanwhile,\nthese advances also enable models to handle more complex prompts, whereas\ncurrent benchmarks remain limited to low scene density and simplified\none-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a\ncomprehensive and complex benchmark that evaluates both composition and\nreasoning capabilities of T2I models. To ensure comprehensiveness, we structure\ncomposition around scene graph elements (instance, attribute, and relation) and\nreasoning around the philosophical framework of inference (deductive,\ninductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To\nincrease complexity, driven by the inherent complexities of real-world\nscenarios, we curate each prompt with high compositional density for\ncomposition and multi-step inference for reasoning. We also pair each prompt\nwith a checklist that specifies individual yes/no questions to assess each\nintended element independently to facilitate fine-grained and reliable\nevaluation. In statistics, our benchmark comprises 1,080 challenging prompts\nand around 13,500 checklist questions. Experiments across 27 current T2I models\nreveal that their composition capability still remains limited in complex\nhigh-density scenarios, while the reasoning capability lags even further behind\nas a critical bottleneck, with all models struggling to infer implicit elements\nfrom prompts. Our project page: https://t2i-corebench.github.io/.",
        "url": "http://arxiv.org/abs/2509.03516v1",
        "published_date": "2025-09-03T17:58:12+00:00",
        "updated_date": "2025-09-03T17:58:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ouxiang Li",
            "Yuan Wang",
            "Xinting Hu",
            "Huijuan Huang",
            "Rui Chen",
            "Jiarong Ou",
            "Xin Tao",
            "Pengfei Wan",
            "Fuli Feng"
        ],
        "tldr": "This paper introduces T2I-CoReBench, a new benchmark designed to comprehensively evaluate the composition and reasoning capabilities of text-to-image models, revealing their current limitations in complex scenarios.",
        "tldr_zh": "该论文提出了T2I-CoReBench，一个新的基准，旨在全面评估文本到图像模型的组合和推理能力，揭示了它们在复杂场景中的当前局限性。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation",
        "summary": "We introduce OneCAT, a unified multimodal model that seamlessly integrates\nunderstanding, generation, and editing within a novel, pure decoder-only\ntransformer architecture. Our framework uniquely eliminates the need for\nexternal components such as Vision Transformers (ViT) or vision tokenizer\nduring inference, leading to significant efficiency gains, especially for\nhigh-resolution inputs. This is achieved through a modality-specific\nMixture-of-Experts (MoE) structure trained with a single autoregressive (AR)\nobjective, which also natively supports dynamic resolutions. Furthermore, we\npioneer a multi-scale visual autoregressive mechanism within the Large Language\nModel (LLM) that drastically reduces decoding steps compared to diffusion-based\nmethods while maintaining state-of-the-art performance. Our findings\ndemonstrate the powerful potential of pure autoregressive modeling as a\nsufficient and elegant foundation for unified multimodal intelligence. As a\nresult, OneCAT sets a new performance standard, outperforming existing\nopen-source unified multimodal models across benchmarks for multimodal\ngeneration, editing, and understanding.",
        "url": "http://arxiv.org/abs/2509.03498v1",
        "published_date": "2025-09-03T17:29:50+00:00",
        "updated_date": "2025-09-03T17:29:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Han Li",
            "Xinyu Peng",
            "Yaoming Wang",
            "Zelin Peng",
            "Xin Chen",
            "Rongxiang Weng",
            "Jingang Wang",
            "Xunliang Cai",
            "Wenrui Dai",
            "Hongkai Xiong"
        ],
        "tldr": "OneCAT is a decoder-only transformer architecture for unified multimodal understanding, generation, and editing, achieving efficiency and SOTA performance by eliminating external vision components and using a multi-scale visual autoregressive mechanism.",
        "tldr_zh": "OneCAT是一个纯解码器Transformer架构，用于统一的多模态理解、生成和编辑。它通过消除外部视觉组件并使用多尺度视觉自回归机制，实现了效率提升和新的SOTA性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Temporally-Aware Diffusion Model for Brain Progression Modelling with Bidirectional Temporal Regularisation",
        "summary": "Generating realistic MRIs to accurately predict future changes in the\nstructure of brain is an invaluable tool for clinicians in assessing clinical\noutcomes and analysing the disease progression at the patient level. However,\ncurrent existing methods present some limitations: (i) some approaches fail to\nexplicitly capture the relationship between structural changes and time\nintervals, especially when trained on age-imbalanced datasets; (ii) others rely\nonly on scan interpolation, which lack clinical utility, as they generate\nintermediate images between timepoints rather than future pathological\nprogression; and (iii) most approaches rely on 2D slice-based architectures,\nthereby disregarding full 3D anatomical context, which is essential for\naccurate longitudinal predictions. We propose a 3D Temporally-Aware Diffusion\nModel (TADM-3D), which accurately predicts brain progression on MRI volumes. To\nbetter model the relationship between time interval and brain changes, TADM-3D\nuses a pre-trained Brain-Age Estimator (BAE) that guides the diffusion model in\nthe generation of MRIs that accurately reflect the expected age difference\nbetween baseline and generated follow-up scans. Additionally, to further\nimprove the temporal awareness of TADM-3D, we propose the Back-In-Time\nRegularisation (BITR), by training TADM-3D to predict bidirectionally from the\nbaseline to follow-up (forward), as well as from the follow-up to baseline\n(backward). Although predicting past scans has limited clinical applications,\nthis regularisation helps the model generate temporally more accurate scans. We\ntrain and evaluate TADM-3D on the OASIS-3 dataset, and we validate the\ngeneralisation performance on an external test set from the NACC dataset. The\ncode will be available upon acceptance.",
        "url": "http://arxiv.org/abs/2509.03141v1",
        "published_date": "2025-09-03T08:51:38+00:00",
        "updated_date": "2025-09-03T08:51:38+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Mattia Litrico",
            "Francesco Guarnera",
            "Mario Valerio Giuffrida",
            "Daniele Ravì",
            "Sebastiano Battiato"
        ],
        "tldr": "This paper introduces a 3D Temporally-Aware Diffusion Model (TADM-3D) for predicting brain progression in MRI volumes, using a pre-trained Brain-Age Estimator and a novel Back-In-Time Regularisation technique to improve temporal accuracy.",
        "tldr_zh": "本文介绍了一种用于预测MRI脑部进展的3D时间感知扩散模型(TADM-3D)，该模型使用预训练的脑年龄估计器和一种新的时间倒流正则化技术来提高时间准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]