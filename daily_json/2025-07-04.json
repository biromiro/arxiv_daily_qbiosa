[
    {
        "title": "RefTok: Reference-Based Tokenization for Video Generation",
        "summary": "Effectively handling temporal redundancy remains a key challenge in learning\nvideo models. Prevailing approaches often treat each set of frames\nindependently, failing to effectively capture the temporal dependencies and\nredundancies inherent in videos. To address this limitation, we introduce\nRefTok, a novel reference-based tokenization method capable of capturing\ncomplex temporal dynamics and contextual information. Our method encodes and\ndecodes sets of frames conditioned on an unquantized reference frame. When\ndecoded, RefTok preserves the continuity of motion and the appearance of\nobjects across frames. For example, RefTok retains facial details despite head\nmotion, reconstructs text correctly, preserves small patterns, and maintains\nthe legibility of handwriting from the context. Across 4 video datasets (K600,\nUCF-101, BAIR Robot Pushing, and DAVIS), RefTok significantly outperforms\ncurrent state-of-the-art tokenizers (Cosmos and MAGVIT) and improves all\nevaluated metrics (PSNR, SSIM, LPIPS) by an average of 36.7% at the same or\nhigher compression ratios. When a video generation model is trained using\nRefTok's latents on the BAIR Robot Pushing task, the generations not only\noutperform MAGVIT-B but the larger MAGVIT-L, which has 4x more parameters,\nacross all generation metrics by an average of 27.9%.",
        "url": "http://arxiv.org/abs/2507.02862v1",
        "published_date": "2025-07-03T17:59:55+00:00",
        "updated_date": "2025-07-03T17:59:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiang Fan",
            "Xiaohang Sun",
            "Kushan Thakkar",
            "Zhu Liu",
            "Vimal Bhat",
            "Ranjay Krishna",
            "Xiang Hao"
        ],
        "tldr": "The paper introduces RefTok, a novel reference-based tokenization method for video generation that captures temporal dynamics and contextual information by encoding and decoding frames conditioned on a reference frame, achieving significant improvements over state-of-the-art tokenizers.",
        "tldr_zh": "该论文介绍了一种名为RefTok的新型基于参考的视频生成tokenization方法。 此方法通过对以参考帧为条件的帧进行编码和解码来捕获时间动态和上下文信息，从而显著优于最先进的tokenization方法。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "AnyI2V: Animating Any Conditional Image with Motion Control",
        "summary": "Recent advancements in video generation, particularly in diffusion models,\nhave driven notable progress in text-to-video (T2V) and image-to-video (I2V)\nsynthesis. However, challenges remain in effectively integrating dynamic motion\nsignals and flexible spatial constraints. Existing T2V methods typically rely\non text prompts, which inherently lack precise control over the spatial layout\nof generated content. In contrast, I2V methods are limited by their dependence\non real images, which restricts the editability of the synthesized content.\nAlthough some methods incorporate ControlNet to introduce image-based\nconditioning, they often lack explicit motion control and require\ncomputationally expensive training. To address these limitations, we propose\nAnyI2V, a training-free framework that animates any conditional images with\nuser-defined motion trajectories. AnyI2V supports a broader range of modalities\nas the conditional image, including data types such as meshes and point clouds\nthat are not supported by ControlNet, enabling more flexible and versatile\nvideo generation. Additionally, it supports mixed conditional inputs and\nenables style transfer and editing via LoRA and text prompts. Extensive\nexperiments demonstrate that the proposed AnyI2V achieves superior performance\nand provides a new perspective in spatial- and motion-controlled video\ngeneration. Code is available at https://henghuiding.com/AnyI2V/.",
        "url": "http://arxiv.org/abs/2507.02857v1",
        "published_date": "2025-07-03T17:59:02+00:00",
        "updated_date": "2025-07-03T17:59:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziye Li",
            "Hao Luo",
            "Xincheng Shuai",
            "Henghui Ding"
        ],
        "tldr": "The paper introduces AnyI2V, a training-free framework for animating conditional images with user-defined motion, supporting various input modalities and offering spatial and motion control in video generation.",
        "tldr_zh": "该论文介绍了一种名为 AnyI2V 的免训练框架，它可以使用用户定义的运动来动画处理条件图像，支持多种输入模态，并在视频生成中提供空间和运动控制。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]