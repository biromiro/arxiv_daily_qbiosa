[
    {
        "title": "DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization",
        "summary": "Customized text-to-video generation with pre-trained large-scale models has\nrecently garnered significant attention through focusing on identity and motion\nconsistency. Existing works typically follow the isolated customized paradigm,\nwhere the subject identity or motion dynamics are customized exclusively.\nHowever, this paradigm completely ignores the intrinsic mutual constraints and\nsynergistic interdependencies between identity and motion, resulting in\nidentity-motion conflicts throughout the generation process that systematically\ndegrades. To address this, we introduce DualReal, a novel framework that,\nemploys adaptive joint training to collaboratively construct interdependencies\nbetween dimensions. Specifically, DualReal is composed of two units: (1)\nDual-aware Adaptation dynamically selects a training phase (i.e., identity or\nmotion), learns the current information guided by the frozen dimension prior,\nand employs a regularization strategy to avoid knowledge leakage; (2)\nStageBlender Controller leverages the denoising stages and Diffusion\nTransformer depths to guide different dimensions with adaptive granularity,\navoiding conflicts at various stages and ultimately achieving lossless fusion\nof identity and motion patterns. We constructed a more comprehensive benchmark\nthan existing methods. The experimental results show that DualReal improves\nCLIP-I and DINO-I metrics by 21.7% and 31.8% on average, and achieves top\nperformance on nearly all motion quality metrics.",
        "url": "http://arxiv.org/abs/2505.02192v1",
        "published_date": "2025-05-04T17:19:20+00:00",
        "updated_date": "2025-05-04T17:19:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Wenchuan Wang",
            "Mengqi Huang",
            "Yijing Tu",
            "Zhendong Mao"
        ],
        "tldr": "the paper introduces dualreal, a framework that addresses identity-motion conflicts in customized text-to-video generation by adaptively and jointly training identity and motion, achieving improved performance on identity and motion quality metrics.",
        "tldr_zh": "该论文介绍了dualreal，一个通过自适应地联合训练身份和运动来解决定制文本到视频生成中身份-运动冲突的框架，并在身份和运动质量指标上实现了性能的提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Regression s all you need for medical image translation",
        "summary": "The acquisition of information-rich images within a limited time budget is\ncrucial in medical imaging. Medical image translation (MIT) can help enhance\nand supplement existing datasets by generating synthetic images from acquired\ndata. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have\nachieved remarkable success in natural image generation, their benefits -\ncreativity and image realism - do not necessarily transfer to medical\napplications where highly accurate anatomical information is required. In fact,\nthe imitation of acquisition noise or content hallucination hinder clinical\nutility. Here, we introduce YODA (You Only Denoise once - or Average), a novel\n2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and\nregression paradigms to produce realistic or noise-free outputs. Furthermore,\nwe propose Expectation-Approximation (ExpA) DM sampling, which draws\ninspiration from MRI signal averaging. ExpA-sampling suppresses generated noise\nand, thus, eliminates noise from biasing the evaluation of image quality.\nThrough extensive experiments on four diverse multi-modal datasets - comprising\nmulti-contrast brain MRI and pelvic MRI-CT - we show that diffusion and\nregression sampling yield similar results in practice. As such, the\ncomputational overhead of diffusion sampling does not provide systematic\nbenefits in medical information translation. Building on these insights, we\ndemonstrate that YODA outperforms several state-of-the-art GAN and DM methods.\nNotably, YODA-generated images are shown to be interchangeable with, or even\nsuperior to, physical acquisitions for several downstream tasks. Our findings\nchallenge the presumed advantages of DMs in MIT and pave the way for the\npractical application of MIT in medical imaging.",
        "url": "http://arxiv.org/abs/2505.02048v1",
        "published_date": "2025-05-04T09:57:10+00:00",
        "updated_date": "2025-05-04T09:57:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sebastian Rassmann",
            "David Kügler",
            "Christian Ewert",
            "Martin Reuter"
        ],
        "tldr": "this paper introduces yoda, a 2.5d diffusion-based framework for medical image translation, showing that a regression approach can outperform more complex diffusion models and gans while also being computationally efficient.",
        "tldr_zh": "本文介绍了一种名为yoda的2.5d扩散医学图像转换框架。研究表明，回归方法能够胜过更复杂的扩散模型和gan，同时兼顾计算效率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GenSync: A Generalized Talking Head Framework for Audio-driven Multi-Subject Lip-Sync using 3D Gaussian Splatting",
        "summary": "We introduce GenSync, a novel framework for multi-identity lip-synced video\nsynthesis using 3D Gaussian Splatting. Unlike most existing 3D methods that\nrequire training a new model for each identity , GenSync learns a unified\nnetwork that synthesizes lip-synced videos for multiple speakers. By\nincorporating a Disentanglement Module, our approach separates\nidentity-specific features from audio representations, enabling efficient\nmulti-identity video synthesis. This design reduces computational overhead and\nachieves 6.8x faster training compared to state-of-the-art models, while\nmaintaining high lip-sync accuracy and visual quality.",
        "url": "http://arxiv.org/abs/2505.01928v1",
        "published_date": "2025-05-03T21:44:59+00:00",
        "updated_date": "2025-05-03T21:44:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Anushka Agarwal",
            "Muhammad Yusuf Hassan",
            "Talha Chafekar"
        ],
        "tldr": "gensync is a new framework using 3d gaussian splatting for multi-identity lip-synced video synthesis, offering faster training and competitive quality compared to single-identity models.",
        "tldr_zh": "gensync是一种新的框架，使用3d高斯溅射进行多身份唇形同步视频合成，与单身份模型相比，它提供了更快的训练速度和具有竞争力的质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SignSplat: Rendering Sign Language via Gaussian Splatting",
        "summary": "State-of-the-art approaches for conditional human body rendering via Gaussian\nsplatting typically focus on simple body motions captured from many views. This\nis often in the context of dancing or walking. However, for more complex use\ncases, such as sign language, we care less about large body motion and more\nabout subtle and complex motions of the hands and face. The problems of\nbuilding high fidelity models are compounded by the complexity of capturing\nmulti-view data of sign. The solution is to make better use of sequence data,\nensuring that we can overcome the limited information from only a few views by\nexploiting temporal variability. Nevertheless, learning from sequence-level\ndata requires extremely accurate and consistent model fitting to ensure that\nappearance is consistent across complex motions. We focus on how to achieve\nthis, constraining mesh parameters to build an accurate Gaussian splatting\nframework from few views capable of modelling subtle human motion. We leverage\nregularization techniques on the Gaussian parameters to mitigate overfitting\nand rendering artifacts. Additionally, we propose a new adaptive control method\nto densify Gaussians and prune splat points on the mesh surface. To demonstrate\nthe accuracy of our approach, we render novel sequences of sign language video,\nbuilding on neural machine translation approaches to sign stitching. On\nbenchmark datasets, our approach achieves state-of-the-art performance; and on\nhighly articulated and complex sign language motion, we significantly\noutperform competing approaches.",
        "url": "http://arxiv.org/abs/2505.02108v1",
        "published_date": "2025-05-04T13:28:49+00:00",
        "updated_date": "2025-05-04T13:28:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Maksym Ivashechkin",
            "Oscar Mendez",
            "Richard Bowden"
        ],
        "tldr": "this paper presents signsplat, a gaussian splatting framework for rendering sign language from few views, focusing on subtle hand and face movements by using sequence data and regularization techniques. it achieves state-of-the-art results on sign language datasets.",
        "tldr_zh": "本文提出signsplat，一个高斯溅射框架，用于从少数视角渲染手语，侧重于细微的手部和面部动作，通过使用序列数据和正则化技术来实现。 在手语数据集上实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "HandOcc: NeRF-based Hand Rendering with Occupancy Networks",
        "summary": "We propose HandOcc, a novel framework for hand rendering based upon\noccupancy. Popular rendering methods such as NeRF are often combined with\nparametric meshes to provide deformable hand models. However, in doing so, such\napproaches present a trade-off between the fidelity of the mesh and the\ncomplexity and dimensionality of the parametric model. The simplicity of\nparametric mesh structures is appealing, but the underlying issue is that it\nbinds methods to mesh initialization, making it unable to generalize to objects\nwhere a parametric model does not exist. It also means that estimation is tied\nto mesh resolution and the accuracy of mesh fitting. This paper presents a\npipeline for meshless 3D rendering, which we apply to the hands. By providing\nonly a 3D skeleton, the desired appearance is extracted via a convolutional\nmodel. We do this by exploiting a NeRF renderer conditioned upon an\noccupancy-based representation. The approach uses the hand occupancy to resolve\nhand-to-hand interactions further improving results, allowing fast rendering,\nand excellent hand appearance transfer. On the benchmark InterHand2.6M dataset,\nwe achieved state-of-the-art results.",
        "url": "http://arxiv.org/abs/2505.02079v1",
        "published_date": "2025-05-04T12:06:54+00:00",
        "updated_date": "2025-05-04T12:06:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Maksym Ivashechkin",
            "Oscar Mendez",
            "Richard Bowden"
        ],
        "tldr": "handocc proposes a novel meshless hand rendering pipeline using nerf conditioned on an occupancy-based representation, achieving state-of-the-art results on the interhand2.6m dataset.",
        "tldr_zh": "handocc 提出了一种新颖的无网格手部渲染流程，该流程使用以 occupancy 为基础表示的 nerf，并在 interhand2.6m 数据集上取得了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "OT-Talk: Animating 3D Talking Head with Optimal Transportation",
        "summary": "Animating 3D head meshes using audio inputs has significant applications in\nAR/VR, gaming, and entertainment through 3D avatars. However, bridging the\nmodality gap between speech signals and facial dynamics remains a challenge,\noften resulting in incorrect lip syncing and unnatural facial movements. To\naddress this, we propose OT-Talk, the first approach to leverage optimal\ntransportation to optimize the learning model in talking head animation.\nBuilding on existing learning frameworks, we utilize a pre-trained Hubert model\nto extract audio features and a transformer model to process temporal\nsequences. Unlike previous methods that focus solely on vertex coordinates or\ndisplacements, we introduce Chebyshev Graph Convolution to extract geometric\nfeatures from triangulated meshes. To measure mesh dissimilarities, we go\nbeyond traditional mesh reconstruction errors and velocity differences between\nadjacent frames. Instead, we represent meshes as probability measures and\napproximate their surfaces. This allows us to leverage the sliced Wasserstein\ndistance for modeling mesh variations. This approach facilitates the learning\nof smooth and accurate facial motions, resulting in coherent and natural facial\nanimations. Our experiments on two public audio-mesh datasets demonstrate that\nour method outperforms state-of-the-art techniques both quantitatively and\nqualitatively in terms of mesh reconstruction accuracy and temporal alignment.\nIn addition, we conducted a user perception study with 20 volunteers to further\nassess the effectiveness of our approach.",
        "url": "http://arxiv.org/abs/2505.01932v1",
        "published_date": "2025-05-03T21:49:23+00:00",
        "updated_date": "2025-05-03T21:49:23+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Xinmu Wang",
            "Xiang Gao",
            "Xiyun Song",
            "Heather Yu",
            "Zongfang Lin",
            "Liang Peng",
            "Xianfeng Gu"
        ],
        "tldr": "the paper introduces ot-talk, a novel approach using optimal transportation and geometric feature extraction to improve the accuracy and naturalness of 3d talking head animation from audio inputs, demonstrating superior performance compared to state-of-the-art methods.",
        "tldr_zh": "该论文介绍了 ot-talk，一种利用最优传输和几何特征提取的新方法，旨在提高从音频输入生成 3d 说话头动画的准确性和自然性，实验证明其性能优于现有技术。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Small Clips, Big Gains: Learning Long-Range Refocused Temporal Information for Video Super-Resolution",
        "summary": "Video super-resolution (VSR) can achieve better performance compared to\nsingle image super-resolution by additionally leveraging temporal information.\nIn particular, the recurrent-based VSR model exploits long-range temporal\ninformation during inference and achieves superior detail restoration. However,\neffectively learning these long-term dependencies within long videos remains a\nkey challenge. To address this, we propose LRTI-VSR, a novel training framework\nfor recurrent VSR that efficiently leverages Long-Range Refocused Temporal\nInformation. Our framework includes a generic training strategy that utilizes\ntemporal propagation features from long video clips while training on shorter\nvideo clips. Additionally, we introduce a refocused intra&inter-frame\ntransformer block which allows the VSR model to selectively prioritize useful\ntemporal information through its attention module while further improving\ninter-frame information utilization in the FFN module. We evaluate LRTI-VSR on\nboth CNN and transformer-based VSR architectures, conducting extensive ablation\nstudies to validate the contribution of each component. Experiments on\nlong-video test sets demonstrate that LRTI-VSR achieves state-of-the-art\nperformance while maintaining training and computational efficiency.",
        "url": "http://arxiv.org/abs/2505.02159v1",
        "published_date": "2025-05-04T15:46:34+00:00",
        "updated_date": "2025-05-04T15:46:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xingyu Zhou",
            "Wei Long",
            "Jingbo Lu",
            "Shiyin Jiang",
            "Weiyi You",
            "Haifeng Wu",
            "Shuhang Gu"
        ],
        "tldr": "the paper introduces lrti-vsr, a novel training framework for recurrent video super-resolution that efficiently leverages long-range temporal information by training on short clips while utilizing temporal propagation features from longer clips and a refocused transformer block, achieving sota results.",
        "tldr_zh": "本文提出了一种名为lrti-vsr的循环视频超分辨率训练框架，通过在短视频片段上训练，同时利用来自长视频片段的时间传播特征和一个重新聚焦的transformer模块，有效地利用了长距离的时间信息，并实现了最先进的结果。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Hierarchical Compact Clustering Attention (COCA) for Unsupervised Object-Centric Learning",
        "summary": "We propose the Compact Clustering Attention (COCA) layer, an effective\nbuilding block that introduces a hierarchical strategy for object-centric\nrepresentation learning, while solving the unsupervised object discovery task\non single images. COCA is an attention-based clustering module capable of\nextracting object-centric representations from multi-object scenes, when\ncascaded into a bottom-up hierarchical network architecture, referred to as\nCOCA-Net. At its core, COCA utilizes a novel clustering algorithm that\nleverages the physical concept of compactness, to highlight distinct object\ncentroids in a scene, providing a spatial inductive bias. Thanks to this\nstrategy, COCA-Net generates high-quality segmentation masks on both the\ndecoder side and, notably, the encoder side of its pipeline. Additionally,\nCOCA-Net is not bound by a predetermined number of object masks that it\ngenerates and handles the segmentation of background elements better than its\ncompetitors. We demonstrate COCA-Net's segmentation performance on six widely\nadopted datasets, achieving superior or competitive results against the\nstate-of-the-art models across nine different evaluation metrics.",
        "url": "http://arxiv.org/abs/2505.02071v1",
        "published_date": "2025-05-04T11:42:04+00:00",
        "updated_date": "2025-05-04T11:42:04+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Can Küçüksözen",
            "Yücel Yemez"
        ],
        "tldr": "the paper introduces coca and coca-net, a hierarchical attention-based clustering approach for unsupervised object-centric representation learning and segmentation in single images, achieving competitive or superior results on several benchmark datasets.",
        "tldr_zh": "该论文介绍了coca和coca-net，一种用于无监督的以物体为中心的表征学习和单张图像分割的层级式、基于注意力的聚类方法，并在多个基准数据集上取得了具有竞争力的或更优异的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation",
        "summary": "Reasoning stands as a cornerstone of intelligence, enabling the synthesis of\nexisting knowledge to solve complex problems. Despite remarkable progress,\nexisting reasoning benchmarks often fail to rigorously evaluate the nuanced\nreasoning capabilities required for complex, real-world problemsolving,\nparticularly in multi-disciplinary and multimodal contexts. In this paper, we\nintroduce a graduate-level, multi-disciplinary, EnglishChinese benchmark,\ndubbed as Reasoning Bench (R-Bench), for assessing the reasoning capability of\nboth language and multimodal models. RBench spans 1,094 questions across 108\nsubjects for language model evaluation and 665 questions across 83 subjects for\nmultimodal model testing in both English and Chinese. These questions are\nmeticulously curated to ensure rigorous difficulty calibration, subject\nbalance, and crosslinguistic alignment, enabling the assessment to be an\nOlympiad-level multi-disciplinary benchmark. We evaluate widely used models,\nincluding OpenAI o1, GPT-4o, DeepSeek-R1, etc. Experimental results indicate\nthat advanced models perform poorly on complex reasoning, especially multimodal\nreasoning. Even the top-performing model OpenAI o1 achieves only 53.2% accuracy\non our multimodal evaluation. Data and code are made publicly available at\nhere.",
        "url": "http://arxiv.org/abs/2505.02018v1",
        "published_date": "2025-05-04T07:48:36+00:00",
        "updated_date": "2025-05-04T07:48:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Meng-Hao Guo",
            "Jiajun Xu",
            "Yi Zhang",
            "Jiaxi Song",
            "Haoyang Peng",
            "Yi-Xuan Deng",
            "Xinzhi Dong",
            "Kiyohiro Nakayama",
            "Zhengyang Geng",
            "Chen Wang",
            "Bolin Ni",
            "Guo-Wei Yang",
            "Yongming Rao",
            "Houwen Peng",
            "Han Hu",
            "Gordon Wetzstein",
            "Shi-min Hu"
        ],
        "tldr": "the paper introduces r-bench, a new graduate-level, multi-disciplinary, english-chinese benchmark for evaluating the complex reasoning capabilities of llms and mllms, revealing the limitations of even state-of-the-art models.",
        "tldr_zh": "该论文介绍了r-bench，一个新的研究生水平、多学科、英汉双语的基准测试，用于评估llm和mllm的复杂推理能力，揭示了即使是最先进模型的局限性。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields",
        "summary": "Recent NeRF methods on large-scale scenes have underlined the importance of\nscene decomposition for scalable NeRFs. Although achieving reasonable\nscalability, there are several critical problems remaining unexplored, i.e.,\nlearnable decomposition, modeling scene heterogeneity, and modeling efficiency.\nIn this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash\nExperts (HMoHE) network that addresses these challenges within a unified\nframework. It is a highly scalable NeRF that learns heterogeneous decomposition\nand heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end\nmanner. In our framework, a gating network learns to decomposes scenes and\nallocates 3D points to specialized NeRF experts. This gating network is\nco-optimized with the experts, by our proposed Sparsely Gated Mixture of\nExperts (MoE) NeRF framework. We incorporate a hash-based gating network and\ndistinct heterogeneous hash experts. The hash-based gating efficiently learns\nthe decomposition of the large-scale scene. The distinct heterogeneous hash\nexperts consist of hash grids of different resolution ranges, enabling\neffective learning of the heterogeneous representation of different scene\nparts. These design choices make our framework an end-to-end and highly\nscalable NeRF solution for real-world large-scale scene modeling to achieve\nboth quality and efficiency. We evaluate our accuracy and scalability on\nexisting large-scale NeRF datasets and a new dataset with very large-scale\nscenes ($>6.5km^2$) from UrbanBIS. Extensive experiments demonstrate that our\napproach can be easily scaled to various large-scale scenes and achieve\nstate-of-the-art scene rendering accuracy. Furthermore, our method exhibits\nsignificant efficiency, with an 8x acceleration in training and a 16x\nacceleration in rendering compared to Switch-NeRF. Codes will be released in\nhttps://github.com/MiZhenxing/Switch-NeRF.",
        "url": "http://arxiv.org/abs/2505.02005v1",
        "published_date": "2025-05-04T06:25:14+00:00",
        "updated_date": "2025-05-04T06:25:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenxing Mi",
            "Ping Yin",
            "Xue Xiao",
            "Dan Xu"
        ],
        "tldr": "this paper introduces switch-nerf++, a highly scalable nerf method for large-scale scene modeling that uses a heterogeneous mixture of hash experts to achieve state-of-the-art rendering accuracy with significant training and rendering efficiency improvements.",
        "tldr_zh": "本文介绍了switch-nerf++，一种可高度扩展的nerf方法，用于大规模场景建模。该方法使用异构哈希专家混合模型（heterogeneous mixture of hash experts），实现了最先进的渲染精度，并显著提高了训练和渲染效率。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "Visual Dominance and Emerging Multimodal Approaches in Distracted Driving Detection: A Review of Machine Learning Techniques",
        "summary": "Distracted driving continues to be a significant cause of road traffic\ninjuries and fatalities worldwide, even with advancements in driver monitoring\ntechnologies. Recent developments in machine learning (ML) and deep learning\n(DL) have primarily focused on visual data to detect distraction, often\nneglecting the complex, multimodal nature of driver behavior. This systematic\nreview assesses 74 peer-reviewed studies from 2019 to 2024 that utilize ML/DL\ntechniques for distracted driving detection across visual, sensor-based,\nmultimodal, and emerging modalities. The review highlights a significant\nprevalence of visual-only models, particularly convolutional neural networks\n(CNNs) and temporal architectures, which achieve high accuracy but show limited\ngeneralizability in real-world scenarios. Sensor-based and physiological models\nprovide complementary strengths by capturing internal states and vehicle\ndynamics, while emerging techniques, such as auditory sensing and radio\nfrequency (RF) methods, offer privacy-aware alternatives. Multimodal\narchitecture consistently surpasses unimodal baselines, demonstrating enhanced\nrobustness, context awareness, and scalability by integrating diverse data\nstreams. These findings emphasize the need to move beyond visual-only\napproaches and adopt multimodal systems that combine visual, physiological, and\nvehicular cues while keeping in checking the need to balance computational\nrequirements. Future research should focus on developing lightweight,\ndeployable multimodal frameworks, incorporating personalized baselines, and\nestablishing cross-modality benchmarks to ensure real-world reliability in\nadvanced driver assistance systems (ADAS) and road safety interventions.",
        "url": "http://arxiv.org/abs/2505.01973v1",
        "published_date": "2025-05-04T02:51:00+00:00",
        "updated_date": "2025-05-04T02:51:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Anthony Dontoh",
            "Stephanie Ivey",
            "Logan Sirbaugh",
            "Andrews Danyo",
            "Armstrong Aboah"
        ],
        "tldr": "this paper reviews machine learning techniques for distracted driving detection, highlighting the prevalence of visual-only models and advocating for multimodal approaches that integrate visual, physiological, and vehicular cues for improved robustness and real-world applicability.",
        "tldr_zh": "本文回顾了用于检测驾驶分心的机器学习技术，强调了纯视觉模型的主导地位，并提倡采用集成视觉、生理和车辆线索的多模态方法，以提高鲁棒性和现实应用性。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 5
    },
    {
        "title": "CMAWRNet: Multiple Adverse Weather Removal via a Unified Quaternion Neural Architecture",
        "summary": "Images used in real-world applications such as image or video retrieval,\noutdoor surveillance, and autonomous driving suffer from poor weather\nconditions. When designing robust computer vision systems, removing adverse\nweather such as haze, rain, and snow is a significant problem. Recently,\ndeep-learning methods offered a solution for a single type of degradation.\nCurrent state-of-the-art universal methods struggle with combinations of\ndegradations, such as haze and rain-streak. Few algorithms have been developed\nthat perform well when presented with images containing multiple adverse\nweather conditions. This work focuses on developing an efficient solution for\nmultiple adverse weather removal using a unified quaternion neural architecture\ncalled CMAWRNet. It is based on a novel texture-structure decomposition block,\na novel lightweight encoder-decoder quaternion transformer architecture, and an\nattentive fusion block with low-light correction. We also introduce a\nquaternion similarity loss function to preserve color information better. The\nquantitative and qualitative evaluation of the current state-of-the-art\nbenchmarking datasets and real-world images shows the performance advantages of\nthe proposed CMAWRNet compared to other state-of-the-art weather removal\napproaches dealing with multiple weather artifacts. Extensive computer\nsimulations validate that CMAWRNet improves the performance of downstream\napplications such as object detection. This is the first time the decomposition\napproach has been applied to the universal weather removal task.",
        "url": "http://arxiv.org/abs/2505.01882v1",
        "published_date": "2025-05-03T18:02:19+00:00",
        "updated_date": "2025-05-03T18:02:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Vladimir Frants",
            "Sos Agaian",
            "Karen Panetta",
            "Peter Huang"
        ],
        "tldr": "the paper introduces cmawrnet, a quaternion neural network architecture for removing multiple adverse weather conditions from images, outperforming existing methods and improving downstream object detection performance. it uses a texture-structure decomposition block, a lightweight encoder-decoder quaternion transformer architecture, and an attentive fusion block with low-light correction.",
        "tldr_zh": "该论文介绍了一种名为 cmawrnet 的四元数神经网络架构，用于去除图像中的多种不利天气条件，性能优于现有方法，并提高了下游目标检测的性能。它使用纹理-结构分解块、轻量级编码器-解码器四元数 transformer 架构和带有弱光校正的注意力融合块。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "Local Herb Identification Using Transfer Learning: A CNN-Powered Mobile Application for Nepalese Flora",
        "summary": "Herb classification presents a critical challenge in botanical research,\nparticularly in regions with rich biodiversity such as Nepal. This study\nintroduces a novel deep learning approach for classifying 60 different herb\nspecies using Convolutional Neural Networks (CNNs) and transfer learning\ntechniques. Using a manually curated dataset of 12,000 herb images, we\ndeveloped a robust machine learning model that addresses existing limitations\nin herb recognition methodologies. Our research employed multiple model\narchitectures, including DenseNet121, 50-layer Residual Network (ResNet50),\n16-layer Visual Geometry Group Network (VGG16), InceptionV3, EfficientNetV2,\nand Vision Transformer (VIT), with DenseNet121 ultimately demonstrating\nsuperior performance. Data augmentation and regularization techniques were\napplied to mitigate overfitting and enhance the generalizability of the model.\nThis work advances herb classification techniques, preserving traditional\nbotanical knowledge and promoting sustainable herb utilization.",
        "url": "http://arxiv.org/abs/2505.02147v1",
        "published_date": "2025-05-04T15:14:44+00:00",
        "updated_date": "2025-05-04T15:14:44+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "I.4.9"
        ],
        "authors": [
            "Prajwal Thapa",
            "Mridul Sharma",
            "Jinu Nyachhyon",
            "Yagya Raj Pandeya"
        ],
        "tldr": "this paper presents a cnn-based mobile application for local herb identification in nepal, utilizing transfer learning and a curated dataset to classify 60 herb species, with densenet121 showing the best performance.",
        "tldr_zh": "本文介绍了一种基于cnn的手机应用程序，用于尼泊尔当地草药识别，利用迁移学习和精选数据集对60种草药进行分类，其中densenet121表现最佳。",
        "relevance_score": 2,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 4
    }
]