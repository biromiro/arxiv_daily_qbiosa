[
    {
        "title": "Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation",
        "summary": "Synthesizing interactive 3D scenes from text is essential for gaming, virtual\nreality, and embodied AI. However, existing methods face several challenges.\nLearning-based approaches depend on small-scale indoor datasets, limiting the\nscene diversity and layout complexity. While large language models (LLMs) can\nleverage diverse text-domain knowledge, they struggle with spatial realism,\noften producing unnatural object placements that fail to respect common sense.\nOur key insight is that vision perception can bridge this gap by providing\nrealistic spatial guidance that LLMs lack. To this end, we introduce\nScenethesis, a training-free agentic framework that integrates LLM-based scene\nplanning with vision-guided layout refinement. Given a text prompt, Scenethesis\nfirst employs an LLM to draft a coarse layout. A vision module then refines it\nby generating an image guidance and extracting scene structure to capture\ninter-object relations. Next, an optimization module iteratively enforces\naccurate pose alignment and physical plausibility, preventing artifacts like\nobject penetration and instability. Finally, a judge module verifies spatial\ncoherence. Comprehensive experiments show that Scenethesis generates diverse,\nrealistic, and physically plausible 3D interactive scenes, making it valuable\nfor virtual content creation, simulation environments, and embodied AI\nresearch.",
        "url": "http://arxiv.org/abs/2505.02836v1",
        "published_date": "2025-05-05T17:59:58+00:00",
        "updated_date": "2025-05-05T17:59:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lu Ling",
            "Chen-Hsuan Lin",
            "Tsung-Yi Lin",
            "Yifan Ding",
            "Yu Zeng",
            "Yichen Sheng",
            "Yunhao Ge",
            "Ming-Yu Liu",
            "Aniket Bera",
            "Zhaoshuo Li"
        ],
        "tldr": "scenethesis is a training-free agentic framework that leverages llms and vision perception to generate realistic and physically plausible 3d scenes from text prompts, addressing limitations of existing methods.",
        "tldr_zh": "scenethesis是一个无需训练的agentic框架，它利用llm和视觉感知来从文本提示生成逼真且物理上合理的3d场景，解决了现有方法的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves",
        "summary": "Recent studies have demonstrated that learning a meaningful internal\nrepresentation can both accelerate generative training and enhance generation\nquality of the diffusion transformers. However, existing approaches necessitate\nto either introduce an additional and complex representation training framework\nor rely on a large-scale, pre-trained representation foundation model to\nprovide representation guidance during the original generative training\nprocess. In this study, we posit that the unique discriminative process\ninherent to diffusion transformers enables them to offer such guidance without\nrequiring external representation components. We therefore propose\nSelf-Representation A}lignment (SRA), a simple yet straightforward method that\nobtain representation guidance through a self-distillation manner.\nSpecifically, SRA aligns the output latent representation of the diffusion\ntransformer in earlier layer with higher noise to that in later layer with\nlower noise to progressively enhance the overall representation learning during\nonly generative training process. Experimental results indicate that applying\nSRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRA\nnot only significantly outperforms approaches relying on auxiliary, complex\nrepresentation training frameworks but also achieves performance comparable to\nmethods that heavily dependent on powerful external representation priors.",
        "url": "http://arxiv.org/abs/2505.02831v1",
        "published_date": "2025-05-05T17:58:05+00:00",
        "updated_date": "2025-05-05T17:58:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dengyang Jiang",
            "Mengmeng Wang",
            "Liuzhuozheng Li",
            "Lei Zhang",
            "Haoyu Wang",
            "Wei Wei",
            "Guang Dai",
            "Yanning Zhang",
            "Jingdong Wang"
        ],
        "tldr": "this paper proposes self-representation alignment (sra), a method to improve diffusion transformer performance by using the model's inherent discriminative properties for representation guidance, avoiding the need for external representation components or pre-trained models.",
        "tldr_zh": "该论文提出自表示对齐（sra）方法，通过利用扩散转换器固有的判别特性来进行表示引导，从而提高扩散转换器的性能，避免了对外部表示组件或预训练模型的需求。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models",
        "summary": "Text-to-image (T2I) diffusion models have rapidly advanced, enabling\nhigh-quality image generation conditioned on textual prompts. However, the\ngrowing trend of fine-tuning pre-trained models for personalization raises\nserious concerns about unauthorized dataset usage. To combat this, dataset\nownership verification (DOV) has emerged as a solution, embedding watermarks\ninto the fine-tuning datasets using backdoor techniques. These watermarks\nremain inactive under benign samples but produce owner-specified outputs when\ntriggered. Despite the promise of DOV for T2I diffusion models, its robustness\nagainst copyright evasion attacks (CEA) remains unexplored. In this paper, we\nexplore how attackers can bypass these mechanisms through CEA, allowing models\nto circumvent watermarks even when trained on watermarked datasets. We propose\nthe first copyright evasion attack (i.e., CEAT2I) specifically designed to\nundermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three\nstages: watermarked sample detection, trigger identification, and efficient\nwatermark mitigation. A key insight driving our approach is that T2I models\nexhibit faster convergence on watermarked samples during the fine-tuning,\nevident through intermediate feature deviation. Leveraging this, CEAT2I can\nreliably detect the watermarked samples. Then, we iteratively ablate tokens\nfrom the prompts of detected watermarked samples and monitor shifts in\nintermediate features to pinpoint the exact trigger tokens. Finally, we adopt a\nclosed-form concept erasure method to remove the injected watermark. Extensive\nexperiments show that our CEAT2I effectively evades DOV mechanisms while\npreserving model performance.",
        "url": "http://arxiv.org/abs/2505.02824v1",
        "published_date": "2025-05-05T17:51:55+00:00",
        "updated_date": "2025-05-05T17:51:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CR"
        ],
        "authors": [
            "Kuofeng Gao",
            "Yufei Zhu",
            "Yiming Li",
            "Jiawang Bai",
            "Yong Yang",
            "Zhifeng Li",
            "Shu-Tao Xia"
        ],
        "tldr": "this paper introduces a novel copyright evasion attack (ceat2i) against dataset ownership verification (dov) mechanisms in personalized text-to-image diffusion models, demonstrating its effectiveness in bypassing watermarks.",
        "tldr_zh": "本文提出了一种针对个性化文本到图像扩散模型中数据集所有权验证（dov）机制的新型版权规避攻击（ceat2i），并证明其在绕过水印方面的有效性。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset via Attention Routing",
        "summary": "Current multi-subject customization approaches encounter two critical\nchallenges: the difficulty in acquiring diverse multi-subject training data,\nand attribute entanglement across different subjects. To bridge these gaps, we\npropose MUSAR - a simple yet effective framework to achieve robust\nmulti-subject customization while requiring only single-subject training data.\nFirstly, to break the data limitation, we introduce debiased diptych learning.\nIt constructs diptych training pairs from single-subject images to facilitate\nmulti-subject learning, while actively correcting the distribution bias\nintroduced by diptych construction via static attention routing and dual-branch\nLoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic\nattention routing mechanism, which adaptively establishes bijective mappings\nbetween generated images and conditional subjects. This design not only\nachieves decoupling of multi-subject representations but also maintains\nscalable generalization performance with increasing reference subjects.\nComprehensive experiments demonstrate that our MUSAR outperforms existing\nmethods - even those trained on multi-subject dataset - in image quality,\nsubject consistency, and interaction naturalness, despite requiring only\nsingle-subject dataset.",
        "url": "http://arxiv.org/abs/2505.02823v1",
        "published_date": "2025-05-05T17:50:24+00:00",
        "updated_date": "2025-05-05T17:50:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zinan Guo",
            "Pengze Zhang",
            "Yanze Wu",
            "Chong Mou",
            "Songtao Zhao",
            "Qian He"
        ],
        "tldr": "the paper introduces musar, a framework for multi-subject image customization using only single-subject training data, addressing data scarcity and subject entanglement challenges through debiased diptych learning and dynamic attention routing; it claims superior performance over methods trained on multi-subject data.",
        "tldr_zh": "该论文介绍了musar，一个仅使用单主体训练数据进行多主体图像定制的框架。它通过去偏二联画学习和动态注意力路由来解决数据稀缺和主体纠缠的挑战，并声称优于在多主体数据上训练的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MCCD: Multi-Agent Collaboration-based Compositional Diffusion for Complex Text-to-Image Generation",
        "summary": "Diffusion models have shown excellent performance in text-to-image\ngeneration. Nevertheless, existing methods often suffer from performance\nbottlenecks when handling complex prompts that involve multiple objects,\ncharacteristics, and relations. Therefore, we propose a Multi-agent\nCollaboration-based Compositional Diffusion (MCCD) for text-to-image generation\nfor complex scenes. Specifically, we design a multi-agent collaboration-based\nscene parsing module that generates an agent system comprising multiple agents\nwith distinct tasks, utilizing MLLMs to extract various scene elements\neffectively. In addition, Hierarchical Compositional diffusion utilizes a\nGaussian mask and filtering to refine bounding box regions and enhance objects\nthrough region enhancement, resulting in the accurate and high-fidelity\ngeneration of complex scenes. Comprehensive experiments demonstrate that our\nMCCD significantly improves the performance of the baseline models in a\ntraining-free manner, providing a substantial advantage in complex scene\ngeneration.",
        "url": "http://arxiv.org/abs/2505.02648v1",
        "published_date": "2025-05-05T13:50:03+00:00",
        "updated_date": "2025-05-05T13:50:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingcheng Li",
            "Xiaolu Hou",
            "Ziyang Liu",
            "Dingkang Yang",
            "Ziyun Qian",
            "Jiawei Chen",
            "Jinjie Wei",
            "Yue Jiang",
            "Qingyao Xu",
            "Lihua Zhang"
        ],
        "tldr": "mccd introduces a multi-agent collaboration and hierarchical compositional diffusion approach to improve text-to-image generation for complex prompts by effectively parsing scenes and enhancing object regions.",
        "tldr_zh": "mccd 提出了一种基于多智能体协作和分层组合扩散的方法，通过有效解析场景和强化对象区域，来改进复杂提示的文本到图像生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
        "summary": "Recent years have seen remarkable progress in both multimodal understanding\nmodels and image generation models. Despite their respective successes, these\ntwo domains have evolved independently, leading to distinct architectural\nparadigms: While autoregressive-based architectures have dominated multimodal\nunderstanding, diffusion-based models have become the cornerstone of image\ngeneration. Recently, there has been growing interest in developing unified\nframeworks that integrate these tasks. The emergence of GPT-4o's new\ncapabilities exemplifies this trend, highlighting the potential for\nunification. However, the architectural differences between the two domains\npose significant challenges. To provide a clear overview of current efforts\ntoward unification, we present a comprehensive survey aimed at guiding future\nresearch. First, we introduce the foundational concepts and recent advancements\nin multimodal understanding and text-to-image generation models. Next, we\nreview existing unified models, categorizing them into three main architectural\nparadigms: diffusion-based, autoregressive-based, and hybrid approaches that\nfuse autoregressive and diffusion mechanisms. For each category, we analyze the\nstructural designs and innovations introduced by related works. Additionally,\nwe compile datasets and benchmarks tailored for unified models, offering\nresources for future exploration. Finally, we discuss the key challenges facing\nthis nascent field, including tokenization strategy, cross-modal attention, and\ndata. As this area is still in its early stages, we anticipate rapid\nadvancements and will regularly update this survey. Our goal is to inspire\nfurther research and provide a valuable reference for the community. The\nreferences associated with this survey will be available on GitHub soon.",
        "url": "http://arxiv.org/abs/2505.02567v1",
        "published_date": "2025-05-05T11:18:03+00:00",
        "updated_date": "2025-05-05T11:18:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinjie Zhang",
            "Jintao Guo",
            "Shanshan Zhao",
            "Minghao Fu",
            "Lunhao Duan",
            "Guo-Hua Wang",
            "Qing-Guo Chen",
            "Zhao Xu",
            "Weihua Luo",
            "Kaifu Zhang"
        ],
        "tldr": "this paper surveys the emerging field of unified multimodal understanding and generation models, categorizing existing approaches (diffusion, autoregressive, hybrid), highlighting key challenges, and providing resources for future research. it aims to guide research in this rapidly evolving area.",
        "tldr_zh": "本文综述了统一多模态理解和生成模型的新兴领域，对现有方法（扩散模型、自回归模型、混合模型）进行分类，强调了关键挑战，并为未来研究提供了资源。旨在指导这一快速发展领域的研究。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Text to Image Generation and Editing: A Survey",
        "summary": "Text-to-image generation (T2I) refers to the text-guided generation of\nhigh-quality images. In the past few years, T2I has attracted widespread\nattention and numerous works have emerged. In this survey, we comprehensively\nreview 141 works conducted from 2021 to 2024. First, we introduce four\nfoundation model architectures of T2I (autoregression, non-autoregression, GAN\nand diffusion) and the commonly used key technologies (autoencoder, attention\nand classifier-free guidance). Secondly, we systematically compare the methods\nof these studies in two directions, T2I generation and T2I editing, including\nthe encoders and the key technologies they use. In addition, we also compare\nthe performance of these researches side by side in terms of datasets,\nevaluation metrics, training resources, and inference speed. In addition to the\nfour foundation models, we survey other works on T2I, such as energy-based\nmodels and recent Mamba and multimodality. We also investigate the potential\nsocial impact of T2I and provide some solutions. Finally, we propose unique\ninsights of improving the performance of T2I models and possible future\ndevelopment directions. In summary, this survey is the first systematic and\ncomprehensive overview of T2I, aiming to provide a valuable guide for future\nresearchers and stimulate continued progress in this field.",
        "url": "http://arxiv.org/abs/2505.02527v1",
        "published_date": "2025-05-05T10:08:31+00:00",
        "updated_date": "2025-05-05T10:08:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pengfei Yang",
            "Ngai-Man Cheung",
            "Xinda Ma"
        ],
        "tldr": "this survey paper comprehensively reviews text-to-image generation (t2i) methods from 2021-2024, covering architectures, key technologies, performance comparisons, and potential social impact, also providing insights and future directions.",
        "tldr_zh": "这篇综述全面回顾了2021-2024年的文本到图像生成(t2i)方法，涵盖架构、关键技术、性能比较和社会影响，并提供了见解和未来方向。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction",
        "summary": "We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a\nnewly designed unified visual generator and a native multimodal autoregressive\nmodel tailored for unifying vision and language. Specifically, this project\nprovides an open-source implementation of the integrated MetaQueries and\nM2-omni framework, while introducing the novel multi-scale learnable tokens and\nmulti-scale representation alignment strategy. By leveraging a fixed MLLM and a\nlearnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to\nperform both text-to-image generation and instruction based image editing\ntasks, expanding their capabilities beyond pure visual understanding. Our\nexperimental results demonstrate the strong performance of Ming-Lite-Uni and\nillustrate the impressive fluid nature of its interactive process. All code and\nmodel weights are open-sourced to foster further exploration within the\ncommunity. Notably, this work aligns with concurrent multimodal AI milestones -\nsuch as ChatGPT-4o with native image generation updated in March 25, 2025 -\nunderscoring the broader significance of unified models like Ming-Lite-Uni on\nthe path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further\nrefined.",
        "url": "http://arxiv.org/abs/2505.02471v1",
        "published_date": "2025-05-05T08:56:12+00:00",
        "updated_date": "2025-05-05T08:56:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Biao Gong",
            "Cheng Zou",
            "Dandan Zheng",
            "Hu Yu",
            "Jingdong Chen",
            "Jianxin Sun",
            "Junbo Zhao",
            "Jun Zhou",
            "Kaixiang Ji",
            "Lixiang Ru",
            "Libin Wang",
            "Qingpei Guo",
            "Rui Liu",
            "Weilong Chai",
            "Xinyu Xiao",
            "Ziyuan Huang"
        ],
        "tldr": "ming-lite-uni is a new open-source multimodal framework featuring a unified visual generator and a native multimodal autoregressive model for vision and language tasks, enabling text-to-image generation and instruction-based image editing.",
        "tldr_zh": "ming-lite-uni 是一个新的开源多模态框架，具有统一的视觉生成器和原生多模态自回归模型，用于视觉和语言任务，支持文本到图像的生成和基于指令的图像编辑。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based Image Editing",
        "summary": "Due to the challenges of manually collecting accurate editing data, existing\ndatasets are typically constructed using various automated methods, leading to\nnoisy supervision signals caused by the mismatch between editing instructions\nand original-edited image pairs. Recent efforts attempt to improve editing\nmodels through generating higher-quality edited images, pre-training on\nrecognition tasks, or introducing vision-language models (VLMs) but fail to\nresolve this fundamental issue. In this paper, we offer a novel solution by\nconstructing more effective editing instructions for given image pairs. This\nincludes rectifying the editing instructions to better align with the\noriginal-edited image pairs and using contrastive editing instructions to\nfurther enhance their effectiveness. Specifically, we find that editing models\nexhibit specific generation attributes at different inference steps,\nindependent of the text. Based on these prior attributes, we define a unified\nguide for VLMs to rectify editing instructions. However, there are some\nchallenging editing scenarios that cannot be resolved solely with rectified\ninstructions. To this end, we further construct contrastive supervision signals\nwith positive and negative instructions and introduce them into the model\ntraining using triplet loss, thereby further facilitating supervision\neffectiveness. Our method does not require the VLM modules or pre-training\ntasks used in previous work, offering a more direct and efficient way to\nprovide better supervision signals, and providing a novel, simple, and\neffective solution for instruction-based image editing. Results on multiple\nbenchmarks demonstrate that our method significantly outperforms existing\napproaches. Compared with previous SOTA SmartEdit, we achieve 9.19%\nimprovements on the Real-Edit benchmark with 30x less training data and 13x\nsmaller model size.",
        "url": "http://arxiv.org/abs/2505.02370v1",
        "published_date": "2025-05-05T05:19:40+00:00",
        "updated_date": "2025-05-05T05:19:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Ming Li",
            "Xin Gu",
            "Fan Chen",
            "Xiaoying Xing",
            "Longyin Wen",
            "Chen Chen",
            "Sijie Zhu"
        ],
        "tldr": "the paper introduces superedit, a method for improving instruction-based image editing by rectifying editing instructions using vlm guidance based on observed generation attributes and incorporating contrastive supervision with triplet loss, achieving superior performance with less data and smaller model size compared to sota methods.",
        "tldr_zh": "该论文介绍了superedit，一种通过修正编辑指令来改进基于指令的图像编辑的方法。该方法利用视觉语言模型（vlm）的指导，根据观察到的生成属性来修正编辑指令，并结合三元组损失进行对比监督。相比于现有最佳方法，superedit在更少的数据和更小的模型尺寸下实现了更卓越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing AI Face Realism: Cost-Efficient Quality Improvement in Distilled Diffusion Models with a Fully Synthetic Dataset",
        "summary": "This study presents a novel approach to enhance the cost-to-quality ratio of\nimage generation with diffusion models. We hypothesize that differences between\ndistilled (e.g. FLUX.1-schnell) and baseline (e.g. FLUX.1-dev) models are\nconsistent and, therefore, learnable within a specialized domain, like portrait\ngeneration. We generate a synthetic paired dataset and train a fast\nimage-to-image translation head. Using two sets of low- and high-quality\nsynthetic images, our model is trained to refine the output of a distilled\ngenerator (e.g., FLUX.1-schnell) to a level comparable to a baseline model like\nFLUX.1-dev, which is more computationally intensive. Our results show that the\npipeline, which combines a distilled version of a large generative model with\nour enhancement layer, delivers similar photorealistic portraits to the\nbaseline version with up to an 82% decrease in computational cost compared to\nFLUX.1-dev. This study demonstrates the potential for improving the efficiency\nof AI solutions involving large-scale image generation.",
        "url": "http://arxiv.org/abs/2505.02255v1",
        "published_date": "2025-05-04T21:28:21+00:00",
        "updated_date": "2025-05-04T21:28:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jakub Wąsala",
            "Bartłomiej Wrzalski",
            "Kornelia Noculak",
            "Yuliia Tarasenko",
            "Oliwer Krupa",
            "Jan Kocoń",
            "Grzegorz Chodak"
        ],
        "tldr": "this paper introduces a cost-effective method to improve the quality of images generated by distilled diffusion models by training an image-to-image translation head on a synthetic dataset, achieving comparable quality to baseline models with significantly reduced computational cost.",
        "tldr_zh": "本文提出了一种经济高效的方法，通过在合成数据集上训练图像到图像的翻译头，来提高蒸馏扩散模型生成的图像质量，从而以显著降低的计算成本实现与基线模型相当的质量。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Quantizing Diffusion Models from a Sampling-Aware Perspective",
        "summary": "Diffusion models have recently emerged as the dominant approach in visual\ngeneration tasks. However, the lengthy denoising chains and the computationally\nintensive noise estimation networks hinder their applicability in low-latency\nand resource-limited environments. Previous research has endeavored to address\nthese limitations in a decoupled manner, utilizing either advanced samplers or\nefficient model quantization techniques. In this study, we uncover that\nquantization-induced noise disrupts directional estimation at each sampling\nstep, further distorting the precise directional estimations of higher-order\nsamplers when solving the sampling equations through discretized numerical\nmethods, thereby altering the optimal sampling trajectory. To attain dual\nacceleration with high fidelity, we propose a sampling-aware quantization\nstrategy, wherein a Mixed-Order Trajectory Alignment technique is devised to\nimpose a more stringent constraint on the error bounds at each sampling step,\nfacilitating a more linear probability flow. Extensive experiments on\nsparse-step fast sampling across multiple datasets demonstrate that our\napproach preserves the rapid convergence characteristics of high-speed samplers\nwhile maintaining superior generation quality. Code will be made publicly\navailable soon.",
        "url": "http://arxiv.org/abs/2505.02242v1",
        "published_date": "2025-05-04T20:50:44+00:00",
        "updated_date": "2025-05-04T20:50:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qian Zeng",
            "Jie Song",
            "Yuanyu Wan",
            "Huiqiong Wang",
            "Mingli Song"
        ],
        "tldr": "this paper proposes a sampling-aware quantization strategy, specifically mixed-order trajectory alignment, for diffusion models to improve both speed and generation quality in resource-constrained environments by mitigating quantization-induced noise during sampling.",
        "tldr_zh": "本文提出了一种采样感知量化策略，即混合阶轨迹对齐，用于扩散模型，通过减轻采样过程中量化引起的噪声，从而在资源受限的环境中提高速度和生成质量。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improving Physical Object State Representation in Text-to-Image Generative Systems",
        "summary": "Current text-to-image generative models struggle to accurately represent\nobject states (e.g., \"a table without a bottle,\" \"an empty tumbler\"). In this\nwork, we first design a fully-automatic pipeline to generate high-quality\nsynthetic data that accurately captures objects in varied states. Next, we\nfine-tune several open-source text-to-image models on this synthetic data. We\nevaluate the performance of the fine-tuned models by quantifying the alignment\nof the generated images to their prompts using GPT4o-mini, and achieve an\naverage absolute improvement of 8+% across four models on the public\nGenAI-Bench dataset. We also curate a collection of 200 prompts with a specific\nfocus on common objects in various physical states. We demonstrate a\nsignificant improvement of an average of 24+% over the baseline on this\ndataset. We release all evaluation prompts and code.",
        "url": "http://arxiv.org/abs/2505.02236v1",
        "published_date": "2025-05-04T20:24:57+00:00",
        "updated_date": "2025-05-04T20:24:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tianle Chen",
            "Chaitanya Chakka",
            "Deepti Ghadiyaram"
        ],
        "tldr": "the paper addresses the problem of text-to-image models struggling to represent object states by generating synthetic data and fine-tuning existing models, resulting in significant improvements in image-prompt alignment.",
        "tldr_zh": "该论文通过生成合成数据并微调现有模型，解决了文本到图像模型难以表示对象状态的问题，从而显著提高了图像与提示的对齐程度。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning",
        "summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs.",
        "url": "http://arxiv.org/abs/2505.02835v1",
        "published_date": "2025-05-05T17:59:50+00:00",
        "updated_date": "2025-05-05T17:59:50+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Yi-Fan Zhang",
            "Xingyu Lu",
            "Xiao Hu",
            "Chaoyou Fu",
            "Bin Wen",
            "Tianke Zhang",
            "Changyi Liu",
            "Kaiyu Jiang",
            "Kaibing Chen",
            "Kaiyu Tang",
            "Haojie Ding",
            "Jiankang Chen",
            "Fan Yang",
            "Zhang Zhang",
            "Tingting Gao",
            "Liang Wang"
        ],
        "tldr": "the paper introduces r1-reward, a multimodal reward model trained using a novel stablereinforce algorithm, demonstrating significant performance improvements on multimodal reward modeling benchmarks.",
        "tldr_zh": "该论文介绍了r1-reward，一个使用新型stablereinforce算法训练的多模态奖励模型，并在多模态奖励建模基准上展示了显著的性能提升。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Sim2Real in endoscopy segmentation with a novel structure aware image translation",
        "summary": "Automatic segmentation of anatomical landmarks in endoscopic images can\nprovide assistance to doctors and surgeons for diagnosis, treatments or medical\ntraining. However, obtaining the annotations required to train commonly used\nsupervised learning methods is a tedious and difficult task, in particular for\nreal images. While ground truth annotations are easier to obtain for synthetic\ndata, models trained on such data often do not generalize well to real data.\nGenerative approaches can add realistic texture to it, but face difficulties to\nmaintain the structure of the original scene. The main contribution in this\nwork is a novel image translation model that adds realistic texture to\nsimulated endoscopic images while keeping the key scene layout information. Our\napproach produces realistic images in different endoscopy scenarios. We\ndemonstrate these images can effectively be used to successfully train a model\nfor a challenging end task without any real labeled data. In particular, we\ndemonstrate our approach for the task of fold segmentation in colonoscopy\nimages. Folds are key anatomical landmarks that can occlude parts of the colon\nmucosa and possible polyps. Our approach generates realistic images maintaining\nthe shape and location of the original folds, after the\nimage-style-translation, better than existing methods. We run experiments both\non a novel simulated dataset for fold segmentation, and real data from the\nEndoMapper (EM) dataset. All our new generated data and new EM metadata is\nbeing released to facilitate further research, as no public benchmark is\ncurrently available for the task of fold segmentation.",
        "url": "http://arxiv.org/abs/2505.02654v1",
        "published_date": "2025-05-05T13:56:59+00:00",
        "updated_date": "2025-05-05T13:56:59+00:00",
        "categories": [
            "cs.CV",
            "I.2.10; I.4.6"
        ],
        "authors": [
            "Clara Tomasini",
            "Luis Riazuelo",
            "Ana C. Murillo"
        ],
        "tldr": "this paper presents a novel image translation model for sim2real endoscopy segmentation, adding realistic texture to simulated images while preserving structural information, which allows training a segmentation model without real labeled data. the authors also release a new simulated dataset as well as new metadata for the endomapper dataset.",
        "tldr_zh": "本文提出了一种新的图像转换模型，用于内窥镜分割中的sim2real，在为模拟图像添加真实纹理的同时保留结构信息，从而无需真实标记数据即可训练分割模型。作者还发布了一个新的模拟数据集以及endomapper数据集的新元数据。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Detect, Classify, Act: Categorizing Industrial Anomalies with Multi-Modal Large Language Models",
        "summary": "Recent advances in visual industrial anomaly detection have demonstrated\nexceptional performance in identifying and segmenting anomalous regions while\nmaintaining fast inference speeds. However, anomaly\nclassification-distinguishing different types of anomalies-remains largely\nunexplored despite its critical importance in real-world inspection tasks. To\naddress this gap, we propose VELM, a novel LLM-based pipeline for anomaly\nclassification. Given the critical importance of inference speed, we first\napply an unsupervised anomaly detection method as a vision expert to assess the\nnormality of an observation. If an anomaly is detected, the LLM then classifies\nits type. A key challenge in developing and evaluating anomaly classification\nmodels is the lack of precise annotations of anomaly classes in existing\ndatasets. To address this limitation, we introduce MVTec-AC and VisA-AC,\nrefined versions of the widely used MVTec-AD and VisA datasets, which include\naccurate anomaly class labels for rigorous evaluation. Our approach achieves a\nstate-of-the-art anomaly classification accuracy of 80.4% on MVTec-AD,\nexceeding the prior baselines by 5%, and 84% on MVTec-AC, demonstrating the\neffectiveness of VELM in understanding and categorizing anomalies. We hope our\nmethodology and benchmark inspire further research in anomaly classification,\nhelping bridge the gap between detection and comprehensive anomaly\ncharacterization.",
        "url": "http://arxiv.org/abs/2505.02626v1",
        "published_date": "2025-05-05T13:08:25+00:00",
        "updated_date": "2025-05-05T13:08:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sassan Mokhtar",
            "Arian Mousakhan",
            "Silvio Galesso",
            "Jawad Tayyub",
            "Thomas Brox"
        ],
        "tldr": "the paper introduces velm, a multi-modal llm-based pipeline for anomaly classification in industrial settings, and provides two new datasets, mvtec-ac and visa-ac, with accurate anomaly class labels for evaluation.",
        "tldr_zh": "本文介绍了一种基于多模态llm的异常分类管道velm，用于工业环境中的异常分类，并提供了两个新的数据集mvtec-ac和visa-ac，其中包含准确的异常类别标签，用于评估。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation",
        "summary": "Chest X-rays (CXRs) are the most frequently performed imaging examinations in\nclinical settings. Recent advancements in Large Multimodal Models (LMMs) have\nenabled automated CXR interpretation, enhancing diagnostic accuracy and\nefficiency. However, despite their strong visual understanding, current Medical\nLMMs (MLMMs) still face two major challenges: (1) Insufficient region-level\nunderstanding and interaction, and (2) Limited accuracy and interpretability\ndue to single-step reasoning. In this paper, we empower MLMMs with\nanatomy-centric reasoning capabilities to enhance their interactivity and\nexplainability. Specifically, we first propose an Anatomical Ontology-Guided\nReasoning (AOR) framework, which centers on cross-modal region-level\ninformation to facilitate multi-step reasoning. Next, under the guidance of\nexpert physicians, we develop AOR-Instruction, a large instruction dataset for\nMLMMs training. Our experiments demonstrate AOR's superior performance in both\nVQA and report generation tasks.",
        "url": "http://arxiv.org/abs/2505.02830v1",
        "published_date": "2025-05-05T17:57:07+00:00",
        "updated_date": "2025-05-05T17:57:07+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Qingqiu Li",
            "Zihang Cui",
            "Seongsu Bae",
            "Jilan Xu",
            "Runtian Yuan",
            "Yuejie Zhang",
            "Rui Feng",
            "Quanli Shen",
            "Xiaobo Zhang",
            "Junjun He",
            "Shujun Wang"
        ],
        "tldr": "the paper introduces an anatomical ontology-guided reasoning (aor) framework to improve medical large multimodal models' (mlmms) performance in chest x-ray interpretation by enhancing region-level understanding and multi-step reasoning.",
        "tldr_zh": "该论文介绍了一种解剖本体引导推理 (aor) 框架，通过增强区域级理解和多步推理，来提高医学大型多模态模型 (mlmm) 在胸部 x 光片解释方面的性能。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 4
    },
    {
        "title": "CSASN: A Multitask Attention-Based Framework for Heterogeneous Thyroid Carcinoma Classification in Ultrasound Images",
        "summary": "Heterogeneous morphological features and data imbalance pose significant\nchallenges in rare thyroid carcinoma classification using ultrasound imaging.\nTo address this issue, we propose a novel multitask learning framework,\nChannel-Spatial Attention Synergy Network (CSASN), which integrates a\ndual-branch feature extractor - combining EfficientNet for local spatial\nencoding and ViT for global semantic modeling, with a cascaded channel-spatial\nattention refinement module. A residual multiscale classifier and dynamically\nweighted loss function further enhance classification stability and accuracy.\nTrained on a multicenter dataset comprising more than 2000 patients from four\nclinical institutions, our framework leverages a residual multiscale classifier\nand dynamically weighted loss function to enhance classification stability and\naccuracy. Extensive ablation studies demonstrate that each module contributes\nsignificantly to model performance, particularly in recognizing rare subtypes\nsuch as FTC and MTC carcinomas. Experimental results show that CSASN\noutperforms existing single-stream CNN or Transformer-based models, achieving a\nsuperior balance between precision and recall under class-imbalanced\nconditions. This framework provides a promising strategy for AI-assisted\nthyroid cancer diagnosis.",
        "url": "http://arxiv.org/abs/2505.02211v1",
        "published_date": "2025-05-04T18:23:03+00:00",
        "updated_date": "2025-05-04T18:23:03+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Peiqi Li",
            "Yincheng Gao",
            "Renxing Li",
            "Haojie Yang",
            "Yunyun Liu",
            "Boji Liu",
            "Jiahui Ni",
            "Ying Zhang",
            "Yulu Wu",
            "Xiaowei Fang",
            "Lehang Guo",
            "Liping Sun",
            "Jiangang Chen"
        ],
        "tldr": "the paper introduces csasn, a multitask attention-based framework for classifying heterogeneous thyroid carcinoma in ultrasound images, using a dual-branch feature extractor and a cascaded channel-spatial attention refinement module to address data imbalance and improve classification accuracy.",
        "tldr_zh": "该论文介绍了一种名为csasn的多任务注意力框架，用于对超声图像中异构性甲状腺癌进行分类。该框架采用双分支特征提取器和级联通道空间注意力细化模块，旨在解决数据不平衡问题并提高分类准确性。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    }
]