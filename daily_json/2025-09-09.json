[
    {
        "title": "Interleaving Reasoning for Better Text-to-Image Generation",
        "summary": "Unified multimodal understanding and generation models recently have achieve\nsignificant improvement in image generation capability, yet a large gap remains\nin instruction following and detail preservation compared to systems that\ntightly couple comprehension with generation such as GPT-4o. Motivated by\nrecent advances in interleaving reasoning, we explore whether such reasoning\ncan further improve Text-to-Image (T2I) generation. We introduce Interleaving\nReasoning Generation (IRG), a framework that alternates between text-based\nthinking and image synthesis: the model first produces a text-based thinking to\nguide an initial image, then reflects on the result to refine fine-grained\ndetails, visual quality, and aesthetics while preserving semantics. To train\nIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),\nwhich targets two sub-goals: (1) strengthening the initial think-and-generate\nstage to establish core content and base quality, and (2) enabling high-quality\ntextual reflection and faithful implementation of those refinements in a\nsubsequent image. We curate IRGL-300K, a dataset organized into six decomposed\nlearning modes that jointly cover learning text-based thinking, and full\nthinking-image trajectories. Starting from a unified foundation model that\nnatively emits interleaved text-image outputs, our two-stage training first\nbuilds robust thinking and reflection, then efficiently tunes the IRG pipeline\nin the full thinking-image trajectory data. Extensive experiments show SoTA\nperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,\nGenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality\nand fine-grained fidelity. The code, model weights and datasets will be\nreleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation .",
        "url": "http://arxiv.org/abs/2509.06945v1",
        "published_date": "2025-09-08T17:56:23+00:00",
        "updated_date": "2025-09-08T17:56:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Wenxuan Huang",
            "Shuang Chen",
            "Zheyong Xie",
            "Shaosheng Cao",
            "Shixiang Tang",
            "Yufan Shen",
            "Qingyu Yin",
            "Wenbo Hu",
            "Xiaoman Wang",
            "Yuntian Tang",
            "Junbo Qiao",
            "Yue Guo",
            "Yao Hu",
            "Zhenfei Yin",
            "Philip Torr",
            "Yu Cheng",
            "Wanli Ouyang",
            "Shaohui Lin"
        ],
        "tldr": "The paper introduces Interleaving Reasoning Generation (IRG), a framework for text-to-image generation that alternates between text-based reasoning and image synthesis, trained using a curated dataset and achieving state-of-the-art performance.",
        "tldr_zh": "该论文介绍了交错推理生成 (IRG)，一个用于文本到图像生成的框架，它在基于文本的推理和图像合成之间交替进行，使用精心策划的数据集进行训练，并实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via Test-Time Training",
        "summary": "Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos\nthat adhere to user-specified motion instructions. Existing methods typically\nrely on computationally expensive fine-tuning on scarce annotated datasets.\nAlthough some zero-shot methods attempt to trajectory control in the latent\nspace, they may yield unrealistic motion by neglecting 3D perspective and\ncreating a misalignment between the manipulated latents and the network's noise\npredictions. To address these challenges, we introduce Zo3T, a novel zero-shot\ntest-time-training framework for trajectory-guided generation with three core\ninnovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging\ninferring scene depth to derive perspective-correct affine transformations for\ntarget regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a\nmechanism that dynamically injects and optimizes ephemeral LoRA adapters into\nthe denoising network alongside the latent state. Driven by a regional feature\nconsistency loss, this co-adaptation effectively enforces motion constraints\nwhile allowing the pre-trained model to locally adapt its internal\nrepresentations to the manipulated latent, thereby ensuring generative fidelity\nand on-manifold adherence. Finally, we develop Guidance Field Rectification,\nwhich refines the denoising evolutionary path by optimizing the conditional\nguidance field through a one-step lookahead strategy, ensuring efficient\ngenerative progression towards the target trajectory. Zo3T significantly\nenhances 3D realism and motion accuracy in trajectory-controlled I2V\ngeneration, demonstrating superior performance over existing training-based and\nzero-shot approaches.",
        "url": "http://arxiv.org/abs/2509.06723v1",
        "published_date": "2025-09-08T14:21:45+00:00",
        "updated_date": "2025-09-08T14:21:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruicheng Zhang",
            "Jun Zhou",
            "Zunnan Xu",
            "Zihao Liu",
            "Jiehui Huang",
            "Mingyang Zhang",
            "Yu Sun",
            "Xiu Li"
        ],
        "tldr": "The paper introduces Zo3T, a zero-shot test-time training framework for trajectory-guided image-to-video generation that improves 3D realism and motion accuracy through 3D-aware kinematic projection, trajectory-guided LoRA adaptation, and guidance field rectification.",
        "tldr_zh": "本文介绍了一种名为Zo3T的零样本测试时训练框架，用于轨迹引导的图像到视频生成，通过3D感知的运动学投影、轨迹引导的LoRA适配和引导场校正，提高了3D真实感和运动精度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]