[
    {
        "title": "Context-Aware Autoregressive Models for Multi-Conditional Image Generation",
        "summary": "Autoregressive transformers have recently shown impressive image generation\nquality and efficiency on par with state-of-the-art diffusion models. Unlike\ndiffusion architectures, autoregressive models can naturally incorporate\narbitrary modalities into a single, unified token sequence--offering a concise\nsolution for multi-conditional image generation tasks. In this work, we propose\n$\\textbf{ContextAR}$, a flexible and effective framework for multi-conditional\nimage generation. ContextAR embeds diverse conditions (e.g., canny edges, depth\nmaps, poses) directly into the token sequence, preserving modality-specific\nsemantics. To maintain spatial alignment while enhancing discrimination among\ndifferent condition types, we introduce hybrid positional encodings that fuse\nRotary Position Embedding with Learnable Positional Embedding. We design\nConditional Context-aware Attention to reduces computational complexity while\npreserving effective intra-condition perception. Without any fine-tuning,\nContextAR supports arbitrary combinations of conditions during inference time.\nExperimental results demonstrate the powerful controllability and versatility\nof our approach, and show that the competitive perpormance than diffusion-based\nmulti-conditional control approaches the existing autoregressive baseline\nacross diverse multi-condition driven scenarios. Project page:\n$\\href{https://context-ar.github.io/}{https://context-ar.github.io/.}$",
        "url": "http://arxiv.org/abs/2505.12274v1",
        "published_date": "2025-05-18T07:27:02+00:00",
        "updated_date": "2025-05-18T07:27:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yixiao Chen",
            "Zhiyuan Ma",
            "Guoli Jia",
            "Che Jiang",
            "Jianjun Li",
            "Bowen Zhou"
        ],
        "tldr": "The paper introduces ContextAR, a flexible autoregressive framework for multi-conditional image generation, which uses hybrid positional encodings and conditional context-aware attention to effectively integrate diverse conditions into the image generation process.",
        "tldr_zh": "本文介绍了一种灵活的自回归框架ContextAR，用于多条件图像生成，它采用混合位置编码和条件上下文感知注意力，有效地将多种条件整合到图像生成过程中。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Video-GPT via Next Clip Diffusion",
        "summary": "GPT has shown its remarkable success in natural language processing. However,\nthe language sequence is not sufficient to describe spatial-temporal details in\nthe visual world. Alternatively, the video sequence is good at capturing such\ndetails. Motivated by this fact, we propose a concise Video-GPT in this paper\nby treating video as new language for visual world modeling. By analogy to next\ntoken prediction in GPT, we introduce a novel next clip diffusion paradigm for\npretraining Video-GPT. Different from the previous works, this distinct\nparadigm allows Video-GPT to tackle both short-term generation and long-term\nprediction, by autoregressively denoising the noisy clip according to the clean\nclips in the history. Extensive experiments show our Video-GPT achieves the\nstate-of-the-art performance on video prediction, which is the key factor\ntowards world modeling (Physics-IQ Benchmark: Video-GPT 34.97 vs. Kling 23.64\nvs. Wan 20.89). Moreover, it can be well adapted on 6 mainstream video tasks in\nboth video generation and understanding, showing its great generalization\ncapacity in downstream. The project page is at https://Video-GPT.github.io.",
        "url": "http://arxiv.org/abs/2505.12489v1",
        "published_date": "2025-05-18T16:22:58+00:00",
        "updated_date": "2025-05-18T16:22:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shaobin Zhuang",
            "Zhipeng Huang",
            "Ying Zhang",
            "Fangyikang Wang",
            "Canmiao Fu",
            "Binxin Yang",
            "Chong Sun",
            "Chen Li",
            "Yali Wang"
        ],
        "tldr": "The paper introduces Video-GPT, a novel approach for video understanding and generation based on a next clip diffusion paradigm, achieving state-of-the-art results on video prediction and demonstrating strong generalization across various downstream tasks.",
        "tldr_zh": "该论文介绍了 Video-GPT，这是一种基于下一个片段扩散范例的视频理解和生成的新方法，在视频预测方面取得了最先进的结果，并在各种下游任务中展示了强大的泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Guiding Diffusion with Deep Geometric Moments: Balancing Fidelity and Variation",
        "summary": "Text-to-image generation models have achieved remarkable capabilities in\nsynthesizing images, but often struggle to provide fine-grained control over\nthe output. Existing guidance approaches, such as segmentation maps and depth\nmaps, introduce spatial rigidity that restricts the inherent diversity of\ndiffusion models. In this work, we introduce Deep Geometric Moments (DGM) as a\nnovel form of guidance that encapsulates the subject's visual features and\nnuances through a learned geometric prior. DGMs focus specifically on the\nsubject itself compared to DINO or CLIP features, which suffer from\noveremphasis on global image features or semantics. Unlike ResNets, which are\nsensitive to pixel-wise perturbations, DGMs rely on robust geometric moments.\nOur experiments demonstrate that DGM effectively balance control and diversity\nin diffusion-based image generation, allowing a flexible control mechanism for\nsteering the diffusion process.",
        "url": "http://arxiv.org/abs/2505.12486v1",
        "published_date": "2025-05-18T16:19:27+00:00",
        "updated_date": "2025-05-18T16:19:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sangmin Jung",
            "Utkarsh Nath",
            "Yezhou Yang",
            "Giulia Pedrielli",
            "Joydeep Biswas",
            "Amy Zhang",
            "Hassan Ghasemzadeh",
            "Pavan Turaga"
        ],
        "tldr": "This paper introduces Deep Geometric Moments (DGM), a novel guidance method for text-to-image diffusion models that balances control and diversity by leveraging learned geometric priors of the subject.",
        "tldr_zh": "本文介绍了一种名为深度几何矩 (DGM) 的新型文本到图像扩散模型引导方法，它通过利用学习到的对象的几何先验来平衡控制和多样性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "NOFT: Test-Time Noise Finetune via Information Bottleneck for Highly Correlated Asset Creation",
        "summary": "The diffusion model has provided a strong tool for implementing text-to-image\n(T2I) and image-to-image (I2I) generation. Recently, topology and texture\ncontrol are popular explorations, e.g., ControlNet, IP-Adapter, Ctrl-X, and\nDSG. These methods explicitly consider high-fidelity controllable editing based\non external signals or diffusion feature manipulations. As for diversity, they\ndirectly choose different noise latents. However, the diffused noise is capable\nof implicitly representing the topological and textural manifold of the\ncorresponding image. Moreover, it's an effective workbench to conduct the\ntrade-off between content preservation and controllable variations. Previous\nT2I and I2I diffusion works do not explore the information within the\ncompressed contextual latent. In this paper, we first propose a plug-and-play\nnoise finetune NOFT module employed by Stable Diffusion to generate highly\ncorrelated and diverse images. We fine-tune seed noise or inverse noise through\nan optimal-transported (OT) information bottleneck (IB) with around only 14K\ntrainable parameters and 10 minutes of training. Our test-time NOFT is good at\nproducing high-fidelity image variations considering topology and texture\nalignments. Comprehensive experiments demonstrate that NOFT is a powerful\ngeneral reimagine approach to efficiently fine-tune the 2D/3D AIGC assets with\ntext or image guidance.",
        "url": "http://arxiv.org/abs/2505.12235v1",
        "published_date": "2025-05-18T05:09:47+00:00",
        "updated_date": "2025-05-18T05:09:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jia Li",
            "Nan Gao",
            "Huaibo Huang",
            "Ran He"
        ],
        "tldr": "This paper introduces NOFT, a plug-and-play module for Stable Diffusion that fine-tunes seed noise using an information bottleneck to generate diverse and highly correlated images with topology and texture alignment within 10 minutes using 14K parameters.",
        "tldr_zh": "该论文介绍了NOFT，一个即插即用的Stable Diffusion模块，它通过信息瓶颈微调种子噪声，从而快速生成具有拓扑和纹理对齐的，多样化且高度相关的图像，且仅需使用1.4万参数在10分钟内完成训练。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]