[
    {
        "title": "FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA",
        "summary": "Recent advances in diffusion-based generative models have demonstrated\nsignificant potential in augmenting scarce datasets for object detection tasks.\nNevertheless, most recent models rely on resource-intensive full fine-tuning of\nlarge-scale diffusion models, requiring enterprise-grade GPUs (e.g., NVIDIA\nV100) and thousands of synthetic images. To address these limitations, we\npropose Flux LoRA Augmentation (FLORA), a lightweight synthetic data generation\npipeline. Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned\nexclusively through Low-Rank Adaptation (LoRA). This dramatically reduces\ncomputational requirements, enabling synthetic dataset generation with a\nconsumer-grade GPU (e.g., NVIDIA RTX 4090). We empirically evaluate our\napproach on seven diverse object detection datasets. Our results demonstrate\nthat training object detectors with just 500 synthetic images generated by our\napproach yields superior detection performance compared to models trained on\n5000 synthetic images from the ODGEN baseline, achieving improvements of up to\n21.3% in mAP@.50:.95. This work demonstrates that it is possible to surpass\nstate-of-the-art performance with far greater efficiency, as FLORA achieves\nsuperior results using only 10% of the data and a fraction of the computational\ncost. This work demonstrates that a quality and efficiency-focused approach is\nmore effective than brute-force generation, making advanced synthetic data\ncreation more practical and accessible for real-world scenarios.",
        "url": "http://arxiv.org/abs/2508.21712v1",
        "published_date": "2025-08-29T15:29:06+00:00",
        "updated_date": "2025-08-29T15:29:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Alvaro Patricio",
            "Atabak Dehban",
            "Rodrigo Ventura"
        ],
        "tldr": "The paper introduces FLORA, a lightweight synthetic data generation pipeline for object detection using LoRA fine-tuning of a diffusion model, achieving superior performance with significantly reduced computational cost and data requirements compared to full fine-tuning methods.",
        "tldr_zh": "该论文介绍了FLORA，一种轻量级的合成数据生成管线，通过LoRA微调扩散模型用于目标检测，与完全微调方法相比，以显著降低的计算成本和数据需求实现了卓越的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Temporal Flow Matching for Learning Spatio-Temporal Trajectories in 4D Longitudinal Medical Imaging",
        "summary": "Understanding temporal dynamics in medical imaging is crucial for\napplications such as disease progression modeling, treatment planning and\nanatomical development tracking. However, most deep learning methods either\nconsider only single temporal contexts, or focus on tasks like classification\nor regression, limiting their ability for fine-grained spatial predictions.\nWhile some approaches have been explored, they are often limited to single\ntimepoints, specific diseases or have other technical restrictions. To address\nthis fundamental gap, we introduce Temporal Flow Matching (TFM), a unified\ngenerative trajectory method that (i) aims to learn the underlying temporal\ndistribution, (ii) by design can fall back to a nearest image predictor, i.e.\npredicting the last context image (LCI), as a special case, and (iii) supports\n$3D$ volumes, multiple prior scans, and irregular sampling. Extensive\nbenchmarks on three public longitudinal datasets show that TFM consistently\nsurpasses spatio-temporal methods from natural imaging, establishing a new\nstate-of-the-art and robust baseline for $4D$ medical image prediction.",
        "url": "http://arxiv.org/abs/2508.21580v1",
        "published_date": "2025-08-29T12:34:28+00:00",
        "updated_date": "2025-08-29T12:34:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nico Albert Disch",
            "Yannick Kirchhoff",
            "Robin Peretzke",
            "Maximilian Rokuss",
            "Saikat Roy",
            "Constantin Ulrich",
            "David Zimmerer",
            "Klaus Maier-Hein"
        ],
        "tldr": "The paper introduces Temporal Flow Matching (TFM) for generating spatio-temporal trajectories in 4D longitudinal medical imaging, outperforming existing methods on multiple datasets and establishing a new state-of-the-art baseline.",
        "tldr_zh": "本文介绍了一种名为时间流匹配（TFM）的方法，用于生成4D纵向医学图像中的时空轨迹。该方法在多个数据集上优于现有方法，并建立了一个新的最先进的基线。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Complete Gaussian Splats from a Single Image with Denoising Diffusion Models",
        "summary": "Gaussian splatting typically requires dense observations of the scene and can\nfail to reconstruct occluded and unobserved areas. We propose a latent\ndiffusion model to reconstruct a complete 3D scene with Gaussian splats,\nincluding the occluded parts, from only a single image during inference.\nCompleting the unobserved surfaces of a scene is challenging due to the\nambiguity of the plausible surfaces. Conventional methods use a\nregression-based formulation to predict a single \"mode\" for occluded and\nout-of-frustum surfaces, leading to blurriness, implausibility, and failure to\ncapture multiple possible explanations. Thus, they often address this problem\npartially, focusing either on objects isolated from the background,\nreconstructing only visible surfaces, or failing to extrapolate far from the\ninput views. In contrast, we propose a generative formulation to learn a\ndistribution of 3D representations of Gaussian splats conditioned on a single\ninput image. To address the lack of ground-truth training data, we propose a\nVariational AutoReconstructor to learn a latent space only from 2D images in a\nself-supervised manner, over which a diffusion model is trained. Our method\ngenerates faithful reconstructions and diverse samples with the ability to\ncomplete the occluded surfaces for high-quality 360-degree renderings.",
        "url": "http://arxiv.org/abs/2508.21542v1",
        "published_date": "2025-08-29T11:55:47+00:00",
        "updated_date": "2025-08-29T11:55:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Ziwei Liao",
            "Mohamed Sayed",
            "Steven L. Waslander",
            "Sara Vicente",
            "Daniyar Turmukhambetov",
            "Michael Firman"
        ],
        "tldr": "This paper proposes a diffusion model for completing 3D Gaussian splat representations from a single image, enabling the reconstruction of occluded areas and generation of diverse 360-degree renderings.",
        "tldr_zh": "该论文提出了一种扩散模型，用于从单个图像补全3D高斯溅射表示，从而实现对遮挡区域的重建和生成多样化的360度渲染。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]