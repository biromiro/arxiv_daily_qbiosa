[
    {
        "title": "Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at the Speed of Light",
        "summary": "Many sparse attention mechanisms such as Neighborhood Attention have\ntypically failed to consistently deliver speedup over the self attention\nbaseline. This is largely due to the level of complexity in attention\ninfrastructure, and the rapid evolution of AI hardware architecture. At the\nsame time, many state-of-the-art foundational models, particularly in computer\nvision, are heavily bound by attention, and need reliable sparsity to escape\nthe O(n^2) complexity. In this paper, we study a class of promising sparse\nattention mechanisms that focus on locality, and aim to develop a better\nanalytical model of their performance improvements. We first introduce\nGeneralized Neighborhood Attention (GNA), which can describe sliding window,\nstrided sliding window, and blocked attention. We then consider possible design\nchoices in implementing these approaches, and create a simulator that can\nprovide much more realistic speedup upper bounds for any given setting.\nFinally, we implement GNA on top of a state-of-the-art fused multi-headed\nattention (FMHA) kernel designed for the NVIDIA Blackwell architecture in\nCUTLASS. Our implementation can fully realize the maximum speedup theoretically\npossible in many perfectly block-sparse cases, and achieves an effective\nutilization of 1.3 petaFLOPs/second in FP16. In addition, we plug various GNA\nconfigurations into off-the-shelf generative models, such as Cosmos-7B,\nHunyuanVideo, and FLUX, and show that it can deliver 28% to 46% end-to-end\nspeedup on B200 without any fine-tuning. We will open source our simulator and\nBlackwell kernels directly through the NATTEN project.",
        "url": "http://arxiv.org/abs/2504.16922v1",
        "published_date": "2025-04-23T17:49:53+00:00",
        "updated_date": "2025-04-23T17:49:53+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Ali Hassani",
            "Fengzhe Zhou",
            "Aditya Kane",
            "Jiannan Huang",
            "Chieh-Yun Chen",
            "Min Shi",
            "Steven Walton",
            "Markus Hoehnerbach",
            "Vijay Thakkar",
            "Michael Isaev",
            "Qinsheng Zhang",
            "Bing Xu",
            "Haicheng Wu",
            "Wen-mei Hwu",
            "Ming-Yu Liu",
            "Humphrey Shi"
        ],
        "tldr": "this paper introduces generalized neighborhood attention (gna), a sparse attention mechanism optimized for modern hardware like nvidia blackwell, achieving significant speedups in generative models without fine-tuning, showing its potential to alleviate the o(n^2) complexity of attention.",
        "tldr_zh": "本文介绍了广义邻域注意力（gna），这是一种针对nvidia blackwell等现代硬件优化的稀疏注意力机制，在生成模型中实现了显著的加速，无需微调，展示了其缓解注意力o(n^2)复杂性的潜力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "DreamO: A Unified Framework for Image Customization",
        "summary": "Recently, extensive research on image customization (e.g., identity, subject,\nstyle, background, etc.) demonstrates strong customization capabilities in\nlarge-scale generative models. However, most approaches are designed for\nspecific tasks, restricting their generalizability to combine different types\nof condition. Developing a unified framework for image customization remains an\nopen challenge. In this paper, we present DreamO, an image customization\nframework designed to support a wide range of tasks while facilitating seamless\nintegration of multiple conditions. Specifically, DreamO utilizes a diffusion\ntransformer (DiT) framework to uniformly process input of different types.\nDuring training, we construct a large-scale training dataset that includes\nvarious customization tasks, and we introduce a feature routing constraint to\nfacilitate the precise querying of relevant information from reference images.\nAdditionally, we design a placeholder strategy that associates specific\nplaceholders with conditions at particular positions, enabling control over the\nplacement of conditions in the generated results. Moreover, we employ a\nprogressive training strategy consisting of three stages: an initial stage\nfocused on simple tasks with limited data to establish baseline consistency, a\nfull-scale training stage to comprehensively enhance the customization\ncapabilities, and a final quality alignment stage to correct quality biases\nintroduced by low-quality data. Extensive experiments demonstrate that the\nproposed DreamO can effectively perform various image customization tasks with\nhigh quality and flexibly integrate different types of control conditions.",
        "url": "http://arxiv.org/abs/2504.16915v1",
        "published_date": "2025-04-23T17:41:44+00:00",
        "updated_date": "2025-04-23T17:41:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chong Mou",
            "Yanze Wu",
            "Wenxu Wu",
            "Zinan Guo",
            "Pengze Zhang",
            "Yufeng Cheng",
            "Yiming Luo",
            "Fei Ding",
            "Shiwen Zhang",
            "Xinghui Li",
            "Mengtian Li",
            "Songtao Zhao",
            "Jian Zhang",
            "Qian He",
            "Xinglong Wu"
        ],
        "tldr": "dreamo is a unified diffusion-transformer framework for image customization, enabling flexible integration of multiple conditions and high-quality results through a specifically designed training strategy and data handling.",
        "tldr_zh": "dreamo是一个统一的扩散-transformer框架，用于图像定制，通过专门设计的训练策略和数据处理，能够灵活地集成多种条件并产生高质量的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation",
        "summary": "Text-to-video (T2V) generative models have rapidly advanced and found\nwidespread applications across fields like entertainment, education, and\nmarketing. However, the adversarial vulnerabilities of these models remain\nrarely explored. We observe that in T2V generation tasks, the generated videos\noften contain substantial redundant information not explicitly specified in the\ntext prompts, such as environmental elements, secondary objects, and additional\ndetails, providing opportunities for malicious attackers to embed hidden\nharmful content. Exploiting this inherent redundancy, we introduce BadVideo,\nthe first backdoor attack framework tailored for T2V generation. Our attack\nfocuses on designing target adversarial outputs through two key strategies: (1)\nSpatio-Temporal Composition, which combines different spatiotemporal features\nto encode malicious information; (2) Dynamic Element Transformation, which\nintroduces transformations in redundant elements over time to convey malicious\ninformation. Based on these strategies, the attacker's malicious target\nseamlessly integrates with the user's textual instructions, providing high\nstealthiness. Moreover, by exploiting the temporal dimension of videos, our\nattack successfully evades traditional content moderation systems that\nprimarily analyze spatial information within individual frames. Extensive\nexperiments demonstrate that BadVideo achieves high attack success rates while\npreserving original semantics and maintaining excellent performance on clean\ninputs. Overall, our work reveals the adversarial vulnerability of T2V models,\ncalling attention to potential risks and misuse. Our project page is at\nhttps://wrt2000.github.io/BadVideo2025/.",
        "url": "http://arxiv.org/abs/2504.16907v1",
        "published_date": "2025-04-23T17:34:48+00:00",
        "updated_date": "2025-04-23T17:34:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ruotong Wang",
            "Mingli Zhu",
            "Jiarong Ou",
            "Rui Chen",
            "Xin Tao",
            "Pengfei Wan",
            "Baoyuan Wu"
        ],
        "tldr": "the paper introduces badvideo, a novel backdoor attack framework against text-to-video generation models, exploiting inherent redundancies in generated videos to embed malicious content that evades traditional content moderation.",
        "tldr_zh": "该论文介绍了badvideo，一种针对文本到视频生成模型的新型后门攻击框架，利用生成视频中固有的冗余来嵌入恶意内容，从而逃避传统的的内容审查。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Explainable AI: Multi-Modal Transformer for Video-based Image Description Generation",
        "summary": "Understanding and analyzing video actions are essential for producing\ninsightful and contextualized descriptions, especially for video-based\napplications like intelligent monitoring and autonomous systems. The proposed\nwork introduces a novel framework for generating natural language descriptions\nfrom video datasets by combining textual and visual modalities. The suggested\narchitecture makes use of ResNet50 to extract visual features from video frames\nthat are taken from the Microsoft Research Video Description Corpus (MSVD), and\nBerkeley DeepDrive eXplanation (BDD-X) datasets. The extracted visual\ncharacteristics are converted into patch embeddings and then run through an\nencoder-decoder model based on Generative Pre-trained Transformer-2 (GPT-2). In\norder to align textual and visual representations and guarantee high-quality\ndescription production, the system uses multi-head self-attention and\ncross-attention techniques. The model's efficacy is demonstrated by performance\nevaluation using BLEU (1-4), CIDEr, METEOR, and ROUGE-L. The suggested\nframework outperforms traditional methods with BLEU-4 scores of 0.755 (BDD-X)\nand 0.778 (MSVD), CIDEr scores of 1.235 (BDD-X) and 1.315 (MSVD), METEOR scores\nof 0.312 (BDD-X) and 0.329 (MSVD), and ROUGE-L scores of 0.782 (BDD-X) and\n0.795 (MSVD). By producing human-like, contextually relevant descriptions,\nstrengthening interpretability, and improving real-world applications, this\nresearch advances explainable AI.",
        "url": "http://arxiv.org/abs/2504.16788v1",
        "published_date": "2025-04-23T15:03:37+00:00",
        "updated_date": "2025-04-23T15:03:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Lakshita Agarwal",
            "Bindu Verma"
        ],
        "tldr": "this paper introduces a multi-modal transformer framework using resnet50 and gpt-2 for generating natural language descriptions from video datasets, achieving state-of-the-art results on msvd and bdd-x datasets.",
        "tldr_zh": "本文介绍了一个多模态transformer框架，使用resnet50和gpt-2从视频数据集中生成自然语言描述，并在msvd和bdd-x数据集上取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Advanced Chest X-Ray Analysis via Transformer-Based Image Descriptors and Cross-Model Attention Mechanism",
        "summary": "The examination of chest X-ray images is a crucial component in detecting\nvarious thoracic illnesses. This study introduces a new image description\ngeneration model that integrates a Vision Transformer (ViT) encoder with\ncross-modal attention and a GPT-4-based transformer decoder. The ViT captures\nhigh-quality visual features from chest X-rays, which are fused with text data\nthrough cross-modal attention to improve the accuracy, context, and richness of\nimage descriptions. The GPT-4 decoder transforms these fused features into\naccurate and relevant captions. The model was tested on the National Institutes\nof Health (NIH) and Indiana University (IU) Chest X-ray datasets. On the IU\ndataset, it achieved scores of 0.854 (B-1), 0.883 (CIDEr), 0.759 (METEOR), and\n0.712 (ROUGE-L). On the NIH dataset, it achieved the best performance on all\nmetrics: BLEU 1--4 (0.825, 0.788, 0.765, 0.752), CIDEr (0.857), METEOR (0.726),\nand ROUGE-L (0.705). This framework has the potential to enhance chest X-ray\nevaluation, assisting radiologists in more precise and efficient diagnosis.",
        "url": "http://arxiv.org/abs/2504.16774v1",
        "published_date": "2025-04-23T14:46:10+00:00",
        "updated_date": "2025-04-23T14:46:10+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Lakshita Agarwal",
            "Bindu Verma"
        ],
        "tldr": "this paper introduces a novel chest x-ray image description generation model using a vision transformer encoder, cross-modal attention, and a gpt-4 decoder, demonstrating strong performance on nih and iu datasets. it shows potential in improving diagnostic accuracy for radiologists.",
        "tldr_zh": "该论文介绍了一种新型胸部x光图像描述生成模型，该模型使用vision transformer编码器、交叉模态注意力机制和gpt-4解码器，并在nih和iu数据集上表现出强大的性能。它显示出提高放射科医生诊断准确性的潜力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Subject-driven Video Generation via Disentangled Identity and Motion",
        "summary": "We propose to train a subject-driven customized video generation model\nthrough decoupling the subject-specific learning from temporal dynamics in\nzero-shot without additional tuning. A traditional method for video\ncustomization that is tuning-free often relies on large, annotated video\ndatasets, which are computationally expensive and require extensive annotation.\nIn contrast to the previous approach, we introduce the use of an image\ncustomization dataset directly on training video customization models,\nfactorizing the video customization into two folds: (1) identity injection\nthrough image customization dataset and (2) temporal modeling preservation with\na small set of unannotated videos through the image-to-video training method.\nAdditionally, we employ random image token dropping with randomized image\ninitialization during image-to-video fine-tuning to mitigate the copy-and-paste\nissue. To further enhance learning, we introduce stochastic switching during\njoint optimization of subject-specific and temporal features, mitigating\ncatastrophic forgetting. Our method achieves strong subject consistency and\nscalability, outperforming existing video customization models in zero-shot\nsettings, demonstrating the effectiveness of our framework.",
        "url": "http://arxiv.org/abs/2504.17816v1",
        "published_date": "2025-04-23T06:48:31+00:00",
        "updated_date": "2025-04-23T06:48:31+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Daneul Kim",
            "Jingxu Zhang",
            "Wonjoon Jin",
            "Sunghyun Cho",
            "Qi Dai",
            "Jaesik Park",
            "Chong Luo"
        ],
        "tldr": "this paper introduces a subject-driven video generation model that decouples identity learning from temporal dynamics using an image customization dataset and image-to-video fine-tuning, achieving strong subject consistency and scalability in zero-shot settings.",
        "tldr_zh": "本文提出了一种主体驱动的视频生成模型，该模型通过使用图像定制数据集和图像到视频的微调，将身份学习与时间动态解耦，从而在零样本设置中实现了强大的主体一致性和可扩展性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "High-Quality Cloud-Free Optical Image Synthesis Using Multi-Temporal SAR and Contaminated Optical Data",
        "summary": "Addressing gaps caused by cloud cover and the long revisit cycle of\nsatellites is vital for providing essential data to support remote sensing\napplications. This paper tackles the challenges of missing optical data\nsynthesis, particularly in complex scenarios with cloud cover. We propose\nCRSynthNet, a novel image synthesis network that incorporates innovative\ndesigned modules such as the DownUp Block and Fusion Attention to enhance\naccuracy. Experimental results validate the effectiveness of CRSynthNet,\ndemonstrating substantial improvements in restoring structural details,\npreserving spectral consist, and achieving superior visual effects that far\nexceed those produced by comparison methods. It achieves quantitative\nimprovements across multiple metrics: a peak signal-to-noise ratio (PSNR) of\n26.978, a structural similarity index measure (SSIM) of 0.648, and a root mean\nsquare error (RMSE) of 0.050. Furthermore, this study creates the TCSEN12\ndataset, a valuable resource specifically designed to address cloud cover\nchallenges in missing optical data synthesis study. The dataset uniquely\nincludes cloud-covered images and leverages earlier image to predict later\nimage, offering a realistic representation of real-world scenarios. This study\noffer practical method and valuable resources for optical satellite image\nsynthesis task.",
        "url": "http://arxiv.org/abs/2504.16870v1",
        "published_date": "2025-04-23T16:44:53+00:00",
        "updated_date": "2025-04-23T16:44:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenxi Duan"
        ],
        "tldr": "the paper introduces crsynthnet, a novel network for synthesizing high-quality cloud-free optical images using multi-temporal sar and contaminated optical data, along with a new dataset tcsen12 for training and evaluation.",
        "tldr_zh": "该论文介绍了crsynthnet，一种使用多时相sar和受污染的光学数据合成高质量无云光学图像的新型网络，并提供了一个新的数据集tcsen12用于训练和评估。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Seeing The Words: Evaluating AI-generated Biblical Art",
        "summary": "The past years witnessed a significant amount of Artificial Intelligence (AI)\ntools that can generate images from texts. This triggers the discussion of\nwhether AI can generate accurate images using text from the Bible with respect\nto the corresponding biblical contexts and backgrounds. Despite some existing\nattempts at a small scale, little work has been done to systematically evaluate\nthese generated images. In this work, we provide a large dataset of over 7K\nimages using biblical text as prompts. These images were evaluated with\nmultiple neural network-based tools on various aspects. We provide an\nassessment of accuracy and some analysis from the perspective of religion and\naesthetics. Finally, we discuss the use of the generated images and reflect on\nthe performance of the AI generators.",
        "url": "http://arxiv.org/abs/2504.16974v1",
        "published_date": "2025-04-23T16:11:55+00:00",
        "updated_date": "2025-04-23T16:11:55+00:00",
        "categories": [
            "cs.CY",
            "cs.CV",
            "cs.MM",
            "I.4.8; I.4.0; I.3.3; I.3.0"
        ],
        "authors": [
            "Hidde Makimei",
            "Shuai Wang",
            "Willem van Peursen"
        ],
        "tldr": "this paper presents a dataset of 7k ai-generated images from biblical text prompts and evaluates their accuracy and aesthetic qualities with neural network tools, providing an assessment from religious and aesthetic perspectives.",
        "tldr_zh": "本文介绍了一个包含 7 千张由圣经文本提示生成的 ai 图像的数据集，并使用神经网络工具评估其准确性和审美质量，从宗教和美学角度进行评估。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "Tri-FusionNet: Enhancing Image Description Generation with Transformer-based Fusion Network and Dual Attention Mechanism",
        "summary": "Image description generation is essential for accessibility and AI\nunderstanding of visual content. Recent advancements in deep learning have\nsignificantly improved natural language processing and computer vision. In this\nwork, we propose Tri-FusionNet, a novel image description generation model that\nintegrates transformer modules: a Vision Transformer (ViT) encoder module with\ndual-attention mechanism, a Robustly Optimized BERT Approach (RoBERTa) decoder\nmodule, and a Contrastive Language-Image Pre-Training (CLIP) integrating\nmodule. The ViT encoder, enhanced with dual attention, focuses on relevant\nspatial regions and linguistic context, improving image feature extraction. The\nRoBERTa decoder is employed to generate precise textual descriptions. CLIP's\nintegrating module aligns visual and textual data through contrastive learning,\nensuring effective combination of both modalities. This fusion of ViT, RoBERTa,\nand CLIP, along with dual attention, enables the model to produce more\naccurate, contextually rich, and flexible descriptions. The proposed framework\ndemonstrated competitive performance on the Flickr30k and Flickr8k datasets,\nwith BLEU scores ranging from 0.767 to 0.456 and 0.784 to 0.479, CIDEr scores\nof 1.679 and 1.483, METEOR scores of 0.478 and 0.358, and ROUGE-L scores of\n0.567 and 0.789, respectively. On MS-COCO, the framework obtained BLEU scores\nof 0.893 (B-1), 0.821 (B-2), 0.794 (B-3), and 0.725 (B-4). The results\ndemonstrate the effectiveness of Tri-FusionNet in generating high-quality image\ndescriptions.",
        "url": "http://arxiv.org/abs/2504.16761v1",
        "published_date": "2025-04-23T14:33:29+00:00",
        "updated_date": "2025-04-23T14:33:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lakshita Agarwal",
            "Bindu Verma"
        ],
        "tldr": "the paper introduces tri-fusionnet, a novel image description generation model leveraging vit, roberta, and clip with a dual attention mechanism. it demonstrates competitive performance on several benchmark datasets.",
        "tldr_zh": "该论文介绍了一种名为tri-fusionnet的新型图像描述生成模型，该模型利用vit、roberta和clip，并结合双重注意力机制。 并在多个基准数据集上展示了有竞争力的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Beyond Anonymization: Object Scrubbing for Privacy-Preserving 2D and 3D Vision Tasks",
        "summary": "We introduce ROAR (Robust Object Removal and Re-annotation), a scalable\nframework for privacy-preserving dataset obfuscation that eliminates sensitive\nobjects instead of modifying them. Our method integrates instance segmentation\nwith generative inpainting to remove identifiable entities while preserving\nscene integrity. Extensive evaluations on 2D COCO-based object detection show\nthat ROAR achieves 87.5% of the baseline detection average precision (AP),\nwhereas image dropping achieves only 74.2% of the baseline AP, highlighting the\nadvantage of scrubbing in preserving dataset utility. The degradation is even\nmore severe for small objects due to occlusion and loss of fine-grained\ndetails. Furthermore, in NeRF-based 3D reconstruction, our method incurs a PSNR\nloss of at most 1.66 dB while maintaining SSIM and improving LPIPS,\ndemonstrating superior perceptual quality. Our findings establish object\nremoval as an effective privacy framework, achieving strong privacy guarantees\nwith minimal performance trade-offs. The results highlight key challenges in\ngenerative inpainting, occlusion-robust segmentation, and task-specific\nscrubbing, setting the foundation for future advancements in privacy-preserving\nvision systems.",
        "url": "http://arxiv.org/abs/2504.16557v1",
        "published_date": "2025-04-23T09:33:10+00:00",
        "updated_date": "2025-04-23T09:33:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Murat Bilgehan Ertan",
            "Ronak Sahu",
            "Phuong Ha Nguyen",
            "Kaleel Mahmood",
            "Marten van Dijk"
        ],
        "tldr": "the paper introduces roar, a framework for privacy-preserving dataset obfuscation by removing sensitive objects using instance segmentation and generative inpainting, demonstrating preservation of dataset utility in 2d object detection and 3d reconstruction tasks.",
        "tldr_zh": "该论文介绍了一种名为roar的框架，通过使用实例分割和生成式修复技术来移除敏感对象，从而实现保护隐私的数据集模糊化。实验证明，该方法在二维物体检测和三维重建任务中能够保持数据集的有效性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Visibility-Uncertainty-guided 3D Gaussian Inpainting via Scene Conceptional Learning",
        "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful and efficient 3D\nrepresentation for novel view synthesis. This paper extends 3DGS capabilities\nto inpainting, where masked objects in a scene are replaced with new contents\nthat blend seamlessly with the surroundings. Unlike 2D image inpainting, 3D\nGaussian inpainting (3DGI) is challenging in effectively leveraging\ncomplementary visual and semantic cues from multiple input views, as occluded\nareas in one view may be visible in others. To address this, we propose a\nmethod that measures the visibility uncertainties of 3D points across different\ninput views and uses them to guide 3DGI in utilizing complementary visual cues.\nWe also employ uncertainties to learn a semantic concept of scene without the\nmasked object and use a diffusion model to fill masked objects in input images\nbased on the learned concept. Finally, we build a novel 3DGI framework, VISTA,\nby integrating VISibility-uncerTainty-guided 3DGI with scene conceptuAl\nlearning. VISTA generates high-quality 3DGS models capable of synthesizing\nartifact-free and naturally inpainted novel views. Furthermore, our approach\nextends to handling dynamic distractors arising from temporal object changes,\nenhancing its versatility in diverse scene reconstruction scenarios. We\ndemonstrate the superior performance of our method over state-of-the-art\ntechniques using two challenging datasets: the SPIn-NeRF dataset, featuring 10\ndiverse static 3D inpainting scenes, and an underwater 3D inpainting dataset\nderived from UTB180, including fast-moving fish as inpainting targets.",
        "url": "http://arxiv.org/abs/2504.17815v1",
        "published_date": "2025-04-23T06:21:11+00:00",
        "updated_date": "2025-04-23T06:21:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingxuan Cui",
            "Qing Guo",
            "Yuyi Wang",
            "Hongkai Yu",
            "Di Lin",
            "Qin Zou",
            "Ming-Ming Cheng",
            "Xi Li"
        ],
        "tldr": "this paper presents vista, a novel 3d gaussian inpainting framework that leverages visibility uncertainties and scene concept learning with a diffusion model to seamlessly fill masked objects in 3d scenes, even handling dynamic distractors.",
        "tldr_zh": "本文提出了一种新的3d高斯修复框架vista，它利用可见性不确定性和场景概念学习，结合扩散模型来无缝地填充3d场景中被遮蔽的对象，甚至可以处理动态干扰物。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "RouteWinFormer: A Route-Window Transformer for Middle-range Attention in Image Restoration",
        "summary": "Transformer models have recently garnered significant attention in image\nrestoration due to their ability to capture long-range pixel dependencies.\nHowever, long-range attention often results in computational overhead without\npractical necessity, as degradation and context are typically localized.\nNormalized average attention distance across various degradation datasets shows\nthat middle-range attention is enough for image restoration. Building on this\ninsight, we propose RouteWinFormer, a novel window-based Transformer that\nmodels middle-range context for image restoration. RouteWinFormer incorporates\nRoute-Windows Attnetion Module, which dynamically selects relevant nearby\nwindows based on regional similarity for attention aggregation, extending the\nreceptive field to a mid-range size efficiently. In addition, we introduce\nMulti-Scale Structure Regularization during training, enabling the sub-scale of\nthe U-shaped network to focus on structural information, while the\noriginal-scale learns degradation patterns based on generalized image structure\npriors. Extensive experiments demonstrate that RouteWinFormer outperforms\nstate-of-the-art methods across 9 datasets in various image restoration tasks.",
        "url": "http://arxiv.org/abs/2504.16637v1",
        "published_date": "2025-04-23T11:57:22+00:00",
        "updated_date": "2025-04-23T11:57:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qifan Li",
            "Tianyi Liang",
            "Xingtao Wang",
            "Xiaopeng Fan"
        ],
        "tldr": "the paper introduces routewinformer, a window-based transformer that efficiently models middle-range context using a route-windows attention module and multi-scale structure regularization for image restoration, achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了一种名为routewinformer的基于窗口的transformer，它利用route-windows注意力模块和多尺度结构正则化，高效地建模中等范围的上下文用于图像恢复，并取得了最先进的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "Federated EndoViT: Pretraining Vision Transformers via Federated Learning on Endoscopic Image Collections",
        "summary": "Purpose: In this study, we investigate the training of foundation models\nusing federated learning to address data-sharing limitations and enable\ncollaborative model training without data transfer for minimally invasive\nsurgery. Methods: Inspired by the EndoViT study, we adapt the Masked\nAutoencoder for federated learning, enhancing it with adaptive Sharpness-Aware\nMinimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is\npretrained on the Endo700k dataset collection and later fine-tuned and\nevaluated for tasks such as Semantic Segmentation, Action Triplet Recognition,\nand Surgical Phase Recognition. Results: Our findings demonstrate that\nintegrating adaptive FedSAM into the federated MAE approach improves\npretraining, leading to a reduction in reconstruction loss per patch. The\napplication of FL-EndoViT in surgical downstream tasks results in performance\ncomparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over\nCEN-EndoViT in surgical scene segmentation when data is limited and in action\ntriplet recognition when large datasets are used. Conclusion: These findings\nhighlight the potential of federated learning for privacy-preserving training\nof surgical foundation models, offering a robust and generalizable solution for\nsurgical data science. Effective collaboration requires adapting federated\nlearning methods, such as the integration of FedSAM, which can accommodate the\ninherent data heterogeneity across institutions. In future, exploring FL in\nvideo-based models may enhance these capabilities by incorporating\nspatiotemporal dynamics crucial for real-world surgical environments.",
        "url": "http://arxiv.org/abs/2504.16612v1",
        "published_date": "2025-04-23T10:54:32+00:00",
        "updated_date": "2025-04-23T10:54:32+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Max Kirchner",
            "Alexander C. Jenke",
            "Sebastian Bodenstedt",
            "Fiona R. Kolbinger",
            "Oliver Saldanha",
            "Jakob N. Kather",
            "Martin Wagner",
            "Stefanie Speidel"
        ],
        "tldr": "this paper explores federated learning for pretraining vision transformers on endoscopic images, enhancing it with fedsam and swa, achieving comparable or better performance to centralized training in surgical tasks.",
        "tldr_zh": "本文探索了使用联邦学习在内窥镜图像上预训练视觉转换器，通过fedsam和swa进行增强，在外科任务中实现了与集中式训练相当或更好的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "TraveLLaMA: Facilitating Multi-modal Large Language Models to Understand Urban Scenes and Provide Travel Assistance",
        "summary": "Tourism and travel planning increasingly rely on digital assistance, yet\nexisting multimodal AI systems often lack specialized knowledge and contextual\nunderstanding of urban environments. We present TraveLLaMA, a specialized\nmultimodal language model designed for urban scene understanding and travel\nassistance. Our work addresses the fundamental challenge of developing\npractical AI travel assistants through a novel large-scale dataset of 220k\nquestion-answer pairs. This comprehensive dataset uniquely combines 130k text\nQA pairs meticulously curated from authentic travel forums with GPT-enhanced\nresponses, alongside 90k vision-language QA pairs specifically focused on map\nunderstanding and scene comprehension. Through extensive fine-tuning\nexperiments on state-of-the-art vision-language models (LLaVA, Qwen-VL,\nShikra), we demonstrate significant performance improvements ranging from\n6.5\\%-9.4\\% in both pure text travel understanding and visual question\nanswering tasks. Our model exhibits exceptional capabilities in providing\ncontextual travel recommendations, interpreting map locations, and\nunderstanding place-specific imagery while offering practical information such\nas operating hours and visitor reviews. Comparative evaluations show TraveLLaMA\nsignificantly outperforms general-purpose models in travel-specific tasks,\nestablishing a new benchmark for multi-modal travel assistance systems.",
        "url": "http://arxiv.org/abs/2504.16505v1",
        "published_date": "2025-04-23T08:32:25+00:00",
        "updated_date": "2025-04-23T08:32:25+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Meng Chu",
            "Yukang Chen",
            "Haokun Gui",
            "Shaozuo Yu",
            "Yi Wang",
            "Jiaya Jia"
        ],
        "tldr": "travellama is a specialized multimodal language model for urban scene understanding and travel assistance, fine-tuned on a large-scale dataset of qa pairs, demonstrating significant performance improvements in travel-specific tasks.",
        "tldr_zh": "travellama是一个专门用于城市场景理解和旅行协助的多模态语言模型，它通过在一个大型问答数据集上进行微调，在旅行特定任务中表现出显着的性能提升。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "RGB-D Video Object Segmentation via Enhanced Multi-store Feature Memory",
        "summary": "The RGB-Depth (RGB-D) Video Object Segmentation (VOS) aims to integrate the\nfine-grained texture information of RGB with the spatial geometric clues of\ndepth modality, boosting the performance of segmentation. However,\noff-the-shelf RGB-D segmentation methods fail to fully explore cross-modal\ninformation and suffer from object drift during long-term prediction. In this\npaper, we propose a novel RGB-D VOS method via multi-store feature memory for\nrobust segmentation. Specifically, we design the hierarchical modality\nselection and fusion, which adaptively combines features from both modalities.\nAdditionally, we develop a segmentation refinement module that effectively\nutilizes the Segmentation Anything Model (SAM) to refine the segmentation mask,\nensuring more reliable results as memory to guide subsequent segmentation\ntasks. By leveraging spatio-temporal embedding and modality embedding, mixed\nprompts and fused images are fed into SAM to unleash its potential in RGB-D\nVOS. Experimental results show that the proposed method achieves\nstate-of-the-art performance on the latest RGB-D VOS benchmark.",
        "url": "http://arxiv.org/abs/2504.16471v1",
        "published_date": "2025-04-23T07:31:37+00:00",
        "updated_date": "2025-04-23T07:31:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Boyue Xu",
            "Ruichao Hou",
            "Tongwei Ren",
            "Gangshan Wu"
        ],
        "tldr": "this paper introduces a novel rgb-d video object segmentation method using a multi-store feature memory and sam for improved segmentation accuracy and robustness, achieving state-of-the-art results on rgb-d vos benchmarks.",
        "tldr_zh": "该论文提出了一种新的rgb-d视频对象分割方法，该方法采用多存储特征记忆和sam以提高分割精度和鲁棒性，并在rgb-d vos基准测试中实现了最先进的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "Automating tumor-infiltrating lymphocyte assessment in breast cancer histopathology images using QuPath: a transparent and accessible machine learning pipeline",
        "summary": "In this study, we built an end-to-end tumor-infiltrating lymphocytes (TILs)\nassessment pipeline within QuPath, demonstrating the potential of easily\naccessible tools to perform complex tasks in a fully automatic fashion. First,\nwe trained a pixel classifier to segment tumor, tumor-associated stroma, and\nother tissue compartments in breast cancer H&E-stained whole-slide images (WSI)\nto isolate tumor-associated stroma for subsequent analysis. Next, we applied a\npre-trained StarDist deep learning model in QuPath for cell detection and used\nthe extracted cell features to train a binary classifier distinguishing TILs\nfrom other cells. To evaluate our TILs assessment pipeline, we calculated the\nTIL density in each WSI and categorized them as low, medium, or high TIL\nlevels. Our pipeline was evaluated against pathologist-assigned TIL scores,\nachieving a Cohen's kappa of 0.71 on the external test set, corroborating\nprevious research findings. These results confirm that existing software can\noffer a practical solution for the assessment of TILs in H&E-stained WSIs of\nbreast cancer.",
        "url": "http://arxiv.org/abs/2504.16979v1",
        "published_date": "2025-04-23T17:54:59+00:00",
        "updated_date": "2025-04-23T17:54:59+00:00",
        "categories": [
            "q-bio.QM",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Masoud Tafavvoghi",
            "Lars Ailo Bongo",
            "André Berli Delgado",
            "Nikita Shvetsov",
            "Anders Sildnes",
            "Line Moi",
            "Lill-Tove Rasmussen Busund",
            "Kajsa Møllersen"
        ],
        "tldr": "this paper presents an automated pipeline within qupath for assessing tumor-infiltrating lymphocytes (tils) in breast cancer histopathology images, achieving a cohen's kappa of 0.71 against pathologist scores.",
        "tldr_zh": "该论文提出了一个在qupath中自动评估乳腺癌组织病理学图像中肿瘤浸润淋巴细胞（tils）的流程，与病理学家评分相比，cohen's kappa值为0.71。",
        "relevance_score": 2,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 4
    },
    {
        "title": "Federated Learning of Low-Rank One-Shot Image Detection Models in Edge Devices with Scalable Accuracy and Compute Complexity",
        "summary": "This paper introduces a novel federated learning framework termed LoRa-FL\ndesigned for training low-rank one-shot image detection models deployed on edge\ndevices. By incorporating low-rank adaptation techniques into one-shot\ndetection architectures, our method significantly reduces both computational\nand communication overhead while maintaining scalable accuracy. The proposed\nframework leverages federated learning to collaboratively train lightweight\nimage recognition models, enabling rapid adaptation and efficient deployment\nacross heterogeneous, resource-constrained devices. Experimental evaluations on\nthe MNIST and CIFAR10 benchmark datasets, both in an\nindependent-and-identically-distributed (IID) and non-IID setting, demonstrate\nthat our approach achieves competitive detection performance while\nsignificantly reducing communication bandwidth and compute complexity. This\nmakes it a promising solution for adaptively reducing the communication and\ncompute power overheads, while not sacrificing model accuracy.",
        "url": "http://arxiv.org/abs/2504.16515v1",
        "published_date": "2025-04-23T08:40:44+00:00",
        "updated_date": "2025-04-23T08:40:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Abdul Hannaan",
            "Zubair Shah",
            "Aiman Erbad",
            "Amr Mohamed",
            "Ali Safa"
        ],
        "tldr": "the paper proposes a federated learning framework (lora-fl) for training low-rank one-shot image detection models on edge devices, aiming to reduce communication and computation costs while maintaining accuracy. it demonstrates effectiveness on mnist and cifar10 datasets.",
        "tldr_zh": "该论文提出了一种联邦学习框架 (lora-fl)，用于在边缘设备上训练低秩的一次性图像检测模型，旨在降低通信和计算成本，同时保持准确性。它在 mnist 和 cifar10 数据集上展示了有效性。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    },
    {
        "title": "CLPSTNet: A Progressive Multi-Scale Convolutional Steganography Model Integrating Curriculum Learning",
        "summary": "In recent years, a large number of works have introduced Convolutional Neural\nNetworks (CNNs) into image steganography, which transform traditional\nsteganography methods such as hand-crafted features and prior knowledge design\ninto steganography methods that neural networks autonomically learn information\nembedding. However, due to the inherent complexity of digital images, issues of\ninvisibility and security persist when using CNN models for information\nembedding. In this paper, we propose Curriculum Learning Progressive Steganophy\nNetwork (CLPSTNet). The network consists of multiple progressive multi-scale\nconvolutional modules that integrate Inception structures and dilated\nconvolutions. The module contains multiple branching pathways, starting from a\nsmaller convolutional kernel and dilatation rate, extracting the basic, local\nfeature information from the feature map, and gradually expanding to the\nconvolution with a larger convolutional kernel and dilatation rate for\nperceiving the feature information of a larger receptive field, so as to\nrealize the multi-scale feature extraction from shallow to deep, and from fine\nto coarse, allowing the shallow secret information features to be refined in\ndifferent fusion stages. The experimental results show that the proposed\nCLPSTNet not only has high PSNR , SSIM metrics and decoding accuracy on three\nlarge public datasets, ALASKA2, VOC2012 and ImageNet, but also the\nsteganographic images generated by CLPSTNet have low steganalysis scores.You\ncan find our code at\n\\href{https://github.com/chaos-boops/CLPSTNet}{https://github.com/chaos-boops/CLPSTNet}.",
        "url": "http://arxiv.org/abs/2504.16364v1",
        "published_date": "2025-04-23T02:34:25+00:00",
        "updated_date": "2025-04-23T02:34:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CR"
        ],
        "authors": [
            "Fengchun Liu",
            "Tong Zhang",
            "Chunying Zhang"
        ],
        "tldr": "the paper introduces clpstnet, a novel convolutional steganography network using multi-scale feature extraction and curriculum learning, achieving high psnr, ssim, and low steganalysis scores.",
        "tldr_zh": "该论文介绍了一种名为clpstnet的新型卷积隐写网络，该网络采用多尺度特征提取和课程学习，实现了高psnr、ssim和低隐写分析得分。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    }
]