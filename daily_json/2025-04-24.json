[
    {
        "title": "Token-Shuffle: Towards High-Resolution Image Generation with Autoregressive Models",
        "summary": "Autoregressive (AR) models, long dominant in language generation, are\nincreasingly applied to image synthesis but are often considered less\ncompetitive than Diffusion-based models. A primary limitation is the\nsubstantial number of image tokens required for AR models, which constrains\nboth training and inference efficiency, as well as image resolution. To address\nthis, we present Token-Shuffle, a novel yet simple method that reduces the\nnumber of image tokens in Transformer. Our key insight is the dimensional\nredundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),\nwhere low-dimensional visual codes from visual encoder are directly mapped to\nhigh-dimensional language vocabularies. Leveraging this, we consider two key\noperations: token-shuffle, which merges spatially local tokens along channel\ndimension to decrease the input token number, and token-unshuffle, which\nuntangles the inferred tokens after Transformer blocks to restore the spatial\narrangement for output. Jointly training with textual prompts, our strategy\nrequires no additional pretrained text-encoder and enables MLLMs to support\nextremely high-resolution image synthesis in a unified next-token prediction\nway while maintaining efficient training and inference. For the first time, we\npush the boundary of AR text-to-image generation to a resolution of 2048x2048\nwith gratifying generation performance. In GenAI-benchmark, our 2.7B model\nachieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen\nby 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human\nevaluations also demonstrate our prominent image generation ability in terms of\ntext-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle\ncan serve as a foundational design for efficient high-resolution image\ngeneration within MLLMs.",
        "url": "http://arxiv.org/abs/2504.17789v1",
        "published_date": "2025-04-24T17:59:56+00:00",
        "updated_date": "2025-04-24T17:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xu Ma",
            "Peize Sun",
            "Haoyu Ma",
            "Hao Tang",
            "Chih-Yao Ma",
            "Jialiang Wang",
            "Kunpeng Li",
            "Xiaoliang Dai",
            "Yujun Shi",
            "Xuan Ju",
            "Yushi Hu",
            "Artsiom Sanakoyeu",
            "Felix Juefei-Xu",
            "Ji Hou",
            "Junjiao Tian",
            "Tao Xu",
            "Tingbo Hou",
            "Yen-Cheng Liu",
            "Zecheng He",
            "Zijian He",
            "Matt Feiszli",
            "Peizhao Zhang",
            "Peter Vajda",
            "Sam Tsai",
            "Yun Fu"
        ],
        "tldr": "the paper introduces token-shuffle, a novel method for efficient high-resolution image generation using autoregressive models within mllms, achieving state-of-the-art results at 2048x2048 resolution.",
        "tldr_zh": "该论文介绍了token-shuffle，一种在多模态大语言模型中使用自回归模型进行高效高分辨率图像生成的新方法，并在2048x2048分辨率下实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Step1X-Edit: A Practical Framework for General Image Editing",
        "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
        "url": "http://arxiv.org/abs/2504.17761v1",
        "published_date": "2025-04-24T17:25:12+00:00",
        "updated_date": "2025-04-24T17:25:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shiyu Liu",
            "Yucheng Han",
            "Peng Xing",
            "Fukun Yin",
            "Rui Wang",
            "Wei Cheng",
            "Jiaqi Liao",
            "Yingming Wang",
            "Honghao Fu",
            "Chunrui Han",
            "Guopeng Li",
            "Yuang Peng",
            "Quan Sun",
            "Jingwei Wu",
            "Yan Cai",
            "Zheng Ge",
            "Ranchen Ming",
            "Lei Xia",
            "Xianfang Zeng",
            "Yibo Zhu",
            "Binxing Jiao",
            "Xiangyu Zhang",
            "Gang Yu",
            "Daxin Jiang"
        ],
        "tldr": "the paper introduces step1x-edit, an open-source image editing model aiming to rival the performance of closed-source models like gpt-4o and gemini2 flash, using a multimodal llm and diffusion decoder trained on a generated high-quality dataset and evaluated on a new benchmark.",
        "tldr_zh": "该论文介绍了step1x-edit，一个旨在与gpt-4o和gemini2 flash等闭源模型竞争的开源图像编辑模型，它使用多模态llm和扩散解码器，并在生成的高质量数据集上进行训练，并通过一个新的benchmark进行评估。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "DPMambaIR:All-in-One Image Restoration via Degradation-Aware Prompt State Space Model",
        "summary": "All-in-One image restoration aims to address multiple image degradation\nproblems using a single model, significantly reducing training costs and\ndeployment complexity compared to traditional methods that design dedicated\nmodels for each degradation type. Existing approaches typically rely on\nDegradation-specific models or coarse-grained degradation prompts to guide\nimage restoration. However, they lack fine-grained modeling of degradation\ninformation and face limitations in balancing multi-task conflicts. To overcome\nthese limitations, we propose DPMambaIR, a novel All-in-One image restoration\nframework. By integrating a Degradation-Aware Prompt State Space Model (DP-SSM)\nand a High-Frequency Enhancement Block (HEB), DPMambaIR enables fine-grained\nmodeling of complex degradation information and efficient global integration,\nwhile mitigating the loss of high-frequency details caused by task competition.\nSpecifically, the DP-SSM utilizes a pre-trained degradation extractor to\ncapture fine-grained degradation features and dynamically incorporates them\ninto the state space modeling process, enhancing the model's adaptability to\ndiverse degradation types. Concurrently, the HEB supplements high-frequency\ninformation, effectively addressing the loss of critical details, such as edges\nand textures, in multi-task image restoration scenarios. Extensive experiments\non a mixed dataset containing seven degradation types show that DPMambaIR\nachieves the best performance, with 27.69dB and 0.893 in PSNR and SSIM,\nrespectively. These results highlight the potential and superiority of\nDPMambaIR as a unified solution for All-in-One image restoration.",
        "url": "http://arxiv.org/abs/2504.17732v1",
        "published_date": "2025-04-24T16:46:32+00:00",
        "updated_date": "2025-04-24T16:46:32+00:00",
        "categories": [
            "cs.CV",
            "I.4.4"
        ],
        "authors": [
            "Zhanwen Liu",
            "Sai Zhou",
            "Yuchao Dai",
            "Yang Wang",
            "Yisheng An",
            "Xiangmo Zhao"
        ],
        "tldr": "the paper introduces dpmambair, an all-in-one image restoration framework utilizing a degradation-aware prompt state space model (dp-ssm) and a high-frequency enhancement block (heb) to address multiple image degradation problems with fine-grained degradation modeling while preserving high-frequency details.",
        "tldr_zh": "该论文介绍了 dpmambair，一个一体化图像修复框架，使用感知退化的提示状态空间模型 (dp-ssm) 和高频增强块 (heb) 来解决多个图像退化问题，同时实现细粒度的退化建模并保留高频细节。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "Generative Fields: Uncovering Hierarchical Feature Control for StyleGAN via Inverted Receptive Fields",
        "summary": "StyleGAN has demonstrated the ability of GANs to synthesize highly-realistic\nfaces of imaginary people from random noise. One limitation of GAN-based image\ngeneration is the difficulty of controlling the features of the generated\nimage, due to the strong entanglement of the low-dimensional latent space.\nPrevious work that aimed to control StyleGAN with image or text prompts\nmodulated sampling in W latent space, which is more expressive than Z latent\nspace. However, W space still has restricted expressivity since it does not\ncontrol the feature synthesis directly; also the feature embedding in W space\nrequires a pre-training process to reconstruct the style signal, limiting its\napplication. This paper introduces the concept of \"generative fields\" to\nexplain the hierarchical feature synthesis in StyleGAN, inspired by the\nreceptive fields of convolution neural networks (CNNs). Additionally, we\npropose a new image editing pipeline for StyleGAN using generative field theory\nand the channel-wise style latent space S, utilizing the intrinsic structural\nfeature of CNNs to achieve disentangled control of feature synthesis at\nsynthesis time.",
        "url": "http://arxiv.org/abs/2504.17712v1",
        "published_date": "2025-04-24T16:15:02+00:00",
        "updated_date": "2025-04-24T16:15:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhuo He",
            "Paul Henderson",
            "Nicolas Pugeault"
        ],
        "tldr": "this paper introduces \"generative fields\" to improve feature control in stylegan by leveraging channel-wise style latent space s, offering a new image editing pipeline with better disentanglement.",
        "tldr_zh": "本文引入了\"生成场\"的概念，通过利用通道级风格潜在空间s来改进stylegan中的特征控制，提供了一种具有更好解耦性的图像编辑新方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Labels: Zero-Shot Diabetic Foot Ulcer Wound Segmentation with Self-attention Diffusion Models and the Potential for Text-Guided Customization",
        "summary": "Diabetic foot ulcers (DFUs) pose a significant challenge in healthcare,\nrequiring precise and efficient wound assessment to enhance patient outcomes.\nThis study introduces the Attention Diffusion Zero-shot Unsupervised System\n(ADZUS), a novel text-guided diffusion model that performs wound segmentation\nwithout relying on labeled training data. Unlike conventional deep learning\nmodels, which require extensive annotation, ADZUS leverages zero-shot learning\nto dynamically adapt segmentation based on descriptive prompts, offering\nenhanced flexibility and adaptability in clinical applications. Experimental\nevaluations demonstrate that ADZUS surpasses traditional and state-of-the-art\nsegmentation models, achieving an IoU of 86.68\\% and the highest precision of\n94.69\\% on the chronic wound dataset, outperforming supervised approaches such\nas FUSegNet. Further validation on a custom-curated DFU dataset reinforces its\nrobustness, with ADZUS achieving a median DSC of 75\\%, significantly surpassing\nFUSegNet's 45\\%. The model's text-guided segmentation capability enables\nreal-time customization of segmentation outputs, allowing targeted analysis of\nwound characteristics based on clinical descriptions. Despite its competitive\nperformance, the computational cost of diffusion-based inference and the need\nfor potential fine-tuning remain areas for future improvement. ADZUS represents\na transformative step in wound segmentation, providing a scalable, efficient,\nand adaptable AI-driven solution for medical imaging.",
        "url": "http://arxiv.org/abs/2504.17628v1",
        "published_date": "2025-04-24T14:50:10+00:00",
        "updated_date": "2025-04-24T14:50:10+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Abderrachid Hamrani",
            "Daniela Leizaola",
            "Renato Sousa",
            "Jose P. Ponce",
            "Stanley Mathis",
            "David G. Armstrong",
            "Anuradha Godavarty"
        ],
        "tldr": "the paper introduces adzus, a text-guided diffusion model for zero-shot diabetic foot ulcer segmentation, achieving state-of-the-art performance and offering real-time customization based on clinical descriptions, though computational cost is a limitation.",
        "tldr_zh": "该论文介绍了一种名为adzus的文本引导扩散模型，用于零样本糖尿病足溃疡分割，实现了最先进的性能，并提供了基于临床描述的实时定制，但计算成本是一个限制。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing CNNs robustness to occlusions with bioinspired filters for border completion",
        "summary": "We exploit the mathematical modeling of the visual cortex mechanism for\nborder completion to define custom filters for CNNs. We see a consistent\nimprovement in performance, particularly in accuracy, when our modified LeNet 5\nis tested with occluded MNIST images.",
        "url": "http://arxiv.org/abs/2504.17619v1",
        "published_date": "2025-04-24T14:43:55+00:00",
        "updated_date": "2025-04-24T14:43:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Catarina P. Coutinho",
            "Aneeqa Merhab",
            "Janko Petkovic",
            "Ferdinando Zanchetta",
            "Rita Fioresi"
        ],
        "tldr": "this paper introduces bio-inspired filters for cnns to improve their robustness to occlusions, demonstrating enhanced accuracy on occluded mnist images using a modified lenet-5.",
        "tldr_zh": "本文介绍了一种受生物启发的cnn滤波器，以提高其对遮挡的鲁棒性，并通过改进的lenet-5在被遮挡的mnist图像上展示了更高的准确率。",
        "relevance_score": 2,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 4,
        "overall_priority_score": 4
    },
    {
        "title": "When Gaussian Meets Surfel: Ultra-fast High-fidelity Radiance Field Rendering",
        "summary": "We introduce Gaussian-enhanced Surfels (GESs), a bi-scale representation for\nradiance field rendering, wherein a set of 2D opaque surfels with\nview-dependent colors represent the coarse-scale geometry and appearance of\nscenes, and a few 3D Gaussians surrounding the surfels supplement fine-scale\nappearance details. The rendering with GESs consists of two passes -- surfels\nare first rasterized through a standard graphics pipeline to produce depth and\ncolor maps, and then Gaussians are splatted with depth testing and color\naccumulation on each pixel order independently. The optimization of GESs from\nmulti-view images is performed through an elaborate coarse-to-fine procedure,\nfaithfully capturing rich scene appearance. The entirely sorting-free rendering\nof GESs not only achieves very fast rates, but also produces view-consistent\nimages, successfully avoiding popping artifacts under view changes. The basic\nGES representation can be easily extended to achieve anti-aliasing in rendering\n(Mip-GES), boosted rendering speeds (Speedy-GES) and compact storage\n(Compact-GES), and reconstruct better scene geometries by replacing 3D\nGaussians with 2D Gaussians (2D-GES). Experimental results show that GESs\nadvance the state-of-the-arts as a compelling representation for ultra-fast\nhigh-fidelity radiance field rendering.",
        "url": "http://arxiv.org/abs/2504.17545v1",
        "published_date": "2025-04-24T13:32:58+00:00",
        "updated_date": "2025-04-24T13:32:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Keyang Ye",
            "Tianjia Shao",
            "Kun Zhou"
        ],
        "tldr": "this paper introduces gaussian-enhanced surfels (gess), a novel bi-scale representation for radiance field rendering that achieves ultra-fast, high-fidelity results by combining 2d surfels and 3d gaussians. it also presents several extensions to the basic ges representation for anti-aliasing, speed boosting, storage compactness, and geometry improvements.",
        "tldr_zh": "本文介绍了一种名为高斯增强surfels (gess) 的新型双尺度光场渲染表示方法，通过结合2d surfel和3d高斯分布，实现了超快速、高保真的渲染效果。 它还提出了ges表示法的几种扩展，以实现抗锯齿、速度提升、存储紧凑性和几何改进。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "Text-to-Image Alignment in Denoising-Based Models through Step Selection",
        "summary": "Visual generative AI models often encounter challenges related to text-image\nalignment and reasoning limitations. This paper presents a novel method for\nselectively enhancing the signal at critical denoising steps, optimizing image\ngeneration based on input semantics. Our approach addresses the shortcomings of\nearly-stage signal modifications, demonstrating that adjustments made at later\nstages yield superior results. We conduct extensive experiments to validate the\neffectiveness of our method in producing semantically aligned images on\nDiffusion and Flow Matching model, achieving state-of-the-art performance. Our\nresults highlight the importance of a judicious choice of sampling stage to\nimprove performance and overall image alignment.",
        "url": "http://arxiv.org/abs/2504.17525v1",
        "published_date": "2025-04-24T13:10:32+00:00",
        "updated_date": "2025-04-24T13:10:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Paul Grimal",
            "Hervé Le Borgne",
            "Olivier Ferret"
        ],
        "tldr": "this paper introduces a novel method for improving text-to-image alignment in denoising-based generative models by selectively enhancing the signal at critical denoising steps, demonstrating state-of-the-art performance.",
        "tldr_zh": "本文提出了一种新颖的方法，通过选择性地增强关键去噪步骤中的信号，来提高基于去噪的生成模型中的文本到图像的对齐，并展示了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ESDiff: Encoding Strategy-inspired Diffusion Model with Few-shot Learning for Color Image Inpainting",
        "summary": "Image inpainting is a technique used to restore missing or damaged regions of\nan image. Traditional methods primarily utilize information from adjacent\npixels for reconstructing missing areas, while they struggle to preserve\ncomplex details and structures. Simultaneously, models based on deep learning\nnecessitate substantial amounts of training data. To address this challenge, an\nencoding strategy-inspired diffusion model with few-shot learning for color\nimage inpainting is proposed in this paper. The main idea of this novel\nencoding strategy is the deployment of a \"virtual mask\" to construct\nhigh-dimensional objects through mutual perturbations between channels. This\napproach enables the diffusion model to capture diverse image representations\nand detailed features from limited training samples. Moreover, the encoding\nstrategy leverages redundancy between channels, integrates with low-rank\nmethods during iterative inpainting, and incorporates the diffusion model to\nachieve accurate information output. Experimental results indicate that our\nmethod exceeds current techniques in quantitative metrics, and the\nreconstructed images quality has been improved in aspects of texture and\nstructural integrity, leading to more precise and coherent results.",
        "url": "http://arxiv.org/abs/2504.17524v1",
        "published_date": "2025-04-24T13:08:36+00:00",
        "updated_date": "2025-04-24T13:08:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junyan Zhang",
            "Yan Li",
            "Mengxiao Geng",
            "Liu Shi",
            "Qiegen Liu"
        ],
        "tldr": "this paper introduces an encoding strategy-inspired diffusion model with few-shot learning for color image inpainting, leveraging channel perturbations and low-rank methods to improve reconstruction quality and detail preservation with limited training data.",
        "tldr_zh": "该论文提出了一种编码策略启发的扩散模型，结合少样本学习用于彩色图像修复，利用通道扰动和低秩方法，在有限的训练数据下，提升重建质量和细节保持。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation",
        "summary": "Subject-driven text-to-image (T2I) generation aims to produce images that\nalign with a given textual description, while preserving the visual identity\nfrom a referenced subject image. Despite its broad downstream applicability --\nranging from enhanced personalization in image generation to consistent\ncharacter representation in video rendering -- progress in this field is\nlimited by the lack of reliable automatic evaluation. Existing methods either\nassess only one aspect of the task (i.e., textual alignment or subject\npreservation), misalign with human judgments, or rely on costly API-based\nevaluation. To address this, we introduce RefVNLI, a cost-effective metric that\nevaluates both textual alignment and subject preservation in a single\nprediction. Trained on a large-scale dataset derived from video-reasoning\nbenchmarks and image perturbations, RefVNLI outperforms or matches existing\nbaselines across multiple benchmarks and subject categories (e.g.,\n\\emph{Animal}, \\emph{Object}), achieving up to 6.4-point gains in textual\nalignment and 8.5-point gains in subject consistency. It also excels with\nlesser-known concepts, aligning with human preferences at over 87\\% accuracy.",
        "url": "http://arxiv.org/abs/2504.17502v1",
        "published_date": "2025-04-24T12:44:51+00:00",
        "updated_date": "2025-04-24T12:44:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aviv Slobodkin",
            "Hagai Taitelbaum",
            "Yonatan Bitton",
            "Brian Gordon",
            "Michal Sokolik",
            "Nitzan Bitton Guetta",
            "Almog Gueta",
            "Royi Rassin",
            "Itay Laish",
            "Dani Lischinski",
            "Idan Szpektor"
        ],
        "tldr": "the paper introduces refvnli, a cost-effective metric for evaluating subject-driven text-to-image generation, addressing the limitations of existing methods in assessing both textual alignment and subject preservation.",
        "tldr_zh": "本文介绍了一种名为refvnli的低成本指标，用于评估主体驱动的文本到图像生成。该指标旨在解决现有方法在评估文本对齐和主体保留方面的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhanced Sample Selection with Confidence Tracking: Identifying Correctly Labeled yet Hard-to-Learn Samples in Noisy Data",
        "summary": "We propose a novel sample selection method for image classification in the\npresence of noisy labels. Existing methods typically consider small-loss\nsamples as correctly labeled. However, some correctly labeled samples are\ninherently difficult for the model to learn and can exhibit high loss similar\nto mislabeled samples in the early stages of training. Consequently, setting a\nthreshold on per-sample loss to select correct labels results in a trade-off\nbetween precision and recall in sample selection: a lower threshold may miss\nmany correctly labeled hard-to-learn samples (low recall), while a higher\nthreshold may include many mislabeled samples (low precision). To address this\nissue, our goal is to accurately distinguish correctly labeled yet\nhard-to-learn samples from mislabeled ones, thus alleviating the trade-off\ndilemma. We achieve this by considering the trends in model prediction\nconfidence rather than relying solely on loss values. Empirical observations\nshow that only for correctly labeled samples, the model's prediction confidence\nfor the annotated labels typically increases faster than for any other classes.\nBased on this insight, we propose tracking the confidence gaps between the\nannotated labels and other classes during training and evaluating their trends\nusing the Mann-Kendall Test. A sample is considered potentially correctly\nlabeled if all its confidence gaps tend to increase. Our method functions as a\nplug-and-play component that can be seamlessly integrated into existing sample\nselection techniques. Experiments on several standard benchmarks and real-world\ndatasets demonstrate that our method enhances the performance of existing\nmethods for learning with noisy labels.",
        "url": "http://arxiv.org/abs/2504.17474v1",
        "published_date": "2025-04-24T12:07:14+00:00",
        "updated_date": "2025-04-24T12:07:14+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Weiran Pan",
            "Wei Wei",
            "Feida Zhu",
            "Yong Deng"
        ],
        "tldr": "this paper introduces a novel sample selection method for image classification with noisy labels that uses the trend of the model's prediction confidence to distinguish correctly labeled, hard-to-learn samples from mislabeled ones, improving upon existing loss-based methods.",
        "tldr_zh": "本文提出了一种新的带噪标签图像分类的样本选择方法，该方法使用模型预测置信度的趋势来区分正确标记的、难以学习的样本和错误标记的样本，从而改进了现有的基于损失的方法。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 4
    },
    {
        "title": "FRAG: Frame Selection Augmented Generation for Long Video and Long Document Understanding",
        "summary": "There has been impressive progress in Large Multimodal Models (LMMs). Recent\nworks extend these models to long inputs, including multi-page documents and\nlong videos. However, the model size and performance of these long context\nmodels are still limited due to the computational cost in both training and\ninference. In this work, we explore an orthogonal direction and process long\ninputs without long context LMMs. We propose Frame Selection Augmented\nGeneration (FRAG), where the model first selects relevant frames within the\ninput, and then only generates the final outputs based on the selected frames.\nThe core of the selection process is done by scoring each frame independently,\nwhich does not require long context processing. The frames with the highest\nscores are then selected by a simple Top-K selection. We show that this\nfrustratingly simple framework is applicable to both long videos and multi-page\ndocuments using existing LMMs without any fine-tuning. We consider two models,\nLLaVA-OneVision and InternVL2, in our experiments and show that FRAG\nconsistently improves the performance and achieves state-of-the-art\nperformances for both long video and long document understanding. For videos,\nFRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on\nVideo-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA\ncompared with recent LMMs specialized in long document understanding. Code is\navailable at: https://github.com/NVlabs/FRAG",
        "url": "http://arxiv.org/abs/2504.17447v1",
        "published_date": "2025-04-24T11:19:18+00:00",
        "updated_date": "2025-04-24T11:19:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "De-An Huang",
            "Subhashree Radhakrishnan",
            "Zhiding Yu",
            "Jan Kautz"
        ],
        "tldr": "the paper introduces frag, a method that selects relevant frames from long videos or documents before using an lmm for understanding, achieving state-of-the-art results without fine-tuning.",
        "tldr_zh": "该论文介绍了一种名为frag的方法，该方法在利用大型多模态模型进行理解之前，先从长视频或文档中选择相关帧，无需微调即可达到最先进的效果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models",
        "summary": "Video try-on replaces clothing in videos with target garments. Existing\nmethods struggle to generate high-quality and temporally consistent results\nwhen handling complex clothing patterns and diverse body poses. We present\n3DV-TON, a novel diffusion-based framework for generating high-fidelity and\ntemporally consistent video try-on results. Our approach employs generated\nanimatable textured 3D meshes as explicit frame-level guidance, alleviating the\nissue of models over-focusing on appearance fidelity at the expanse of motion\ncoherence. This is achieved by enabling direct reference to consistent garment\ntexture movements throughout video sequences. The proposed method features an\nadaptive pipeline for generating dynamic 3D guidance: (1) selecting a keyframe\nfor initial 2D image try-on, followed by (2) reconstructing and animating a\ntextured 3D mesh synchronized with original video poses. We further introduce a\nrobust rectangular masking strategy that successfully mitigates artifact\npropagation caused by leaking clothing information during dynamic human and\ngarment movements. To advance video try-on research, we introduce HR-VVT, a\nhigh-resolution benchmark dataset containing 130 videos with diverse clothing\ntypes and scenarios. Quantitative and qualitative results demonstrate our\nsuperior performance over existing methods. The project page is at this link\nhttps://2y7c3.github.io/3DV-TON/",
        "url": "http://arxiv.org/abs/2504.17414v1",
        "published_date": "2025-04-24T10:12:40+00:00",
        "updated_date": "2025-04-24T10:12:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Min Wei",
            "Chaohui Yu",
            "Jingkai Zhou",
            "Fan Wang"
        ],
        "tldr": "this paper introduces 3dv-ton, a diffusion-based video try-on framework employing textured 3d meshes for improved temporal consistency and fidelity, along with a new high-resolution video try-on dataset (hr-vvt). it addresses challenges in handling complex clothing patterns and diverse poses.",
        "tldr_zh": "这篇论文介绍了一种名为3dv-ton的基于扩散模型的视频试穿框架，该框架使用纹理化的3d网格以提高时间一致性和保真度，并提出了一个新的高分辨率视频试穿数据集（hr-vvt）。它解决了处理复杂服装图案和多样姿势的挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SDVPT: Semantic-Driven Visual Prompt Tuning for Open-World Object Counting",
        "summary": "Open-world object counting leverages the robust text-image alignment of\npre-trained vision-language models (VLMs) to enable counting of arbitrary\ncategories in images specified by textual queries. However, widely adopted\nnaive fine-tuning strategies concentrate exclusively on text-image consistency\nfor categories contained in training, which leads to limited generalizability\nfor unseen categories. In this work, we propose a plug-and-play Semantic-Driven\nVisual Prompt Tuning framework (SDVPT) that transfers knowledge from the\ntraining set to unseen categories with minimal overhead in parameters and\ninference time. First, we introduce a two-stage visual prompt learning strategy\ncomposed of Category-Specific Prompt Initialization (CSPI) and Topology-Guided\nPrompt Refinement (TGPR). The CSPI generates category-specific visual prompts,\nand then TGPR distills latent structural patterns from the VLM's text encoder\nto refine these prompts. During inference, we dynamically synthesize the visual\nprompts for unseen categories based on the semantic correlation between unseen\nand training categories, facilitating robust text-image alignment for unseen\ncategories. Extensive experiments integrating SDVPT with all available\nopen-world object counting models demonstrate its effectiveness and\nadaptability across three widely used datasets: FSC-147, CARPK, and PUCPR+.",
        "url": "http://arxiv.org/abs/2504.17395v1",
        "published_date": "2025-04-24T09:31:08+00:00",
        "updated_date": "2025-04-24T09:31:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiming Zhao",
            "Guorong Li",
            "Laiyun Qing",
            "Amin Beheshti",
            "Jian Yang",
            "Michael Sheng",
            "Yuankai Qi",
            "Qingming Huang"
        ],
        "tldr": "the paper introduces a semantic-driven visual prompt tuning framework (sdvpt) for open-world object counting, improving generalization to unseen categories by transferring knowledge from training data and distilling structural patterns. it dynamically generates visual prompts based on semantic correlation during inference.",
        "tldr_zh": "该论文介绍了一种用于开放世界对象计数的语义驱动视觉提示调优框架 (sdvpt)，通过从训练数据中迁移知识并提取结构模式，从而提高对未见类别的泛化能力。它在推理过程中基于语义相关性动态生成视觉提示。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation",
        "summary": "Soccer is a globally popular sporting event, typically characterized by long\nmatches and distinctive highlight moments. Recent advances in Multimodal Large\nLanguage Models (MLLMs) offer promising capabilities in temporal grounding and\nvideo understanding, soccer commentary generation often requires precise\ntemporal localization and semantically rich descriptions over long-form video.\nHowever, existing soccer MLLMs often rely on the temporal a priori for caption\ngeneration, so they cannot process the soccer video end-to-end. While some\ntraditional approaches follow a two-step paradigm that is complex and fails to\ncapture the global context to achieve suboptimal performance. To solve the\nabove issues, we present TimeSoccer, the first end-to-end soccer MLLM for\nSingle-anchor Dense Video Captioning (SDVC) in full-match soccer videos.\nTimeSoccer jointly predicts timestamps and generates captions in a single pass,\nenabling global context modeling across 45-minute matches. To support long\nvideo understanding of soccer matches, we introduce MoFA-Select, a\ntraining-free, motion-aware frame compression module that adaptively selects\nrepresentative frames via a coarse-to-fine strategy, and incorporates\ncomplementary training paradigms to strengthen the model's ability to handle\nlong temporal sequences. Extensive experiments demonstrate that our TimeSoccer\nachieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end\nform, generating high-quality commentary with accurate temporal alignment and\nstrong semantic relevance.",
        "url": "http://arxiv.org/abs/2504.17365v1",
        "published_date": "2025-04-24T08:27:42+00:00",
        "updated_date": "2025-04-24T08:27:42+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Ling You",
            "Wenxuan Huang",
            "Xinni Xie",
            "Xiangyi Wei",
            "Bangyan Li",
            "Shaohui Lin",
            "Yang Li",
            "Changbo Wang"
        ],
        "tldr": "the paper introduces timesoccer, an end-to-end mllm for soccer commentary generation that jointly predicts timestamps and generates captions, achieving state-of-the-art performance using a motion-aware frame compression module for long video understanding.",
        "tldr_zh": "该论文介绍了 timesoccer，一个端到端的用于足球评论生成的 mllm，它联合预测时间戳和生成字幕，并利用运动感知帧压缩模块实现长视频理解，从而达到最先进的性能水平。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "I-INR: Iterative Implicit Neural Representations",
        "summary": "Implicit Neural Representations (INRs) have revolutionized signal processing\nand computer vision by modeling signals as continuous, differentiable functions\nparameterized by neural networks. However, their inherent formulation as a\nregression problem makes them prone to regression to the mean, limiting their\nability to capture fine details, retain high-frequency information, and handle\nnoise effectively. To address these challenges, we propose Iterative Implicit\nNeural Representations (I-INRs) a novel plug-and-play framework that enhances\nsignal reconstruction through an iterative refinement process. I-INRs\neffectively recover high-frequency details, improve robustness to noise, and\nachieve superior reconstruction quality. Our framework seamlessly integrates\nwith existing INR architectures, delivering substantial performance gains\nacross various tasks. Extensive experiments show that I-INRs outperform\nbaseline methods, including WIRE, SIREN, and Gauss, in diverse computer vision\napplications such as image restoration, image denoising, and object occupancy\nprediction.",
        "url": "http://arxiv.org/abs/2504.17364v1",
        "published_date": "2025-04-24T08:27:22+00:00",
        "updated_date": "2025-04-24T08:27:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ali Haider",
            "Muhammad Salman Ali",
            "Maryam Qamar",
            "Tahir Khalil",
            "Soo Ye Kim",
            "Jihyong Oh",
            "Enzo Tartaglione",
            "Sung-Ho Bae"
        ],
        "tldr": "the paper introduces iterative implicit neural representations (i-inrs), a plug-and-play framework improving signal reconstruction, especially for high-frequency details and noise robustness, in implicit neural representations.",
        "tldr_zh": "该论文介绍了迭代隐式神经表示 (i-inrs)，这是一个即插即用框架，通过迭代细化过程来增强信号重建，特别是在隐式神经表示中对高频细节和噪声鲁棒性，提高重建质量。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "DRC: Enhancing Personalized Image Generation via Disentangled Representation Composition",
        "summary": "Personalized image generation has emerged as a promising direction in\nmultimodal content creation. It aims to synthesize images tailored to\nindividual style preferences (e.g., color schemes, character appearances,\nlayout) and semantic intentions (e.g., emotion, action, scene contexts) by\nleveraging user-interacted history images and multimodal instructions. Despite\nnotable progress, existing methods -- whether based on diffusion models, large\nlanguage models, or Large Multimodal Models (LMMs) -- struggle to accurately\ncapture and fuse user style preferences and semantic intentions. In particular,\nthe state-of-the-art LMM-based method suffers from the entanglement of visual\nfeatures, leading to Guidance Collapse, where the generated images fail to\npreserve user-preferred styles or reflect the specified semantics.\n  To address these limitations, we introduce DRC, a novel personalized image\ngeneration framework that enhances LMMs through Disentangled Representation\nComposition. DRC explicitly extracts user style preferences and semantic\nintentions from history images and the reference image, respectively, to form\nuser-specific latent instructions that guide image generation within LMMs.\nSpecifically, it involves two critical learning stages: 1) Disentanglement\nlearning, which employs a dual-tower disentangler to explicitly separate style\nand semantic features, optimized via a reconstruction-driven paradigm with\ndifficulty-aware importance sampling; and 2) Personalized modeling, which\napplies semantic-preserving augmentations to effectively adapt the disentangled\nrepresentations for robust personalized generation. Extensive experiments on\ntwo benchmarks demonstrate that DRC shows competitive performance while\neffectively mitigating the guidance collapse issue, underscoring the importance\nof disentangled representation learning for controllable and effective\npersonalized image generation.",
        "url": "http://arxiv.org/abs/2504.17349v1",
        "published_date": "2025-04-24T08:10:10+00:00",
        "updated_date": "2025-04-24T08:10:10+00:00",
        "categories": [
            "cs.CV",
            "cs.IR"
        ],
        "authors": [
            "Yiyan Xu",
            "Wuqiang Zheng",
            "Wenjie Wang",
            "Fengbin Zhu",
            "Xinting Hu",
            "Yang Zhang",
            "Fuli Feng",
            "Tat-Seng Chua"
        ],
        "tldr": "the paper introduces drc, a novel framework for personalized image generation that mitigates guidance collapse in lmms by employing disentangled representation composition to separate and fuse user style preferences and semantic intentions.",
        "tldr_zh": "该论文介绍了drc，一种新颖的用于个性化图像生成的框架，通过采用解耦表示组合来分离和融合用户风格偏好和语义意图，从而缓解了lmm中的指导崩溃问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DIMT25@ICDAR2025: HW-TSC's End-to-End Document Image Machine Translation System Leveraging Large Vision-Language Model",
        "summary": "This paper presents the technical solution proposed by Huawei Translation\nService Center (HW-TSC) for the \"End-to-End Document Image Machine Translation\nfor Complex Layouts\" competition at the 19th International Conference on\nDocument Analysis and Recognition (DIMT25@ICDAR2025). Leveraging\nstate-of-the-art open-source large vision-language model (LVLM), we introduce a\ntraining framework that combines multi-task learning with perceptual\nchain-of-thought to develop a comprehensive end-to-end document translation\nsystem. During the inference phase, we apply minimum Bayesian decoding and\npost-processing strategies to further enhance the system's translation\ncapabilities. Our solution uniquely addresses both OCR-based and OCR-free\ndocument image translation tasks within a unified framework. This paper\nsystematically details the training methods, inference strategies, LVLM base\nmodels, training data, experimental setups, and results, demonstrating an\neffective approach to document image machine translation.",
        "url": "http://arxiv.org/abs/2504.17315v1",
        "published_date": "2025-04-24T07:17:59+00:00",
        "updated_date": "2025-04-24T07:17:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhanglin Wu",
            "Tengfei Song",
            "Ning Xie",
            "Weidong Zhang",
            "Pengfei Li",
            "Shuang Wu",
            "Chong Li",
            "Junhao Zhu",
            "Hao Yang"
        ],
        "tldr": "this paper introduces an end-to-end document image machine translation system developed for the dimt25@icdar2025 competition, leveraging a large vision-language model with multi-task learning and perceptual chain-of-thought, handling both ocr-based and ocr-free tasks.",
        "tldr_zh": "本文介绍了一个为dimt25@icdar2025竞赛开发的端到端文档图像机器翻译系统，该系统利用大型视觉语言模型，结合多任务学习和感知链式思维，能处理基于ocr和无ocr的任务。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 5,
        "overall_priority_score": 5
    },
    {
        "title": "Towards Generalized and Training-Free Text-Guided Semantic Manipulation",
        "summary": "Text-guided semantic manipulation refers to semantically editing an image\ngenerated from a source prompt to match a target prompt, enabling the desired\nsemantic changes (e.g., addition, removal, and style transfer) while preserving\nirrelevant contents. With the powerful generative capabilities of the diffusion\nmodel, the task has shown the potential to generate high-fidelity visual\ncontent. Nevertheless, existing methods either typically require time-consuming\nfine-tuning (inefficient), fail to accomplish multiple semantic manipulations\n(poorly extensible), and/or lack support for different modality tasks (limited\ngeneralizability). Upon further investigation, we find that the geometric\nproperties of noises in the diffusion model are strongly correlated with the\nsemantic changes. Motivated by this, we propose a novel $\\textit{GTF}$ for\ntext-guided semantic manipulation, which has the following attractive\ncapabilities: 1) $\\textbf{Generalized}$: our $\\textit{GTF}$ supports multiple\nsemantic manipulations (e.g., addition, removal, and style transfer) and can be\nseamlessly integrated into all diffusion-based methods (i.e., Plug-and-play)\nacross different modalities (i.e., modality-agnostic); and 2)\n$\\textbf{Training-free}$: $\\textit{GTF}$ produces high-fidelity results via\nsimply controlling the geometric relationship between noises without tuning or\noptimization. Our extensive experiments demonstrate the efficacy of our\napproach, highlighting its potential to advance the state-of-the-art in\nsemantics manipulation.",
        "url": "http://arxiv.org/abs/2504.17269v1",
        "published_date": "2025-04-24T05:54:56+00:00",
        "updated_date": "2025-04-24T05:54:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu Hong",
            "Xiao Cai",
            "Pengpeng Zeng",
            "Shuai Zhang",
            "Jingkuan Song",
            "Lianli Gao",
            "Heng Tao Shen"
        ],
        "tldr": "this paper introduces a training-free and generalized method (gtf) for text-guided semantic image manipulation using diffusion models by controlling the geometric properties of noises, supporting various manipulations and modalities without fine-tuning.",
        "tldr_zh": "本文介绍了一种训练自由且通用的文本引导语义图像操纵方法（gtf），该方法通过控制噪声的几何特性来使用扩散模型，支持各种操纵和模式，而无需进行微调。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Generalizable Deepfake Detection with Spatial-Frequency Collaborative Learning and Hierarchical Cross-Modal Fusion",
        "summary": "The rapid evolution of deep generative models poses a critical challenge to\ndeepfake detection, as detectors trained on forgery-specific artifacts often\nsuffer significant performance degradation when encountering unseen forgeries.\nWhile existing methods predominantly rely on spatial domain analysis, frequency\ndomain operations are primarily limited to feature-level augmentation, leaving\nfrequency-native artifacts and spatial-frequency interactions insufficiently\nexploited. To address this limitation, we propose a novel detection framework\nthat integrates multi-scale spatial-frequency analysis for universal deepfake\ndetection. Our framework comprises three key components: (1) a local spectral\nfeature extraction pipeline that combines block-wise discrete cosine transform\nwith cascaded multi-scale convolutions to capture subtle spectral artifacts;\n(2) a global spectral feature extraction pipeline utilizing scale-invariant\ndifferential accumulation to identify holistic forgery distribution patterns;\nand (3) a multi-stage cross-modal fusion mechanism that incorporates\nshallow-layer attention enhancement and deep-layer dynamic modulation to model\nspatial-frequency interactions. Extensive evaluations on widely adopted\nbenchmarks demonstrate that our method outperforms state-of-the-art deepfake\ndetection methods in both accuracy and generalizability.",
        "url": "http://arxiv.org/abs/2504.17223v1",
        "published_date": "2025-04-24T03:23:35+00:00",
        "updated_date": "2025-04-24T03:23:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mengyu Qiao",
            "Runze Tian",
            "Yang Wang"
        ],
        "tldr": "this paper proposes a novel deepfake detection framework using spatial-frequency collaborative learning with multi-stage cross-modal fusion, achieving improved accuracy and generalizability compared to state-of-the-art methods.",
        "tldr_zh": "该论文提出了一种新颖的深度伪造检测框架，该框架采用空间-频率协同学习以及多阶段跨模态融合，与最先进的方法相比，提高了准确性和泛化性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback",
        "summary": "Current text-to-video (T2V) generation models are increasingly popular due to\ntheir ability to produce coherent videos from textual prompts. However, these\nmodels often struggle to generate semantically and temporally consistent videos\nwhen dealing with longer, more complex prompts involving multiple objects or\nsequential events. Additionally, the high computational costs associated with\ntraining or fine-tuning make direct improvements impractical. To overcome these\nlimitations, we introduce \\(\\projectname\\), a novel zero-training video\nrefinement pipeline that leverages neuro-symbolic feedback to automatically\nenhance video generation, achieving superior alignment with the prompts. Our\napproach first derives the neuro-symbolic feedback by analyzing a formal video\nrepresentation and pinpoints semantically inconsistent events, objects, and\ntheir corresponding frames. This feedback then guides targeted edits to the\noriginal video. Extensive empirical evaluations on both open-source and\nproprietary T2V models demonstrate that \\(\\projectname\\) significantly enhances\ntemporal and logical alignment across diverse prompts by almost $40\\%$.",
        "url": "http://arxiv.org/abs/2504.17180v1",
        "published_date": "2025-04-24T01:34:12+00:00",
        "updated_date": "2025-04-24T01:34:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Minkyu Choi",
            "S P Sharan",
            "Harsh Goel",
            "Sahil Shah",
            "Sandeep Chinchali"
        ]
    },
    {
        "title": "AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models",
        "summary": "Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately\ndetect objects and interpret their surroundings. However, even when trained\nusing millions of miles of real-world data, AVs are often unable to detect rare\nfailure modes (RFMs). The problem of RFMs is commonly referred to as the\n\"long-tail challenge\", due to the distribution of data including many instances\nthat are very rarely seen. In this paper, we present a novel approach that\nutilizes advanced generative and explainable AI techniques to aid in\nunderstanding RFMs. Our methods can be used to enhance the robustness and\nreliability of AVs when combined with both downstream model training and\ntesting. We extract segmentation masks for objects of interest (e.g., cars) and\ninvert them to create environmental masks. These masks, combined with carefully\ncrafted text prompts, are fed into a custom diffusion model. We leverage the\nStable Diffusion inpainting model guided by adversarial noise optimization to\ngenerate images containing diverse environments designed to evade object\ndetection models and expose vulnerabilities in AI systems. Finally, we produce\nnatural language descriptions of the generated RFMs that can guide developers\nand policymakers to improve the safety and reliability of AV systems.",
        "url": "http://arxiv.org/abs/2504.17179v1",
        "published_date": "2025-04-24T01:31:13+00:00",
        "updated_date": "2025-04-24T01:31:13+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.RO",
            "68T45, 68T05 68T45, 68T05 68T45, 68T05",
            "I.2.6; I.2.10; I.4.8"
        ],
        "authors": [
            "Mohammad Zarei",
            "Melanie A Jutras",
            "Eliana Evans",
            "Mike Tan",
            "Omid Aaramoon"
        ],
        "tldr": "this paper introduces a novel approach using adversarially guided diffusion models to generate rare failure modes in autonomous vehicle perception systems, aiming to improve their robustness and reliability.",
        "tldr_zh": "该论文提出了一种新颖的方法，利用对抗引导的扩散模型生成自动驾驶汽车感知系统中罕见的故障模式，旨在提高其鲁棒性和可靠性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]