[
    {
        "title": "Audio-Driven Talking Face Video Generation with Joint Uncertainty Learning",
        "summary": "Talking face video generation with arbitrary speech audio is a significant\nchallenge within the realm of digital human technology. The previous studies\nhave emphasized the significance of audio-lip synchronization and visual\nquality. Currently, limited attention has been given to the learning of visual\nuncertainty, which creates several issues in existing systems, including\ninconsistent visual quality and unreliable performance across different input\nconditions. To address the problem, we propose a Joint Uncertainty Learning\nNetwork (JULNet) for high-quality talking face video generation, which\nincorporates a representation of uncertainty that is directly related to visual\nerror. Specifically, we first design an uncertainty module to individually\npredict the error map and uncertainty map after obtaining the generated image.\nThe error map represents the difference between the generated image and the\nground truth image, while the uncertainty map is used to predict the\nprobability of incorrect estimates. Furthermore, to match the uncertainty\ndistribution with the error distribution through a KL divergence term, we\nintroduce a histogram technique to approximate the distributions. By jointly\noptimizing error and uncertainty, the performance and robustness of our model\ncan be enhanced. Extensive experiments demonstrate that our method achieves\nsuperior high-fidelity and audio-lip synchronization in talking face video\ngeneration compared to previous methods.",
        "url": "http://arxiv.org/abs/2504.18810v1",
        "published_date": "2025-04-26T05:45:38+00:00",
        "updated_date": "2025-04-26T05:45:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yifan Xie",
            "Fei Ma",
            "Yi Bin",
            "Ying He",
            "Fei Yu"
        ],
        "tldr": "this paper introduces a joint uncertainty learning network (julnet) for audio-driven talking face video generation, focusing on mitigating visual uncertainty to improve visual quality and audio-lip synchronization.",
        "tldr_zh": "本文介绍了一种用于音频驱动的说话人脸视频生成的联合不确定性学习网络（julnet），专注于减轻视觉不确定性，以提高视觉质量和音频唇部同步。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Validation and Calibration of Semi-Analytical Models for the Event Horizon Telescope Observations of Sagittarius A*",
        "summary": "The Event Horizon Telescope (EHT) enables the exploration of black hole\naccretion flows at event-horizon scales. Fitting ray-traced physical models to\nEHT observations requires the generation of synthetic images, a task that is\ncomputationally demanding. This study leverages \\alinet, a generative machine\nlearning model, to efficiently produce radiatively inefficient accretion flow\n(RIAF) images as a function of the specified physical parameters. \\alinet has\npreviously been shown to be able to interpolate black hole images and their\nassociated physical parameters after training on a computationally tractable\nset of library images. We utilize this model to estimate the uncertainty\nintroduced by a number of anticipated unmodeled physical effects, including\ninterstellar scattering and intrinsic source variability. We then use this to\ncalibrate physical parameter estimates and their associated uncertainties from\nRIAF model fits to mock EHT data via a library of general relativistic\nmagnetohydrodynamics models.",
        "url": "http://arxiv.org/abs/2504.18624v1",
        "published_date": "2025-04-25T18:00:04+00:00",
        "updated_date": "2025-04-25T18:00:04+00:00",
        "categories": [
            "astro-ph.HE",
            "astro-ph.IM",
            "cs.CV",
            "cs.LG",
            "85A99, 35Q75, 65C60, 62F15",
            "I.2.6; G.1.10; I.4.10"
        ],
        "authors": [
            "Ali SaraerToosi",
            "Avery Broderick"
        ],
        "tldr": "the paper utilizes a generative machine learning model (alinet) to efficiently generate black hole accretion flow images for event horizon telescope data analysis, addressing the computational cost of fitting physical models to observations and calibrating parameter estimates.",
        "tldr_zh": "该论文利用生成式机器学习模型（alinet）高效生成黑洞吸积流图像，用于事件视界望远镜的数据分析，解决了将物理模型拟合到观测数据时的计算成本问题，并校准了参数估计。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Video CLIP Model for Multi-View Echocardiography Interpretation",
        "summary": "Echocardiography involves recording videos of the heart using ultrasound,\nenabling clinicians to evaluate its condition. Recent advances in large-scale\nvision-language models (VLMs) have garnered attention for automating the\ninterpretation of echocardiographic videos. However, most existing VLMs\nproposed for medical interpretation thus far rely on single-frame (i.e., image)\ninputs. Consequently, these image-based models often exhibit lower diagnostic\naccuracy for conditions identifiable through cardiac motion. Moreover,\nechocardiographic videos are recorded from various views that depend on the\ndirection of ultrasound emission, and certain views are more suitable than\nothers for interpreting specific conditions. Incorporating multiple views could\npotentially yield further improvements in accuracy. In this study, we developed\na video-language model that takes five different views and full video sequences\nas input, training it on pairs of echocardiographic videos and clinical reports\nfrom 60,747 cases. Our experiments demonstrate that this expanded approach\nachieves higher interpretation accuracy than models trained with only\nsingle-view videos or with still images.",
        "url": "http://arxiv.org/abs/2504.18800v1",
        "published_date": "2025-04-26T05:11:15+00:00",
        "updated_date": "2025-04-26T05:11:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ryo Takizawa",
            "Satoshi Kodera",
            "Tempei Kabayama",
            "Ryo Matsuoka",
            "Yuta Ando",
            "Yuto Nakamura",
            "Haruki Settai",
            "Norihiko Takeda"
        ],
        "tldr": "this paper introduces a video-language model using multiple echocardiography views and full video sequences to improve diagnostic accuracy in cardiac condition interpretation, outperforming single-view video or still image-based models.",
        "tldr_zh": "本文介绍了一种视频语言模型，该模型使用多个超声心动图视图和完整的视频序列来提高心脏状况解释的诊断准确性，优于基于单视图视频或静态图像的模型。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Dream-Box: Object-wise Outlier Generation for Out-of-Distribution Detection",
        "summary": "Deep neural networks have demonstrated great generalization capabilities for\ntasks whose training and test sets are drawn from the same distribution.\nNevertheless, out-of-distribution (OOD) detection remains a challenging task\nthat has received significant attention in recent years. Specifically, OOD\ndetection refers to the detection of instances that do not belong to the\ntraining distribution, while still having good performance on the\nin-distribution task (e.g., classification or object detection). Recent work\nhas focused on generating synthetic outliers and using them to train an outlier\ndetector, generally achieving improved OOD detection than traditional OOD\nmethods. In this regard, outliers can be generated either in feature or pixel\nspace. Feature space driven methods have shown strong performance on both the\nclassification and object detection tasks, at the expense that the\nvisualization of training outliers remains unknown, making further analysis on\nOOD failure modes challenging. On the other hand, pixel space outlier\ngeneration techniques enabled by diffusion models have been used for image\nclassification using, providing improved OOD detection performance and outlier\nvisualization, although their adaption to the object detection task is as yet\nunexplored. We therefore introduce Dream-Box, a method that provides a link to\nobject-wise outlier generation in the pixel space for OOD detection.\nSpecifically, we use diffusion models to generate object-wise outliers that are\nused to train an object detector for an in-distribution task and OOD detection.\nOur method achieves comparable performance to previous traditional methods\nwhile being the first technique to provide concrete visualization of generated\nOOD objects.",
        "url": "http://arxiv.org/abs/2504.18746v1",
        "published_date": "2025-04-25T23:52:27+00:00",
        "updated_date": "2025-04-25T23:52:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Brian K. S. Isaac-Medina",
            "Toby P. Breckon"
        ],
        "tldr": "the paper introduces dream-box, a method for object-wise outlier generation using diffusion models for out-of-distribution (ood) object detection, providing visualization of generated ood objects.",
        "tldr_zh": "该论文介绍了 dream-box，一种使用扩散模型进行目标级别的异常值生成的方法，用于异常检测的目标检测，并提供了生成异常目标的视觉呈现。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "PiercingEye: Dual-Space Video Violence Detection with Hyperbolic Vision-Language Guidance",
        "summary": "Existing weakly supervised video violence detection (VVD) methods primarily\nrely on Euclidean representation learning, which often struggles to distinguish\nvisually similar yet semantically distinct events due to limited hierarchical\nmodeling and insufficient ambiguous training samples. To address this\nchallenge, we propose PiercingEye, a novel dual-space learning framework that\nsynergizes Euclidean and hyperbolic geometries to enhance discriminative\nfeature representation. Specifically, PiercingEye introduces a layer-sensitive\nhyperbolic aggregation strategy with hyperbolic Dirichlet energy constraints to\nprogressively model event hierarchies, and a cross-space attention mechanism to\nfacilitate complementary feature interactions between Euclidean and hyperbolic\nspaces. Furthermore, to mitigate the scarcity of ambiguous samples, we leverage\nlarge language models to generate logic-guided ambiguous event descriptions,\nenabling explicit supervision through a hyperbolic vision-language contrastive\nloss that prioritizes high-confusion samples via dynamic similarity-aware\nweighting. Extensive experiments on XD-Violence and UCF-Crime benchmarks\ndemonstrate that PiercingEye achieves state-of-the-art performance, with\nparticularly strong results on a newly curated ambiguous event subset,\nvalidating its superior capability in fine-grained violence detection.",
        "url": "http://arxiv.org/abs/2504.18866v1",
        "published_date": "2025-04-26T09:29:10+00:00",
        "updated_date": "2025-04-26T09:29:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaxu Leng",
            "Zhanjie Wu",
            "Mingpi Tan",
            "Mengjingcheng Mo",
            "Jiankang Zheng",
            "Qingqing Li",
            "Ji Gan",
            "Xinbo Gao"
        ],
        "tldr": "the paper introduces piercingeye, a dual-space (euclidean and hyperbolic) learning framework for video violence detection that utilizes large language models to generate ambiguous event descriptions for better training, achieving state-of-the-art performance.",
        "tldr_zh": "该论文介绍了一种名为piercingeye的双空间（欧几里得和双曲）学习框架，用于视频暴力检测。该框架利用大型语言模型生成模糊事件描述以进行更好的训练，从而实现最先进的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "Co-Training with Active Contrastive Learning and Meta-Pseudo-Labeling on 2D Projections for Deep Semi-Supervised Learning",
        "summary": "A major challenge that prevents the training of DL models is the limited\navailability of accurately labeled data. This shortcoming is highlighted in\nareas where data annotation becomes a time-consuming and error-prone task. In\nthis regard, SSL tackles this challenge by capitalizing on scarce labeled and\nabundant unlabeled data; however, SoTA methods typically depend on pre-trained\nfeatures and large validation sets to learn effective representations for\nclassification tasks. In addition, the reduced set of labeled data is often\nrandomly sampled, neglecting the selection of more informative samples. Here,\nwe present active-DeepFA, a method that effectively combines CL,\nteacher-student-based meta-pseudo-labeling and AL to train non-pretrained CNN\narchitectures for image classification in scenarios of scarcity of labeled and\nabundance of unlabeled data. It integrates DeepFA into a co-training setup that\nimplements two cooperative networks to mitigate confirmation bias from\npseudo-labels. The method starts with a reduced set of labeled samples by\nwarming up the networks with supervised CL. Afterward and at regular epoch\nintervals, label propagation is performed on the 2D projections of the\nnetworks' deep features. Next, the most reliable pseudo-labels are exchanged\nbetween networks in a cross-training fashion, while the most meaningful samples\nare annotated and added into the labeled set. The networks independently\nminimize an objective loss function comprising supervised contrastive,\nsupervised and semi-supervised loss components, enhancing the representations\ntowards image classification. Our approach is evaluated on three challenging\nbiological image datasets using only 5% of labeled samples, improving baselines\nand outperforming six other SoTA methods. In addition, it reduces annotation\neffort by achieving comparable results to those of its counterparts with only\n3% of labeled data.",
        "url": "http://arxiv.org/abs/2504.18666v1",
        "published_date": "2025-04-25T19:41:45+00:00",
        "updated_date": "2025-04-25T19:41:45+00:00",
        "categories": [
            "cs.CV",
            "68T07",
            "I.4.10; I.5.1"
        ],
        "authors": [
            "David Aparco-Cardenas",
            "Jancarlo F. Gomes",
            "Alexandre X. Falcão",
            "Pedro J. de Rezende"
        ],
        "tldr": "this paper proposes an active co-training method (active-deepfa) combining contrastive learning, meta-pseudo-labeling, and active learning on 2d feature projections to train cnns for image classification with limited labeled data, achieving sota results on biological image datasets.",
        "tldr_zh": "本文提出了一种主动协同训练方法 (active-deepfa)，结合了对比学习、元伪标签和在 2d 特征投影上的主动学习，用于在标记数据有限的情况下训练 cnn 进行图像分类，并在生物图像数据集上取得了 sota 效果。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy",
        "summary": "Generalizable dexterous grasping with suitable grasp types is a fundamental\nskill for intelligent robots. Developing such skills requires a large-scale and\nhigh-quality dataset that covers numerous grasp types (i.e., at least those\ncategorized by the GRASP taxonomy), but collecting such data is extremely\nchallenging. Existing automatic grasp synthesis methods are often limited to\nspecific grasp types or object categories, hindering scalability. This work\nproposes an efficient pipeline capable of synthesizing contact-rich,\npenetration-free, and physically plausible grasps for any grasp type, object,\nand articulated hand. Starting from a single human-annotated template for each\nhand and grasp type, our pipeline tackles the complicated synthesis problem\nwith two stages: optimize the object to fit the hand template first, and then\nlocally refine the hand to fit the object in simulation. To validate the\nsynthesized grasps, we introduce a contact-aware control strategy that allows\nthe hand to apply the appropriate force at each contact point to the object.\nThose validated grasps can also be used as new grasp templates to facilitate\nfuture synthesis. Experiments show that our method significantly outperforms\nprevious type-unaware grasp synthesis baselines in simulation. Using our\nalgorithm, we construct a dataset containing 10.7k objects and 9.5M grasps,\ncovering 31 grasp types in the GRASP taxonomy. Finally, we train a\ntype-conditional generative model that successfully performs the desired grasp\ntype from single-view object point clouds, achieving an 82.3% success rate in\nreal-world experiments. Project page: https://pku-epic.github.io/Dexonomy.",
        "url": "http://arxiv.org/abs/2504.18829v1",
        "published_date": "2025-04-26T07:32:59+00:00",
        "updated_date": "2025-04-26T07:32:59+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Jiayi Chen",
            "Yubin Ke",
            "Lin Peng",
            "He Wang"
        ],
        "tldr": "this paper introduces a method for synthesizing a large-scale dataset of dexterous grasps covering the grasp taxonomy, and demonstrates its use by training a type-conditional generative model for grasping from single-view object point clouds, achieving good real-world performance.",
        "tldr_zh": "本文介绍了一种合成大规模灵巧抓取数据集的方法，该数据集涵盖了grasp分类，并通过训练一个类型条件生成模型，从单视图物体点云中进行抓取，证明了其在现实世界中的良好性能。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 4
    },
    {
        "title": "Depth as Points: Center Point-based Depth Estimation",
        "summary": "The perception of vehicles and pedestrians in urban scenarios is crucial for\nautonomous driving. This process typically involves complicated data\ncollection, imposes high computational and hardware demands. To address these\nlimitations, we first develop a highly efficient method for generating virtual\ndatasets, which enables the creation of task- and scenario-specific datasets in\na short time. Leveraging this method, we construct the virtual depth estimation\ndataset VirDepth, a large-scale, multi-task autonomous driving dataset.\nSubsequently, we propose CenterDepth, a lightweight architecture for monocular\ndepth estimation that ensures high operational efficiency and exhibits superior\nperformance in depth estimation tasks with highly imbalanced height-scale\ndistributions. CenterDepth integrates global semantic information through the\ninnovative Center FC-CRFs algorithm, aggregates multi-scale features based on\nobject key points, and enables detection-based depth estimation of targets.\nExperiments demonstrate that our proposed method achieves superior performance\nin terms of both computational speed and prediction accuracy.",
        "url": "http://arxiv.org/abs/2504.18773v1",
        "published_date": "2025-04-26T03:04:05+00:00",
        "updated_date": "2025-04-26T03:04:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiheng Tu",
            "Xinjian Huang",
            "Yong He",
            "Ruiyang Zhou",
            "Bo Du",
            "Weitao Wu"
        ],
        "tldr": "this paper introduces a method for generating virtual depth estimation datasets and a lightweight architecture called centerdepth for monocular depth estimation, achieving high efficiency and accuracy, especially in imbalanced height-scale distributions.",
        "tldr_zh": "本文介绍了一种生成虚拟深度估计数据集的方法，以及一种名为centerdepth的轻量级单目深度估计架构，该架构具有高效和准确性，尤其是在高度比例分布不平衡的情况下。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    }
]