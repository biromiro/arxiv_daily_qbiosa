[
    {
        "title": "Automated Learning of Semantic Embedding Representations for Diffusion Models",
        "summary": "Generative models capture the true distribution of data, yielding\nsemantically rich representations. Denoising diffusion models (DDMs) exhibit\nsuperior generative capabilities, though efficient representation learning for\nthem are lacking. In this work, we employ a multi-level denoising autoencoder\nframework to expand the representation capacity of DDMs, which introduces\nsequentially consistent Diffusion Transformers and an additional\ntimestep-dependent encoder to acquire embedding representations on the\ndenoising Markov chain through self-conditional diffusion learning.\nIntuitively, the encoder, conditioned on the entire diffusion process,\ncompresses high-dimensional data into directional vectors in latent under\ndifferent noise levels, facilitating the learning of image embeddings across\nall timesteps. To verify the semantic adequacy of embeddings generated through\nthis approach, extensive experiments are conducted on various datasets,\ndemonstrating that optimally learned embeddings by DDMs surpass\nstate-of-the-art self-supervised representation learning methods in most cases,\nachieving remarkable discriminative semantic representation quality. Our work\njustifies that DDMs are not only suitable for generative tasks, but also\npotentially advantageous for general-purpose deep learning applications.",
        "url": "http://arxiv.org/abs/2505.05732v1",
        "published_date": "2025-05-09T02:10:46+00:00",
        "updated_date": "2025-05-09T02:10:46+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Limai Jiang",
            "Yunpeng Cai"
        ],
        "tldr": "this paper introduces a multi-level denoising autoencoder framework for diffusion models to learn semantically rich image embeddings, achieving state-of-the-art performance in self-supervised representation learning.",
        "tldr_zh": "本文介绍了一种用于扩散模型的多级去噪自动编码器框架，以学习语义丰富的图像嵌入，并在自监督表征学习中实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition",
        "summary": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc.",
        "url": "http://arxiv.org/abs/2505.05829v1",
        "published_date": "2025-05-09T06:56:17+00:00",
        "updated_date": "2025-05-09T06:56:17+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "eess.IV"
        ],
        "authors": [
            "Zhiyuan Chen",
            "Keyi Li",
            "Yifan Jia",
            "Le Ye",
            "Yufei Ma"
        ],
        "tldr": "this paper introduces a training-free acceleration method for diffusion transformers (dit) called increment-calibrated caching with channel-aware svd, which reduces computation and improves image generation quality compared to naive caching approaches.",
        "tldr_zh": "本文介绍了一种用于扩散transformer (dit) 的免训练加速方法，称为增量校准缓存与通道感知svd，与朴素缓存方法相比，该方法降低了计算量并提高了图像生成质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "InstanceGen: Image Generation with Instance-level Instructions",
        "summary": "Despite rapid advancements in the capabilities of generative models,\npretrained text-to-image models still struggle in capturing the semantics\nconveyed by complex prompts that compound multiple objects and instance-level\nattributes. Consequently, we are witnessing growing interests in integrating\nadditional structural constraints, %leveraging additional structural inputs\ntypically in the form of coarse bounding boxes, to better guide the generation\nprocess in such challenging cases. In this work, we take the idea of structural\nguidance a step further by making the observation that contemporary image\ngeneration models can directly provide a plausible \\emph{fine-grained}\nstructural initialization. We propose a technique that couples this image-based\nstructural guidance with LLM-based instance-level instructions, yielding output\nimages that adhere to all parts of the text prompt, including object counts,\ninstance-level attributes, and spatial relations between instances.",
        "url": "http://arxiv.org/abs/2505.05678v1",
        "published_date": "2025-05-08T22:31:23+00:00",
        "updated_date": "2025-05-08T22:31:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Etai Sella",
            "Yanir Kleiman",
            "Hadar Averbuch-Elor"
        ],
        "tldr": "the paper introduces instancegen, a technique that combines image-based structural guidance with llm-based instance-level instructions to improve text-to-image generation with complex prompts involving multiple objects and attributes.",
        "tldr_zh": "该论文介绍了一种名为 instancegen 的技术，它将基于图像的结构引导与基于 llm 的实例级指令相结合，以改进文本到图像的生成，尤其是在处理涉及多个对象和属性的复杂提示时。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Looking Beyond Language Priors: Enhancing Visual Comprehension and Attention in Multimodal Models",
        "summary": "Achieving deep alignment between vision and language remains a central\nchallenge for Multimodal Large Language Models (MLLMs). These models often fail\nto fully leverage visual input, defaulting to strong language priors. Our\napproach first provides insights into how MLLMs internally build visual\nunderstanding of image regions and then introduces techniques to amplify this\ncapability. Specifically, we explore techniques designed both to deepen the\nmodel's understanding of visual content and to ensure that these visual\ninsights actively guide language generation. We demonstrate the superior\nmultimodal understanding of our resultant model through a detailed upstream\nanalysis quantifying its ability to predict visually-dependent tokens as well\nas 10 pt boost on visually challenging tasks.",
        "url": "http://arxiv.org/abs/2505.05626v1",
        "published_date": "2025-05-08T20:04:27+00:00",
        "updated_date": "2025-05-08T20:04:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Aarti Ghatkesar",
            "Uddeshya Upadhyay",
            "Ganesh Venkatesh"
        ],
        "tldr": "this paper addresses the problem of mllms relying too heavily on language priors, proposing techniques to enhance visual understanding and attention, leading to improved performance on visually challenging tasks. it boosts multimodal understanding by improving visually-dependent token prediction.",
        "tldr_zh": "本文旨在解决多模态大语言模型过度依赖语言先验的问题，提出增强视觉理解和注意力的方法，从而提升在视觉挑战性任务上的表现。 通过改进视觉依赖性标记的预测来增强多模态理解。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Prompt to Polyp: Clinically-Aware Medical Image Synthesis with Diffusion Models",
        "summary": "The generation of realistic medical images from text descriptions has\nsignificant potential to address data scarcity challenges in healthcare AI\nwhile preserving patient privacy. This paper presents a comprehensive study of\ntext-to-image synthesis in the medical domain, comparing two distinct\napproaches: (1) fine-tuning large pre-trained latent diffusion models and (2)\ntraining small, domain-specific models. We introduce a novel model named MSDM,\nan optimized architecture based on Stable Diffusion that integrates a clinical\ntext encoder, variational autoencoder, and cross-attention mechanisms to better\nalign medical text prompts with generated images. Our study compares two\napproaches: fine-tuning large pre-trained models (FLUX, Kandinsky) versus\ntraining compact domain-specific models (MSDM). Evaluation across colonoscopy\n(MedVQA-GI) and radiology (ROCOv2) datasets reveals that while large models\nachieve higher fidelity, our optimized MSDM delivers comparable quality with\nlower computational costs. Quantitative metrics and qualitative evaluations by\nmedical experts reveal strengths and limitations of each approach.",
        "url": "http://arxiv.org/abs/2505.05573v1",
        "published_date": "2025-05-08T18:07:16+00:00",
        "updated_date": "2025-05-08T18:07:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "68T07, 68U10, 92C55",
            "I.2.10; I.4.8; J.3"
        ],
        "authors": [
            "Mikhail Chaichuk",
            "Sushant Gautam",
            "Steven Hicks",
            "Elena Tutubalina"
        ],
        "tldr": "this paper compares fine-tuning large diffusion models vs. training small, domain-specific models for medical image generation from text, introducing an optimized stable diffusion-based architecture (msdm) that achieves comparable quality to large models with lower computational cost.",
        "tldr_zh": "本文比较了微调大型扩散模型与训练小型、特定领域模型在基于文本的医学图像生成方面的效果，提出了一种基于优化后的stable diffusion架构（msdm），在计算成本较低的情况下实现了与大型模型相当的质量。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Photovoltaic Defect Image Generator with Boundary Alignment Smoothing Constraint for Domain Shift Mitigation",
        "summary": "Accurate defect detection of photovoltaic (PV) cells is critical for ensuring\nquality and efficiency in intelligent PV manufacturing systems. However, the\nscarcity of rich defect data poses substantial challenges for effective model\ntraining. While existing methods have explored generative models to augment\ndatasets, they often suffer from instability, limited diversity, and domain\nshifts. To address these issues, we propose PDIG, a Photovoltaic Defect Image\nGenerator based on Stable Diffusion (SD). PDIG leverages the strong priors\nlearned from large-scale datasets to enhance generation quality under limited\ndata. Specifically, we introduce a Semantic Concept Embedding (SCE) module that\nincorporates text-conditioned priors to capture the relational concepts between\ndefect types and their appearances. To further enrich the domain distribution,\nwe design a Lightweight Industrial Style Adaptor (LISA), which injects\nindustrial defect characteristics into the SD model through cross-disentangled\nattention. At inference, we propose a Text-Image Dual-Space Constraints (TIDSC)\nmodule, enforcing the quality of generated images via positional consistency\nand spatial smoothing alignment. Extensive experiments demonstrate that PDIG\nachieves superior realism and diversity compared to state-of-the-art methods.\nSpecifically, our approach improves Frechet Inception Distance (FID) by 19.16\npoints over the second-best method and significantly enhances the performance\nof downstream defect detection tasks.",
        "url": "http://arxiv.org/abs/2505.06117v1",
        "published_date": "2025-05-09T15:16:42+00:00",
        "updated_date": "2025-05-09T15:16:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dongying Li",
            "Binyi Su",
            "Hua Zhang",
            "Yong Li",
            "Haiyong Chen"
        ],
        "tldr": "the paper introduces pdig, a stable diffusion-based image generator that creates synthetic photovoltaic defect images. it uses a semantic concept embedding module, a lightweight industrial style adaptor, and a text-image dual-space constraints module to improve the realism and diversity of generated images for effective defect detection.",
        "tldr_zh": "该论文介绍了一种基于stable diffusion的光伏缺陷图像生成器pdig, 用于生成合成光伏缺陷图像。它利用语义概念嵌入模块、轻量级工业风格适配器以及文本-图像双空间约束模块，以提高生成图像的真实感和多样性, 从而实现有效的缺陷检测。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Noise-Consistent Siamese-Diffusion for Medical Image Synthesis and Segmentation",
        "summary": "Deep learning has revolutionized medical image segmentation, yet its full\npotential remains constrained by the paucity of annotated datasets. While\ndiffusion models have emerged as a promising approach for generating synthetic\nimage-mask pairs to augment these datasets, they paradoxically suffer from the\nsame data scarcity challenges they aim to mitigate. Traditional mask-only\nmodels frequently yield low-fidelity images due to their inability to\nadequately capture morphological intricacies, which can critically compromise\nthe robustness and reliability of segmentation models. To alleviate this\nlimitation, we introduce Siamese-Diffusion, a novel dual-component model\ncomprising Mask-Diffusion and Image-Diffusion. During training, a Noise\nConsistency Loss is introduced between these components to enhance the\nmorphological fidelity of Mask-Diffusion in the parameter space. During\nsampling, only Mask-Diffusion is used, ensuring diversity and scalability.\nComprehensive experiments demonstrate the superiority of our method.\nSiamese-Diffusion boosts SANet's mDice and mIoU by 3.6% and 4.4% on the Polyps,\nwhile UNet improves by 1.52% and 1.64% on the ISIC2018. Code is available at\nGitHub.",
        "url": "http://arxiv.org/abs/2505.06068v1",
        "published_date": "2025-05-09T14:07:27+00:00",
        "updated_date": "2025-05-09T14:07:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kunpeng Qiu",
            "Zhiqiang Gao",
            "Zhiying Zhou",
            "Mingjie Sun",
            "Yongxin Guo"
        ],
        "tldr": "the paper introduces siamese-diffusion, a novel dual-component diffusion model for medical image synthesis and segmentation, using a noise consistency loss to improve morphological fidelity and achieve better segmentation results with limited annotated data.",
        "tldr_zh": "该论文介绍了一种名为siamese-diffusion的新型双组分扩散模型，用于医学图像合成和分割，通过噪声一致性损失来提高形态保真度，并在带注释数据有限的情况下实现更好的分割效果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Towards Better Cephalometric Landmark Detection with Diffusion Data Generation",
        "summary": "Cephalometric landmark detection is essential for orthodontic diagnostics and\ntreatment planning. Nevertheless, the scarcity of samples in data collection\nand the extensive effort required for manual annotation have significantly\nimpeded the availability of diverse datasets. This limitation has restricted\nthe effectiveness of deep learning-based detection methods, particularly those\nbased on large-scale vision models. To address these challenges, we have\ndeveloped an innovative data generation method capable of producing diverse\ncephalometric X-ray images along with corresponding annotations without human\nintervention. To achieve this, our approach initiates by constructing new\ncephalometric landmark annotations using anatomical priors. Then, we employ a\ndiffusion-based generator to create realistic X-ray images that correspond\nclosely with these annotations. To achieve precise control in producing samples\nwith different attributes, we introduce a novel prompt cephalometric X-ray\nimage dataset. This dataset includes real cephalometric X-ray images and\ndetailed medical text prompts describing the images. By leveraging these\ndetailed prompts, our method improves the generation process to control\ndifferent styles and attributes. Facilitated by the large, diverse generated\ndata, we introduce large-scale vision detection models into the cephalometric\nlandmark detection task to improve accuracy. Experimental results demonstrate\nthat training with the generated data substantially enhances the performance.\nCompared to methods without using the generated data, our approach improves the\nSuccess Detection Rate (SDR) by 6.5%, attaining a notable 82.2%. All code and\ndata are available at: https://um-lab.github.io/cepha-generation",
        "url": "http://arxiv.org/abs/2505.06055v1",
        "published_date": "2025-05-09T13:50:27+00:00",
        "updated_date": "2025-05-09T13:50:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dongqian Guo",
            "Wencheng Han",
            "Pang Lyu",
            "Yuxi Zhou",
            "Jianbing Shen"
        ],
        "tldr": "the paper introduces a diffusion-based data generation method for cephalometric x-ray images, utilizing anatomical priors and prompt engineering to improve the performance of landmark detection models, achieving a 6.5% sdr improvement.",
        "tldr_zh": "该论文介绍了一种基于扩散的头影测量x射线图像数据生成方法，利用解剖学先验和提示工程来提高地标检测模型的性能，实现了6.5%的成功检测率提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding",
        "summary": "Understanding visual art requires reasoning across multiple perspectives --\ncultural, historical, and stylistic -- beyond mere object recognition. While\nrecent multimodal large language models (MLLMs) perform well on general image\ncaptioning, they often fail to capture the nuanced interpretations that fine\nart demands. We propose ArtRAG, a novel, training-free framework that combines\nstructured knowledge with retrieval-augmented generation (RAG) for\nmulti-perspective artwork explanation. ArtRAG automatically constructs an Art\nContext Knowledge Graph (ACKG) from domain-specific textual sources, organizing\nentities such as artists, movements, themes, and historical events into a rich,\ninterpretable graph. At inference time, a multi-granular structured retriever\nselects semantically and topologically relevant subgraphs to guide generation.\nThis enables MLLMs to produce contextually grounded, culturally informed art\ndescriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG\noutperforms several heavily trained baselines. Human evaluations further\nconfirm that ArtRAG generates coherent, insightful, and culturally enriched\ninterpretations.",
        "url": "http://arxiv.org/abs/2505.06020v1",
        "published_date": "2025-05-09T13:08:27+00:00",
        "updated_date": "2025-05-09T13:08:27+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Shuai Wang",
            "Ivona Najdenkoska",
            "Hongyi Zhu",
            "Stevan Rudinac",
            "Monika Kackovic",
            "Nachoem Wijnberg",
            "Marcel Worring"
        ],
        "tldr": "the paper introduces artrag, a training-free framework using retrieval-augmented generation with a structured knowledge graph to enhance mllms' ability to generate culturally informed descriptions of visual art.",
        "tldr_zh": "该论文介绍了artrag，一个无需训练的框架，它利用检索增强生成技术和一个结构化知识图谱来增强多模态大型语言模型（mllms）生成具有文化内涵的视觉艺术描述的能力。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PICD: Versatile Perceptual Image Compression with Diffusion Rendering",
        "summary": "Recently, perceptual image compression has achieved significant advancements,\ndelivering high visual quality at low bitrates for natural images. However, for\nscreen content, existing methods often produce noticeable artifacts when\ncompressing text. To tackle this challenge, we propose versatile perceptual\nscreen image compression with diffusion rendering (PICD), a codec that works\nwell for both screen and natural images. More specifically, we propose a\ncompression framework that encodes the text and image separately, and renders\nthem into one image using diffusion model. For this diffusion rendering, we\nintegrate conditional information into diffusion models at three distinct\nlevels: 1). Domain level: We fine-tune the base diffusion model using text\ncontent prompts with screen content. 2). Adaptor level: We develop an efficient\nadaptor to control the diffusion model using compressed image and text as\ninput. 3). Instance level: We apply instance-wise guidance to further enhance\nthe decoding process. Empirically, our PICD surpasses existing perceptual\ncodecs in terms of both text accuracy and perceptual quality. Additionally,\nwithout text conditions, our approach serves effectively as a perceptual codec\nfor natural images.",
        "url": "http://arxiv.org/abs/2505.05853v1",
        "published_date": "2025-05-09T07:45:01+00:00",
        "updated_date": "2025-05-09T07:45:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tongda Xu",
            "Jiahao Li",
            "Bin Li",
            "Yan Wang",
            "Ya-Qin Zhang",
            "Yan Lu"
        ],
        "tldr": "the paper introduces picd, a perceptual image compression codec using diffusion rendering, specifically designed to handle both natural and screen content with improved text accuracy and perceptual quality.",
        "tldr_zh": "该论文介绍了picd，一种使用扩散渲染的感知图像压缩编解码器，专门用于处理自然图像和屏幕内容，并提高了文本准确性和感知质量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "The Moon's Many Faces: A Single Unified Transformer for Multimodal Lunar Reconstruction",
        "summary": "Multimodal learning is an emerging research topic across multiple disciplines\nbut has rarely been applied to planetary science. In this contribution, we\nidentify that reflectance parameter estimation and image-based 3D\nreconstruction of lunar images can be formulated as a multimodal learning\nproblem. We propose a single, unified transformer architecture trained to learn\nshared representations between multiple sources like grayscale images, digital\nelevation models, surface normals, and albedo maps. The architecture supports\nflexible translation from any input modality to any target modality. Predicting\nDEMs and albedo maps from grayscale images simultaneously solves the task of 3D\nreconstruction of planetary surfaces and disentangles photometric parameters\nand height information. Our results demonstrate that our foundation model\nlearns physically plausible relations across these four modalities. Adding more\ninput modalities in the future will enable tasks such as photometric\nnormalization and co-registration.",
        "url": "http://arxiv.org/abs/2505.05644v1",
        "published_date": "2025-05-08T20:55:02+00:00",
        "updated_date": "2025-05-08T20:55:02+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Tom Sander",
            "Moritz Tenthoff",
            "Kay Wohlfarth",
            "Christian Wöhler"
        ],
        "tldr": "this paper introduces a unified transformer architecture for multimodal lunar reconstruction, learning shared representations between grayscale images, dems, surface normals, and albedo maps to enable flexible translation between modalities and improve 3d reconstruction and photometric parameter disentanglement.",
        "tldr_zh": "本文介绍了一种用于多模态月球重建的统一transformer架构，通过学习灰度图像、数字高程模型(dem)、表面法线和反照率图之间的共享表示，实现模态之间的灵活转换，并改进三维重建和光度参数分解。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ReactDance: Progressive-Granular Representation for Long-Term Coherent Reactive Dance Generation",
        "summary": "Reactive dance generation (RDG) produces follower movements conditioned on\nguiding dancer and music while ensuring spatial coordination and temporal\ncoherence. However, existing methods overemphasize global constraints and\noptimization, overlooking local information, such as fine-grained spatial\ninteractions and localized temporal context. Therefore, we present ReactDance,\na novel diffusion-based framework for high-fidelity RDG with long-term\ncoherence and multi-scale controllability. Unlike existing methods that\nstruggle with interaction fidelity, synchronization, and temporal consistency\nin duet synthesis, our approach introduces two key innovations: 1)Group\nResidual Finite Scalar Quantization (GRFSQ), a multi-scale disentangled motion\nrepresentation that captures interaction semantics from coarse body rhythms to\nfine-grained joint dynamics, and 2)Blockwise Local Context (BLC), a sampling\nstrategy eliminating error accumulation in long sequence generation via local\nblock causal masking and periodic positional encoding. Built on the decoupled\nmulti-scale GRFSQ representation, we implement a diffusion model\nwithLayer-Decoupled Classifier-free Guidance (LDCFG), allowing granular control\nover motion semantics across scales. Extensive experiments on standard\nbenchmarks demonstrate that ReactDance surpasses existing methods, achieving\nstate-of-the-art performance.",
        "url": "http://arxiv.org/abs/2505.05589v1",
        "published_date": "2025-05-08T18:42:38+00:00",
        "updated_date": "2025-05-08T18:42:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Jingzhong Lin",
            "Yuanyuan Qi",
            "Xinru Li",
            "Wenxuan Huang",
            "Xiangfeng Xu",
            "Bangyan Li",
            "Xuejiao Wang",
            "Gaoqi He"
        ],
        "tldr": "the paper introduces reactdance, a diffusion-based framework for reactive dance generation that achieves state-of-the-art performance by capturing fine-grained spatial interactions and localized temporal context through novel motion representation and sampling strategies.",
        "tldr_zh": "本文介绍了一种名为reactdance的基于扩散模型的反应式舞蹈生成框架。该框架通过新颖的运动表示和采样策略捕捉细粒度的空间交互和局部时间上下文，从而达到最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Describe Anything in Medical Images",
        "summary": "Localized image captioning has made significant progress with models like the\nDescribe Anything Model (DAM), which can generate detailed region-specific\ndescriptions without explicit region-text supervision. However, such\ncapabilities have yet to be widely applied to specialized domains like medical\nimaging, where diagnostic interpretation relies on subtle regional findings\nrather than global understanding. To mitigate this gap, we propose MedDAM, the\nfirst comprehensive framework leveraging large vision-language models for\nregion-specific captioning in medical images. MedDAM employs medical\nexpert-designed prompts tailored to specific imaging modalities and establishes\na robust evaluation benchmark comprising a customized assessment protocol, data\npre-processing pipeline, and specialized QA template library. This benchmark\nevaluates both MedDAM and other adaptable large vision-language models,\nfocusing on clinical factuality through attribute-level verification tasks,\nthereby circumventing the absence of ground-truth region-caption pairs in\nmedical datasets. Extensive experiments on the VinDr-CXR, LIDC-IDRI, and\nSkinCon datasets demonstrate MedDAM's superiority over leading peers (including\nGPT-4o, Claude 3.7 Sonnet, LLaMA-3.2 Vision, Qwen2.5-VL, GPT-4Rol, and\nOMG-LLaVA) in the task, revealing the importance of region-level semantic\nalignment in medical image understanding and establishing MedDAM as a promising\nfoundation for clinical vision-language integration.",
        "url": "http://arxiv.org/abs/2505.05804v1",
        "published_date": "2025-05-09T05:45:31+00:00",
        "updated_date": "2025-05-09T05:45:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xi Xiao",
            "Yunbei Zhang",
            "Thanh-Huy Nguyen",
            "Ba-Thinh Lam",
            "Janet Wang",
            "Jihun Hamm",
            "Tianyang Wang",
            "Xingjian Li",
            "Xiao Wang",
            "Hao Xu",
            "Tianming Liu",
            "Min Xu"
        ],
        "tldr": "the paper introduces meddam, a novel framework for region-specific captioning of medical images using large vision-language models and a new evaluation benchmark emphasizing clinical factuality.",
        "tldr_zh": "该论文介绍了meddam，一个用于医学图像区域特定描述的新框架，它使用大型视觉语言模型和一个新的评估基准，着重强调临床事实性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications",
        "summary": "The scarcity of high-quality multimodal biomedical data limits the ability to\neffectively fine-tune pretrained Large Language Models (LLMs) for specialized\nbiomedical tasks. To address this challenge, we introduce MINT (Multimodal\nIntegrated kNowledge Transfer), a framework that aligns unimodal large decoder\nmodels with domain-specific decision patterns from multimodal biomedical data\nthrough preference optimization. While MINT supports different optimization\ntechniques, we primarily implement it with the Odds Ratio Preference\nOptimization (ORPO) framework as its backbone. This strategy enables the\naligned LLMs to perform predictive tasks using text-only or image-only inputs\nwhile retaining knowledge learnt from multimodal data. MINT leverages an\nupstream multimodal machine learning (MML) model trained on high-quality\nmultimodal data to transfer domain-specific insights to downstream text-only or\nimage-only LLMs. We demonstrate its effectiveness through two key applications:\n(1) Rare genetic disease prediction from texts, where MINT uses a multimodal\nencoder model, trained on facial photos and clinical notes, to generate a\npreference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despite\nrelying on text input only, the MINT-derived model outperforms models trained\nwith SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissue\ntype classification using cell nucleus images, where MINT uses a\nvision-language foundation model as the preference generator, containing\nknowledge learnt from both text and histopathological images to align\ndownstream image-only models. The resulting MINT-derived model significantly\nimproves the performance of Llama 3.2-Vision-11B-Instruct on tissue type\nclassification. In summary, MINT provides an effective strategy to align\nunimodal LLMs with high-quality multimodal expertise through preference\noptimization.",
        "url": "http://arxiv.org/abs/2505.05736v1",
        "published_date": "2025-05-09T02:28:41+00:00",
        "updated_date": "2025-05-09T02:28:41+00:00",
        "categories": [
            "q-bio.QM",
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Da Wu",
            "Zhanliang Wang",
            "Quan Nguyen",
            "Zhuoran Xu",
            "Kai Wang"
        ],
        "tldr": "the paper introduces mint, a framework that transfers knowledge from multimodal data to unimodal llms via preference optimization, demonstrating improved performance in biomedical tasks like rare disease prediction and tissue type classification.",
        "tldr_zh": "该论文介绍了mint框架，它通过偏好优化将多模态数据中的知识转移到单模态llm，在罕见疾病预测和组织类型分类等生物医学任务中表现出更好的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "Hybrid Learning: A Novel Combination of Self-Supervised and Supervised Learning for MRI Reconstruction without High-Quality Training Reference",
        "summary": "Purpose: Deep learning has demonstrated strong potential for MRI\nreconstruction, but conventional supervised learning methods require\nhigh-quality reference images, which are often unavailable in practice.\nSelf-supervised learning offers an alternative, yet its performance degrades at\nhigh acceleration rates. To overcome these limitations, we propose hybrid\nlearning, a novel two-stage training framework that combines self-supervised\nand supervised learning for robust image reconstruction.\n  Methods: Hybrid learning is implemented in two sequential stages. In the\nfirst stage, self-supervised learning is employed to generate improved images\nfrom noisy or undersampled reference data. These enhanced images then serve as\npseudo-ground truths for the second stage, which uses supervised learning to\nrefine reconstruction performance and support higher acceleration rates. We\nevaluated hybrid learning in two representative applications: (1) accelerated\n0.55T spiral-UTE lung MRI using noisy reference data, and (2) 3D T1 mapping of\nthe brain without access to fully sampled ground truth.\n  Results: For spiral-UTE lung MRI, hybrid learning consistently improved image\nquality over both self-supervised and conventional supervised methods across\ndifferent acceleration rates, as measured by SSIM and NMSE. For 3D T1 mapping,\nhybrid learning achieved superior T1 quantification accuracy across a wide\ndynamic range, outperforming self-supervised learning in all tested conditions.\n  Conclusions: Hybrid learning provides a practical and effective solution for\ntraining deep MRI reconstruction networks when only low-quality or incomplete\nreference data are available. It enables improved image quality and accurate\nquantitative mapping across different applications and field strengths,\nrepresenting a promising technique toward broader clinical deployment of deep\nlearning-based MRI.",
        "url": "http://arxiv.org/abs/2505.05703v1",
        "published_date": "2025-05-09T00:35:14+00:00",
        "updated_date": "2025-05-09T00:35:14+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Haoyang Pei",
            "Ding Xia",
            "Xiang Xu",
            "William Moore",
            "Yao Wang",
            "Hersh Chandarana",
            "Li Feng"
        ],
        "tldr": "this paper proposes a hybrid learning approach, combining self-supervised and supervised learning, for mri reconstruction when high-quality training data is unavailable, demonstrating improved image quality and accuracy.",
        "tldr_zh": "该论文提出了一种混合学习方法，结合自监督和监督学习，用于在缺乏高质量训练数据的情况下进行 mri 重建，展示了改进的图像质量和准确性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "VR-RAG: Open-vocabulary Species Recognition with RAG-Assisted Large Multi-Modal Models",
        "summary": "Open-vocabulary recognition remains a challenging problem in computer vision,\nas it requires identifying objects from an unbounded set of categories. This is\nparticularly relevant in nature, where new species are discovered every year.\nIn this work, we focus on open-vocabulary bird species recognition, where the\ngoal is to classify species based on their descriptions without being\nconstrained to a predefined set of taxonomic categories. Traditional benchmarks\nlike CUB-200-2011 and Birdsnap have been evaluated in a closed-vocabulary\nparadigm, limiting their applicability to real-world scenarios where novel\nspecies continually emerge. We show that the performance of current systems\nwhen evaluated under settings closely aligned with open-vocabulary drops by a\nhuge margin. To address this gap, we propose a scalable framework integrating\nstructured textual knowledge from Wikipedia articles of 11,202 bird species\ndistilled via GPT-4o into concise, discriminative summaries. We propose Visual\nRe-ranking Retrieval-Augmented Generation(VR-RAG), a novel, retrieval-augmented\ngeneration framework that uses visual similarities to rerank the top m\ncandidates retrieved by a set of multimodal vision language encoders. This\nallows for the recognition of unseen taxa. Extensive experiments across five\nestablished classification benchmarks show that our approach is highly\neffective. By integrating VR-RAG, we improve the average performance of\nstate-of-the-art Large Multi-Modal Model QWEN2.5-VL by 15.4% across five\nbenchmarks. Our approach outperforms conventional VLM-based approaches, which\nstruggle with unseen species. By bridging the gap between encyclopedic\nknowledge and visual recognition, our work advances open-vocabulary\nrecognition, offering a flexible, scalable solution for biodiversity monitoring\nand ecological research.",
        "url": "http://arxiv.org/abs/2505.05635v1",
        "published_date": "2025-05-08T20:33:31+00:00",
        "updated_date": "2025-05-08T20:33:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Faizan Farooq Khan",
            "Jun Chen",
            "Youssef Mohamed",
            "Chun-Mei Feng",
            "Mohamed Elhoseiny"
        ],
        "tldr": "this paper introduces vr-rag, a retrieval-augmented generation framework for open-vocabulary bird species recognition, which significantly improves the performance of large multimodal models on unseen species by integrating visual similarity re-ranking and knowledge from wikipedia. the approach offers a scalable solution for biodiversity monitoring and ecological research.",
        "tldr_zh": "该论文介绍了vr-rag，一种用于开放词汇鸟类物种识别的检索增强生成框架，通过整合视觉相似性重排序和维基百科的知识显著提高了大型多模态模型在未见物种上的性能。该方法为生物多样性监测和生态研究提供了一种可扩展的解决方案。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "Document Image Rectification Bases on Self-Adaptive Multitask Fusion",
        "summary": "Deformed document image rectification is essential for real-world document\nunderstanding tasks, such as layout analysis and text recognition. However,\ncurrent multi-task methods -- such as background removal, 3D coordinate\nprediction, and text line segmentation -- often overlook the complementary\nfeatures between tasks and their interactions. To address this gap, we propose\na self-adaptive learnable multi-task fusion rectification network named\nSalmRec. This network incorporates an inter-task feature aggregation module\nthat adaptively improves the perception of geometric distortions, enhances\nfeature complementarity, and reduces negative interference. We also introduce a\ngating mechanism to balance features both within global tasks and between local\ntasks effectively. Experimental results on two English benchmarks (DIR300 and\nDocUNet) and one Chinese benchmark (DocReal) demonstrate that our method\nsignificantly improves rectification performance. Ablation studies further\nhighlight the positive impact of different tasks on dewarping and the\neffectiveness of our proposed module.",
        "url": "http://arxiv.org/abs/2505.06038v1",
        "published_date": "2025-05-09T13:35:25+00:00",
        "updated_date": "2025-05-09T13:35:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Heng Li",
            "Xiangping Wu",
            "Qingcai Chen"
        ],
        "tldr": "the paper presents salmrec, a self-adaptive multi-task learning network for deformed document image rectification, which improves feature complementarity and reduces negative interference between tasks like background removal and text line segmentation.",
        "tldr_zh": "本文提出了一种名为salmrec的自适应多任务学习网络，用于校正变形的文档图像，提高特征互补性并减少诸如背景去除和文本行分割等任务之间的负面干扰。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    },
    {
        "title": "Efficient Quantum Convolutional Neural Networks for Image Classification: Overcoming Hardware Constraints",
        "summary": "While classical convolutional neural networks (CNNs) have revolutionized\nimage classification, the emergence of quantum computing presents new\nopportunities for enhancing neural network architectures. Quantum CNNs (QCNNs)\nleverage quantum mechanical properties and hold potential to outperform\nclassical approaches. However, their implementation on current noisy\nintermediate-scale quantum (NISQ) devices remains challenging due to hardware\nlimitations. In our research, we address this challenge by introducing an\nencoding scheme that significantly reduces the input dimensionality. We\ndemonstrate that a primitive QCNN architecture with 49 qubits is sufficient to\ndirectly process $28\\times 28$ pixel MNIST images, eliminating the need for\nclassical dimensionality reduction pre-processing. Additionally, we propose an\nautomated framework based on expressibility, entanglement, and complexity\ncharacteristics to identify the building blocks of QCNNs, parameterized quantum\ncircuits (PQCs). Our approach demonstrates advantages in accuracy and\nconvergence speed with a similar parameter count compared to both hybrid QCNNs\nand classical CNNs. We validated our experiments on IBM's Heron r2 quantum\nprocessor, achieving $96.08\\%$ classification accuracy, surpassing the\n$71.74\\%$ benchmark of traditional approaches under identical training\nconditions. These results represent one of the first implementations of image\nclassifications on real quantum hardware and validate the potential of quantum\ncomputing in this area.",
        "url": "http://arxiv.org/abs/2505.05957v1",
        "published_date": "2025-05-09T11:09:52+00:00",
        "updated_date": "2025-05-09T11:09:52+00:00",
        "categories": [
            "quant-ph",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Peter Röseler",
            "Oliver Schaudt",
            "Helmut Berg",
            "Christian Bauckhage",
            "Matthias Koch"
        ],
        "tldr": "this paper introduces an efficient quantum convolutional neural network architecture for image classification on nisq devices, achieving competitive accuracy on mnist with minimal classical pre-processing and an automated framework for identifying optimal quantum circuit building blocks.",
        "tldr_zh": "本文介绍了一种高效的量子卷积神经网络架构，用于在nisq设备上进行图像分类，通过最少的经典预处理和用于识别最佳量子电路构建块的自动化框架，在mnist上实现了具有竞争力的准确性。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    },
    {
        "title": "Achieving 3D Attention via Triplet Squeeze and Excitation Block",
        "summary": "The emergence of ConvNeXt and its variants has reaffirmed the conceptual and\nstructural suitability of CNN-based models for vision tasks, re-establishing\nthem as key players in image classification in general, and in facial\nexpression recognition (FER) in particular. In this paper, we propose a new set\nof models that build on these advancements by incorporating a new set of\nattention mechanisms that combines Triplet attention with\nSqueeze-and-Excitation (TripSE) in four different variants. We demonstrate the\neffectiveness of these variants by applying them to the ResNet18, DenseNet and\nConvNext architectures to validate their versatility and impact. Our study\nshows that incorporating a TripSE block in these CNN models boosts their\nperformances, particularly for the ConvNeXt architecture, indicating its\nutility. We evaluate the proposed mechanisms and associated models across four\ndatasets, namely CIFAR100, ImageNet, FER2013 and AffectNet datasets, where\nConvNext with TripSE achieves state-of-the-art results with an accuracy of\n\\textbf{78.27\\%} on the popular FER2013 dataset, a new feat for this dataset.",
        "url": "http://arxiv.org/abs/2505.05943v1",
        "published_date": "2025-05-09T10:36:30+00:00",
        "updated_date": "2025-05-09T10:36:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Maan Alhazmi",
            "Abdulrahman Altahhan"
        ],
        "tldr": "this paper introduces a novel attention mechanism, tripse, combining triplet attention with squeeze-and-excitation, and demonstrates its effectiveness in improving the performance of various cnn architectures, achieving state-of-the-art results on the fer2013 dataset.",
        "tldr_zh": "本文介绍了一种新的注意力机制 tripse，它结合了 triplet 注意力和 squeeze-and-excitation，并展示了它在提高各种 cnn 架构性能方面的有效性，并在 fer2013 数据集上取得了最先进的结果。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4
    },
    {
        "title": "Dual-level Fuzzy Learning with Patch Guidance for Image Ordinal Regression",
        "summary": "Ordinal regression bridges regression and classification by assigning objects\nto ordered classes. While human experts rely on discriminative patch-level\nfeatures for decisions, current approaches are limited by the availability of\nonly image-level ordinal labels, overlooking fine-grained patch-level\ncharacteristics. In this paper, we propose a Dual-level Fuzzy Learning with\nPatch Guidance framework, named DFPG that learns precise feature-based grading\nboundaries from ambiguous ordinal labels, with patch-level supervision.\nSpecifically, we propose patch-labeling and filtering strategies to enable the\nmodel to focus on patch-level features exclusively with only image-level\nordinal labels available. We further design a dual-level fuzzy learning module,\nwhich leverages fuzzy logic to quantitatively capture and handle label\nambiguity from both patch-wise and channel-wise perspectives. Extensive\nexperiments on various image ordinal regression datasets demonstrate the\nsuperiority of our proposed method, further confirming its ability in\ndistinguishing samples from difficult-to-classify categories. The code is\navailable at https://github.com/ZJUMAI/DFPG-ord.",
        "url": "http://arxiv.org/abs/2505.05834v1",
        "published_date": "2025-05-09T07:01:14+00:00",
        "updated_date": "2025-05-09T07:01:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chunlai Dong",
            "Haochao Ying",
            "Qibo Qiu",
            "Jinhong Wang",
            "Danny Chen",
            "Jian Wu"
        ],
        "tldr": "the paper introduces a dual-level fuzzy learning framework (dfpg) for image ordinal regression that leverages patch-level supervision to improve feature learning from ambiguous ordinal labels, demonstrating superior performance on various datasets.",
        "tldr_zh": "该论文提出了一个用于图像序数回归的双层模糊学习框架(dfpg)，利用patch级别的监督来改进从模糊序数标签中学习的特征，并在各种数据集上展示了优越的性能。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    },
    {
        "title": "Predicting Diabetic Macular Edema Treatment Responses Using OCT: Dataset and Methods of APTOS Competition",
        "summary": "Diabetic macular edema (DME) significantly contributes to visual impairment\nin diabetic patients. Treatment responses to intravitreal therapies vary,\nhighlighting the need for patient stratification to predict therapeutic\nbenefits and enable personalized strategies. To our knowledge, this study is\nthe first to explore pre-treatment stratification for predicting DME treatment\nresponses. To advance this research, we organized the 2nd Asia-Pacific\nTele-Ophthalmology Society (APTOS) Big Data Competition in 2021. The\ncompetition focused on improving predictive accuracy for anti-VEGF therapy\nresponses using ophthalmic OCT images. We provided a dataset containing tens of\nthousands of OCT images from 2,000 patients with labels across four sub-tasks.\nThis paper details the competition's structure, dataset, leading methods, and\nevaluation metrics. The competition attracted strong scientific community\nparticipation, with 170 teams initially registering and 41 reaching the final\nround. The top-performing team achieved an AUC of 80.06%, highlighting the\npotential of AI in personalized DME treatment and clinical decision-making.",
        "url": "http://arxiv.org/abs/2505.05768v1",
        "published_date": "2025-05-09T04:12:05+00:00",
        "updated_date": "2025-05-09T04:12:05+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Weiyi Zhang",
            "Peranut Chotcomwongse",
            "Yinwen Li",
            "Pusheng Xu",
            "Ruijie Yao",
            "Lianhao Zhou",
            "Yuxuan Zhou",
            "Hui Feng",
            "Qiping Zhou",
            "Xinyue Wang",
            "Shoujin Huang",
            "Zihao Jin",
            "Florence H. T. Chung",
            "Shujun Wang",
            "Yalin Zheng",
            "Mingguang He",
            "Danli Shi",
            "Paisan Ruamviboonsuk"
        ],
        "tldr": "this paper introduces a dataset of oct images for predicting diabetic macular edema (dme) treatment responses using ai, as part of the aptos competition, with the top-performing team achieving an auc of 80.06%. the competition aimed to improve personalized dme treatment strategies.",
        "tldr_zh": "本文介绍了一个oct图像数据集，用于使用ai预测糖尿病性黄斑水肿（dme）的治疗反应，这是aptos竞赛的一部分，其中表现最佳的团队达到了80.06%的auc。该竞赛旨在改进个性化的dme治疗策略。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 4
    },
    {
        "title": "Equivariant Imaging Biomarkers for Robust Unsupervised Segmentation of Histopathology",
        "summary": "Histopathology evaluation of tissue specimens through microscopic examination\nis essential for accurate disease diagnosis and prognosis. However, traditional\nmanual analysis by specially trained pathologists is time-consuming,\nlabor-intensive, cost-inefficient, and prone to inter-rater variability,\npotentially affecting diagnostic consistency and accuracy. As digital pathology\nimages continue to proliferate, there is a pressing need for automated analysis\nto address these challenges. Recent advancements in artificial\nintelligence-based tools such as machine learning (ML) models, have\nsignificantly enhanced the precision and efficiency of analyzing\nhistopathological slides. However, despite their impressive performance, ML\nmodels are invariant only to translation, lacking invariance to rotation and\nreflection. This limitation restricts their ability to generalize effectively,\nparticularly in histopathology, where images intrinsically lack meaningful\norientation. In this study, we develop robust, equivariant histopathological\nbiomarkers through a novel symmetric convolutional kernel via unsupervised\nsegmentation. The approach is validated using prostate tissue micro-array (TMA)\nimages from 50 patients in the Gleason 2019 Challenge public dataset. The\nbiomarkers extracted through this approach demonstrate enhanced robustness and\ngeneralizability against rotation compared to models using standard convolution\nkernels, holding promise for enhancing the accuracy, consistency, and\nrobustness of ML models in digital pathology. Ultimately, this work aims to\nimprove diagnostic and prognostic capabilities of histopathology beyond\nprostate cancer through equivariant imaging.",
        "url": "http://arxiv.org/abs/2505.05689v1",
        "published_date": "2025-05-08T23:19:21+00:00",
        "updated_date": "2025-05-08T23:19:21+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Fuyao Chen",
            "Yuexi Du",
            "Tal Zeevi",
            "Nicha C. Dvornek",
            "John A. Onofrey"
        ],
        "tldr": "this paper introduces equivariant convolutional kernels for unsupervised segmentation of histopathology images, demonstrating improved robustness to rotation and generalizability compared to standard convolutional methods, with a focus on prostate cancer diagnosis.",
        "tldr_zh": "本文提出了一种用于组织病理图像无监督分割的等变卷积核，与标准卷积方法相比，该方法表现出更好的旋转鲁棒性和泛化能力，主要应用于前列腺癌的诊断。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 4
    }
]