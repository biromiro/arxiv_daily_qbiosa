[
    {
        "title": "Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields",
        "summary": "Flow matching casts sample generation as learning a continuous-time velocity\nfield that transports noise to data. Existing flow matching networks typically\npredict each point's velocity independently, considering only its location and\ntime along its flow trajectory, and ignoring neighboring points. However, this\npointwise approach may overlook correlations between points along the\ngeneration trajectory that could enhance velocity predictions, thereby\nimproving downstream generation quality. To address this, we propose Graph Flow\nMatching (GFM), a lightweight enhancement that decomposes the learned velocity\ninto a reaction term -- any standard flow matching network -- and a diffusion\nterm that aggregates neighbor information via a graph neural module. This\nreaction-diffusion formulation retains the scalability of deep flow models\nwhile enriching velocity predictions with local context, all at minimal\nadditional computational cost. Operating in the latent space of a pretrained\nvariational autoencoder, GFM consistently improves Fr\\'echet Inception Distance\n(FID) and recall across five image generation benchmarks (LSUN Church, LSUN\nBedroom, FFHQ, AFHQ-Cat, and CelebA-HQ at $256\\times256$), demonstrating its\neffectiveness as a modular enhancement to existing flow matching architectures.",
        "url": "http://arxiv.org/abs/2505.24434v1",
        "published_date": "2025-05-30T10:17:50+00:00",
        "updated_date": "2025-05-30T10:17:50+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Md Shahriar Rahim Siddiqui",
            "Moshe Eliasof",
            "Eldad Haber"
        ],
        "tldr": "The paper introduces Graph Flow Matching (GFM) to improve image generation by incorporating neighbor information into flow matching networks, demonstrating improved FID and recall scores across several benchmarks.",
        "tldr_zh": "该论文介绍了图流匹配（GFM），通过将邻域信息融入流匹配网络中来改善图像生成，并在多个基准测试中展示了改进的FID和召回率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Unleashing High-Quality Image Generation in Diffusion Sampling Using Second-Order Levenberg-Marquardt-Langevin",
        "summary": "The diffusion models (DMs) have demonstrated the remarkable capability of\ngenerating images via learning the noised score function of data distribution.\nCurrent DM sampling techniques typically rely on first-order Langevin dynamics\nat each noise level, with efforts concentrated on refining inter-level\ndenoising strategies. While leveraging additional second-order Hessian geometry\nto enhance the sampling quality of Langevin is a common practice in Markov\nchain Monte Carlo (MCMC), the naive attempts to utilize Hessian geometry in\nhigh-dimensional DMs lead to quadratic-complexity computational costs,\nrendering them non-scalable. In this work, we introduce a novel\nLevenberg-Marquardt-Langevin (LML) method that approximates the diffusion\nHessian geometry in a training-free manner, drawing inspiration from the\ncelebrated Levenberg-Marquardt optimization algorithm. Our approach introduces\ntwo key innovations: (1) A low-rank approximation of the diffusion Hessian,\nleveraging the DMs' inherent structure and circumventing explicit\nquadratic-complexity computations; (2) A damping mechanism to stabilize the\napproximated Hessian. This LML approximated Hessian geometry enables the\ndiffusion sampling to execute more accurate steps and improve the image\ngeneration quality. We further conduct a theoretical analysis to substantiate\nthe approximation error bound of low-rank approximation and the convergence\nproperty of the damping mechanism. Extensive experiments across multiple\npretrained DMs validate that the LML method significantly improves image\ngeneration quality, with negligible computational overhead.",
        "url": "http://arxiv.org/abs/2505.24222v1",
        "published_date": "2025-05-30T05:21:44+00:00",
        "updated_date": "2025-05-30T05:21:44+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Fangyikang Wang",
            "Hubery Yin",
            "Lei Qian",
            "Yinan Li",
            "Shaobin Zhuang",
            "Huminhao Zhu",
            "Yilin Zhang",
            "Yanlong Tang",
            "Chao Zhang",
            "Hanbin Zhao",
            "Hui Qian",
            "Chen Li"
        ],
        "tldr": "This paper introduces a Levenberg-Marquardt-Langevin (LML) method to improve diffusion model sampling by approximating the Hessian geometry with low-rank approximation and damping mechanism, leading to improved image generation quality with negligible overhead.",
        "tldr_zh": "本文提出了一种 Levenberg-Marquardt-Langevin (LML) 方法，通过低秩逼近和阻尼机制来近似 Hessian 几何，从而改进扩散模型采样，以可忽略的开销提高图像生成质量。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ComposeAnything: Composite Object Priors for Text-to-Image Generation",
        "summary": "Generating images from text involving complex and novel object arrangements\nremains a significant challenge for current text-to-image (T2I) models.\nAlthough prior layout-based methods improve object arrangements using spatial\nconstraints with 2D layouts, they often struggle to capture 3D positioning and\nsacrifice quality and coherence. In this work, we introduce ComposeAnything, a\nnovel framework for improving compositional image generation without retraining\nexisting T2I models. Our approach first leverages the chain-of-thought\nreasoning abilities of LLMs to produce 2.5D semantic layouts from text,\nconsisting of 2D object bounding boxes enriched with depth information and\ndetailed captions. Based on this layout, we generate a spatial and depth aware\ncoarse composite of objects that captures the intended composition, serving as\na strong and interpretable prior that replaces stochastic noise initialization\nin diffusion-based T2I models. This prior guides the denoising process through\nobject prior reinforcement and spatial-controlled denoising, enabling seamless\ngeneration of compositional objects and coherent backgrounds, while allowing\nrefinement of inaccurate priors. ComposeAnything outperforms state-of-the-art\nmethods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D\nspatial arrangements, high object counts, and surreal compositions. Human\nevaluations further demonstrate that our model generates high-quality images\nwith compositions that faithfully reflect the text.",
        "url": "http://arxiv.org/abs/2505.24086v1",
        "published_date": "2025-05-30T00:13:36+00:00",
        "updated_date": "2025-05-30T00:13:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeeshan Khan",
            "Shizhe Chen",
            "Cordelia Schmid"
        ],
        "tldr": "ComposeAnything improves text-to-image generation by using LLMs to create 2.5D semantic layouts and object priors, guiding diffusion models for better compositional image generation.",
        "tldr_zh": "ComposeAnything 通过使用LLM创建2.5D语义布局和对象先验来改进文本到图像的生成，从而引导扩散模型以实现更好的组合图像生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]