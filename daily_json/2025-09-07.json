[
    {
        "title": "FAVAE-Effective Frequency Aware Latent Tokenizer",
        "summary": "Latent generative models have shown remarkable progress in high-fidelity\nimage synthesis, typically using a two-stage training process that involves\ncompressing images into latent embeddings via learned tokenizers in the first\nstage. The quality of generation strongly depends on how expressive and\nwell-optimized these latent embeddings are. While various methods have been\nproposed to learn effective latent representations, the reconstructed images\noften lack realism, particularly in textured regions with sharp transitions,\ndue to loss of fine details governed by high frequencies. We conduct a detailed\nfrequency decomposition of existing state-of-the-art (SOTA) latent tokenizers\nand show that conventional objectives inherently prioritize low-frequency\nreconstruction, often at the expense of high-frequency fidelity. Our analysis\nreveals these latent tokenizers exhibit a bias toward low-frequency\ninformation, when jointly optimized, leading to over-smoothed outputs and\nvisual artifacts that diminish perceptual quality. To address this, we propose\na wavelet-based, frequency-aware variational autoencoder (FA-VAE) framework\nthat explicitly decouples the optimization of low- and high-frequency\ncomponents. This decoupling enables improved reconstruction of fine textures\nwhile preserving global structure. Our approach bridges the fidelity gap in\ncurrent latent tokenizers and emphasizes the importance of frequency-aware\noptimization for realistic image representation, with broader implications for\napplications in content creation, neural rendering, and medical imaging.",
        "url": "http://arxiv.org/abs/2509.05441v1",
        "published_date": "2025-09-05T18:49:08+00:00",
        "updated_date": "2025-09-05T18:49:08+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Tejaswini Medi",
            "Hsien-Yi Wang",
            "Arianna Rampini",
            "Margret Keuper"
        ],
        "tldr": "The paper introduces a frequency-aware variational autoencoder (FA-VAE) to improve image reconstruction fidelity in latent generative models by decoupling the optimization of low- and high-frequency components, addressing the bias towards low-frequency information in existing latent tokenizers.",
        "tldr_zh": "该论文介绍了一种频率感知变分自编码器（FA-VAE），通过解耦低频和高频分量的优化，来提高潜在生成模型中的图像重建保真度，从而解决了现有潜在标记器中对低频信息的偏见。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EditIDv2: Editable ID Customization with Data-Lubricated ID Feature Integration for Text-to-Image Generation",
        "summary": "We propose EditIDv2, a tuning-free solution specifically designed for\nhigh-complexity narrative scenes and long text inputs. Existing character\nediting methods perform well under simple prompts, but often suffer from\ndegraded editing capabilities, semantic understanding biases, and identity\nconsistency breakdowns when faced with long text narratives containing multiple\nsemantic layers, temporal logic, and complex contextual relationships. In\nEditID, we analyzed the impact of the ID integration module on editability. In\nEditIDv2, we further explore and address the influence of the ID feature\nintegration module. The core of EditIDv2 is to discuss the issue of editability\ninjection under minimal data lubrication. Through a sophisticated decomposition\nof PerceiverAttention, the introduction of ID loss and joint dynamic training\nwith the diffusion model, as well as an offline fusion strategy for the\nintegration module, we achieve deep, multi-level semantic editing while\nmaintaining identity consistency in complex narrative environments using only a\nsmall amount of data lubrication. This meets the demands of long prompts and\nhigh-quality image generation, and achieves excellent results in the IBench\nevaluation.",
        "url": "http://arxiv.org/abs/2509.05659v1",
        "published_date": "2025-09-06T09:29:48+00:00",
        "updated_date": "2025-09-06T09:29:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guandong Li",
            "Zhaobin Chu"
        ],
        "tldr": "EditIDv2 addresses the problem of maintaining identity consistency and editability in text-to-image generation with long and complex prompts, using a data-lubricated approach to ID feature integration. It focuses on improving editing capabilities in complex narrative scenes.",
        "tldr_zh": "EditIDv2提出了一种数据润滑的ID特征融合方法，解决了长文本和复杂叙事场景下文本到图像生成中身份一致性和可编辑性维护的问题。它侧重于提高复杂叙事场景中的编辑能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]