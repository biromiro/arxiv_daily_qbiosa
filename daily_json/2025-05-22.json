[
    {
        "title": "MMaDA: Multimodal Large Diffusion Language Models",
        "summary": "We introduce MMaDA, a novel class of multimodal diffusion foundation models\ndesigned to achieve superior performance across diverse domains such as textual\nreasoning, multimodal understanding, and text-to-image generation. The approach\nis distinguished by three key innovations: (i) MMaDA adopts a unified diffusion\narchitecture with a shared probabilistic formulation and a modality-agnostic\ndesign, eliminating the need for modality-specific components. This\narchitecture ensures seamless integration and processing across different data\ntypes. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning\nstrategy that curates a unified CoT format across modalities. By aligning\nreasoning processes between textual and visual domains, this strategy\nfacilitates cold-start training for the final reinforcement learning (RL)\nstage, thereby enhancing the model's ability to handle complex tasks from the\noutset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm\nspecifically tailored for diffusion foundation models. Utilizing diversified\nreward modeling, UniGRPO unifies post-training across both reasoning and\ngeneration tasks, ensuring consistent performance improvements. Experimental\nresults demonstrate that MMaDA-8B exhibits strong generalization capabilities\nas a unified multimodal foundation model. It surpasses powerful models like\nLLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in\nmultimodal understanding, and excels over SDXL and Janus in text-to-image\ngeneration. These achievements highlight MMaDA's effectiveness in bridging the\ngap between pretraining and post-training within unified diffusion\narchitectures, providing a comprehensive framework for future research and\ndevelopment. We open-source our code and trained models at:\nhttps://github.com/Gen-Verse/MMaDA",
        "url": "http://arxiv.org/abs/2505.15809v1",
        "published_date": "2025-05-21T17:59:05+00:00",
        "updated_date": "2025-05-21T17:59:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ling Yang",
            "Ye Tian",
            "Bowen Li",
            "Xinchen Zhang",
            "Ke Shen",
            "Yunhai Tong",
            "Mengdi Wang"
        ],
        "tldr": "MMaDA introduces a unified multimodal diffusion foundation model with a shared architecture and training strategy for textual reasoning, multimodal understanding, and text-to-image generation, outperforming state-of-the-art models in each domain.",
        "tldr_zh": "MMaDA 提出了一种统一的多模态扩散基础模型，具有共享架构和训练策略，用于文本推理、多模态理解和文本到图像生成，并在每个领域都优于最先进的模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Harnessing Caption Detailness for Data-Efficient Text-to-Image Generation",
        "summary": "Training text-to-image (T2I) models with detailed captions can significantly\nimprove their generation quality. Existing methods often rely on simplistic\nmetrics like caption length to represent the detailness of the caption in the\nT2I training set. In this paper, we propose a new metric to estimate caption\ndetailness based on two aspects: image coverage rate (ICR), which evaluates\nwhether the caption covers all regions/objects in the image, and average object\ndetailness (AOD), which quantifies the detailness of each object's description.\nThrough experiments on the COCO dataset using ShareGPT4V captions, we\ndemonstrate that T2I models trained on high-ICR and -AOD captions achieve\nsuperior performance on DPG and other benchmarks. Notably, our metric enables\nmore effective data selection-training on only 20% of full data surpasses both\nfull-dataset training and length-based selection method, improving alignment\nand reconstruction ability. These findings highlight the critical role of\ndetail-aware metrics over length-based heuristics in caption selection for T2I\ntasks.",
        "url": "http://arxiv.org/abs/2505.15172v1",
        "published_date": "2025-05-21T06:42:17+00:00",
        "updated_date": "2025-05-21T06:42:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinran Wang",
            "Muxi Diao",
            "Yuanzhi Liu",
            "Chunyu Wang",
            "Kongming Liang",
            "Zhanyu Ma",
            "Jun Guo"
        ],
        "tldr": "This paper introduces a novel metric (ICR and AOD) for assessing caption detailness in text-to-image datasets, demonstrating that training on data selected using this metric significantly improves generation quality and data efficiency compared to length-based selection.",
        "tldr_zh": "本文提出了一种新的度量标准（ICR 和 AOD）来评估文本到图像数据集中标题的详细程度，表明与基于长度的选择相比，使用此度量标准选择的数据进行训练可显著提高生成质量和数据效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scaling Diffusion Transformers Efficiently via $μ$P",
        "summary": "Diffusion Transformers have emerged as the foundation for vision generative\nmodels, but their scalability is limited by the high cost of hyperparameter\n(HP) tuning at large scales. Recently, Maximal Update Parametrization ($\\mu$P)\nwas proposed for vanilla Transformers, which enables stable HP transfer from\nsmall to large language models, and dramatically reduces tuning costs. However,\nit remains unclear whether $\\mu$P of vanilla Transformers extends to diffusion\nTransformers, which differ architecturally and objectively. In this work, we\ngeneralize standard $\\mu$P to diffusion Transformers and validate its\neffectiveness through large-scale experiments. First, we rigorously prove that\n$\\mu$P of mainstream diffusion Transformers, including DiT, U-ViT,\nPixArt-$\\alpha$, and MMDiT, aligns with that of the vanilla Transformer,\nenabling the direct application of existing $\\mu$P methodologies. Leveraging\nthis result, we systematically demonstrate that DiT-$\\mu$P enjoys robust HP\ntransferability. Notably, DiT-XL-2-$\\mu$P with transferred learning rate\nachieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we\nvalidate the effectiveness of $\\mu$P on text-to-image generation by scaling\nPixArt-$\\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases,\nmodels under $\\mu$P outperform their respective baselines while requiring small\ntuning cost, only 5.5% of one training run for PixArt-$\\alpha$ and 3% of\nconsumption by human experts for MMDiT-18B. These results establish $\\mu$P as a\nprincipled and efficient framework for scaling diffusion Transformers.",
        "url": "http://arxiv.org/abs/2505.15270v1",
        "published_date": "2025-05-21T08:49:03+00:00",
        "updated_date": "2025-05-21T08:49:03+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Chenyu Zheng",
            "Xinyu Zhang",
            "Rongzhen Wang",
            "Wei Huang",
            "Zhi Tian",
            "Weilin Huang",
            "Jun Zhu",
            "Chongxuan Li"
        ]
    }
]