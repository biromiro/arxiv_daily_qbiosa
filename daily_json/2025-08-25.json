[
    {
        "title": "T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image Generation",
        "summary": "We propose T2I-ReasonBench, a benchmark evaluating reasoning capabilities of\ntext-to-image (T2I) models. It consists of four dimensions: Idiom\nInterpretation, Textual Image Design, Entity-Reasoning and\nScientific-Reasoning. We propose a two-stage evaluation protocol to assess the\nreasoning accuracy and image quality. We benchmark various T2I generation\nmodels, and provide comprehensive analysis on their performances.",
        "url": "http://arxiv.org/abs/2508.17472v1",
        "published_date": "2025-08-24T17:59:38+00:00",
        "updated_date": "2025-08-24T17:59:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaiyue Sun",
            "Rongyao Fang",
            "Chengqi Duan",
            "Xian Liu",
            "Xihui Liu"
        ],
        "tldr": "The paper introduces T2I-ReasonBench, a new benchmark to evaluate the reasoning capabilities of text-to-image models across four dimensions, and benchmarks existing models using a two-stage evaluation protocol.",
        "tldr_zh": "该论文介绍了T2I-ReasonBench，一个用于评估文本到图像模型推理能力的新基准，它包含四个维度，并使用两阶段评估协议对现有模型进行了基准测试。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "An LLM-LVLM Driven Agent for Iterative and Fine-Grained Image Editing",
        "summary": "Despite the remarkable capabilities of text-to-image (T2I) generation models,\nreal-world applications often demand fine-grained, iterative image editing that\nexisting methods struggle to provide. Key challenges include granular\ninstruction understanding, robust context preservation during modifications,\nand the lack of intelligent feedback mechanisms for iterative refinement. This\npaper introduces RefineEdit-Agent, a novel, training-free intelligent agent\nframework designed to address these limitations by enabling complex, iterative,\nand context-aware image editing. RefineEdit-Agent leverages the powerful\nplanning capabilities of Large Language Models (LLMs) and the advanced visual\nunderstanding and evaluation prowess of Vision-Language Large Models (LVLMs)\nwithin a closed-loop system. Our framework comprises an LVLM-driven instruction\nparser and scene understanding module, a multi-level LLM-driven editing planner\nfor goal decomposition, tool selection, and sequence generation, an iterative\nimage editing module, and a crucial LVLM-driven feedback and evaluation loop.\nTo rigorously evaluate RefineEdit-Agent, we propose LongBench-T2I-Edit, a new\nbenchmark featuring 500 initial images with complex, multi-turn editing\ninstructions across nine visual dimensions. Extensive experiments demonstrate\nthat RefineEdit-Agent significantly outperforms state-of-the-art baselines,\nachieving an average score of 3.67 on LongBench-T2I-Edit, compared to 2.29 for\nDirect Re-Prompting, 2.91 for InstructPix2Pix, 3.16 for GLIGEN-based Edit, and\n3.39 for ControlNet-XL. Ablation studies, human evaluations, and analyses of\niterative refinement, backbone choices, tool usage, and robustness to\ninstruction complexity further validate the efficacy of our agentic design in\ndelivering superior edit fidelity and context preservation.",
        "url": "http://arxiv.org/abs/2508.17435v1",
        "published_date": "2025-08-24T16:28:18+00:00",
        "updated_date": "2025-08-24T16:28:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zihan Liang",
            "Jiahao Sun",
            "Haoran Ma"
        ],
        "tldr": "The paper introduces RefineEdit-Agent, a novel LLM-LVLM driven agent for iterative and fine-grained image editing, outperforming SOTA methods on a new benchmark, LongBench-T2I-Edit.",
        "tldr_zh": "本文介绍了一种名为 RefineEdit-Agent 的新型 LLM-LVLM 驱动的代理，用于迭代和细粒度的图像编辑，并在名为 LongBench-T2I-Edit 的新基准测试中优于 SOTA 方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MoCo: Motion-Consistent Human Video Generation via Structure-Appearance Decoupling",
        "summary": "Generating human videos with consistent motion from text prompts remains a\nsignificant challenge, particularly for whole-body or long-range motion.\nExisting video generation models prioritize appearance fidelity, resulting in\nunrealistic or physically implausible human movements with poor structural\ncoherence. Additionally, most existing human video datasets primarily focus on\nfacial or upper-body motions, or consist of vertically oriented dance videos,\nlimiting the scope of corresponding generation methods to simple movements. To\novercome these challenges, we propose MoCo, which decouples the process of\nhuman video generation into two components: structure generation and appearance\ngeneration. Specifically, our method first employs an efficient 3D structure\ngenerator to produce a human motion sequence from a text prompt. The remaining\nvideo appearance is then synthesized under the guidance of the generated\nstructural sequence. To improve fine-grained control over sparse human\nstructures, we introduce Human-Aware Dynamic Control modules and integrate\ndense tracking constraints during training. Furthermore, recognizing the\nlimitations of existing datasets, we construct a large-scale whole-body human\nvideo dataset featuring complex and diverse motions. Extensive experiments\ndemonstrate that MoCo outperforms existing approaches in generating realistic\nand structurally coherent human videos.",
        "url": "http://arxiv.org/abs/2508.17404v1",
        "published_date": "2025-08-24T15:20:24+00:00",
        "updated_date": "2025-08-24T15:20:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoyu Wang",
            "Hao Tang",
            "Donglin Di",
            "Zhilu Zhang",
            "Wangmeng Zuo",
            "Feng Gao",
            "Siwei Ma",
            "Shiliang Zhang"
        ],
        "tldr": "The paper introduces MoCo, a method for generating human videos from text prompts by decoupling structure and appearance generation, using a 3D structure generator and a new large-scale whole-body motion dataset.",
        "tldr_zh": "该论文介绍了MoCo，一种通过解耦结构和外观生成，利用3D结构生成器和一个新的大规模全身运动数据集，从文本提示生成人体视频的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ShaLa: Multimodal Shared Latent Space Modelling",
        "summary": "This paper presents a novel generative framework for learning shared latent\nrepresentations across multimodal data. Many advanced multimodal methods focus\non capturing all combinations of modality-specific details across inputs, which\ncan inadvertently obscure the high-level semantic concepts that are shared\nacross modalities. Notably, Multimodal VAEs with low-dimensional latent\nvariables are designed to capture shared representations, enabling various\ntasks such as joint multimodal synthesis and cross-modal inference. However,\nmultimodal VAEs often struggle to design expressive joint variational\nposteriors and suffer from low-quality synthesis. In this work, ShaLa addresses\nthese challenges by integrating a novel architectural inference model and a\nsecond-stage expressive diffusion prior, which not only facilitates effective\ninference of shared latent representation but also significantly improves the\nquality of downstream multimodal synthesis. We validate ShaLa extensively\nacross multiple benchmarks, demonstrating superior coherence and synthesis\nquality compared to state-of-the-art multimodal VAEs. Furthermore, ShaLa scales\nto many more modalities while prior multimodal VAEs have fallen short in\ncapturing the increasing complexity of the shared latent space.",
        "url": "http://arxiv.org/abs/2508.17376v1",
        "published_date": "2025-08-24T14:16:22+00:00",
        "updated_date": "2025-08-24T14:16:22+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Jiali Cui",
            "Yan-Ying Chen",
            "Yanxia Zhang",
            "Matthew Klenk"
        ],
        "tldr": "The paper introduces ShaLa, a novel generative framework that uses a new architectural inference model and diffusion prior to learn shared latent representations across multiple modalities, improving both inference and synthesis quality compared to multimodal VAEs.",
        "tldr_zh": "本文介绍了一种名为ShaLa的新型生成框架，该框架使用新的架构推理模型和扩散先验来学习跨多种模态的共享潜在表示，与多模态VAE相比，提高了推理和合成质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Condition Weaving Meets Expert Modulation: Towards Universal and Controllable Image Generation",
        "summary": "The image-to-image generation task aims to produce controllable images by\nleveraging conditional inputs and prompt instructions. However, existing\nmethods often train separate control branches for each type of condition,\nleading to redundant model structures and inefficient use of computational\nresources. To address this, we propose a Unified image-to-image Generation\n(UniGen) framework that supports diverse conditional inputs while enhancing\ngeneration efficiency and expressiveness. Specifically, to tackle the widely\nexisting parameter redundancy and computational inefficiency in controllable\nconditional generation architectures, we propose the Condition Modulated Expert\n(CoMoE) module. This module aggregates semantically similar patch features and\nassigns them to dedicated expert modules for visual representation and\nconditional modeling. By enabling independent modeling of foreground features\nunder different conditions, CoMoE effectively mitigates feature entanglement\nand redundant computation in multi-condition scenarios. Furthermore, to bridge\nthe information gap between the backbone and control branches, we propose\nWeaveNet, a dynamic, snake-like connection mechanism that enables effective\ninteraction between global text-level control from the backbone and\nfine-grained control from conditional branches. Extensive experiments on the\nSubjects-200K and MultiGen-20M datasets across various conditional image\ngeneration tasks demonstrate that our method consistently achieves\nstate-of-the-art performance, validating its advantages in both versatility and\neffectiveness. The code has been uploaded to\nhttps://github.com/gavin-gqzhang/UniGen.",
        "url": "http://arxiv.org/abs/2508.17364v1",
        "published_date": "2025-08-24T13:47:10+00:00",
        "updated_date": "2025-08-24T13:47:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Guoqing Zhang",
            "Xingtong Ge",
            "Lu Shi",
            "Xin Zhang",
            "Muqing Xue",
            "Wanru Xu",
            "Yigang Cen"
        ],
        "tldr": "The paper introduces UniGen, a new image-to-image generation framework with a Condition Modulated Expert (CoMoE) module and WeaveNet to improve efficiency and control in multi-conditional image generation, achieving state-of-the-art results.",
        "tldr_zh": "本文介绍了一种新的图像到图像生成框架UniGen，它具有条件调制专家（CoMoE）模块和WeaveNet，可以提高多条件图像生成的效率和控制能力，并取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DanceEditor: Towards Iterative Editable Music-driven Dance Generation with Open-Vocabulary Descriptions",
        "summary": "Generating coherent and diverse human dances from music signals has gained\ntremendous progress in animating virtual avatars. While existing methods\nsupport direct dance synthesis, they fail to recognize that enabling users to\nedit dance movements is far more practical in real-world choreography\nscenarios. Moreover, the lack of high-quality dance datasets incorporating\niterative editing also limits addressing this challenge. To achieve this goal,\nwe first construct DanceRemix, a large-scale multi-turn editable dance dataset\ncomprising the prompt featuring over 25.3M dance frames and 84.5K pairs. In\naddition, we propose a novel framework for iterative and editable dance\ngeneration coherently aligned with given music signals, namely DanceEditor.\nConsidering the dance motion should be both musical rhythmic and enable\niterative editing by user descriptions, our framework is built upon a\nprediction-then-editing paradigm unifying multi-modal conditions. At the\ninitial prediction stage, our framework improves the authority of generated\nresults by directly modeling dance movements from tailored, aligned music.\nMoreover, at the subsequent iterative editing stages, we incorporate text\ndescriptions as conditioning information to draw the editable results through a\nspecifically designed Cross-modality Editing Module (CEM). Specifically, CEM\nadaptively integrates the initial prediction with music and text prompts as\ntemporal motion cues to guide the synthesized sequences. Thereby, the results\ndisplay music harmonics while preserving fine-grained semantic alignment with\ntext descriptions. Extensive experiments demonstrate that our method\noutperforms the state-of-the-art models on our newly collected DanceRemix\ndataset. Code is available at https://lzvsdy.github.io/DanceEditor/.",
        "url": "http://arxiv.org/abs/2508.17342v1",
        "published_date": "2025-08-24T12:53:09+00:00",
        "updated_date": "2025-08-24T12:53:09+00:00",
        "categories": [
            "cs.GR",
            "cs.CV",
            "cs.MM",
            "cs.SD"
        ],
        "authors": [
            "Hengyuan Zhang",
            "Zhe Li",
            "Xingqun Qi",
            "Mengze Li",
            "Muyi Sun",
            "Man Zhang",
            "Sirui Han"
        ],
        "tldr": "The paper introduces DanceEditor, a framework for iteratively editing music-driven dance generation using user-defined text descriptions, along with a new large-scale dataset, DanceRemix, for training and evaluation.",
        "tldr_zh": "该论文介绍了 DanceEditor，一个利用用户定义的文本描述迭代编辑音乐驱动舞蹈生成的框架，以及一个新的大型数据集 DanceRemix，用于训练和评估。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PosBridge: Multi-View Positional Embedding Transplant for Identity-Aware Image Editing",
        "summary": "Localized subject-driven image editing aims to seamlessly integrate\nuser-specified objects into target scenes. As generative models continue to\nscale, training becomes increasingly costly in terms of memory and computation,\nhighlighting the need for training-free and scalable editing frameworks.To this\nend, we propose PosBridge an efficient and flexible framework for inserting\ncustom objects. A key component of our method is positional embedding\ntransplant, which guides the diffusion model to faithfully replicate the\nstructural characteristics of reference objects.Meanwhile, we introduce the\nCorner Centered Layout, which concatenates reference images and the background\nimage as input to the FLUX.1-Fill model. During progressive denoising,\npositional embedding transplant is applied to guide the noise distribution in\nthe target region toward that of the reference object. In this way, Corner\nCentered Layout effectively directs the FLUX.1-Fill model to synthesize\nidentity-consistent content at the desired location. Extensive experiments\ndemonstrate that PosBridge outperforms mainstream baselines in structural\nconsistency, appearance fidelity, and computational efficiency, showcasing its\npractical value and potential for broad adoption.",
        "url": "http://arxiv.org/abs/2508.17302v1",
        "published_date": "2025-08-24T11:09:01+00:00",
        "updated_date": "2025-08-24T11:09:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peilin Xiong",
            "Junwen Chen",
            "Honghui Yuan",
            "Keiji Yanai"
        ],
        "tldr": "The paper introduces PosBridge, a training-free and efficient framework for subject-driven image editing using positional embedding transplant to insert custom objects into scenes while maintaining identity and structural consistency.",
        "tldr_zh": "该论文介绍了PosBridge，一个无需训练且高效的主体驱动图像编辑框架，它使用位置嵌入移植技术将自定义对象插入场景，同时保持身份和结构一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MMCIG: Multimodal Cover Image Generation for Text-only Documents and Its Dataset Construction via Pseudo-labeling",
        "summary": "In this study, we introduce a novel cover image generation task that produces\nboth a concise summary and a visually corresponding image from a given\ntext-only document. Because no existing datasets are available for this task,\nwe propose a multimodal pseudo-labeling method to construct high-quality\ndatasets at low cost. We first collect documents that contain multiple images\nwith their captions, and their summaries by excluding factually inconsistent\ninstances. Our approach selects one image from the multiple images accompanying\nthe documents. Using the gold summary, we independently rank both the images\nand their captions. Then, we annotate a pseudo-label for an image when both the\nimage and its corresponding caption are ranked first in their respective\nrankings. Finally, we remove documents that contain direct image references\nwithin texts. Experimental results demonstrate that the proposed multimodal\npseudo-labeling method constructs more precise datasets and generates higher\nquality images than text- and image-only pseudo-labeling methods, which\nconsider captions and images separately. We release our code at:\nhttps://github.com/HyeyeeonKim/MMCIG",
        "url": "http://arxiv.org/abs/2508.17199v1",
        "published_date": "2025-08-24T03:24:35+00:00",
        "updated_date": "2025-08-24T03:24:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hyeyeon Kim",
            "Sungwoo Han",
            "Jingun Kwon",
            "Hidetaka Kamigaito",
            "Manabu Okumura"
        ],
        "tldr": "The paper introduces a novel multimodal cover image generation task for text-only documents by creating a dataset using a multimodal pseudo-labeling approach, outperforming unimodal baselines.",
        "tldr_zh": "该论文介绍了一种新的多模态封面图像生成任务，用于仅包含文本的文档。该方法通过多模态伪标签方法创建数据集，并优于单模态基线。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]