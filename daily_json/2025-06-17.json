[
    {
        "title": "Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model",
        "summary": "The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\nexploration of integrating text, vision, and speech modalities to support more\nflexible multimodal interaction. Existing LMMs typically concatenate\nrepresentation of modalities along the sequence dimension and feed them into a\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\nstraightforward for modality integration, it often relies heavily on\nlarge-scale data to learn modality alignments. In this paper, we aim to model\nthe relationships between modalities more purposefully, thereby achieving more\nefficient and flexible modality alignments. To this end, we propose\nStream-Omni, a large language-vision-speech model with efficient modality\nalignments, which can simultaneously support interactions under various\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\nvision and speech to the text based on their relationships. For vision that is\nsemantically complementary to text, Stream-Omni uses sequence-dimension\nconcatenation to achieve vision-text alignment. For speech that is semantically\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\nmodality alignments with less data (especially speech), enabling the transfer\nof text capabilities to other modalities. Experiments on various benchmarks\ndemonstrate that Stream-Omni achieves strong performance on visual\nunderstanding, speech interaction, and vision-grounded speech interaction\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\nprovide intermediate text outputs (such as ASR transcriptions and model\nresponses) during speech interaction, offering users a comprehensive multimodal\nexperience.",
        "url": "http://arxiv.org/abs/2506.13642v1",
        "published_date": "2025-06-16T16:06:45+00:00",
        "updated_date": "2025-06-16T16:06:45+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.SD",
            "eess.AS"
        ],
        "authors": [
            "Shaolei Zhang",
            "Shoutao Guo",
            "Qingkai Fang",
            "Yan Zhou",
            "Yang Feng"
        ],
        "tldr": "Stream-Omni is a large multimodal model that uses both sequence-dimensional concatenation and a novel CTC-based layer-dimensional mapping to efficiently align vision and speech modalities to text, enabling simultaneous multimodal interactions with less data.",
        "tldr_zh": "Stream-Omni是一个大型多模态模型，它使用序列维度连接和一个新的基于CTC的层维度映射，有效地将视觉和语音模态与文本对齐，从而能以更少的数据支持同时多模态交互。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]