[
    {
        "title": "Curriculum Learning for Biological Sequence Prediction: The Case of De Novo Peptide Sequencing",
        "summary": "Peptide sequencing-the process of identifying amino acid sequences from mass\nspectrometry data-is a fundamental task in proteomics. Non-Autoregressive\nTransformers (NATs) have proven highly effective for this task, outperforming\ntraditional methods. Unlike autoregressive models, which generate tokens\nsequentially, NATs predict all positions simultaneously, leveraging\nbidirectional context through unmasked self-attention. However, existing NAT\napproaches often rely on Connectionist Temporal Classification (CTC) loss,\nwhich presents significant optimization challenges due to CTC's complexity and\nincreases the risk of training failures. To address these issues, we propose an\nimproved non-autoregressive peptide sequencing model that incorporates a\nstructured protein sequence curriculum learning strategy. This approach adjusts\nprotein's learning difficulty based on the model's estimated protein\ngenerational capabilities through a sampling process, progressively learning\npeptide generation from simple to complex sequences. Additionally, we introduce\na self-refining inference-time module that iteratively enhances predictions\nusing learned NAT token embeddings, improving sequence accuracy at a\nfine-grained level. Our curriculum learning strategy reduces NAT training\nfailures frequency by more than 90% based on sampled training over various data\ndistributions. Evaluations on nine benchmark species demonstrate that our\napproach outperforms all previous methods across multiple metrics and species.",
        "url": "http://arxiv.org/abs/2506.13485v1",
        "published_date": "2025-06-16T13:44:25+00:00",
        "updated_date": "2025-06-16T13:44:25+00:00",
        "categories": [
            "q-bio.BM",
            "cs.LG"
        ],
        "authors": [
            "Xiang Zhang",
            "Jiaqi Wei",
            "Zijie Qiu",
            "Sheng Xu",
            "Nanqing Dong",
            "Zhiqiang Gao",
            "Siqi Sun"
        ],
        "pdf_url": "http://arxiv.org/pdf/2506.13485v1",
        "tldr": "This paper introduces an improved non-autoregressive Transformer model for de novo peptide sequencing using curriculum learning and a self-refining inference module, achieving state-of-the-art performance on benchmark datasets and reducing training failures.",
        "explanation": "The paper addresses the challenge of de novo peptide sequencing from mass spectrometry data using Non-Autoregressive Transformers (NATs). It proposes a curriculum learning strategy, which progressively increases the difficulty of training sequences based on the model's estimated capabilities. Additionally, it incorporates a self-refining module at inference time to iteratively enhance predictions. The method reportedly reduces training failures and outperforms existing approaches on multiple benchmark datasets.",
        "interests_alignment": "The paper directly aligns with the user's interests in machine learning for peptides and biomolecules. It focuses on applying a transformer-based model to peptide sequencing, a task related to peptide structure and function. Furthermore, the benchmarks and datasets utilized in the research may be of interest, as well as the mention of model code that is made publicly available in their github repo.",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]