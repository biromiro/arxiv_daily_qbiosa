[
    {
        "title": "Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation",
        "summary": "Recent advances in diffusion models have enabled high-quality video\ngeneration, but the additional temporal dimension significantly increases\ncomputational costs, making training and inference on long videos prohibitively\nexpensive. In this paper, we identify a phenomenon we term Spatiotemporal\nEnergy Decay in video diffusion models: post-softmax attention scores diminish\nas spatial and temporal distance between tokens increase, akin to the physical\ndecay of signal or waves over space and time in nature. Motivated by this, we\npropose Radial Attention, a scalable sparse attention mechanism with $O(n \\log\nn)$ complexity that translates energy decay into exponentially decaying compute\ndensity, which is significantly more efficient than standard $O(n^2)$ dense\nattention and more expressive than linear attention. Specifically, Radial\nAttention employs a simple, static attention mask where each token attends to\nspatially nearby tokens, with the attention window size shrinking with temporal\ndistance. Moreover, it allows pre-trained video diffusion models to extend\ntheir generation length with efficient LoRA-based fine-tuning. Extensive\nexperiments show that Radial Attention maintains video quality across\nWan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$\\times$ speedup\nover the original dense attention. With minimal tuning, it enables video\ngeneration up to 4$\\times$ longer while reducing training costs by up to\n4.4$\\times$ compared to direct fine-tuning and accelerating inference by up to\n3.7$\\times$ compared to dense attention inference.",
        "url": "http://arxiv.org/abs/2506.19852v1",
        "published_date": "2025-06-24T17:59:59+00:00",
        "updated_date": "2025-06-24T17:59:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Xingyang Li",
            "Muyang Li",
            "Tianle Cai",
            "Haocheng Xi",
            "Shuo Yang",
            "Yujun Lin",
            "Lvmin Zhang",
            "Songlin Yang",
            "Jinbo Hu",
            "Kelly Peng",
            "Maneesh Agrawala",
            "Ion Stoica",
            "Kurt Keutzer",
            "Song Han"
        ],
        "tldr": "This paper introduces Radial Attention, a computationally efficient sparse attention mechanism for long video generation that leverages the observed spatiotemporal energy decay in video diffusion models, achieving significant speedups and cost reductions.",
        "tldr_zh": "该论文介绍了径向注意力（Radial Attention），这是一种用于长视频生成的高效稀疏注意力机制，它利用了视频扩散模型中观察到的时空能量衰减现象，实现了显著的加速和成本降低。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Improving Progressive Generation with Decomposable Flow Matching",
        "summary": "Generating high-dimensional visual modalities is a computationally intensive\ntask. A common solution is progressive generation, where the outputs are\nsynthesized in a coarse-to-fine spectral autoregressive manner. While diffusion\nmodels benefit from the coarse-to-fine nature of denoising, explicit\nmulti-stage architectures are rarely adopted. These architectures have\nincreased the complexity of the overall approach, introducing the need for a\ncustom diffusion formulation, decomposition-dependent stage transitions,\nadd-hoc samplers, or a model cascade. Our contribution, Decomposable Flow\nMatching (DFM), is a simple and effective framework for the progressive\ngeneration of visual media. DFM applies Flow Matching independently at each\nlevel of a user-defined multi-scale representation (such as Laplacian pyramid).\nAs shown by our experiments, our approach improves visual quality for both\nimages and videos, featuring superior results compared to prior multistage\nframeworks. On Imagenet-1k 512px, DFM achieves 35.2% improvements in FDD scores\nover the base architecture and 26.4% over the best-performing baseline, under\nthe same training compute. When applied to finetuning of large models, such as\nFLUX, DFM shows faster convergence speed to the training distribution.\nCrucially, all these advantages are achieved with a single model, architectural\nsimplicity, and minimal modifications to existing training pipelines.",
        "url": "http://arxiv.org/abs/2506.19839v1",
        "published_date": "2025-06-24T17:58:02+00:00",
        "updated_date": "2025-06-24T17:58:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Moayed Haji-Ali",
            "Willi Menapace",
            "Ivan Skorokhodov",
            "Arpit Sahni",
            "Sergey Tulyakov",
            "Vicente Ordonez",
            "Aliaksandr Siarohin"
        ],
        "tldr": "The paper introduces Decomposable Flow Matching (DFM), a simple and effective framework for progressive image and video generation that improves visual quality and convergence speed compared to existing multistage approaches, using a single model and minimal modifications to existing training pipelines.",
        "tldr_zh": "该论文介绍了可分解流匹配（DFM），这是一种简单高效的渐进式图像和视频生成框架，与现有的多阶段方法相比，它提高了视觉质量和收敛速度，且仅使用单个模型并对现有训练流程进行最小修改。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Diffusion Transformer-to-Mamba Distillation for High-Resolution Image Generation",
        "summary": "The quadratic computational complexity of self-attention in diffusion\ntransformers (DiT) introduces substantial computational costs in\nhigh-resolution image generation. While the linear-complexity Mamba model\nemerges as a potential alternative, direct Mamba training remains empirically\nchallenging. To address this issue, this paper introduces diffusion\ntransformer-to-mamba distillation (T2MD), forming an efficient training\npipeline that facilitates the transition from the self-attention-based\ntransformer to the linear complexity state-space model Mamba. We establish a\ndiffusion self-attention and Mamba hybrid model that simultaneously achieves\nefficiency and global dependencies. With the proposed layer-level teacher\nforcing and feature-based knowledge distillation, T2MD alleviates the training\ndifficulty and high cost of a state space model from scratch. Starting from the\ndistilled 512$\\times$512 resolution base model, we push the generation towards\n2048$\\times$2048 images via lightweight adaptation and high-resolution\nfine-tuning. Experiments demonstrate that our training path leads to low\noverhead but high-quality text-to-image generation. Importantly, our results\nalso justify the feasibility of using sequential and causal Mamba models for\ngenerating non-causal visual output, suggesting the potential for future\nexploration.",
        "url": "http://arxiv.org/abs/2506.18999v1",
        "published_date": "2025-06-23T18:01:19+00:00",
        "updated_date": "2025-06-23T18:01:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuan Yao",
            "Yicong Hong",
            "Difan Liu",
            "Long Mai",
            "Feng Liu",
            "Jiebo Luo"
        ],
        "tldr": "This paper introduces a diffusion transformer-to-Mamba distillation method (T2MD) to improve the efficiency of high-resolution image generation by transferring knowledge from self-attention based transformers to the linear complexity Mamba model, demonstrating high-quality text-to-image generation at 2048x2048 resolution.",
        "tldr_zh": "本文提出了一种扩散Transformer到Mamba的蒸馏方法(T2MD)，通过将知识从基于自注意力的Transformer传递到线性复杂度的Mamba模型，从而提高高分辨率图像生成的效率，并展示了2048x2048分辨率下高质量的文本到图像生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]