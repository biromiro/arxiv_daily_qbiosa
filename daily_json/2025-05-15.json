[
    {
        "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset",
        "summary": "Unifying image understanding and generation has gained growing attention in\nrecent research on multimodal models. Although design choices for image\nunderstanding have been extensively studied, the optimal model architecture and\ntraining recipe for a unified framework with image generation remain\nunderexplored. Motivated by the strong potential of autoregressive and\ndiffusion models for high-quality generation and scalability, we conduct a\ncomprehensive study of their use in unified multimodal settings, with emphasis\non image representations, modeling objectives, and training strategies.\nGrounded in these investigations, we introduce a novel approach that employs a\ndiffusion transformer to generate semantically rich CLIP image features, in\ncontrast to conventional VAE-based representations. This design yields both\nhigher training efficiency and improved generative quality. Furthermore, we\ndemonstrate that a sequential pretraining strategy for unified models-first\ntraining on image understanding and subsequently on image generation-offers\npractical advantages by preserving image understanding capability while\ndeveloping strong image generation ability. Finally, we carefully curate a\nhigh-quality instruction-tuning dataset BLIP3o-60k for image generation by\nprompting GPT-4o with a diverse set of captions covering various scenes,\nobjects, human gestures, and more. Building on our innovative model design,\ntraining recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art\nunified multimodal models. BLIP3-o achieves superior performance across most of\nthe popular benchmarks spanning both image understanding and generation tasks.\nTo facilitate future research, we fully open-source our models, including code,\nmodel weights, training scripts, and pretraining and instruction tuning\ndatasets.",
        "url": "http://arxiv.org/abs/2505.09568v1",
        "published_date": "2025-05-14T17:11:07+00:00",
        "updated_date": "2025-05-14T17:11:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jiuhai Chen",
            "Zhiyang Xu",
            "Xichen Pan",
            "Yushi Hu",
            "Can Qin",
            "Tom Goldstein",
            "Lifu Huang",
            "Tianyi Zhou",
            "Saining Xie",
            "Silvio Savarese",
            "Le Xue",
            "Caiming Xiong",
            "Ran Xu"
        ],
        "tldr": "BLIP3-o introduces a novel, fully open-source unified multimodal model using a diffusion transformer for CLIP feature generation, achieving state-of-the-art performance in both image understanding and generation, trained with a new instruction-tuning dataset.",
        "tldr_zh": "BLIP3-o 介绍了一种新的、完全开源的统一多模态模型，该模型使用扩散转换器来生成 CLIP 特征，在图像理解和生成方面均实现了最先进的性能，并使用新的指令调整数据集进行训练。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    }
]