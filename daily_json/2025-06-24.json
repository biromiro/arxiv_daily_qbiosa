[
    {
        "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
        "summary": "In this work, we introduce OmniGen2, a versatile and open-source generative\nmodel designed to provide a unified solution for diverse generation tasks,\nincluding text-to-image, image editing, and in-context generation. Unlike\nOmniGen v1, OmniGen2 features two distinct decoding pathways for text and image\nmodalities, utilizing unshared parameters and a decoupled image tokenizer. This\ndesign enables OmniGen2 to build upon existing multimodal understanding models\nwithout the need to re-adapt VAE inputs, thereby preserving the original text\ngeneration capabilities. To facilitate the training of OmniGen2, we developed\ncomprehensive data construction pipelines, encompassing image editing and\nin-context generation data. Additionally, we introduce a reflection mechanism\ntailored for image generation tasks and curate a dedicated reflection dataset\nbased on OmniGen2. Despite its relatively modest parameter size, OmniGen2\nachieves competitive results on multiple task benchmarks, including\ntext-to-image and image editing. To further evaluate in-context generation,\nalso referred to as subject-driven tasks, we introduce a new benchmark named\nOmniContext. OmniGen2 achieves state-of-the-art performance among open-source\nmodels in terms of consistency. We will release our models, training code,\ndatasets, and data construction pipeline to support future research in this\nfield. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:\nhttps://github.com/VectorSpaceLab/OmniGen2",
        "url": "http://arxiv.org/abs/2506.18871v1",
        "published_date": "2025-06-23T17:38:54+00:00",
        "updated_date": "2025-06-23T17:38:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Chenyuan Wu",
            "Pengfei Zheng",
            "Ruiran Yan",
            "Shitao Xiao",
            "Xin Luo",
            "Yueze Wang",
            "Wanli Li",
            "Xiyan Jiang",
            "Yexin Liu",
            "Junjie Zhou",
            "Ze Liu",
            "Ziyi Xia",
            "Chaofan Li",
            "Haoge Deng",
            "Jiahao Wang",
            "Kun Luo",
            "Bo Zhang",
            "Defu Lian",
            "Xinlong Wang",
            "Zhongyuan Wang",
            "Tiejun Huang",
            "Zheng Liu"
        ],
        "tldr": "OmniGen2 is a new open-source multimodal generative model that unifies text-to-image, image editing, and in-context generation using separate decoding pathways and achieves competitive performance, especially in subject-driven image generation consistency.",
        "tldr_zh": "OmniGen2是一个新的开源多模态生成模型，它统一了文本到图像、图像编辑和上下文生成，使用单独的解码路径，并实现了具有竞争力的性能，尤其是在受主体驱动的图像生成一致性方面。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations",
        "summary": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com",
        "url": "http://arxiv.org/abs/2506.18898v1",
        "published_date": "2025-06-23T17:59:14+00:00",
        "updated_date": "2025-06-23T17:59:14+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.MM"
        ],
        "authors": [
            "Jiaming Han",
            "Hao Chen",
            "Yang Zhao",
            "Hanyu Wang",
            "Qi Zhao",
            "Ziyan Yang",
            "Hao He",
            "Xiangyu Yue",
            "Lu Jiang"
        ],
        "tldr": "The paper introduces Tar, a multimodal LLM that unifies visual understanding and generation using a text-aligned tokenizer, demonstrating improved performance and efficiency across benchmarks.",
        "tldr_zh": "本文介绍了一种多模态LLM名为Tar，它使用文本对齐的tokenizer统一了视觉理解和生成，并在多个基准测试中展示了改进的性能和效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ShowFlow: From Robust Single Concept to Condition-Free Multi-Concept Generation",
        "summary": "Customizing image generation remains a core challenge in controllable image\nsynthesis. For single-concept generation, maintaining both identity\npreservation and prompt alignment is challenging. In multi-concept scenarios,\nrelying solely on a prompt without additional conditions like layout boxes or\nsemantic masks, often leads to identity loss and concept omission. In this\npaper, we introduce ShowFlow, a comprehensive framework designed to tackle\nthese challenges. We propose ShowFlow-S for single-concept image generation,\nand ShowFlow-M for handling multiple concepts. ShowFlow-S introduces a\nKronA-WED adapter, which integrates a Kronecker adapter with weight and\nembedding decomposition, and employs a disentangled learning approach with a\nnovel attention regularization objective to enhance single-concept generation.\nBuilding on this foundation, ShowFlow-M directly reuses the learned models from\nShowFlow-S to support multi-concept generation without extra conditions,\nincorporating a Subject-Adaptive Matching Attention (SAMA) and a layout\nconsistency strategy as the plug-and-play module. Extensive experiments and\nuser studies validate ShowFlow's effectiveness, highlighting its potential in\nreal-world applications like advertising and virtual dressing.",
        "url": "http://arxiv.org/abs/2506.18493v1",
        "published_date": "2025-06-23T10:44:19+00:00",
        "updated_date": "2025-06-23T10:44:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Trong-Vu Hoang",
            "Quang-Binh Nguyen",
            "Thanh-Toan Do",
            "Tam V. Nguyen",
            "Minh-Triet Tran",
            "Trung-Nghia Le"
        ],
        "tldr": "The paper introduces ShowFlow, a framework comprising ShowFlow-S for single-concept and ShowFlow-M for multi-concept image generation, addressing identity preservation and prompt alignment without requiring explicit conditions.",
        "tldr_zh": "该论文介绍了 ShowFlow，一个包含 ShowFlow-S（用于单概念图像生成）和 ShowFlow-M（用于多概念图像生成）的框架，旨在解决身份保持和提示对齐问题，而无需显式条件。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image Generation",
        "summary": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency.",
        "url": "http://arxiv.org/abs/2506.18226v1",
        "published_date": "2025-06-23T01:27:06+00:00",
        "updated_date": "2025-06-23T01:27:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xunzhi Xiang",
            "Qi Fan"
        ],
        "tldr": "The paper introduces Adaptive Dynamic Sparse Attention (ADSA), a training-free method for optimizing attention computation in autoregressive image generation, significantly reducing memory overhead and computational delays during inference by dynamically selecting crucial historical tokens.",
        "tldr_zh": "该论文介绍了自适应动态稀疏注意力 (ADSA)，一种用于优化自回归图像生成中注意力计算的免训练方法，通过动态选择关键的历史 token，显著降低了推理期间的内存开销和计算延迟。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]