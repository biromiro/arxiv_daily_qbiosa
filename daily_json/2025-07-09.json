[
    {
        "title": "Omni-Video: Democratizing Unified Video Understanding and Generation",
        "summary": "Notable breakthroughs in unified understanding and generation modeling have\nled to remarkable advancements in image understanding, reasoning, production\nand editing, yet current foundational models predominantly focus on processing\nimages, creating a gap in the development of unified models for video\nunderstanding and generation. This report presents Omni-Video, an efficient and\neffective unified framework for video understanding, generation, as well as\ninstruction-based editing. Our key insight is to teach existing multimodal\nlarge language models (MLLMs) to produce continuous visual clues that are used\nas the input of diffusion decoders, which produce high-quality videos\nconditioned on these visual clues. To fully unlock the potential of our system\nfor unified video modeling, we integrate several technical improvements: 1) a\nlightweight architectural design that respectively attaches a vision head on\nthe top of MLLMs and a adapter before the input of diffusion decoders, the\nformer produce visual tokens for the latter, which adapts these visual tokens\nto the conditional space of diffusion decoders; and 2) an efficient multi-stage\ntraining scheme that facilitates a fast connection between MLLMs and diffusion\ndecoders with limited data and computational resources. We empirically\ndemonstrate that our model exhibits satisfactory generalization abilities\nacross video generation, editing and understanding tasks.",
        "url": "http://arxiv.org/abs/2507.06119v1",
        "published_date": "2025-07-08T16:02:16+00:00",
        "updated_date": "2025-07-08T16:02:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiyu Tan",
            "Hao Yang",
            "Luozheng Qin",
            "Jia Gong",
            "Mengping Yang",
            "Hao Li"
        ],
        "tldr": "The paper introduces Omni-Video, a unified framework for video understanding, generation, and editing by connecting multimodal large language models (MLLMs) with diffusion decoders through a lightweight architecture and multi-stage training.",
        "tldr_zh": "该论文介绍了Omni-Video，一个统一的视频理解、生成和编辑框架，通过轻量级架构和多阶段训练将多模态大型语言模型（MLLM）与扩散解码器连接起来。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation",
        "summary": "Recent advances in diffusion transformer models for motion-guided video\ngeneration, such as Tora, have shown significant progress. In this paper, we\npresent Tora2, an enhanced version of Tora, which introduces several design\nimprovements to expand its capabilities in both appearance and motion\ncustomization. Specifically, we introduce a decoupled personalization extractor\nthat generates comprehensive personalization embeddings for multiple open-set\nentities, better preserving fine-grained visual details compared to previous\nmethods. Building on this, we design a gated self-attention mechanism to\nintegrate trajectory, textual description, and visual information for each\nentity. This innovation significantly reduces misalignment in multimodal\nconditioning during training. Moreover, we introduce a contrastive loss that\njointly optimizes trajectory dynamics and entity consistency through explicit\nmapping between motion and personalization embeddings. Tora2 is, to our best\nknowledge, the first method to achieve simultaneous multi-entity customization\nof appearance and motion for video generation. Experimental results demonstrate\nthat Tora2 achieves competitive performance with state-of-the-art customization\nmethods while providing advanced motion control capabilities, which marks a\ncritical advancement in multi-condition video generation. Project page:\nhttps://github.com/alibaba/Tora .",
        "url": "http://arxiv.org/abs/2507.05963v1",
        "published_date": "2025-07-08T13:11:40+00:00",
        "updated_date": "2025-07-08T13:11:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenghao Zhang",
            "Junchao Liao",
            "Xiangyu Meng",
            "Long Qin",
            "Weizhi Wang"
        ],
        "tldr": "Tora2 improves upon Tora for multi-entity video generation by introducing a decoupled personalization extractor, gated self-attention, and contrastive loss for enhanced appearance and motion customization.",
        "tldr_zh": "Tora2在Tora的基础上进行了改进，通过引入解耦的个性化提取器、门控自注意力机制和对比损失，用于增强外观和运动定制的多实体视频生成。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "NeoBabel: A Multilingual Open Tower for Visual Generation",
        "summary": "Text-to-image generation advancements have been predominantly\nEnglish-centric, creating barriers for non-English speakers and perpetuating\ndigital inequities. While existing systems rely on translation pipelines, these\nintroduce semantic drift, computational overhead, and cultural misalignment. We\nintroduce NeoBabel, a novel multilingual image generation framework that sets a\nnew Pareto frontier in performance, efficiency and inclusivity, supporting six\nlanguages: English, Chinese, Dutch, French, Hindi, and Persian. The model is\ntrained using a combination of large-scale multilingual pretraining and\nhigh-resolution instruction tuning. To evaluate its capabilities, we expand two\nEnglish-only benchmarks to multilingual equivalents: m-GenEval and m-DPG.\nNeoBabel achieves state-of-the-art multilingual performance while retaining\nstrong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG.\nNotably, it performs on par with leading models on English tasks while\noutperforming them by +0.11 and +0.09 on multilingual benchmarks, even though\nthese models are built on multilingual base LLMs. This demonstrates the\neffectiveness of our targeted alignment training for preserving and extending\ncrosslingual generalization. We further introduce two new metrics to rigorously\nassess multilingual alignment and robustness to code-mixed prompts. Notably,\nNeoBabel matches or exceeds English-only models while being 2-4x smaller. We\nrelease an open toolkit, including all code, model checkpoints, a curated\ndataset of 124M multilingual text-image pairs, and standardized multilingual\nevaluation protocols, to advance inclusive AI research. Our work demonstrates\nthat multilingual capability is not a trade-off but a catalyst for improved\nrobustness, efficiency, and cultural fidelity in generative AI.",
        "url": "http://arxiv.org/abs/2507.06137v1",
        "published_date": "2025-07-08T16:19:45+00:00",
        "updated_date": "2025-07-08T16:19:45+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Mohammad Mahdi Derakhshani",
            "Dheeraj Varghese",
            "Marzieh Fadaee",
            "Cees G. M. Snoek"
        ],
        "tldr": "NeoBabel is a novel multilingual text-to-image generation framework that supports six languages and achieves state-of-the-art performance on multilingual benchmarks while being more efficient than existing systems.",
        "tldr_zh": "NeoBabel是一个新型的多语言文本到图像生成框架，支持六种语言，并在多语言基准测试中实现了最先进的性能，同时比现有系统更有效。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]