[
    {
        "title": "FantasyHSI: Video-Generation-Centric 4D Human Synthesis In Any Scene through A Graph-based Multi-Agent Framework",
        "summary": "Human-Scene Interaction (HSI) seeks to generate realistic human behaviors\nwithin complex environments, yet it faces significant challenges in handling\nlong-horizon, high-level tasks and generalizing to unseen scenes. To address\nthese limitations, we introduce FantasyHSI, a novel HSI framework centered on\nvideo generation and multi-agent systems that operates without paired data. We\nmodel the complex interaction process as a dynamic directed graph, upon which\nwe build a collaborative multi-agent system. This system comprises a scene\nnavigator agent for environmental perception and high-level path planning, and\na planning agent that decomposes long-horizon goals into atomic actions.\nCritically, we introduce a critic agent that establishes a closed-loop feedback\nmechanism by evaluating the deviation between generated actions and the planned\npath. This allows for the dynamic correction of trajectory drifts caused by the\nstochasticity of the generative model, thereby ensuring long-term logical\nconsistency. To enhance the physical realism of the generated motions, we\nleverage Direct Preference Optimization (DPO) to train the action generator,\nsignificantly reducing artifacts such as limb distortion and foot-sliding.\nExtensive experiments on our custom SceneBench benchmark demonstrate that\nFantasyHSI significantly outperforms existing methods in terms of\ngeneralization, long-horizon task completion, and physical realism. Ours\nproject page: https://fantasy-amap.github.io/fantasy-hsi/",
        "url": "http://arxiv.org/abs/2509.01232v1",
        "published_date": "2025-09-01T08:20:50+00:00",
        "updated_date": "2025-09-01T08:20:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lingzhou Mu",
            "Qiang Wang",
            "Fan Jiang",
            "Mengchao Wang",
            "Yaqi Fan",
            "Mu Xu",
            "Kai Zhang"
        ],
        "tldr": "FantasyHSI is a novel video-generation-centric framework for Human-Scene Interaction that utilizes a graph-based multi-agent system to generate realistic human behaviors in complex environments, achieving superior performance in generalization, long-horizon task completion, and physical realism compared to existing methods.",
        "tldr_zh": "FantasyHSI是一个新颖的以视频生成为中心的Human-Scene Interaction框架，它利用基于图的多智能体系统在复杂环境中生成逼真的人类行为，与现有方法相比，在泛化、长时任务完成度和物理真实感方面表现更优异。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling",
        "summary": "Text-to-image diffusion models are computationally intensive, often requiring\ndozens of forward passes through large transformer backbones. For instance,\nStable Diffusion XL generates high-quality images with 50 evaluations of a\n2.6B-parameter model, an expensive process even for a single batch. Few-step\ndiffusion models reduce this cost to 2-8 denoising steps but still depend on\nlarge, uncompressed U-Net or diffusion transformer backbones, which are often\ntoo costly for full-precision inference without datacenter GPUs. These\nrequirements also limit existing post-training quantization methods that rely\non full-precision calibration. We introduce Q-Sched, a new paradigm for\npost-training quantization that modifies the diffusion model scheduler rather\nthan model weights. By adjusting the few-step sampling trajectory, Q-Sched\nachieves full-precision accuracy with a 4x reduction in model size. To learn\nquantization-aware pre-conditioning coefficients, we propose the JAQ loss,\nwhich combines text-image compatibility with an image quality metric for\nfine-grained optimization. JAQ is reference-free and requires only a handful of\ncalibration prompts, avoiding full-precision inference during calibration.\nQ-Sched delivers substantial gains: a 15.5% FID improvement over the FP16\n4-step Latent Consistency Model and a 16.6% improvement over the FP16 8-step\nPhased Consistency Model, showing that quantization and few-step distillation\nare complementary for high-fidelity generation. A large-scale user study with\nmore than 80,000 annotations further confirms Q-Sched's effectiveness on both\nFLUX.1[schnell] and SDXL-Turbo.",
        "url": "http://arxiv.org/abs/2509.01624v1",
        "published_date": "2025-09-01T17:09:22+00:00",
        "updated_date": "2025-09-01T17:09:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Natalia Frumkin",
            "Diana Marculescu"
        ],
        "tldr": "The paper introduces Q-Sched, a novel post-training quantization paradigm that modifies the diffusion model scheduler to achieve full-precision accuracy with a 4x reduction in model size for few-step diffusion models, showing significant FID improvements and preference in user studies.",
        "tldr_zh": "该论文介绍了一种名为Q-Sched的新型训练后量化方法，通过修改扩散模型的调度器，在模型尺寸缩小 4 倍的情况下，实现了少量步数扩散模型的全精度准确率，并在 FID 指标和用户研究中表现出显著的改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improving Large Vision and Language Models by Learning from a Panel of Peers",
        "summary": "Traditional alignment methods for Large Vision and Language Models (LVLMs)\nprimarily rely on human-curated preference data. Human-generated preference\ndata is costly; machine-generated preference data is limited in quality; and\nself-supervised preference data often introduces hallucinations. To overcome\nthese limitations, we propose a novel Panel-of-Peers learning framework\ninspired by collaborative learning among humans. This approach leverages a\npanel of LVLMs, each evaluating and learning from their collective outputs\nthrough an iterative self-improvement process. By simulating a peer review\nsystem, our models generate, assess, and refine outputs in response to a\ncurated set of prompts, mimicking a classroom learning environment. We\ndemonstrate that this methodology enhances model performance without requiring\nextensive human-labeled datasets. Our experiments show significant improvement\nacross multiple benchmarks, demonstrating the potential of peer evaluations as\na scalable alternative to self-supervised alignment. Notably, we show that\nPanel-of-Peers increases the average score on fifteen benchmarks from 48% to\n57%",
        "url": "http://arxiv.org/abs/2509.01610v1",
        "published_date": "2025-09-01T16:43:48+00:00",
        "updated_date": "2025-09-01T16:43:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jefferson Hernandez",
            "Jing Shi",
            "Simon Jenni",
            "Vicente Ordonez",
            "Kushal Kafle"
        ],
        "tldr": "This paper introduces a 'Panel-of-Peers' learning framework for LVLMs, where a panel of models collaboratively improve each other's outputs through iterative self-improvement and peer review, achieving better performance without extensive human labeling.",
        "tldr_zh": "该论文介绍了一个用于大型视觉语言模型（LVLMs）的“同行小组”学习框架，其中一组模型通过迭代自提升和同行评审协作改进彼此的输出，在没有大量人工标签的情况下实现更好的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "O-DisCo-Edit: Object Distortion Control for Unified Realistic Video Editing",
        "summary": "Diffusion models have recently advanced video editing, yet controllable\nediting remains challenging due to the need for precise manipulation of diverse\nobject properties. Current methods require different control signal for diverse\nediting tasks, which complicates model design and demands significant training\nresources. To address this, we propose O-DisCo-Edit, a unified framework that\nincorporates a novel object distortion control (O-DisCo). This signal, based on\nrandom and adaptive noise, flexibly encapsulates a wide range of editing cues\nwithin a single representation. Paired with a \"copy-form\" preservation module\nfor preserving non-edited regions, O-DisCo-Edit enables efficient,\nhigh-fidelity editing through an effective training paradigm. Extensive\nexperiments and comprehensive human evaluations consistently demonstrate that\nO-DisCo-Edit surpasses both specialized and multitask state-of-the-art methods\nacross various video editing tasks.\nhttps://cyqii.github.io/O-DisCo-Edit.github.io/",
        "url": "http://arxiv.org/abs/2509.01596v1",
        "published_date": "2025-09-01T16:29:39+00:00",
        "updated_date": "2025-09-01T16:29:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuqing Chen",
            "Junjie Wang",
            "Lin Liu",
            "Ruihang Chu",
            "Xiaopeng Zhang",
            "Qi Tian",
            "Yujiu Yang"
        ],
        "tldr": "The paper introduces O-DisCo-Edit, a unified framework for realistic video editing using a novel object distortion control signal within a diffusion model, outperforming existing methods.",
        "tldr_zh": "该论文介绍了 O-DisCo-Edit，一个用于现实视频编辑的统一框架，它在扩散模型中使用了一种新颖的物体失真控制信号，性能优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InfoScale: Unleashing Training-free Variable-scaled Image Generation via Effective Utilization of Information",
        "summary": "Diffusion models (DMs) have become dominant in visual generation but suffer\nperformance drop when tested on resolutions that differ from the training\nscale, whether lower or higher. In fact, the key challenge in generating\nvariable-scale images lies in the differing amounts of information across\nresolutions, which requires information conversion procedures to be varied for\ngenerating variable-scaled images. In this paper, we investigate the issues of\nthree critical aspects in DMs for a unified analysis in variable-scaled\ngeneration: dilated convolution, attention mechanisms, and initial noise.\nSpecifically, 1) dilated convolution in DMs for the higher-resolution\ngeneration loses high-frequency information. 2) Attention for variable-scaled\nimage generation struggles to adjust the information aggregation adaptively. 3)\nThe spatial distribution of information in the initial noise is misaligned with\nvariable-scaled image. To solve the above problems, we propose\n\\textbf{InfoScale}, an information-centric framework for variable-scaled image\ngeneration by effectively utilizing information from three aspects\ncorrespondingly. For information loss in 1), we introduce Progressive Frequency\nCompensation module to compensate for high-frequency information lost by\ndilated convolution in higher-resolution generation. For information\naggregation inflexibility in 2), we introduce Adaptive Information Aggregation\nmodule to adaptively aggregate information in lower-resolution generation and\nachieve an effective balance between local and global information in\nhigher-resolution generation. For information distribution misalignment in 3),\nwe design Noise Adaptation module to re-distribute information in initial noise\nfor variable-scaled generation. Our method is plug-and-play for DMs and\nextensive experiments demonstrate the effectiveness in variable-scaled image\ngeneration.",
        "url": "http://arxiv.org/abs/2509.01421v1",
        "published_date": "2025-09-01T12:27:04+00:00",
        "updated_date": "2025-09-01T12:27:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guohui Zhang",
            "Jiangtong Tan",
            "Linjiang Huang",
            "Zhonghang Yuan",
            "Naishan Zheng",
            "Jie Huang",
            "Feng Zhao"
        ],
        "tldr": "The paper introduces InfoScale, a training-free framework for diffusion models that addresses performance degradation when generating images at different resolutions by tackling information loss, aggregation, and distribution issues. It proposes three plug-and-play modules to enhance variable-scaled image generation.",
        "tldr_zh": "该论文介绍了InfoScale，一个无需训练的扩散模型框架，通过解决信息损失、聚合和分布问题，解决了在不同分辨率下生成图像时的性能下降问题。它提出了三个即插即用的模块来增强可变尺度图像的生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Neural Scene Designer: Self-Styled Semantic Image Manipulation",
        "summary": "Maintaining stylistic consistency is crucial for the cohesion and aesthetic\nappeal of images, a fundamental requirement in effective image editing and\ninpainting. However, existing methods primarily focus on the semantic control\nof generated content, often neglecting the critical task of preserving this\nconsistency. In this work, we introduce the Neural Scene Designer (NSD), a\nnovel framework that enables photo-realistic manipulation of user-specified\nscene regions while ensuring both semantic alignment with user intent and\nstylistic consistency with the surrounding environment. NSD leverages an\nadvanced diffusion model, incorporating two parallel cross-attention mechanisms\nthat separately process text and style information to achieve the dual\nobjectives of semantic control and style consistency. To capture fine-grained\nstyle representations, we propose the Progressive Self-style Representational\nLearning (PSRL) module. This module is predicated on the intuitive premise that\ndifferent regions within a single image share a consistent style, whereas\nregions from different images exhibit distinct styles. The PSRL module employs\na style contrastive loss that encourages high similarity between\nrepresentations from the same image while enforcing dissimilarity between those\nfrom different images. Furthermore, to address the lack of standardized\nevaluation protocols for this task, we establish a comprehensive benchmark.\nThis benchmark includes competing algorithms, dedicated style-related metrics,\nand diverse datasets and settings to facilitate fair comparisons. Extensive\nexperiments conducted on our benchmark demonstrate the effectiveness of the\nproposed framework.",
        "url": "http://arxiv.org/abs/2509.01405v1",
        "published_date": "2025-09-01T11:59:03+00:00",
        "updated_date": "2025-09-01T11:59:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianman Lin",
            "Tianshui Chen",
            "Chunmei Qing",
            "Zhijing Yang",
            "Shuangping Huang",
            "Yuheng Ren",
            "Liang Lin"
        ],
        "tldr": "The paper introduces Neural Scene Designer (NSD), a diffusion model-based framework for semantically-aware image manipulation that ensures stylistic consistency using a progressive self-style representation learning module and contrastive loss, validated through a new benchmark.",
        "tldr_zh": "该论文介绍了神经场景设计师（NSD），一个基于扩散模型的框架，用于进行语义感知的图像编辑，并通过渐进式自风格表示学习模块和对比损失来确保风格的一致性，并通过新的基准进行了验证。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Identity-Preserving Text-to-Video Generation via Training-Free Prompt, Image, and Guidance Enhancement",
        "summary": "Identity-preserving text-to-video (IPT2V) generation creates videos faithful\nto both a reference subject image and a text prompt. While fine-tuning large\npretrained video diffusion models on ID-matched data achieves state-of-the-art\nresults on IPT2V, data scarcity and high tuning costs hinder broader\nimprovement. We thus introduce a Training-Free Prompt, Image, and Guidance\nEnhancement (TPIGE) framework that bridges the semantic gap between the video\ndescription and the reference image and design sampling guidance that enhances\nidentity preservation and video quality, achieving performance gains at minimal\ncost.Specifically, we first propose Face Aware Prompt Enhancement, using GPT-4o\nto enhance the text prompt with facial details derived from the reference\nimage. We then propose Prompt Aware Reference Image Enhancement, leveraging an\nidentity-preserving image generator to refine the reference image, rectifying\nconflicts with the text prompt. The above mutual refinement significantly\nimproves input quality before video generation. Finally, we propose ID-Aware\nSpatiotemporal Guidance Enhancement, utilizing unified gradients to optimize\nidentity preservation and video quality jointly during generation.Our method\noutperforms prior work and is validated by automatic and human evaluations on a\n1000 video test set, winning first place in the ACM Multimedia 2025\nIdentity-Preserving Video Generation Challenge, demonstrating state-of-the-art\nperformance and strong generality. The code is available at\nhttps://github.com/Andyplus1/IPT2V.git.",
        "url": "http://arxiv.org/abs/2509.01362v1",
        "published_date": "2025-09-01T11:03:13+00:00",
        "updated_date": "2025-09-01T11:03:13+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Jiayi Gao",
            "Changcheng Hua",
            "Qingchao Chen",
            "Yuxin Peng",
            "Yang Liu"
        ],
        "tldr": "This paper presents a training-free framework (TPIGE) for identity-preserving text-to-video generation, using prompt, image, and guidance enhancement techniques, achieving state-of-the-art performance without fine-tuning and winning first place in a video generation challenge.",
        "tldr_zh": "本文提出了一种用于保持身份的文本到视频生成的无训练框架 (TPIGE)，该框架采用提示、图像和指导增强技术，无需微调即可实现最先进的性能，并在视频生成挑战赛中获得第一名。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FocusDPO: Dynamic Preference Optimization for Multi-Subject Personalized Image Generation via Adaptive Focus",
        "summary": "Multi-subject personalized image generation aims to synthesize customized\nimages containing multiple specified subjects without requiring test-time\noptimization. However, achieving fine-grained independent control over multiple\nsubjects remains challenging due to difficulties in preserving subject fidelity\nand preventing cross-subject attribute leakage. We present FocusDPO, a\nframework that adaptively identifies focus regions based on dynamic semantic\ncorrespondence and supervision image complexity. During training, our method\nprogressively adjusts these focal areas across noise timesteps, implementing a\nweighted strategy that rewards information-rich patches while penalizing\nregions with low prediction confidence. The framework dynamically adjusts focus\nallocation during the DPO process according to the semantic complexity of\nreference images and establishes robust correspondence mappings between\ngenerated and reference subjects. Extensive experiments demonstrate that our\nmethod substantially enhances the performance of existing pre-trained\npersonalized generation models, achieving state-of-the-art results on both\nsingle-subject and multi-subject personalized image synthesis benchmarks. Our\nmethod effectively mitigates attribute leakage while preserving superior\nsubject fidelity across diverse generation scenarios, advancing the frontier of\ncontrollable multi-subject image synthesis.",
        "url": "http://arxiv.org/abs/2509.01181v1",
        "published_date": "2025-09-01T07:06:36+00:00",
        "updated_date": "2025-09-01T07:06:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Qiaoqiao Jin",
            "Siming Fu",
            "Dong She",
            "Weinan Jia",
            "Hualiang Wang",
            "Mu Liu",
            "Jidong Jiang"
        ],
        "tldr": "The paper introduces FocusDPO, a dynamic preference optimization framework for multi-subject personalized image generation that adaptively identifies focus regions to improve subject fidelity and prevent attribute leakage.",
        "tldr_zh": "该论文介绍了 FocusDPO，一个用于多主体个性化图像生成的动态偏好优化框架，它能自适应地识别焦点区域，从而提高主体保真度并防止属性泄漏。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GPSToken: Gaussian Parameterized Spatially-adaptive Tokenization for Image Representation and Generation",
        "summary": "Effective and efficient tokenization plays an important role in image\nrepresentation and generation. Conventional methods, constrained by uniform\n2D/1D grid tokenization, are inflexible to represent regions with varying\nshapes and textures and at different locations, limiting their efficacy of\nfeature representation. In this work, we propose $\\textbf{GPSToken}$, a novel\n$\\textbf{G}$aussian $\\textbf{P}$arameterized $\\textbf{S}$patially-adaptive\n$\\textbf{Token}$ization framework, to achieve non-uniform image tokenization by\nleveraging parametric 2D Gaussians to dynamically model the shape, position,\nand textures of different image regions. We first employ an entropy-driven\nalgorithm to partition the image into texture-homogeneous regions of variable\nsizes. Then, we parameterize each region as a 2D Gaussian (mean for position,\ncovariance for shape) coupled with texture features. A specialized transformer\nis trained to optimize the Gaussian parameters, enabling continuous adaptation\nof position/shape and content-aware feature extraction. During decoding,\nGaussian parameterized tokens are reconstructed into 2D feature maps through a\ndifferentiable splatting-based renderer, bridging our adaptive tokenization\nwith standard decoders for end-to-end training. GPSToken disentangles spatial\nlayout (Gaussian parameters) from texture features to enable efficient\ntwo-stage generation: structural layout synthesis using lightweight networks,\nfollowed by structure-conditioned texture generation. Experiments demonstrate\nthe state-of-the-art performance of GPSToken, which achieves rFID and FID\nscores of 0.65 and 1.50 on image reconstruction and generation tasks using 128\ntokens, respectively. Codes and models of GPSToken can be found at\n$\\href{https://github.com/xtudbxk/GPSToken}{https://github.com/xtudbxk/GPSToken}$.",
        "url": "http://arxiv.org/abs/2509.01109v1",
        "published_date": "2025-09-01T04:01:37+00:00",
        "updated_date": "2025-09-01T04:01:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhengqiang Zhang",
            "Rongyuan Wu",
            "Lingchen Sun",
            "Lei Zhang"
        ],
        "tldr": "The paper introduces GPSToken, a novel image tokenization method using Gaussian parameterization for spatially-adaptive feature representation and generation, achieving state-of-the-art performance in image reconstruction and generation.",
        "tldr_zh": "该论文介绍了一种新的图像令牌化方法 GPSToken，它使用高斯参数化进行空间自适应特征表示和生成，在图像重建和生成方面取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
        "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
        "url": "http://arxiv.org/abs/2509.01085v1",
        "published_date": "2025-09-01T03:16:52+00:00",
        "updated_date": "2025-09-01T03:16:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenlu Zhan",
            "Wen Li",
            "Chuyu Shen",
            "Jun Zhang",
            "Suhui Wu",
            "Hao Zhang"
        ],
        "tldr": "This paper introduces Bidirectional Sparse Attention (BSA) to accelerate video diffusion transformer training by dynamically sparsifying both Queries and Key-Value pairs within 3D full attention, achieving significant FLOPs reduction and faster training while maintaining generative quality.",
        "tldr_zh": "本文介绍了一种双向稀疏注意力（BSA），通过动态稀疏化 3D 全注意力中的查询和键值对，从而加速视频扩散 Transformer 的训练，在保持生成质量的同时，显著降低 FLOPs 并加快训练速度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CompSlider: Compositional Slider for Disentangled Multiple-Attribute Image Generation",
        "summary": "In text-to-image (T2I) generation, achieving fine-grained control over\nattributes - such as age or smile - remains challenging, even with detailed\ntext prompts. Slider-based methods offer a solution for precise control of\nimage attributes. Existing approaches typically train individual adapter for\neach attribute separately, overlooking the entanglement among multiple\nattributes. As a result, interference occurs among different attributes,\npreventing precise control of multiple attributes together. To address this\nchallenge, we aim to disentangle multiple attributes in slider-based generation\nto enbale more reliable and independent attribute manipulation. Our approach,\nCompSlider, can generate a conditional prior for the T2I foundation model to\ncontrol multiple attributes simultaneously. Furthermore, we introduce novel\ndisentanglement and structure losses to compose multiple attribute changes\nwhile maintaining structural consistency within the image. Since CompSlider\noperates in the latent space of the conditional prior and does not require\nretraining the foundation model, it reduces the computational burden for both\ntraining and inference. We evaluate our approach on a variety of image\nattributes and highlight its generality by extending to video generation.",
        "url": "http://arxiv.org/abs/2509.01028v2",
        "published_date": "2025-08-31T23:36:44+00:00",
        "updated_date": "2025-09-03T15:01:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zixin Zhu",
            "Kevin Duarte",
            "Mamshad Nayeem Rizve",
            "Chengyuan Xu",
            "Ratheesh Kalarot",
            "Junsong Yuan"
        ],
        "tldr": "CompSlider introduces a method for disentangled, multi-attribute image (and video) generation using sliders, operating in the latent space of T2I models without retraining, addressing interference issues in existing approaches.",
        "tldr_zh": "CompSlider 提出了一种解耦的、多属性的图像（和视频）生成方法，通过滑块在 T2I 模型的潜在空间中操作，无需重新训练，解决了现有方法中的干扰问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning",
        "summary": "This paper provides a simplification on OpenVision's architecture and loss\ndesign for enhancing its training efficiency. Following the prior\nvision-language pretraining works CapPa and AIMv2, as well as modern multimodal\ndesigns like LLaVA, our changes are straightforward: we remove the text encoder\n(and therefore the contrastive loss), retaining only the captioning loss as a\npurely generative training signal. We name this new version OpenVision 2. The\ninitial results are promising: despite this simplification, OpenVision 2\ncompetitively matches the original model's performance on a broad set of\nmultimodal benchmarks while substantially cutting both training time and memory\nconsumption. For example, with ViT-L/14, it reduces training time by about 1.5x\n(from 83h to 57h), and memory usage by about 1.8x (from 24.5GB to 13.8GB,\nequivalently allowing the maximum batch size to grow from 2k to 8k). This\nsuperior training efficiency also allows us to scale far beyond the largest\nvision encoder used in OpenVision, reaching more than 1 billion parameters. We\nhold a strong belief that this lightweight, generative-only paradigm is\ncompelling for future vision encoder development in multimodal foundation\nmodels.",
        "url": "http://arxiv.org/abs/2509.01644v1",
        "published_date": "2025-09-01T17:38:21+00:00",
        "updated_date": "2025-09-01T17:38:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanqing Liu",
            "Xianhang Li",
            "Letian Zhang",
            "Zirui Wang",
            "Zeyu Zheng",
            "Yuyin Zhou",
            "Cihang Xie"
        ],
        "tldr": "OpenVision 2 simplifies the original OpenVision architecture by removing the text encoder and contrastive loss, resulting in faster and more memory-efficient training while maintaining performance. This allows for scaling to larger models.",
        "tldr_zh": "OpenVision 2 通过移除文本编码器和对比损失简化了原始 OpenVision 架构，从而在保持性能的同时实现了更快、更省内存的训练。这使得能够扩展到更大的模型。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "RealMat: Realistic Materials with Diffusion and Reinforcement Learning",
        "summary": "Generative models for high-quality materials are particularly desirable to\nmake 3D content authoring more accessible. However, the majority of material\ngeneration methods are trained on synthetic data. Synthetic data provides\nprecise supervision for material maps, which is convenient but also tends to\ncreate a significant visual gap with real-world materials. Alternatively,\nrecent work used a small dataset of real flash photographs to guarantee\nrealism, however such data is limited in scale and diversity. To address these\nlimitations, we propose RealMat, a diffusion-based material generator that\nleverages realistic priors, including a text-to-image model and a dataset of\nrealistic material photos under natural lighting. In RealMat, we first finetune\na pretrained Stable Diffusion XL (SDXL) with synthetic material maps arranged\nin $2 \\times 2$ grids. This way, our model inherits some realism of SDXL while\nlearning the data distribution of the synthetic material grids. Still, this\ncreates a realism gap, with some generated materials appearing synthetic. We\npropose to further finetune our model through reinforcement learning (RL),\nencouraging the generation of realistic materials. We develop a realism reward\nfunction for any material image under natural lighting, by collecting a\nlarge-scale dataset of realistic material images. We show that this approach\nincreases generated materials' realism compared to our base model and related\nwork.",
        "url": "http://arxiv.org/abs/2509.01134v1",
        "published_date": "2025-09-01T05:04:51+00:00",
        "updated_date": "2025-09-01T05:04:51+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Xilong Zhou",
            "Pedro Figueiredo",
            "Miloš Hašan",
            "Valentin Deschaintre",
            "Paul Guerrero",
            "Yiwei Hu",
            "Nima Khademi Kalantari"
        ],
        "tldr": "The paper introduces RealMat, a diffusion-based material generator that combines synthetic and real data, fine-tuning Stable Diffusion XL with reinforcement learning to enhance the realism of generated materials.",
        "tldr_zh": "该论文介绍了RealMat，一种基于扩散的材质生成器，它结合了合成数据和真实数据，通过强化学习微调Stable Diffusion XL，以增强生成材质的真实感。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FICGen: Frequency-Inspired Contextual Disentanglement for Layout-driven Degraded Image Generation",
        "summary": "Layout-to-image (L2I) generation has exhibited promising results in natural\ndomains, but suffers from limited generative fidelity and weak alignment with\nuser-provided layouts when applied to degraded scenes (i.e., low-light,\nunderwater). We primarily attribute these limitations to the \"contextual\nillusion dilemma\" in degraded conditions, where foreground instances are\noverwhelmed by context-dominant frequency distributions. Motivated by this, our\npaper proposes a new Frequency-Inspired Contextual Disentanglement Generative\n(FICGen) paradigm, which seeks to transfer frequency knowledge of degraded\nimages into the latent diffusion space, thereby facilitating the rendering of\ndegraded instances and their surroundings via contextual frequency-aware\nguidance. To be specific, FICGen consists of two major steps. Firstly, we\nintroduce a learnable dual-query mechanism, each paired with a dedicated\nfrequency resampler, to extract contextual frequency prototypes from\npre-collected degraded exemplars in the training set. Secondly, a\nvisual-frequency enhanced attention is employed to inject frequency prototypes\ninto the degraded generation process. To alleviate the contextual illusion and\nattribute leakage, an instance coherence map is developed to regulate\nlatent-space disentanglement between individual instances and their\nsurroundings, coupled with an adaptive spatial-frequency aggregation module to\nreconstruct spatial-frequency mixed degraded representations. Extensive\nexperiments on 5 benchmarks involving a variety of degraded scenarios-from\nsevere low-light to mild blur-demonstrate that FICGen consistently surpasses\nexisting L2I methods in terms of generative fidelity, alignment and downstream\nauxiliary trainability.",
        "url": "http://arxiv.org/abs/2509.01107v1",
        "published_date": "2025-09-01T04:00:22+00:00",
        "updated_date": "2025-09-01T04:00:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenzhuang Wang",
            "Yifan Zhao",
            "Mingcan Ma",
            "Ming Liu",
            "Zhonglin Jiang",
            "Yong Chen",
            "Jia Li"
        ],
        "tldr": "The paper introduces FICGen, a novel layout-to-image generation framework for degraded scenes, using frequency-inspired contextual disentanglement and a frequency-aware guidance mechanism to improve generative fidelity and alignment.",
        "tldr_zh": "该论文介绍了FICGen，一种新颖的布局到图像生成框架，用于处理退化场景。它利用频率启发的上下文解耦和频率感知引导机制，以提高生成保真度和对齐效果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Reinforced Visual Perception with Tools",
        "summary": "Visual reasoning, a cornerstone of human intelligence, encompasses complex\nperceptual and logical processes essential for solving diverse visual problems.\nWhile advances in computer vision have produced powerful models for various\nperceptual tasks, leveraging these for general visual reasoning remains\nchallenging. Prior work demonstrates that augmenting LLMs with vision models\nvia supervised finetuning improves performance, but faces key limitations such\nas expensive data generation, reliance on careful data filtering, and poor\ngeneralization. To address these issues, we propose ReVPT to enhance\nmulti-modal LLMs' abilities to reason about and use visual tools through\nreinforcement learning. We introduce a novel RL algorithm based on GRPO,\ndesigned to train models to reason with a suite of four visual tools. Through\nextensive experiments, we show that our method achieves state-of-the-art\nperformance on several perception-heavy benchmarks, including SAT, CV-Bench,\nBLINK and MMStar, significantly outperforming the supervised and text-based RL\nfinetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the\ninstruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the\ncommunity new insights on RL-based visual tool-usage through extensive\nablations. Our code is available at https://github.com/ls-kelvin/REVPT.",
        "url": "http://arxiv.org/abs/2509.01656v1",
        "published_date": "2025-09-01T17:57:49+00:00",
        "updated_date": "2025-09-01T17:57:49+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Zetong Zhou",
            "Dongping Chen",
            "Zixian Ma",
            "Zhihan Hu",
            "Mingyang Fu",
            "Sinan Wang",
            "Yao Wan",
            "Zhou Zhao",
            "Ranjay Krishna"
        ],
        "tldr": "The paper introduces ReVPT, a reinforcement learning approach for training multi-modal LLMs to reason with and utilize visual tools, achieving state-of-the-art performance on perception-heavy benchmarks.",
        "tldr_zh": "本文介绍了一种名为ReVPT的强化学习方法，用于训练多模态LLM进行视觉工具的推理和使用，并在多个感知基准测试中实现了最先进的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "A Continuous-Time Consistency Model for 3D Point Cloud Generation",
        "summary": "Fast and accurate 3D shape generation from point clouds is essential for\napplications in robotics, AR/VR, and digital content creation. We introduce\nConTiCoM-3D, a continuous-time consistency model that synthesizes 3D shapes\ndirectly in point space, without discretized diffusion steps, pre-trained\nteacher models, or latent-space encodings. The method integrates a\nTrigFlow-inspired continuous noise schedule with a Chamfer Distance-based\ngeometric loss, enabling stable training on high-dimensional point sets while\navoiding expensive Jacobian-vector products. This design supports efficient\none- to two-step inference with high geometric fidelity. In contrast to\nprevious approaches that rely on iterative denoising or latent decoders,\nConTiCoM-3D employs a time-conditioned neural network operating entirely in\ncontinuous time, thereby achieving fast generation. Experiments on the ShapeNet\nbenchmark show that ConTiCoM-3D matches or outperforms state-of-the-art\ndiffusion and latent consistency models in both quality and efficiency,\nestablishing it as a practical framework for scalable 3D shape generation.",
        "url": "http://arxiv.org/abs/2509.01492v1",
        "published_date": "2025-09-01T14:11:59+00:00",
        "updated_date": "2025-09-01T14:11:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sebastian Eilermann",
            "René Heesch",
            "Oliver Niggemann"
        ],
        "tldr": "The paper introduces ConTiCoM-3D, a continuous-time consistency model for fast and accurate 3D shape generation from point clouds, outperforming existing methods on ShapeNet in both quality and efficiency. It operates directly in point space without relying on discretized diffusion steps or latent-space encodings.",
        "tldr_zh": "该论文提出了 ConTiCoM-3D，一种连续时间一致性模型，用于从点云中快速准确地生成 3D 形状，在 ShapeNet 上的质量和效率均优于现有方法。 它直接在点空间中运行，无需依赖离散化扩散步骤或潜在空间编码。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Acoustic Interference Suppression in Ultrasound images for Real-Time HIFU Monitoring Using an Image-Based Latent Diffusion Model",
        "summary": "High-Intensity Focused Ultrasound (HIFU) is a non-invasive therapeutic\ntechnique widely used for treating various diseases. However, the success and\nsafety of HIFU treatments depend on real-time monitoring, which is often\nhindered by interference when using ultrasound to guide HIFU treatment. To\naddress these challenges, we developed HIFU-ILDiff, a novel deep learning-based\napproach leveraging latent diffusion models to suppress HIFU-induced\ninterference in ultrasound images. The HIFU-ILDiff model employs a Vector\nQuantized Variational Autoencoder (VQ-VAE) to encode noisy ultrasound images\ninto a lower-dimensional latent space, followed by a latent diffusion model\nthat iteratively removes interference. The denoised latent vectors are then\ndecoded to reconstruct high-resolution, interference-free ultrasound images. We\nconstructed a comprehensive dataset comprising 18,872 image pairs from in vitro\nphantoms, ex vivo tissues, and in vivo animal data across multiple imaging\nmodalities and HIFU power levels to train and evaluate the model. Experimental\nresults demonstrate that HIFU-ILDiff significantly outperforms the commonly\nused Notch Filter method, achieving a Structural Similarity Index (SSIM) of\n0.796 and Peak Signal-to-Noise Ratio (PSNR) of 23.780 compared to SSIM of 0.443\nand PSNR of 14.420 for the Notch Filter under in vitro scenarios. Additionally,\nHIFU-ILDiff achieves real-time processing at 15 frames per second, markedly\nfaster than the Notch Filter's 5 seconds per frame. These findings indicate\nthat HIFU-ILDiff is able to denoise HIFU interference in ultrasound guiding\nimages for real-time monitoring during HIFU therapy, which will greatly improve\nthe treatment precision in current clinical applications.",
        "url": "http://arxiv.org/abs/2509.01557v1",
        "published_date": "2025-09-01T15:36:17+00:00",
        "updated_date": "2025-09-01T15:36:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dejia Cai",
            "Yao Ran",
            "Kun Yang",
            "Xinwang Shi",
            "Yingying Zhou",
            "Kexian Wu",
            "Yang Xu",
            "Yi Hu",
            "Xiaowei Zhou"
        ],
        "tldr": "The paper introduces HIFU-ILDiff, a latent diffusion model-based approach for real-time suppression of HIFU-induced interference in ultrasound images, significantly outperforming traditional methods in both image quality and processing speed.",
        "tldr_zh": "该论文介绍了一种基于潜在扩散模型的HIFU-ILDiff方法，用于实时抑制超声图像中由HIFU引起的干扰，在图像质量和处理速度方面均显著优于传统方法。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]