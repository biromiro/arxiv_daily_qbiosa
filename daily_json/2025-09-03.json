[
    {
        "title": "Draw-In-Mind: Learning Precise Image Editing via Chain-of-Thought Imagination",
        "summary": "In recent years, integrating multimodal understanding and generation into a\nsingle unified model has emerged as a promising paradigm. While this approach\nachieves strong results in text-to-image (T2I) generation, it still struggles\nwith precise image editing. We attribute this limitation to an imbalanced\ndivision of responsibilities. The understanding module primarily functions as a\ntranslator that encodes user instructions into semantic conditions, while the\ngeneration module must simultaneously act as designer and painter, inferring\nthe original layout, identifying the target editing region, and rendering the\nnew content. This imbalance is counterintuitive because the understanding\nmodule is typically trained with several times more data on complex reasoning\ntasks than the generation module. To address this issue, we introduce\nDraw-In-Mind (DIM), a dataset comprising two complementary subsets: (i)\nDIM-T2I, containing 14M long-context image-text pairs to enhance complex\ninstruction comprehension; and (ii) DIM-Edit, consisting of 233K\nchain-of-thought imaginations generated by GPT-4o, serving as explicit design\nblueprints for image edits. We connect a frozen Qwen2.5-VL-3B with a trainable\nSANA1.5-1.6B via a lightweight two-layer MLP, and train it on the proposed DIM\ndataset, resulting in DIM-4.6B-T2I/Edit. Despite its modest parameter scale,\nDIM-4.6B-Edit achieves SOTA or competitive performance on the ImgEdit and\nGEdit-Bench benchmarks, outperforming much larger models such as UniWorld-V1\nand Step1X-Edit. These findings demonstrate that explicitly assigning the\ndesign responsibility to the understanding module provides significant benefits\nfor image editing. Our dataset and models will be available at\nhttps://github.com/showlab/DIM.",
        "url": "http://arxiv.org/abs/2509.01986v1",
        "published_date": "2025-09-02T06:06:52+00:00",
        "updated_date": "2025-09-02T06:06:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ziyun Zeng",
            "Junhao Zhang",
            "Wei Li",
            "Mike Zheng Shou"
        ],
        "tldr": "The paper introduces Draw-In-Mind (DIM), a dataset and model (DIM-4.6B-T2I/Edit) that improves precise image editing by explicitly assigning design responsibilities to the understanding module, achieving SOTA or competitive performance. The key insight is to balance the responsibilities between understanding and generation via chain-of-thought prompting.",
        "tldr_zh": "本文介绍了Draw-In-Mind (DIM)，一个数据集和模型 (DIM-4.6B-T2I/Edit)，通过明确地将设计职责分配给理解模块，从而改进精确的图像编辑，并实现了SOTA或有竞争力的性能。 关键见解是通过思维链提示来平衡理解和生成之间的职责。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "GenCompositor: Generative Video Compositing with Diffusion Transformer",
        "summary": "Video compositing combines live-action footage to create video production,\nserving as a crucial technique in video creation and film production.\nTraditional pipelines require intensive labor efforts and expert collaboration,\nresulting in lengthy production cycles and high manpower costs. To address this\nissue, we automate this process with generative models, called generative video\ncompositing. This new task strives to adaptively inject identity and motion\ninformation of foreground video to the target video in an interactive manner,\nallowing users to customize the size, motion trajectory, and other attributes\nof the dynamic elements added in final video. Specifically, we designed a novel\nDiffusion Transformer (DiT) pipeline based on its intrinsic properties. To\nmaintain consistency of the target video before and after editing, we revised a\nlight-weight DiT-based background preservation branch with masked token\ninjection. As to inherit dynamic elements from other sources, a DiT fusion\nblock is proposed using full self-attention, along with a simple yet effective\nforeground augmentation for training. Besides, for fusing background and\nforeground videos with different layouts based on user control, we developed a\nnovel position embedding, named Extended Rotary Position Embedding (ERoPE).\nFinally, we curated a dataset comprising 61K sets of videos for our new task,\ncalled VideoComp. This data includes complete dynamic elements and high-quality\ntarget videos. Experiments demonstrate that our method effectively realizes\ngenerative video compositing, outperforming existing possible solutions in\nfidelity and consistency.",
        "url": "http://arxiv.org/abs/2509.02460v1",
        "published_date": "2025-09-02T16:10:13+00:00",
        "updated_date": "2025-09-02T16:10:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuzhou Yang",
            "Xiaoyu Li",
            "Xiaodong Cun",
            "Guangzhi Wang",
            "Lingen Li",
            "Ying Shan",
            "Jian Zhang"
        ],
        "tldr": "The paper introduces GenCompositor, a novel Diffusion Transformer (DiT) based approach for generative video compositing, along with a new dataset called VideoComp, demonstrating improved fidelity and consistency compared to existing solutions.",
        "tldr_zh": "该论文介绍了GenCompositor，一种基于扩散Transformer（DiT）的用于生成视频合成的新方法，并提供了一个名为VideoComp的新数据集，与现有解决方案相比，展示了更高的保真度和一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image Generation",
        "summary": "Text-to-image diffusion models can generate stunning visuals, yet they often\nfail at tasks children find trivial--like placing a dog to the right of a teddy\nbear rather than to the left. When combinations get more unusual--a giraffe\nabove an airplane--these failures become even more pronounced. Existing methods\nattempt to fix these spatial reasoning failures through model fine-tuning or\ntest-time optimization with handcrafted losses that are suboptimal. Rather than\nimposing our assumptions about spatial encoding, we propose learning these\nobjectives directly from the model's internal representations. We introduce\nLearn-to-Steer, a novel framework that learns data-driven objectives for\ntest-time optimization rather than handcrafting them. Our key insight is to\ntrain a lightweight classifier that decodes spatial relationships from the\ndiffusion model's cross-attention maps, then deploy this classifier as a\nlearned loss function during inference. Training such classifiers poses a\nsurprising challenge: they can take shortcuts by detecting linguistic traces\nrather than learning true spatial patterns. We solve this with a dual-inversion\nstrategy that enforces geometric understanding. Our method dramatically\nimproves spatial accuracy: from 0.20 to 0.61 on FLUX.1-dev and from 0.07 to\n0.54 on SD2.1 across standard benchmarks. Moreover, our approach generalizes to\nmultiple relations and significantly improves accuracy.",
        "url": "http://arxiv.org/abs/2509.02295v1",
        "published_date": "2025-09-02T13:17:11+00:00",
        "updated_date": "2025-09-02T13:17:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sapir Esther Yiflach",
            "Yuval Atzmon",
            "Gal Chechik"
        ],
        "tldr": "This paper introduces Learn-to-Steer, a method for learning data-driven loss functions to improve spatial reasoning in text-to-image generation by training classifiers on diffusion model's cross-attention maps and using them for test-time optimization, significantly enhancing spatial accuracy.",
        "tldr_zh": "本文介绍了Learn-to-Steer，一种通过学习数据驱动的损失函数来提升文本到图像生成中空间推理能力的方法。该方法通过在扩散模型的交叉注意力图上训练分类器，并将其应用于测试时优化，从而显著提高了空间准确性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing",
        "summary": "Visual autoregressive models (VAR) have recently emerged as a promising class\nof generative models, achieving performance comparable to diffusion models in\ntext-to-image generation tasks. While conditional generation has been widely\nexplored, the ability to perform prompt-guided image editing without additional\ntraining is equally critical, as it supports numerous practical real-world\napplications. This paper investigates the text-to-image editing capabilities of\nVAR by introducing Visual AutoRegressive Inverse Noise (VARIN), the first noise\ninversion-based editing technique designed explicitly for VAR models. VARIN\nleverages a novel pseudo-inverse function for argmax sampling, named\nLocation-aware Argmax Inversion (LAI), to generate inverse Gumbel noises. These\ninverse noises enable precise reconstruction of the source image and facilitate\ntargeted, controllable edits aligned with textual prompts. Extensive\nexperiments demonstrate that VARIN effectively modifies source images according\nto specified prompts while significantly preserving the original background and\nstructural details, thus validating its efficacy as a practical editing\napproach.",
        "url": "http://arxiv.org/abs/2509.01984v2",
        "published_date": "2025-09-02T06:01:52+00:00",
        "updated_date": "2025-09-03T05:25:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Quan Dao",
            "Xiaoxiao He",
            "Ligong Han",
            "Ngan Hoai Nguyen",
            "Amin Heyrani Nobar",
            "Faez Ahmed",
            "Han Zhang",
            "Viet Anh Nguyen",
            "Dimitris Metaxas"
        ],
        "tldr": "This paper introduces VARIN, a novel noise inversion-based editing technique for Visual Autoregressive models, enabling prompt-guided image editing with precise reconstruction and targeted edits while preserving original details.",
        "tldr_zh": "本文介绍了VARIN，一种为视觉自回归模型设计的新型基于噪声反演的编辑技术，它能够实现基于提示的图像编辑，精确重建图像，进行有针对性的编辑，并同时保留原始细节。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement",
        "summary": "Multi-subject personalized generation presents unique challenges in\nmaintaining identity fidelity and semantic coherence when synthesizing images\nconditioned on multiple reference subjects. Existing methods often suffer from\nidentity blending and attribute leakage due to inadequate modeling of how\ndifferent subjects should interact within shared representation spaces. We\npresent MOSAIC, a representation-centric framework that rethinks multi-subject\ngeneration through explicit semantic correspondence and orthogonal feature\ndisentanglement. Our key insight is that multi-subject generation requires\nprecise semantic alignment at the representation level - knowing exactly which\nregions in the generated image should attend to which parts of each reference.\nTo enable this, we introduce SemAlign-MS, a meticulously annotated dataset\nproviding fine-grained semantic correspondences between multiple reference\nsubjects and target images, previously unavailable in this domain. Building on\nthis foundation, we propose the semantic correspondence attention loss to\nenforce precise point-to-point semantic alignment, ensuring high consistency\nfrom each reference to its designated regions. Furthermore, we develop the\nmulti-reference disentanglement loss to push different subjects into orthogonal\nattention subspaces, preventing feature interference while preserving\nindividual identity characteristics. Extensive experiments demonstrate that\nMOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably,\nwhile existing methods typically degrade beyond 3 subjects, MOSAIC maintains\nhigh fidelity with 4+ reference subjects, opening new possibilities for complex\nmulti-subject synthesis applications.",
        "url": "http://arxiv.org/abs/2509.01977v1",
        "published_date": "2025-09-02T05:40:07+00:00",
        "updated_date": "2025-09-02T05:40:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dong She",
            "Siming Fu",
            "Mushui Liu",
            "Qiaoqiao Jin",
            "Hualiang Wang",
            "Mu Liu",
            "Jidong Jiang"
        ],
        "tldr": "The paper introduces MOSAIC, a novel framework for multi-subject personalized image generation that uses semantic correspondence and feature disentanglement to improve identity fidelity and semantic coherence, particularly with a larger number of subjects. They also introduce a new dataset for fine-grained semantic correspondences.",
        "tldr_zh": "本文介绍了一种名为MOSAIC的新型框架，用于多主体个性化图像生成，该框架利用语义对应和特征解耦来提高身份保真度和语义连贯性，尤其是在处理更多主体时。他们还引入了一个新的数据集，用于细粒度的语义对应。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Conditional-$t^3$VAE: Equitable Latent Space Allocation for Fair Generation",
        "summary": "Variational Autoencoders (VAEs) with global priors mirror the training set's\nclass frequency in latent space, underrepresenting tail classes and reducing\ngenerative fairness on imbalanced datasets. While $t^3$VAE improves robustness\nvia heavy-tailed Student's t-distribution priors, it still allocates latent\nvolume proportionally to the class frequency.In this work, we address this\nissue by explicitly enforcing equitable latent space allocation across classes.\nTo this end, we propose Conditional-$t^3$VAE, which defines a per-class\n\\mbox{Student's t} joint prior over latent and output variables, preventing\ndominance by majority classes. Our model is optimized using a closed-form\nobjective derived from the $\\gamma$-power divergence. Moreover, for\nclass-balanced generation, we derive an equal-weight latent mixture of\nStudent's t-distributions. On SVHN-LT, CIFAR100-LT, and CelebA,\nConditional-$t^3$VAE consistently achieves lower FID scores than both $t^3$VAE\nand Gaussian-based VAE baselines, particularly under severe class imbalance. In\nper-class F1 evaluations, Conditional-$t^3$VAE also outperforms the conditional\nGaussian VAE across all highly imbalanced settings. While Gaussian-based models\nremain competitive under mild imbalance ratio ($\\rho \\lesssim 3$), our approach\nsubstantially improves generative fairness and diversity in more extreme\nregimes.",
        "url": "http://arxiv.org/abs/2509.02154v1",
        "published_date": "2025-09-02T10:03:10+00:00",
        "updated_date": "2025-09-02T10:03:10+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "stat.ML"
        ],
        "authors": [
            "Aymene Mohammed Bouayed",
            "Samuel Deslauriers-Gauthier",
            "Adrian Iaccovelli",
            "David Naccache"
        ],
        "tldr": "The paper introduces Conditional-$t^3$VAE, a novel VAE architecture with a per-class Student's t-distribution prior, designed for fair and diverse generation, particularly on imbalanced datasets, and demonstrates its superior performance on several image datasets.",
        "tldr_zh": "该论文介绍了一种新的VAE架构，名为Conditional-$t^3$VAE，它具有一个逐类的学生t分布先验，专为公平和多样化的生成而设计，尤其是在不平衡的数据集上，并在多个图像数据集上展示了其卓越的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Unifi3D: A Study on 3D Representations for Generation and Reconstruction in a Common Framework",
        "summary": "Following rapid advancements in text and image generation, research has\nincreasingly shifted towards 3D generation. Unlike the well-established\npixel-based representation in images, 3D representations remain diverse and\nfragmented, encompassing a wide variety of approaches such as voxel grids,\nneural radiance fields, signed distance functions, point clouds, or octrees,\neach offering distinct advantages and limitations. In this work, we present a\nunified evaluation framework designed to assess the performance of 3D\nrepresentations in reconstruction and generation. We compare these\nrepresentations based on multiple criteria: quality, computational efficiency,\nand generalization performance. Beyond standard model benchmarking, our\nexperiments aim to derive best practices over all steps involved in the 3D\ngeneration pipeline, including preprocessing, mesh reconstruction, compression\nwith autoencoders, and generation. Our findings highlight that reconstruction\nerrors significantly impact overall performance, underscoring the need to\nevaluate generation and reconstruction jointly. We provide insights that can\ninform the selection of suitable 3D models for various applications,\nfacilitating the development of more robust and application-specific solutions\nin 3D generation. The code for our framework is available at\nhttps://github.com/isl-org/unifi3d.",
        "url": "http://arxiv.org/abs/2509.02474v1",
        "published_date": "2025-09-02T16:25:12+00:00",
        "updated_date": "2025-09-02T16:25:12+00:00",
        "categories": [
            "cs.GR",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Nina Wiedemann",
            "Sainan Liu",
            "Quentin Leboutet",
            "Katelyn Gao",
            "Benjamin Ummenhofer",
            "Michael Paulitsch",
            "Kai Yuan"
        ],
        "tldr": "This paper introduces a unified evaluation framework, Unifi3D, to benchmark different 3D representations for reconstruction and generation, providing insights into best practices and the impact of reconstruction errors on overall performance.",
        "tldr_zh": "本文介绍了一个统一的评估框架Unifi3D，用于评估不同3D表示在重建和生成方面的性能，并深入了解了最佳实践以及重建误差对整体性能的影响。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]