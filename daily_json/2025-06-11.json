[
    {
        "title": "Diffuse and Disperse: Image Generation with Representation Regularization",
        "summary": "The development of diffusion-based generative models over the past decade has\nlargely proceeded independently of progress in representation learning. These\ndiffusion models typically rely on regression-based objectives and generally\nlack explicit regularization. In this work, we propose \\textit{Dispersive\nLoss}, a simple plug-and-play regularizer that effectively improves\ndiffusion-based generative models. Our loss function encourages internal\nrepresentations to disperse in the hidden space, analogous to contrastive\nself-supervised learning, with the key distinction that it requires no positive\nsample pairs and therefore does not interfere with the sampling process used\nfor regression. Compared to the recent method of representation alignment\n(REPA), our approach is self-contained and minimalist, requiring no\npre-training, no additional parameters, and no external data. We evaluate\nDispersive Loss on the ImageNet dataset across a range of models and report\nconsistent improvements over widely used and strong baselines. We hope our work\nwill help bridge the gap between generative modeling and representation\nlearning.",
        "url": "http://arxiv.org/abs/2506.09027v1",
        "published_date": "2025-06-10T17:53:29+00:00",
        "updated_date": "2025-06-10T17:53:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Runqian Wang",
            "Kaiming He"
        ],
        "tldr": "This paper introduces 'Dispersive Loss,' a simple regularization technique for diffusion-based image generation models that encourages dispersed internal representations, improving performance without pre-training or external data.",
        "tldr_zh": "本文介绍了一种名为“Dispersive Loss”的简单正则化技术，用于基于扩散的图像生成模型，该技术鼓励分散的内部表示，从而提高性能，无需预训练或外部数据。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Product of Experts for Visual Generation",
        "summary": "Modern neural models capture rich priors and have complementary knowledge\nover shared data domains, e.g., images and videos. Integrating diverse\nknowledge from multiple sources -- including visual generative models, visual\nlanguage models, and sources with human-crafted knowledge such as graphics\nengines and physics simulators -- remains under-explored. We propose a Product\nof Experts (PoE) framework that performs inference-time knowledge composition\nfrom heterogeneous models. This training-free approach samples from the product\ndistribution across experts via Annealed Importance Sampling (AIS). Our\nframework shows practical benefits in image and video synthesis tasks, yielding\nbetter controllability than monolithic methods and additionally providing\nflexible user interfaces for specifying visual generation goals.",
        "url": "http://arxiv.org/abs/2506.08894v1",
        "published_date": "2025-06-10T15:21:14+00:00",
        "updated_date": "2025-06-10T15:21:14+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yunzhi Zhang",
            "Carson Murtuza-Lanier",
            "Zizhang Li",
            "Yilun Du",
            "Jiajun Wu"
        ],
        "tldr": "The paper introduces a training-free Product of Experts (PoE) framework for image and video synthesis, enabling knowledge composition from heterogeneous models (e.g., generative models, language models, physics simulators) via Annealed Importance Sampling, achieving better controllability and flexible user interfaces.",
        "tldr_zh": "本文介绍了一种无需训练的专家乘积（PoE）框架，用于图像和视频合成。该框架通过退火重要性采样，从异构模型（如生成模型、语言模型、物理模拟器）中进行知识组合，从而实现更好的可控性和灵活的用户界面。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models",
        "summary": "With the rapid development of text-to-vision generation diffusion models,\nclassifier-free guidance has emerged as the most prevalent method for\nconditioning. However, this approach inherently requires twice as many steps\nfor model forwarding compared to unconditional generation, resulting in\nsignificantly higher costs. While previous study has introduced the concept of\nadaptive guidance, it lacks solid analysis and empirical results, making\nprevious method unable to be applied to general diffusion models. In this work,\nwe present another perspective of applying adaptive guidance and propose Step\nAG, which is a simple, universally applicable adaptive guidance strategy. Our\nevaluations focus on both image quality and image-text alignment. whose results\nindicate that restricting classifier-free guidance to the first several\ndenoising steps is sufficient for generating high-quality, well-conditioned\nimages, achieving an average speedup of 20% to 30%. Such improvement is\nconsistent across different settings such as inference steps, and various\nmodels including video generation models, highlighting the superiority of our\nmethod.",
        "url": "http://arxiv.org/abs/2506.08351v1",
        "published_date": "2025-06-10T02:09:48+00:00",
        "updated_date": "2025-06-10T02:09:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Huixuan Zhang",
            "Junzhe Zhang",
            "Xiaojun Wan"
        ],
        "tldr": "This paper introduces Step AG, an adaptive classifier-free guidance strategy for diffusion models that improves inference speed by 20-30% by restricting guidance to the initial denoising steps without sacrificing image quality or text alignment, applicable to both image and video generation.",
        "tldr_zh": "该论文介绍了一种自适应的无分类器引导策略Step AG，用于扩散模型，通过将引导限制在初始去噪步骤中，在不牺牲图像质量或文本对齐的情况下，将推理速度提高20-30%，适用于图像和视频生成。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Highly Compressed Tokenizer Can Generate Without Training",
        "summary": "Commonly used image tokenizers produce a 2D grid of spatially arranged\ntokens. In contrast, so-called 1D image tokenizers represent images as highly\ncompressed one-dimensional sequences of as few as 32 discrete tokens. We find\nthat the high degree of compression achieved by a 1D tokenizer with vector\nquantization enables image editing and generative capabilities through\nheuristic manipulation of tokens, demonstrating that even very crude\nmanipulations -- such as copying and replacing tokens between latent\nrepresentations of images -- enable fine-grained image editing by transferring\nappearance and semantic attributes. Motivated by the expressivity of the 1D\ntokenizer's latent space, we construct an image generation pipeline leveraging\ngradient-based test-time optimization of tokens with plug-and-play loss\nfunctions such as reconstruction or CLIP similarity. Our approach is\ndemonstrated for inpainting and text-guided image editing use cases, and can\ngenerate diverse and realistic samples without requiring training of any\ngenerative model.",
        "url": "http://arxiv.org/abs/2506.08257v1",
        "published_date": "2025-06-09T21:45:03+00:00",
        "updated_date": "2025-06-09T21:45:03+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "L. Lao Beyer",
            "T. Li",
            "X. Chen",
            "S. Karaman",
            "K. He"
        ],
        "tldr": "This paper introduces a method for image editing and generation using a highly compressed 1D image tokenizer combined with test-time optimization, achieving results without training a generative model.",
        "tldr_zh": "本文介绍了一种图像编辑和生成方法，该方法使用高度压缩的一维图像分词器，并结合测试时优化，无需训练生成模型即可实现效果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]