[
    {
        "title": "OptiPrune: Boosting Prompt-Image Consistency with Attention-Guided Noise and Dynamic Token Selection",
        "summary": "Text-to-image diffusion models often struggle to achieve accurate semantic\nalignment between generated images and text prompts while maintaining\nefficiency for deployment on resource-constrained hardware. Existing approaches\neither incur substantial computational overhead through noise optimization or\ncompromise semantic fidelity by aggressively pruning tokens. In this work, we\npropose OptiPrune, a unified framework that combines distribution-aware initial\nnoise optimization with similarity-based token pruning to address both\nchallenges simultaneously. Specifically, (1) we introduce a distribution-aware\nnoise optimization module guided by attention scores to steer the initial\nlatent noise toward semantically meaningful regions, mitigating issues such as\nsubject neglect and feature entanglement; (2) we design a hardware-efficient\ntoken pruning strategy that selects representative base tokens via patch-wise\nsimilarity, injects randomness to enhance generalization, and recovers pruned\ntokens using maximum similarity copying before attention operations. Our method\npreserves the Gaussian prior during noise optimization and enables efficient\ninference without sacrificing alignment quality. Experiments on benchmark\ndatasets, including Animal-Animal, demonstrate that OptiPrune achieves\nstate-of-the-art prompt-image consistency with significantly reduced\ncomputational cost.",
        "url": "http://arxiv.org/abs/2507.00789v1",
        "published_date": "2025-07-01T14:24:40+00:00",
        "updated_date": "2025-07-01T14:24:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziji Lu"
        ],
        "tldr": "OptiPrune addresses the trade-off between prompt-image consistency and computational efficiency in text-to-image diffusion models by combining attention-guided noise optimization with dynamic token selection. The method achieves state-of-the-art results with reduced computational cost.",
        "tldr_zh": "OptiPrune通过结合注意力引导的噪声优化和动态令牌选择，解决了文本到图像扩散模型中提示-图像一致性和计算效率之间的权衡。该方法以降低的计算成本实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Diffusion Classifier Guidance for Non-robust Classifiers",
        "summary": "Classifier guidance is intended to steer a diffusion process such that a\ngiven classifier reliably recognizes the generated data point as a certain\nclass. However, most classifier guidance approaches are restricted to robust\nclassifiers, which were specifically trained on the noise of the diffusion\nforward process. We extend classifier guidance to work with general,\nnon-robust, classifiers that were trained without noise. We analyze the\nsensitivity of both non-robust and robust classifiers to noise of the diffusion\nprocess on the standard CelebA data set, the specialized SportBalls data set\nand the high-dimensional real-world CelebA-HQ data set. Our findings reveal\nthat non-robust classifiers exhibit significant accuracy degradation under\nnoisy conditions, leading to unstable guidance gradients. To mitigate these\nissues, we propose a method that utilizes one-step denoised image predictions\nand implements stabilization techniques inspired by stochastic optimization\nmethods, such as exponential moving averages. Experimental results demonstrate\nthat our approach improves the stability of classifier guidance while\nmaintaining sample diversity and visual quality. This work contributes to\nadvancing conditional sampling techniques in generative models, enabling a\nbroader range of classifiers to be used as guidance classifiers.",
        "url": "http://arxiv.org/abs/2507.00687v1",
        "published_date": "2025-07-01T11:39:41+00:00",
        "updated_date": "2025-07-01T11:39:41+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Philipp Vaeth",
            "Dibyanshu Kumar",
            "Benjamin Paassen",
            "Magda Gregorová"
        ],
        "tldr": "This paper introduces a method to extend classifier guidance in diffusion models to work with general, non-robust classifiers by addressing accuracy degradation under noisy conditions and stabilizing guidance gradients.",
        "tldr_zh": "本文提出了一种方法，通过解决噪声条件下的准确度下降和稳定指导梯度，将扩散模型中的分类器指导扩展到使用通用的非鲁棒分类器。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]