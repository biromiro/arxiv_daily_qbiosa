[
    {
        "title": "SSG-Dit: A Spatial Signal Guided Framework for Controllable Video Generation",
        "summary": "Controllable video generation aims to synthesize video content that aligns\nprecisely with user-provided conditions, such as text descriptions and initial\nimages. However, a significant challenge persists in this domain: existing\nmodels often struggle to maintain strong semantic consistency, frequently\ngenerating videos that deviate from the nuanced details specified in the\nprompts. To address this issue, we propose SSG-DiT (Spatial Signal Guided\nDiffusion Transformer), a novel and efficient framework for high-fidelity\ncontrollable video generation. Our approach introduces a decoupled two-stage\nprocess. The first stage, Spatial Signal Prompting, generates a spatially aware\nvisual prompt by leveraging the rich internal representations of a pre-trained\nmulti-modal model. This prompt, combined with the original text, forms a joint\ncondition that is then injected into a frozen video DiT backbone via our\nlightweight and parameter-efficient SSG-Adapter. This unique design, featuring\na dual-branch attention mechanism, allows the model to simultaneously harness\nits powerful generative priors while being precisely steered by external\nspatial signals. Extensive experiments demonstrate that SSG-DiT achieves\nstate-of-the-art performance, outperforming existing models on multiple key\nmetrics in the VBench benchmark, particularly in spatial relationship control\nand overall consistency.",
        "url": "http://arxiv.org/abs/2508.17062v1",
        "published_date": "2025-08-23T15:30:17+00:00",
        "updated_date": "2025-08-23T15:30:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Peng Hu",
            "Yu Gu",
            "Liang Luo",
            "Fuji Ren"
        ],
        "tldr": "The paper introduces SSG-DiT, a novel two-stage framework for controllable video generation that uses spatial signal prompting to enhance semantic consistency and achieve state-of-the-art performance, especially in spatial relationship control.",
        "tldr_zh": "该论文介绍了SSG-DiT，一种用于可控视频生成的新型两阶段框架，它使用空间信号提示来增强语义一致性，并实现最先进的性能，尤其是在空间关系控制方面。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Styleclone: Face Stylization with Diffusion Based Data Augmentation",
        "summary": "We present StyleClone, a method for training image-to-image translation\nnetworks to stylize faces in a specific style, even with limited style images.\nOur approach leverages textual inversion and diffusion-based guided image\ngeneration to augment small style datasets. By systematically generating\ndiverse style samples guided by both the original style images and real face\nimages, we significantly enhance the diversity of the style dataset. Using this\naugmented dataset, we train fast image-to-image translation networks that\noutperform diffusion-based methods in speed and quality. Experiments on\nmultiple styles demonstrate that our method improves stylization quality,\nbetter preserves source image content, and significantly accelerates inference.\nAdditionally, we provide a systematic evaluation of the augmentation techniques\nand their impact on stylization performance.",
        "url": "http://arxiv.org/abs/2508.17045v1",
        "published_date": "2025-08-23T14:48:18+00:00",
        "updated_date": "2025-08-23T14:48:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Neeraj Matiyali",
            "Siddharth Srivastava",
            "Gaurav Sharma"
        ],
        "tldr": "StyleClone uses diffusion-based data augmentation via textual inversion and guided image generation to train image-to-image translation networks for face stylization with limited style images, achieving faster and better results than diffusion models.",
        "tldr_zh": "StyleClone利用基于扩散的数据增强方法（通过文本反演和引导图像生成）训练图像到图像的转换网络，以在风格图像有限的情况下实现面部风格化，从而比扩散模型更快更好。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HiCache: Training-free Acceleration of Diffusion Models via Hermite Polynomial-based Feature Caching",
        "summary": "Diffusion models have achieved remarkable success in content generation but\nsuffer from prohibitive computational costs due to iterative sampling. While\nrecent feature caching methods tend to accelerate inference through temporal\nextrapolation, these methods still suffer from server quality loss due to the\nfailure in modeling the complex dynamics of feature evolution. To solve this\nproblem, this paper presents HiCache, a training-free acceleration framework\nthat fundamentally improves feature prediction by aligning mathematical tools\nwith empirical properties. Our key insight is that feature derivative\napproximations in Diffusion Transformers exhibit multivariate Gaussian\ncharacteristics, motivating the use of Hermite polynomials-the potentially\ntheoretically optimal basis for Gaussian-correlated processes. Besides, We\nfurther introduce a dual-scaling mechanism that ensures numerical stability\nwhile preserving predictive accuracy. Extensive experiments demonstrate\nHiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding\nbaseline quality, maintaining strong performance across text-to-image, video\ngeneration, and super-resolution tasks. Core implementation is provided in the\nappendix, with complete code to be released upon acceptance.",
        "url": "http://arxiv.org/abs/2508.16984v1",
        "published_date": "2025-08-23T10:35:16+00:00",
        "updated_date": "2025-08-23T10:35:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liang Feng",
            "Shikang Zheng",
            "Jiacheng Liu",
            "Yuqi Lin",
            "Qinming Zhou",
            "Peiliang Cai",
            "Xinyu Wang",
            "Junjie Chen",
            "Chang Zou",
            "Yue Ma",
            "Linfeng Zhang"
        ],
        "tldr": "HiCache is a training-free acceleration framework for diffusion models that utilizes Hermite polynomials for feature prediction, achieving significant speedup and improved quality across various generation tasks.",
        "tldr_zh": "HiCache是一个针对扩散模型的免训练加速框架，它利用Hermite多项式进行特征预测，在各种生成任务中实现了显著的加速和质量提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MDD: A Dataset for Text-and-Music Conditioned Duet Dance Generation",
        "summary": "We introduce Multimodal DuetDance (MDD), a diverse multimodal benchmark\ndataset designed for text-controlled and music-conditioned 3D duet dance motion\ngeneration. Our dataset comprises 620 minutes of high-quality motion capture\ndata performed by professional dancers, synchronized with music, and detailed\nwith over 10K fine-grained natural language descriptions. The annotations\ncapture a rich movement vocabulary, detailing spatial relationships, body\nmovements, and rhythm, making MDD the first dataset to seamlessly integrate\nhuman motions, music, and text for duet dance generation. We introduce two\nnovel tasks supported by our dataset: (1) Text-to-Duet, where given music and a\ntextual prompt, both the leader and follower dance motion are generated (2)\nText-to-Dance Accompaniment, where given music, textual prompt, and the\nleader's motion, the follower's motion is generated in a cohesive, text-aligned\nmanner. We include baseline evaluations on both tasks to support future\nresearch.",
        "url": "http://arxiv.org/abs/2508.16911v1",
        "published_date": "2025-08-23T05:56:37+00:00",
        "updated_date": "2025-08-23T05:56:37+00:00",
        "categories": [
            "cs.GR",
            "cs.CV",
            "cs.MM",
            "cs.SD"
        ],
        "authors": [
            "Prerit Gupta",
            "Jason Alexander Fotso-Puepi",
            "Zhengyuan Li",
            "Jay Mehta",
            "Aniket Bera"
        ],
        "tldr": "The paper introduces MDD, a new dataset for text-and-music conditioned 3D duet dance generation, featuring synchronized motion capture, music, and fine-grained text descriptions, supporting two novel tasks for duet dance creation and accompaniment.",
        "tldr_zh": "该论文介绍了一个新的数据集MDD，用于文本和音乐条件下的3D二重奏舞蹈生成，包含同步的动作捕捉、音乐和细粒度的文本描述，支持二重奏舞蹈创作和伴奏的两种新任务。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Dual Orthogonal Guidance for Robust Diffusion-based Handwritten Text Generation",
        "summary": "Diffusion-based Handwritten Text Generation (HTG) approaches achieve\nimpressive results on frequent, in-vocabulary words observed at training time\nand on regular styles. However, they are prone to memorizing training samples\nand often struggle with style variability and generation clarity. In\nparticular, standard diffusion models tend to produce artifacts or distortions\nthat negatively affect the readability of the generated text, especially when\nthe style is hard to produce. To tackle these issues, we propose a novel\nsampling guidance strategy, Dual Orthogonal Guidance (DOG), that leverages an\northogonal projection of a negatively perturbed prompt onto the original\npositive prompt. This approach helps steer the generation away from artifacts\nwhile maintaining the intended content, and encourages more diverse, yet\nplausible, outputs. Unlike standard Classifier-Free Guidance (CFG), which\nrelies on unconditional predictions and produces noise at high guidance scales,\nDOG introduces a more stable, disentangled direction in the latent space. To\ncontrol the strength of the guidance across the denoising process, we apply a\ntriangular schedule: weak at the start and end of denoising, when the process\nis most sensitive, and strongest in the middle steps. Experimental results on\nthe state-of-the-art DiffusionPen and One-DM demonstrate that DOG improves both\ncontent clarity and style variability, even for out-of-vocabulary words and\nchallenging writing styles.",
        "url": "http://arxiv.org/abs/2508.17017v1",
        "published_date": "2025-08-23T13:09:19+00:00",
        "updated_date": "2025-08-23T13:09:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Konstantina Nikolaidou",
            "George Retsinas",
            "Giorgos Sfikas",
            "Silvia Cascianelli",
            "Rita Cucchiara",
            "Marcus Liwicki"
        ],
        "tldr": "The paper proposes a novel sampling guidance strategy, Dual Orthogonal Guidance (DOG), for diffusion-based handwritten text generation, improving content clarity and style variability, especially for challenging cases.",
        "tldr_zh": "该论文提出了一种新颖的采样引导策略，即双正交引导（DOG），用于基于扩散的手写文本生成，提高了内容清晰度和风格多样性，尤其是在具有挑战性的情况下。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]