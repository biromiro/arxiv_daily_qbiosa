[
    {
        "title": "LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding",
        "summary": "Large-scale scene data is essential for training and testing in robot\nlearning. Neural reconstruction methods have promised the capability of\nreconstructing large physically-grounded outdoor scenes from captured sensor\ndata. However, these methods have baked-in static environments and only allow\nfor limited scene control -- they are functionally constrained in scene and\ntrajectory diversity by the captures from which they are reconstructed. In\ncontrast, generating driving data with recent image or video diffusion models\noffers control, however, at the cost of geometry grounding and causality. In\nthis work, we aim to bridge this gap and present a method that directly\ngenerates large-scale 3D driving scenes with accurate geometry, allowing for\ncausal novel view synthesis with object permanence and explicit 3D geometry\nestimation. The proposed method combines the generation of a proxy geometry and\nenvironment representation with score distillation from learned 2D image\npriors. We find that this approach allows for high controllability, enabling\nthe prompt-guided geometry and high-fidelity texture and structure that can be\nconditioned on map layouts -- producing realistic and geometrically consistent\n3D generations of complex driving scenes.",
        "url": "http://arxiv.org/abs/2508.19204v1",
        "published_date": "2025-08-26T17:04:49+00:00",
        "updated_date": "2025-08-26T17:04:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.GR"
        ],
        "authors": [
            "Julian Ost",
            "Andrea Ramazzina",
            "Amogh Joshi",
            "Maximilian Bömer",
            "Mario Bijelic",
            "Felix Heide"
        ],
        "tldr": "This paper introduces a method, LSD-3D, for generating large-scale, geometrically accurate 3D driving scenes by combining proxy geometry generation with score distillation from 2D image priors, aiming to bridge the gap between controllable image/video diffusion and grounded 3D reconstruction.",
        "tldr_zh": "该论文介绍了LSD-3D，一种用于生成大规模、几何精确的三维驾驶场景的方法。它将代理几何生成与2D图像先验的score distillation相结合，旨在弥合可控图像/视频扩散与基于几何的3D重建之间的差距。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "All-in-One Slider for Attribute Manipulation in Diffusion Models",
        "summary": "Text-to-image (T2I) diffusion models have made significant strides in\ngenerating high-quality images. However, progressively manipulating certain\nattributes of generated images to meet the desired user expectations remains\nchallenging, particularly for content with rich details, such as human faces.\nSome studies have attempted to address this by training slider modules.\nHowever, they follow a One-for-One manner, where an independent slider is\ntrained for each attribute, requiring additional training whenever a new\nattribute is introduced. This not only results in parameter redundancy\naccumulated by sliders but also restricts the flexibility of practical\napplications and the scalability of attribute manipulation. To address this\nissue, we introduce the All-in-One Slider, a lightweight module that decomposes\nthe text embedding space into sparse, semantically meaningful attribute\ndirections. Once trained, it functions as a general-purpose slider, enabling\ninterpretable and fine-grained continuous control over various attributes.\nMoreover, by recombining the learned directions, the All-in-One Slider supports\nzero-shot manipulation of unseen attributes (e.g., races and celebrities) and\nthe composition of multiple attributes. Extensive experiments demonstrate that\nour method enables accurate and scalable attribute manipulation, achieving\nnotable improvements compared to previous methods. Furthermore, our method can\nbe extended to integrate with the inversion framework to perform attribute\nmanipulation on real images, broadening its applicability to various real-world\nscenarios. The code and trained model will be released at:\nhttps://github.com/ywxsuperstar/KSAE-FaceSteer.",
        "url": "http://arxiv.org/abs/2508.19195v1",
        "published_date": "2025-08-26T16:56:30+00:00",
        "updated_date": "2025-08-26T16:56:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weixin Ye",
            "Hongguang Zhu",
            "Wei Wang",
            "Yahui Liu",
            "Mengyu Wang"
        ],
        "tldr": "This paper introduces an \"All-in-One Slider\" for diffusion models that allows for continuous control over multiple attributes of generated images without retraining, enabling zero-shot manipulation and attribute composition.",
        "tldr_zh": "这篇论文介绍了一种用于扩散模型的“全功能滑块”，它可以在不重新训练的情况下，连续控制生成图像的多个属性，从而实现零样本操作和属性组合。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond the Textual: Generating Coherent Visual Options for MCQs",
        "summary": "Multiple-choice questions (MCQs) play a crucial role in fostering deep\nthinking and knowledge integration in education. However, previous research has\nprimarily focused on generating MCQs with textual options, but it largely\noverlooks the visual options. Moreover, generating high-quality distractors\nremains a major challenge due to the high cost and limited scalability of\nmanual authoring. To tackle these problems, we propose a Cross-modal Options\nSynthesis (CmOS), a novel framework for generating educational MCQs with visual\noptions. Our framework integrates Multimodal Chain-of-Thought (MCoT) reasoning\nprocess and Retrieval-Augmented Generation (RAG) to produce semantically\nplausible and visually similar answer and distractors. It also includes a\ndiscrimination module to identify content suitable for visual options.\nExperimental results on test tasks demonstrate the superiority of CmOS in\ncontent discrimination, question generation and visual option generation over\nexisting methods across various subjects and educational levels.",
        "url": "http://arxiv.org/abs/2508.18772v1",
        "published_date": "2025-08-26T07:55:46+00:00",
        "updated_date": "2025-08-26T07:55:46+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Wanqiang Wang",
            "Longzhu He",
            "Wei Zheng"
        ],
        "tldr": "The paper introduces a Cross-modal Options Synthesis (CmOS) framework that combines Multimodal Chain-of-Thought (MCoT) and Retrieval-Augmented Generation (RAG) to automatically generate multiple-choice questions (MCQs) with visual options, addressing the limitations of existing text-based MCQs and the cost of manual authoring.",
        "tldr_zh": "该论文介绍了一种跨模态选项合成（CmOS）框架，该框架结合了多模态思维链（MCoT）和检索增强生成（RAG），以自动生成带有视觉选项的多项选择题（MCQ），从而解决了现有基于文本的MCQ的局限性以及手动创作的成本问题。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Wan-S2V: Audio-Driven Cinematic Video Generation",
        "summary": "Current state-of-the-art (SOTA) methods for audio-driven character animation\ndemonstrate promising performance for scenarios primarily involving speech and\nsinging. However, they often fall short in more complex film and television\nproductions, which demand sophisticated elements such as nuanced character\ninteractions, realistic body movements, and dynamic camera work. To address\nthis long-standing challenge of achieving film-level character animation, we\npropose an audio-driven model, which we refere to as Wan-S2V, built upon Wan.\nOur model achieves significantly enhanced expressiveness and fidelity in\ncinematic contexts compared to existing approaches. We conducted extensive\nexperiments, benchmarking our method against cutting-edge models such as\nHunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate\nthat our approach significantly outperforms these existing solutions.\nAdditionally, we explore the versatility of our method through its applications\nin long-form video generation and precise video lip-sync editing.",
        "url": "http://arxiv.org/abs/2508.18621v1",
        "published_date": "2025-08-26T02:51:31+00:00",
        "updated_date": "2025-08-26T02:51:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xin Gao",
            "Li Hu",
            "Siqi Hu",
            "Mingyang Huang",
            "Chaonan Ji",
            "Dechao Meng",
            "Jinwei Qi",
            "Penchong Qiao",
            "Zhen Shen",
            "Yafei Song",
            "Ke Sun",
            "Linrui Tian",
            "Guangyuan Wang",
            "Qi Wang",
            "Zhongjian Wang",
            "Jiayu Xiao",
            "Sheng Xu",
            "Bang Zhang",
            "Peng Zhang",
            "Xindi Zhang",
            "Zhe Zhang",
            "Jingren Zhou",
            "Lian Zhuo"
        ],
        "tldr": "The paper introduces Wan-S2V, an audio-driven model for generating film-level character animation, claiming significant improvements in expressiveness and fidelity compared to existing methods and demonstrating versatility in long-form video generation and lip-sync editing.",
        "tldr_zh": "该论文提出了Wan-S2V，一个用于生成电影级角色动画的音频驱动模型，声称与现有方法相比在表现力和保真度方面有显著改进，并展示了其在长视频生成和唇形同步编辑方面的多功能性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation",
        "summary": "Visual metaphor generation is a challenging task that aims to generate an\nimage given an input text metaphor. Inherently, it needs language understanding\nto bind a source concept with a target concept, in a way that preserves meaning\nwhile ensuring visual coherence. We propose a self-evaluating visual metaphor\ngeneration framework that focuses on metaphor alignment. Our self-evaluation\napproach combines existing metrics with our newly proposed metaphor\ndecomposition score and a meaning alignment (MA) metric. Within this setup, we\nexplore two novel approaches: a training-free pipeline that explicitly\ndecomposes prompts into source-target-meaning (S-T-M) mapping for image\nsynthesis, and a complementary training-based pipeline that improves alignment\nusing our proposed self-evaluation reward schema, without any large-scale\nretraining. On the held-out test set, the training-free approach surpasses\nstrong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores,\nwith the training-based approach close behind. We evaluate our framework output\nusing a user-facing study, and observed that participants preferred GPT-4o\noverall, while our training-free pipeline led open-source methods and edged\nImagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or\nmore abstract metaphors, with closed models excelling on short, concrete cases;\nwe also observe sensitivity to sampler settings. Overall, structured prompting\nand lightweight RL perform metaphor alignment well under modest compute, and\nremaining gaps to human preference appear driven by aesthetics and sampling.",
        "url": "http://arxiv.org/abs/2508.18569v1",
        "published_date": "2025-08-26T00:04:01+00:00",
        "updated_date": "2025-08-26T00:04:01+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Girish A. Koushik",
            "Fatemeh Nazarieh",
            "Katherine Birch",
            "Shenbin Qian",
            "Diptesh Kanojia"
        ],
        "tldr": "This paper introduces a novel framework for visual metaphor generation using self-evaluation and structured prompting to improve metaphor alignment, achieving competitive results against strong baselines using both training-free and lightweight RL-based approaches.",
        "tldr_zh": "本文提出了一种新颖的视觉隐喻生成框架，该框架使用自评估和结构化 Prompting 来提高隐喻的对齐效果。通过训练自由方法和轻量级的基于 RL 的方法，该框架取得了与强大的基线模型相媲美的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Style4D-Bench: A Benchmark Suite for 4D Stylization",
        "summary": "We introduce Style4D-Bench, the first benchmark suite specifically designed\nfor 4D stylization, with the goal of standardizing evaluation and facilitating\nprogress in this emerging area. Style4D-Bench comprises: 1) a comprehensive\nevaluation protocol measuring spatial fidelity, temporal coherence, and\nmulti-view consistency through both perceptual and quantitative metrics, 2) a\nstrong baseline that make an initial attempt for 4D stylization, and 3) a\ncurated collection of high-resolution dynamic 4D scenes with diverse motions\nand complex backgrounds. To establish a strong baseline, we present Style4D, a\nnovel framework built upon 4D Gaussian Splatting. It consists of three key\ncomponents: a basic 4DGS scene representation to capture reliable geometry, a\nStyle Gaussian Representation that leverages lightweight per-Gaussian MLPs for\ntemporally and spatially aware appearance control, and a Holistic\nGeometry-Preserved Style Transfer module designed to enhance spatio-temporal\nconsistency via contrastive coherence learning and structural content\npreservation. Extensive experiments on Style4D-Bench demonstrate that Style4D\nachieves state-of-the-art performance in 4D stylization, producing fine-grained\nstylistic details with stable temporal dynamics and consistent multi-view\nrendering. We expect Style4D-Bench to become a valuable resource for\nbenchmarking and advancing research in stylized rendering of dynamic 3D scenes.\nProject page: https://becky-catherine.github.io/Style4D . Code:\nhttps://github.com/Becky-catherine/Style4D-Bench .",
        "url": "http://arxiv.org/abs/2508.19243v1",
        "published_date": "2025-08-26T17:59:17+00:00",
        "updated_date": "2025-08-26T17:59:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Beiqi Chen",
            "Shuai Shao",
            "Haitang Feng",
            "Jianhuang Lai",
            "Jianlou Si",
            "Guangcong Wang"
        ],
        "tldr": "The paper introduces Style4D-Bench, a novel benchmark and baseline model (Style4D) for 4D stylization using 4D Gaussian Splatting, offering comprehensive evaluation metrics and high-resolution datasets.",
        "tldr_zh": "该论文介绍了Style4D-Bench，这是一个新的4D风格化基准和基线模型(Style4D)，它使用4D高斯溅射，提供全面的评估指标和高分辨率数据集。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning",
        "summary": "Existing literature typically treats style-driven and subject-driven\ngeneration as two disjoint tasks: the former prioritizes stylistic similarity,\nwhereas the latter insists on subject consistency, resulting in an apparent\nantagonism. We argue that both objectives can be unified under a single\nframework because they ultimately concern the disentanglement and\nre-composition of content and style, a long-standing theme in style-driven\nresearch. To this end, we present USO, a Unified Style-Subject Optimized\ncustomization model. First, we construct a large-scale triplet dataset\nconsisting of content images, style images, and their corresponding stylized\ncontent images. Second, we introduce a disentangled learning scheme that\nsimultaneously aligns style features and disentangles content from style\nthrough two complementary objectives, style-alignment training and\ncontent-style disentanglement training. Third, we incorporate a style\nreward-learning paradigm denoted as SRL to further enhance the model's\nperformance. Finally, we release USO-Bench, the first benchmark that jointly\nevaluates style similarity and subject fidelity across multiple metrics.\nExtensive experiments demonstrate that USO achieves state-of-the-art\nperformance among open-source models along both dimensions of subject\nconsistency and style similarity. Code and model:\nhttps://github.com/bytedance/USO",
        "url": "http://arxiv.org/abs/2508.18966v1",
        "published_date": "2025-08-26T12:10:24+00:00",
        "updated_date": "2025-08-26T12:10:24+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Shaojin Wu",
            "Mengqi Huang",
            "Yufeng Cheng",
            "Wenxu Wu",
            "Jiahe Tian",
            "Yiming Luo",
            "Fei Ding",
            "Qian He"
        ],
        "tldr": "The paper introduces USO, a unified model for style and subject-driven image generation, using disentangled learning and reward learning, along with a new benchmark for evaluation. It claims state-of-the-art performance in subject consistency and style similarity.",
        "tldr_zh": "该论文提出了USO，一个用于风格和主题驱动图像生成的统一模型，它采用了解耦学习和奖励学习，并提供了一个新的评估基准。论文声称在主题一致性和风格相似性方面达到了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and Caption Set Equivalence Reward",
        "summary": "Video captioning aims to generate comprehensive and coherent descriptions of\nthe video content, contributing to the advancement of both video understanding\nand generation. However, existing methods often suffer from motion-detail\nimbalance, as models tend to overemphasize one aspect while neglecting the\nother. This imbalance results in incomplete captions, which in turn leads to a\nlack of consistency in video understanding and generation. To address this\nissue, we propose solutions from two aspects: 1) Data aspect: We constructed\nthe Harmonizing Motion-Detail 270K (HMD-270K) dataset through a two-stage\npipeline: Motion-Detail Fusion (MDF) and Fine-Grained Examination (FGE). 2)\nOptimization aspect: We introduce the Caption Set Equivalence Reward (CSER)\nbased on Group Relative Policy Optimization (GRPO). CSER enhances completeness\nand accuracy in capturing both motion and details through unit-to-set matching\nand bidirectional validation. Based on the HMD-270K supervised fine-tuning and\nGRPO post-training with CSER, we developed OwlCap, a powerful video captioning\nmulti-modal large language model (MLLM) with motion-detail balance.\nExperimental results demonstrate that OwlCap achieves significant improvements\ncompared to baseline models on two benchmarks: the detail-focused VDC (+4.2\nAcc) and the motion-focused DREAM-1K (+4.6 F1). The HMD-270K dataset and OwlCap\nmodel will be publicly released to facilitate video captioning research\ncommunity advancements.",
        "url": "http://arxiv.org/abs/2508.18634v1",
        "published_date": "2025-08-26T03:18:34+00:00",
        "updated_date": "2025-08-26T03:18:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chunlin Zhong",
            "Qiuxia Hou",
            "Zhangjun Zhou",
            "Shuang Hao",
            "Haonan Lu",
            "Yanhao Zhang",
            "He Tang",
            "Xiang Bai"
        ],
        "tldr": "The paper introduces OwlCap, a video captioning model addressing motion-detail imbalance by proposing a new dataset, HMD-270K, and a novel reward function, CSER, achieving state-of-the-art results on motion and detail-focused benchmarks.",
        "tldr_zh": "该论文介绍了OwlCap，一个视频描述模型，通过提出新的数据集HMD-270K和新的奖励函数CSER来解决运动细节不平衡问题，并在运动和细节为中心的基准测试中实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ROSE: Remove Objects with Side Effects in Videos",
        "summary": "Video object removal has achieved advanced performance due to the recent\nsuccess of video generative models. However, when addressing the side effects\nof objects, e.g., their shadows and reflections, existing works struggle to\neliminate these effects for the scarcity of paired video data as supervision.\nThis paper presents ROSE, termed Remove Objects with Side Effects, a framework\nthat systematically studies the object's effects on environment, which can be\ncategorized into five common cases: shadows, reflections, light, translucency\nand mirror. Given the challenges of curating paired videos exhibiting the\naforementioned effects, we leverage a 3D rendering engine for synthetic data\ngeneration. We carefully construct a fully-automatic pipeline for data\npreparation, which simulates a large-scale paired dataset with diverse scenes,\nobjects, shooting angles, and camera trajectories. ROSE is implemented as an\nvideo inpainting model built on diffusion transformer. To localize all\nobject-correlated areas, the entire video is fed into the model for\nreference-based erasing. Moreover, additional supervision is introduced to\nexplicitly predict the areas affected by side effects, which can be revealed\nthrough the differential mask between the paired videos. To fully investigate\nthe model performance on various side effect removal, we presents a new\nbenchmark, dubbed ROSE-Bench, incorporating both common scenarios and the five\nspecial side effects for comprehensive evaluation. Experimental results\ndemonstrate that ROSE achieves superior performance compared to existing video\nobject erasing models and generalizes well to real-world video scenarios. The\nproject page is https://rose2025-inpaint.github.io/.",
        "url": "http://arxiv.org/abs/2508.18633v1",
        "published_date": "2025-08-26T03:18:31+00:00",
        "updated_date": "2025-08-26T03:18:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Chenxuan Miao",
            "Yutong Feng",
            "Jianshu Zeng",
            "Zixiang Gao",
            "Hantang Liu",
            "Yunfeng Yan",
            "Donglian Qi",
            "Xi Chen",
            "Bin Wang",
            "Hengshuang Zhao"
        ],
        "tldr": "The paper introduces ROSE, a video inpainting framework using diffusion transformers and synthetic data to remove objects and their side effects (shadows, reflections, etc.) in videos, outperforming existing methods and generalizing to real-world scenarios.",
        "tldr_zh": "该论文介绍了ROSE，一个视频修复框架，它使用扩散Transformer和合成数据来移除视频中的物体及其副作用（阴影、反射等），性能优于现有方法，并且可以泛化到真实场景。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SemLayoutDiff: Semantic Layout Generation with Diffusion Model for Indoor Scene Synthesis",
        "summary": "We present SemLayoutDiff, a unified model for synthesizing diverse 3D indoor\nscenes across multiple room types. The model introduces a scene layout\nrepresentation combining a top-down semantic map and attributes for each\nobject. Unlike prior approaches, which cannot condition on architectural\nconstraints, SemLayoutDiff employs a categorical diffusion model capable of\nconditioning scene synthesis explicitly on room masks. It first generates a\ncoherent semantic map, followed by a cross-attention-based network to predict\nfurniture placements that respect the synthesized layout. Our method also\naccounts for architectural elements such as doors and windows, ensuring that\ngenerated furniture arrangements remain practical and unobstructed. Experiments\non the 3D-FRONT dataset show that SemLayoutDiff produces spatially coherent,\nrealistic, and varied scenes, outperforming previous methods.",
        "url": "http://arxiv.org/abs/2508.18597v1",
        "published_date": "2025-08-26T02:01:20+00:00",
        "updated_date": "2025-08-26T02:01:20+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Xiaohao Sun",
            "Divyam Goel",
            "Angle X. Chang"
        ],
        "tldr": "SemLayoutDiff introduces a diffusion model for generating diverse and realistic 3D indoor scenes conditioned on room type and architectural constraints, outperforming previous methods on the 3D-FRONT dataset.",
        "tldr_zh": "SemLayoutDiff 提出了一种扩散模型，用于生成基于房间类型和建筑约束的多样且逼真的 3D 室内场景，并在 3D-FRONT 数据集上优于先前的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SAT-SKYLINES: 3D Building Generation from Satellite Imagery and Coarse Geometric Priors",
        "summary": "We present SatSkylines, a 3D building generation approach that takes\nsatellite imagery and coarse geometric priors. Without proper geometric\nguidance, existing image-based 3D generation methods struggle to recover\naccurate building structures from the top-down views of satellite images alone.\nOn the other hand, 3D detailization methods tend to rely heavily on highly\ndetailed voxel inputs and fail to produce satisfying results from simple priors\nsuch as cuboids. To address these issues, our key idea is to model the\ntransformation from interpolated noisy coarse priors to detailed geometries,\nenabling flexible geometric control without additional computational cost. We\nhave further developed Skylines-50K, a large-scale dataset of over 50,000\nunique and stylized 3D building assets in order to support the generations of\ndetailed building models. Extensive evaluations indicate the effectiveness of\nour model and strong generalization ability.",
        "url": "http://arxiv.org/abs/2508.18531v1",
        "published_date": "2025-08-25T22:03:31+00:00",
        "updated_date": "2025-08-25T22:03:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhangyu Jin",
            "Andrew Feng"
        ],
        "tldr": "The paper introduces SatSkylines, a 3D building generation approach from satellite imagery using coarse geometric priors, and a corresponding dataset Skylines-50K. It aims to overcome limitations of both image-based 3D generation and 3D detailization methods.",
        "tldr_zh": "该论文介绍了SatSkylines，一种利用卫星图像和粗略几何先验生成3D建筑的方法，以及相应的数据集Skylines-50K。它旨在克服基于图像的3D生成方法和3D细节化方法的局限性。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Quantum-Circuit-Based Visual Fractal Image Generation in Qiskit and Analytics",
        "summary": "As nature is ascribed as quantum, the fractals also pose some intriguing\nappearance which is found in many micro and macro observable entities or\nphenomena. Fractals show self-similarity across sizes; structures that resemble\nthe entire are revealed when zoomed in. In Quantum systems, the probability\ndensity or wavefunction may exhibit recurring interference patterns at various\nenergy or length scales. Fractals are produced by basic iterative rules (such\nas Mandelbrot or Julia sets), and they provide limitless complexity. Despite\nits simplicity, the Schr\\\"odinger equation in quantum mechanics produces\nincredibly intricate patterns of interference and entanglement, particularly in\nchaotic quantum systems. Quantum computing, the root where lies to the using\nthe principles of quantum-mechanical phenomenon, when applied in fractal image\ngeneration, what outcomes are expected? The paper outlines the generation of a\nJulia set dataset using an approach coupled with building quantum circuit,\nhighlighting the concepts of superposition, randomness, and entanglement as\nfoundational elements to manipulate the generated dataset patterns. As Quantum\ncomputing is finding many application areas, the possibility of using quantum\ncircuits for fractal Julia image generation posits a unique direction of future\nresearch where it can be applied to quantum generative arts across various\necosystems with a customised approach, such as producing an exciting landscape\nbased on a quantum art theme.",
        "url": "http://arxiv.org/abs/2508.18835v1",
        "published_date": "2025-08-26T09:14:19+00:00",
        "updated_date": "2025-08-26T09:14:19+00:00",
        "categories": [
            "quant-ph",
            "cs.CV"
        ],
        "authors": [
            "Hillol Biswas"
        ],
        "tldr": "This paper explores the generation of Julia set fractal images using quantum circuits, leveraging superposition, randomness, and entanglement. It proposes a novel direction for quantum generative art.",
        "tldr_zh": "本文探讨了使用量子电路生成Julia集分形图像，利用叠加、随机性和纠缠。它为量子生成艺术提出了一个新的研究方向。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "FastMesh:Efficient Artistic Mesh Generation via Component Decoupling",
        "summary": "Recent mesh generation approaches typically tokenize triangle meshes into\nsequences of tokens and train autoregressive models to generate these tokens\nsequentially. Despite substantial progress, such token sequences inevitably\nreuse vertices multiple times to fully represent manifold meshes, as each\nvertex is shared by multiple faces. This redundancy leads to excessively long\ntoken sequences and inefficient generation processes. In this paper, we propose\nan efficient framework that generates artistic meshes by treating vertices and\nfaces separately, significantly reducing redundancy. We employ an\nautoregressive model solely for vertex generation, decreasing the token count\nto approximately 23\\% of that required by the most compact existing tokenizer.\nNext, we leverage a bidirectional transformer to complete the mesh in a single\nstep by capturing inter-vertex relationships and constructing the adjacency\nmatrix that defines the mesh faces. To further improve the generation quality,\nwe introduce a fidelity enhancer to refine vertex positioning into more natural\narrangements and propose a post-processing framework to remove undesirable edge\nconnections. Experimental results show that our method achieves more than\n8$\\times$ faster speed on mesh generation compared to state-of-the-art\napproaches, while producing higher mesh quality.",
        "url": "http://arxiv.org/abs/2508.19188v1",
        "published_date": "2025-08-26T16:51:02+00:00",
        "updated_date": "2025-08-26T16:51:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jeonghwan Kim",
            "Yushi Lan",
            "Armando Fortes",
            "Yongwei Chen",
            "Xingang Pan"
        ],
        "tldr": "The paper introduces FastMesh, a novel framework for efficient artistic mesh generation that decouples vertex and face generation, achieving significant speedups and higher mesh quality compared to existing autoregressive methods.",
        "tldr_zh": "该论文介绍了一种名为 FastMesh 的新型框架，用于高效生成艺术网格。该框架解耦了顶点和面的生成过程，与现有的自回归方法相比，实现了显著的速度提升和更高的网格质量。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]