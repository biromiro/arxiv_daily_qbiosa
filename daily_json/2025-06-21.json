[
    {
        "title": "UniFork: Exploring Modality Alignment for Unified Multimodal Understanding and Generation",
        "summary": "Unified image understanding and generation has emerged as a promising\nparadigm in multimodal artificial intelligence. Despite recent progress, the\noptimal architectural design for such unified models remains an open challenge.\nIn this work, we start by analyzing the modality alignment behaviors of\ntask-specific expert models for understanding and generation, as well as\ncurrent unified models. Our analysis reveals a crucial observation:\nunderstanding tasks benefit from a progressively increasing modality alignment\nacross network depth, which helps build up semantic information for better\ncomprehension; In contrast, generation tasks follow a different trend: modality\nalignment increases in the early layers but decreases in the deep layers to\nrecover spatial details. These divergent alignment patterns create a\nfundamental conflict in fully shared Transformer backbones, where a uniform\nrepresentational flow often leads to performance compromises across two tasks.\nMotivated by this finding, we introduce UniFork, a novel Y-shaped architecture\nthat shares the shallow layers for cross-task representation learning, while\nemploying task-specific branches in deeper layers to avoid task interference.\nThis design effectively balances shared learning and task specialization.\nThrough extensive ablation experiments, we demonstrate that Unifork\nconsistently outperforms conventional fully shared Transformer architectures,\nand achieves performance on par with or better than task-specific models.",
        "url": "http://arxiv.org/abs/2506.17202v1",
        "published_date": "2025-06-20T17:52:31+00:00",
        "updated_date": "2025-06-20T17:52:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Teng Li",
            "Quanfeng Lu",
            "Lirui Zhao",
            "Hao Li",
            "Xizhou Zhu",
            "Yu Qiao",
            "Jun Zhang",
            "Wenqi Shao"
        ],
        "tldr": "The paper introduces UniFork, a Y-shaped Transformer architecture for unified multimodal understanding and generation, addressing the conflict between modality alignment requirements of understanding and generation tasks by using shared shallow layers and task-specific deeper branches.",
        "tldr_zh": "该论文介绍了UniFork，一种用于统一多模态理解和生成的Y型Transformer架构。该架构通过共享浅层，并为理解任务和生成任务使用独立分支的方式，解决了理解任务和生成任务对模态对齐的不同需求导致的任务冲突。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]