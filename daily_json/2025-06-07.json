[
    {
        "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis",
        "summary": "We present STARFlow, a scalable generative model based on normalizing flows\nthat achieves strong performance in high-resolution image synthesis. The core\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\nexpressive power of normalizing flows with the structured modeling capabilities\nof Autoregressive Transformers. We first establish the theoretical universality\nof TARFlow for modeling continuous distributions. Building on this foundation,\nwe introduce several key architectural and algorithmic innovations to\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\nTransformer block captures most of the model representational capacity,\ncomplemented by a few shallow Transformer blocks that are computationally\nefficient yet substantially beneficial; (2) modeling in the latent space of\npretrained autoencoders, which proves more effective than direct pixel-level\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\nexact maximum likelihood training in continuous spaces without discretization.\nSTARFlow achieves competitive performance in both class-conditional and\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\nmodels in sample quality. To our knowledge, this work is the first successful\ndemonstration of normalizing flows operating effectively at this scale and\nresolution.",
        "url": "http://arxiv.org/abs/2506.06276v1",
        "published_date": "2025-06-06T17:58:39+00:00",
        "updated_date": "2025-06-06T17:58:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiatao Gu",
            "Tianrong Chen",
            "David Berthelot",
            "Huangjie Zheng",
            "Yuyang Wang",
            "Ruixiang Zhang",
            "Laurent Dinh",
            "Miguel Angel Bautista",
            "Josh Susskind",
            "Shuangfei Zhai"
        ],
        "tldr": "STARFlow introduces a scalable normalizing flow model using Transformer Autoregressive Flows (TARFlow) in the latent space of autoencoders, achieving competitive high-resolution image synthesis results.",
        "tldr_zh": "STARFlow 引入了一种可扩展的归一化流模型，它在自编码器的潜在空间中使用 Transformer 自回归流 (TARFlow)，实现了具有竞争力的高分辨率图像合成结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FocusDiff: Advancing Fine-Grained Text-Image Alignment for Autoregressive Visual Generation through RL",
        "summary": "Recent studies extend the autoregression paradigm to text-to-image\ngeneration, achieving performance comparable to diffusion models. However, our\nnew PairComp benchmark -- featuring test cases of paired prompts with similar\nsyntax but different fine-grained semantics -- reveals that existing models\nstruggle with fine-grained text-image alignment thus failing to realize precise\ncontrol over visual tokens. To address this, we propose FocusDiff, which\nenhances fine-grained text-image semantic alignment by focusing on subtle\ndifferences between similar text-image pairs. We construct a new dataset of\npaired texts and images with similar overall expressions but distinct local\nsemantics, further introducing a novel reinforcement learning algorithm to\nemphasize such fine-grained semantic differences for desired image generation.\nOur approach achieves state-of-the-art performance on existing text-to-image\nbenchmarks and significantly outperforms prior methods on PairComp.",
        "url": "http://arxiv.org/abs/2506.05501v1",
        "published_date": "2025-06-05T18:36:33+00:00",
        "updated_date": "2025-06-05T18:36:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaihang Pan",
            "Wendong Bu",
            "Yuruo Wu",
            "Yang Wu",
            "Kai Shen",
            "Yunfei Li",
            "Hang Zhao",
            "Juncheng Li",
            "Siliang Tang",
            "Yueting Zhuang"
        ],
        "tldr": "FocusDiff uses reinforcement learning to improve fine-grained text-image alignment in autoregressive image generation, addressing limitations identified by a new benchmark called PairComp.",
        "tldr_zh": "FocusDiff使用强化学习来提高自回归图像生成中细粒度的文本-图像对齐，解决了新的PairComp基准测试中发现的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]