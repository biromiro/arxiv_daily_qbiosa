[
    {
        "title": "Scene-Aware Location Modeling for Data Augmentation in Automotive Object Detection",
        "summary": "Generative image models are increasingly being used for training data\naugmentation in vision tasks. In the context of automotive object detection,\nmethods usually focus on producing augmented frames that look as realistic as\npossible, for example by replacing real objects with generated ones. Others try\nto maximize the diversity of augmented frames, for example by pasting lots of\ngenerated objects onto existing backgrounds. Both perspectives pay little\nattention to the locations of objects in the scene. Frame layouts are either\nreused with little or no modification, or they are random and disregard realism\nentirely. In this work, we argue that optimal data augmentation should also\ninclude realistic augmentation of layouts. We introduce a scene-aware\nprobabilistic location model that predicts where new objects can realistically\nbe placed in an existing scene. By then inpainting objects in these locations\nwith a generative model, we obtain much stronger augmentation performance than\nexisting approaches. We set a new state of the art for generative data\naugmentation on two automotive object detection tasks, achieving up to\n$2.8\\times$ higher gains than the best competing approach ($+1.4$ vs. $+0.5$\nmAP boost). We also demonstrate significant improvements for instance\nsegmentation.",
        "url": "http://arxiv.org/abs/2504.17076v1",
        "published_date": "2025-04-23T19:52:47+00:00",
        "updated_date": "2025-04-23T19:52:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jens Petersen",
            "Davide Abati",
            "Amirhossein Habibian",
            "Auke Wiggers"
        ],
        "tldr": "this paper introduces a scene-aware probabilistic location model for data augmentation in automotive object detection, which improves object placement realism and achieves state-of-the-art results in object detection and instance segmentation tasks.",
        "tldr_zh": "本文介绍了一种场景感知的概率位置模型，用于汽车物体检测中的数据增强，该模型提高了物体放置的真实性，并在物体检测和实例分割任务中取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Distilling semantically aware orders for autoregressive image generation",
        "summary": "Autoregressive patch-based image generation has recently shown competitive\nresults in terms of image quality and scalability. It can also be easily\nintegrated and scaled within Vision-Language models. Nevertheless,\nautoregressive models require a defined order for patch generation. While a\nnatural order based on the dictation of the words makes sense for text\ngeneration, there is no inherent generation order that exists for image\ngeneration. Traditionally, a raster-scan order (from top-left to bottom-right)\nguides autoregressive image generation models. In this paper, we argue that\nthis order is suboptimal, as it fails to respect the causality of the image\ncontent: for instance, when conditioned on a visual description of a sunset, an\nautoregressive model may generate clouds before the sun, even though the color\nof clouds should depend on the color of the sun and not the inverse. In this\nwork, we show that first by training a model to generate patches in\nany-given-order, we can infer both the content and the location (order) of each\npatch during generation. Secondly, we use these extracted orders to finetune\nthe any-given-order model to produce better-quality images. Through our\nexperiments, we show on two datasets that this new generation method produces\nbetter images than the traditional raster-scan approach, with similar training\ncosts and no extra annotations.",
        "url": "http://arxiv.org/abs/2504.17069v1",
        "published_date": "2025-04-23T19:33:58+00:00",
        "updated_date": "2025-04-23T19:33:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Rishav Pramanik",
            "Antoine Poupon",
            "Juan A. Rodriguez",
            "Masih Aminbeidokhti",
            "David Vazquez",
            "Christopher Pal",
            "Zhaozheng Yin",
            "Marco Pedersoli"
        ],
        "tldr": "this paper proposes a semantically aware patch generation order for autoregressive image generation, showing improved image quality over raster-scan order by learning content and location dependencies.",
        "tldr_zh": "该论文提出了一种语义感知的自回归图像生成中的图像块生成顺序， 通过学习内容和位置依赖关系，相较于光栅扫描顺序，图像质量有所提高。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at the Speed of Light",
        "summary": "Many sparse attention mechanisms such as Neighborhood Attention have\ntypically failed to consistently deliver speedup over the self attention\nbaseline. This is largely due to the level of complexity in attention\ninfrastructure, and the rapid evolution of AI hardware architecture. At the\nsame time, many state-of-the-art foundational models, particularly in computer\nvision, are heavily bound by attention, and need reliable sparsity to escape\nthe O(n^2) complexity. In this paper, we study a class of promising sparse\nattention mechanisms that focus on locality, and aim to develop a better\nanalytical model of their performance improvements. We first introduce\nGeneralized Neighborhood Attention (GNA), which can describe sliding window,\nstrided sliding window, and blocked attention. We then consider possible design\nchoices in implementing these approaches, and create a simulator that can\nprovide much more realistic speedup upper bounds for any given setting.\nFinally, we implement GNA on top of a state-of-the-art fused multi-headed\nattention (FMHA) kernel designed for the NVIDIA Blackwell architecture in\nCUTLASS. Our implementation can fully realize the maximum speedup theoretically\npossible in many perfectly block-sparse cases, and achieves an effective\nutilization of 1.3 petaFLOPs/second in FP16. In addition, we plug various GNA\nconfigurations into off-the-shelf generative models, such as Cosmos-7B,\nHunyuanVideo, and FLUX, and show that it can deliver 28% to 46% end-to-end\nspeedup on B200 without any fine-tuning. We will open source our simulator and\nBlackwell kernels directly through the NATTEN project.",
        "url": "http://arxiv.org/abs/2504.16922v1",
        "published_date": "2025-04-23T17:49:53+00:00",
        "updated_date": "2025-04-23T17:49:53+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Ali Hassani",
            "Fengzhe Zhou",
            "Aditya Kane",
            "Jiannan Huang",
            "Chieh-Yun Chen",
            "Min Shi",
            "Steven Walton",
            "Markus Hoehnerbach",
            "Vijay Thakkar",
            "Michael Isaev",
            "Qinsheng Zhang",
            "Bing Xu",
            "Haicheng Wu",
            "Wen-mei Hwu",
            "Ming-Yu Liu",
            "Humphrey Shi"
        ],
        "tldr": "the paper introduces generalized neighborhood attention (gna), a fast and efficient sparse attention mechanism, and demonstrates its speedup in generative models on nvidia blackwell architecture; code and simulator will be open-sourced.",
        "tldr_zh": "该论文介绍了广义邻域注意力（gna），一种快速高效的稀疏注意力机制，并展示了其在nvidia blackwell架构上的生成模型中的加速效果; 代码和模拟器将开源。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "DreamO: A Unified Framework for Image Customization",
        "summary": "Recently, extensive research on image customization (e.g., identity, subject,\nstyle, background, etc.) demonstrates strong customization capabilities in\nlarge-scale generative models. However, most approaches are designed for\nspecific tasks, restricting their generalizability to combine different types\nof condition. Developing a unified framework for image customization remains an\nopen challenge. In this paper, we present DreamO, an image customization\nframework designed to support a wide range of tasks while facilitating seamless\nintegration of multiple conditions. Specifically, DreamO utilizes a diffusion\ntransformer (DiT) framework to uniformly process input of different types.\nDuring training, we construct a large-scale training dataset that includes\nvarious customization tasks, and we introduce a feature routing constraint to\nfacilitate the precise querying of relevant information from reference images.\nAdditionally, we design a placeholder strategy that associates specific\nplaceholders with conditions at particular positions, enabling control over the\nplacement of conditions in the generated results. Moreover, we employ a\nprogressive training strategy consisting of three stages: an initial stage\nfocused on simple tasks with limited data to establish baseline consistency, a\nfull-scale training stage to comprehensively enhance the customization\ncapabilities, and a final quality alignment stage to correct quality biases\nintroduced by low-quality data. Extensive experiments demonstrate that the\nproposed DreamO can effectively perform various image customization tasks with\nhigh quality and flexibly integrate different types of control conditions.",
        "url": "http://arxiv.org/abs/2504.16915v1",
        "published_date": "2025-04-23T17:41:44+00:00",
        "updated_date": "2025-04-23T17:41:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chong Mou",
            "Yanze Wu",
            "Wenxu Wu",
            "Zinan Guo",
            "Pengze Zhang",
            "Yufeng Cheng",
            "Yiming Luo",
            "Fei Ding",
            "Shiwen Zhang",
            "Xinghui Li",
            "Mengtian Li",
            "Songtao Zhao",
            "Jian Zhang",
            "Qian He",
            "Xinglong Wu"
        ],
        "tldr": "dreamo is a unified image customization framework using a diffusion transformer and a multi-stage training approach to handle a wide range of image customization tasks and seamlessly integrate multiple conditions.",
        "tldr_zh": "dreamo是一个统一的图像定制框架，它使用扩散transformer和多阶段训练方法来处理各种图像定制任务，并无缝集成多个条件。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation",
        "summary": "Text-to-video (T2V) generative models have rapidly advanced and found\nwidespread applications across fields like entertainment, education, and\nmarketing. However, the adversarial vulnerabilities of these models remain\nrarely explored. We observe that in T2V generation tasks, the generated videos\noften contain substantial redundant information not explicitly specified in the\ntext prompts, such as environmental elements, secondary objects, and additional\ndetails, providing opportunities for malicious attackers to embed hidden\nharmful content. Exploiting this inherent redundancy, we introduce BadVideo,\nthe first backdoor attack framework tailored for T2V generation. Our attack\nfocuses on designing target adversarial outputs through two key strategies: (1)\nSpatio-Temporal Composition, which combines different spatiotemporal features\nto encode malicious information; (2) Dynamic Element Transformation, which\nintroduces transformations in redundant elements over time to convey malicious\ninformation. Based on these strategies, the attacker's malicious target\nseamlessly integrates with the user's textual instructions, providing high\nstealthiness. Moreover, by exploiting the temporal dimension of videos, our\nattack successfully evades traditional content moderation systems that\nprimarily analyze spatial information within individual frames. Extensive\nexperiments demonstrate that BadVideo achieves high attack success rates while\npreserving original semantics and maintaining excellent performance on clean\ninputs. Overall, our work reveals the adversarial vulnerability of T2V models,\ncalling attention to potential risks and misuse. Our project page is at\nhttps://wrt2000.github.io/BadVideo2025/.",
        "url": "http://arxiv.org/abs/2504.16907v1",
        "published_date": "2025-04-23T17:34:48+00:00",
        "updated_date": "2025-04-23T17:34:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ruotong Wang",
            "Mingli Zhu",
            "Jiarong Ou",
            "Rui Chen",
            "Xin Tao",
            "Pengfei Wan",
            "Baoyuan Wu"
        ],
        "tldr": "the paper introduces badvideo, a novel backdoor attack framework that exploits redundant information in text-to-video generation to embed hidden malicious content, bypassing traditional content moderation.",
        "tldr_zh": "该论文介绍了一种新颖的后门攻击框架 badvideo，它利用文本到视频生成中固有的冗余信息来嵌入隐藏的恶意内容，从而绕过传统的的内容审核机制。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Advanced Chest X-Ray Analysis via Transformer-Based Image Descriptors and Cross-Model Attention Mechanism",
        "summary": "The examination of chest X-ray images is a crucial component in detecting\nvarious thoracic illnesses. This study introduces a new image description\ngeneration model that integrates a Vision Transformer (ViT) encoder with\ncross-modal attention and a GPT-4-based transformer decoder. The ViT captures\nhigh-quality visual features from chest X-rays, which are fused with text data\nthrough cross-modal attention to improve the accuracy, context, and richness of\nimage descriptions. The GPT-4 decoder transforms these fused features into\naccurate and relevant captions. The model was tested on the National Institutes\nof Health (NIH) and Indiana University (IU) Chest X-ray datasets. On the IU\ndataset, it achieved scores of 0.854 (B-1), 0.883 (CIDEr), 0.759 (METEOR), and\n0.712 (ROUGE-L). On the NIH dataset, it achieved the best performance on all\nmetrics: BLEU 1--4 (0.825, 0.788, 0.765, 0.752), CIDEr (0.857), METEOR (0.726),\nand ROUGE-L (0.705). This framework has the potential to enhance chest X-ray\nevaluation, assisting radiologists in more precise and efficient diagnosis.",
        "url": "http://arxiv.org/abs/2504.16774v1",
        "published_date": "2025-04-23T14:46:10+00:00",
        "updated_date": "2025-04-23T14:46:10+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Lakshita Agarwal",
            "Bindu Verma"
        ],
        "tldr": "this paper introduces a transformer-based model using vit, cross-modal attention, and a gpt-4 decoder for generating image descriptions of chest x-rays, achieving state-of-the-art results on nih and iu datasets.",
        "tldr_zh": "本文介绍了一种基于transformer的模型，该模型使用vit、跨模态注意力和gpt-4解码器来生成胸部x光片的图像描述，并在nih和iu数据集上取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Tri-FusionNet: Enhancing Image Description Generation with Transformer-based Fusion Network and Dual Attention Mechanism",
        "summary": "Image description generation is essential for accessibility and AI\nunderstanding of visual content. Recent advancements in deep learning have\nsignificantly improved natural language processing and computer vision. In this\nwork, we propose Tri-FusionNet, a novel image description generation model that\nintegrates transformer modules: a Vision Transformer (ViT) encoder module with\ndual-attention mechanism, a Robustly Optimized BERT Approach (RoBERTa) decoder\nmodule, and a Contrastive Language-Image Pre-Training (CLIP) integrating\nmodule. The ViT encoder, enhanced with dual attention, focuses on relevant\nspatial regions and linguistic context, improving image feature extraction. The\nRoBERTa decoder is employed to generate precise textual descriptions. CLIP's\nintegrating module aligns visual and textual data through contrastive learning,\nensuring effective combination of both modalities. This fusion of ViT, RoBERTa,\nand CLIP, along with dual attention, enables the model to produce more\naccurate, contextually rich, and flexible descriptions. The proposed framework\ndemonstrated competitive performance on the Flickr30k and Flickr8k datasets,\nwith BLEU scores ranging from 0.767 to 0.456 and 0.784 to 0.479, CIDEr scores\nof 1.679 and 1.483, METEOR scores of 0.478 and 0.358, and ROUGE-L scores of\n0.567 and 0.789, respectively. On MS-COCO, the framework obtained BLEU scores\nof 0.893 (B-1), 0.821 (B-2), 0.794 (B-3), and 0.725 (B-4). The results\ndemonstrate the effectiveness of Tri-FusionNet in generating high-quality image\ndescriptions.",
        "url": "http://arxiv.org/abs/2504.16761v1",
        "published_date": "2025-04-23T14:33:29+00:00",
        "updated_date": "2025-04-23T14:33:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lakshita Agarwal",
            "Bindu Verma"
        ],
        "tldr": "the paper introduces tri-fusionnet, a novel image description generation model leveraging vit, roberta, and clip with dual attention, achieving competitive performance on standard datasets.",
        "tldr_zh": "该论文介绍了tri-fusionnet，一种新颖的图像描述生成模型，它利用vit、roberta和clip以及双重注意力机制，在标准数据集上取得了有竞争力的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Latent Video Dataset Distillation",
        "summary": "Dataset distillation has demonstrated remarkable effectiveness in\nhigh-compression scenarios for image datasets. While video datasets inherently\ncontain greater redundancy, existing video dataset distillation methods\nprimarily focus on compression in the pixel space, overlooking advances in the\nlatent space that have been widely adopted in modern text-to-image and\ntext-to-video models. In this work, we bridge this gap by introducing a novel\nvideo dataset distillation approach that operates in the latent space using a\nstate-of-the-art variational encoder. Furthermore, we employ a diversity-aware\ndata selection strategy to select both representative and diverse samples.\nAdditionally, we introduce a simple, training-free method to further compress\nthe distilled latent dataset. By combining these techniques, our approach\nachieves a new state-of-the-art performance in dataset distillation,\noutperforming prior methods on all datasets, e.g. on HMDB51 IPC 1, we achieve a\n2.6% performance increase; on MiniUCF IPC 5, we achieve a 7.8% performance\nincrease.",
        "url": "http://arxiv.org/abs/2504.17132v1",
        "published_date": "2025-04-23T22:50:39+00:00",
        "updated_date": "2025-04-23T22:50:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ning Li",
            "Antai Andy Liu",
            "Jingran Zhang",
            "Justin Cui"
        ],
        "tldr": "this paper presents a novel video dataset distillation approach that operates in the latent space using a variational encoder and diversity-aware data selection, achieving state-of-the-art performance.",
        "tldr_zh": "该论文提出了一种新的视频数据集提炼方法，该方法利用变分编码器和多样性感知数据选择在潜在空间中运行，实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "PPS-Ctrl: Controllable Sim-to-Real Translation for Colonoscopy Depth Estimation",
        "summary": "Accurate depth estimation enhances endoscopy navigation and diagnostics, but\nobtaining ground-truth depth in clinical settings is challenging. Synthetic\ndatasets are often used for training, yet the domain gap limits generalization\nto real data. We propose a novel image-to-image translation framework that\npreserves structure while generating realistic textures from clinical data. Our\nkey innovation integrates Stable Diffusion with ControlNet, conditioned on a\nlatent representation extracted from a Per-Pixel Shading (PPS) map. PPS\ncaptures surface lighting effects, providing a stronger structural constraint\nthan depth maps. Experiments show our approach produces more realistic\ntranslations and improves depth estimation over GAN-based MI-CycleGAN. Our code\nis publicly accessible at https://github.com/anaxqx/PPS-Ctrl.",
        "url": "http://arxiv.org/abs/2504.17067v1",
        "published_date": "2025-04-23T19:28:58+00:00",
        "updated_date": "2025-04-23T19:28:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinqi Xiong",
            "Andrea Dunn Beltran",
            "Jun Myeong Choi",
            "Marc Niethammer",
            "Roni Sengupta"
        ],
        "tldr": "the paper introduces a novel image-to-image translation framework, pps-ctrl, using stable diffusion and controlnet conditioned on per-pixel shading maps to generate realistic colonoscopy images from synthetic data for improved depth estimation.",
        "tldr_zh": "该论文介绍了一种新颖的图像到图像翻译框架pps-ctrl，它使用stable diffusion和controlnet，以逐像素着色图为条件，从合成数据生成逼真的结肠镜图像，从而提高深度估计。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ePBR: Extended PBR Materials in Image Synthesis",
        "summary": "Realistic indoor or outdoor image synthesis is a core challenge in computer\nvision and graphics. The learning-based approach is easy to use but lacks\nphysical consistency, while traditional Physically Based Rendering (PBR) offers\nhigh realism but is computationally expensive. Intrinsic image representation\noffers a well-balanced trade-off, decomposing images into fundamental\ncomponents (intrinsic channels) such as geometry, materials, and illumination\nfor controllable synthesis. However, existing PBR materials struggle with\ncomplex surface models, particularly high-specular and transparent surfaces. In\nthis work, we extend intrinsic image representations to incorporate both\nreflection and transmission properties, enabling the synthesis of transparent\nmaterials such as glass and windows. We propose an explicit intrinsic\ncompositing framework that provides deterministic, interpretable image\nsynthesis. With the Extended PBR (ePBR) Materials, we can effectively edit the\nmaterials with precise controls.",
        "url": "http://arxiv.org/abs/2504.17062v1",
        "published_date": "2025-04-23T19:15:42+00:00",
        "updated_date": "2025-04-23T19:15:42+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Yu Guo",
            "Zhiqiang Lao",
            "Xiyun Song",
            "Yubin Zhou",
            "Zongfang Lin",
            "Heather Yu"
        ],
        "tldr": "this paper introduces an extension to pbr materials within intrinsic image representations (epbr) to better handle complex, transparent surfaces in image synthesis, offering more control during material editing.",
        "tldr_zh": "本文介绍了一种在固有图像表示中对 pbr 材料的扩展（epbr），以更好地处理图像合成中复杂的透明表面，并在材料编辑期间提供更多控制。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "High-Quality Cloud-Free Optical Image Synthesis Using Multi-Temporal SAR and Contaminated Optical Data",
        "summary": "Addressing gaps caused by cloud cover and the long revisit cycle of\nsatellites is vital for providing essential data to support remote sensing\napplications. This paper tackles the challenges of missing optical data\nsynthesis, particularly in complex scenarios with cloud cover. We propose\nCRSynthNet, a novel image synthesis network that incorporates innovative\ndesigned modules such as the DownUp Block and Fusion Attention to enhance\naccuracy. Experimental results validate the effectiveness of CRSynthNet,\ndemonstrating substantial improvements in restoring structural details,\npreserving spectral consist, and achieving superior visual effects that far\nexceed those produced by comparison methods. It achieves quantitative\nimprovements across multiple metrics: a peak signal-to-noise ratio (PSNR) of\n26.978, a structural similarity index measure (SSIM) of 0.648, and a root mean\nsquare error (RMSE) of 0.050. Furthermore, this study creates the TCSEN12\ndataset, a valuable resource specifically designed to address cloud cover\nchallenges in missing optical data synthesis study. The dataset uniquely\nincludes cloud-covered images and leverages earlier image to predict later\nimage, offering a realistic representation of real-world scenarios. This study\noffer practical method and valuable resources for optical satellite image\nsynthesis task.",
        "url": "http://arxiv.org/abs/2504.16870v1",
        "published_date": "2025-04-23T16:44:53+00:00",
        "updated_date": "2025-04-23T16:44:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenxi Duan"
        ],
        "tldr": "this paper introduces crsynthnet, a novel image synthesis network for generating high-quality, cloud-free optical images by leveraging multi-temporal sar and contaminated optical data, along with a new dataset (tcsen12) designed for this task.",
        "tldr_zh": "本文介绍了crsynthnet，一种新颖的图像合成网络，通过利用多时相sar和受污染的光学数据生成高质量的无云光学图像，并为此任务设计了一个新的数据集（tcsen12）。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Seeing The Words: Evaluating AI-generated Biblical Art",
        "summary": "The past years witnessed a significant amount of Artificial Intelligence (AI)\ntools that can generate images from texts. This triggers the discussion of\nwhether AI can generate accurate images using text from the Bible with respect\nto the corresponding biblical contexts and backgrounds. Despite some existing\nattempts at a small scale, little work has been done to systematically evaluate\nthese generated images. In this work, we provide a large dataset of over 7K\nimages using biblical text as prompts. These images were evaluated with\nmultiple neural network-based tools on various aspects. We provide an\nassessment of accuracy and some analysis from the perspective of religion and\naesthetics. Finally, we discuss the use of the generated images and reflect on\nthe performance of the AI generators.",
        "url": "http://arxiv.org/abs/2504.16974v1",
        "published_date": "2025-04-23T16:11:55+00:00",
        "updated_date": "2025-04-23T16:11:55+00:00",
        "categories": [
            "cs.CY",
            "cs.CV",
            "cs.MM",
            "I.4.8; I.4.0; I.3.3; I.3.0"
        ],
        "authors": [
            "Hidde Makimei",
            "Shuai Wang",
            "Willem van Peursen"
        ],
        "tldr": "this paper introduces a dataset of ai-generated biblical art and evaluates the accuracy and aesthetics of these images using neural network-based tools, discussing potential uses and performance of ai generators in this context.",
        "tldr_zh": "本文介绍了一个由ai生成的圣经艺术数据集，并使用基于神经网络的工具评估这些图像的准确性和美学，讨论了ai生成器在此背景下的潜在用途和性能。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs",
        "summary": "We present DyMU, an efficient, training-free framework that dynamically\nreduces the computational burden of vision-language models (VLMs) while\nmaintaining high task performance. Our approach comprises two key components.\nFirst, Dynamic Token Merging (DToMe) reduces the number of visual token\nembeddings by merging similar tokens based on image complexity, addressing the\ninherent inefficiency of fixed-length outputs in vision transformers. Second,\nVirtual Token Unmerging (VTU) simulates the expected token sequence for large\nlanguage models (LLMs) by efficiently reconstructing the attention dynamics of\na full sequence, thus preserving the downstream performance without additional\nfine-tuning. Unlike previous approaches, our method dynamically adapts token\ncompression to the content of the image and operates completely training-free,\nmaking it readily applicable to most state-of-the-art VLM architectures.\nExtensive experiments on image and video understanding tasks demonstrate that\nDyMU can reduce the average visual token count by 32%-85% while achieving\ncomparable performance to full-length models across diverse VLM architectures,\nincluding the recently popularized AnyRes-based visual encoders. Furthermore,\nthrough qualitative analyses, we demonstrate that DToMe effectively adapts\ntoken reduction based on image complexity and, unlike existing systems,\nprovides users more control over computational costs. Project page:\nhttps://mikewangwzhl.github.io/dymu/.",
        "url": "http://arxiv.org/abs/2504.17040v1",
        "published_date": "2025-04-23T18:38:18+00:00",
        "updated_date": "2025-04-23T18:38:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhenhailong Wang",
            "Senthil Purushwalkam",
            "Caiming Xiong",
            "Silvio Savarese",
            "Heng Ji",
            "Ran Xu"
        ],
        "tldr": "the paper introduces dymu, a training-free framework for efficient vlms that dynamically reduces visual tokens via merging and virtual unmerging, achieving comparable performance with significantly fewer tokens.",
        "tldr_zh": "该论文介绍了 dymu，一个高效的 vlm 框架，无需训练，通过动态合并和虚拟解合并视觉 tokens 来减少计算量，并以明显更少的 tokens 实现了可比的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "RouteWinFormer: A Route-Window Transformer for Middle-range Attention in Image Restoration",
        "summary": "Transformer models have recently garnered significant attention in image\nrestoration due to their ability to capture long-range pixel dependencies.\nHowever, long-range attention often results in computational overhead without\npractical necessity, as degradation and context are typically localized.\nNormalized average attention distance across various degradation datasets shows\nthat middle-range attention is enough for image restoration. Building on this\ninsight, we propose RouteWinFormer, a novel window-based Transformer that\nmodels middle-range context for image restoration. RouteWinFormer incorporates\nRoute-Windows Attnetion Module, which dynamically selects relevant nearby\nwindows based on regional similarity for attention aggregation, extending the\nreceptive field to a mid-range size efficiently. In addition, we introduce\nMulti-Scale Structure Regularization during training, enabling the sub-scale of\nthe U-shaped network to focus on structural information, while the\noriginal-scale learns degradation patterns based on generalized image structure\npriors. Extensive experiments demonstrate that RouteWinFormer outperforms\nstate-of-the-art methods across 9 datasets in various image restoration tasks.",
        "url": "http://arxiv.org/abs/2504.16637v1",
        "published_date": "2025-04-23T11:57:22+00:00",
        "updated_date": "2025-04-23T11:57:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qifan Li",
            "Tianyi Liang",
            "Xingtao Wang",
            "Xiaopeng Fan"
        ],
        "tldr": "routewinformer, a novel window-based transformer, is proposed for image restoration, using a route-windows attention module and multi-scale structure regularization to achieve state-of-the-art results across various datasets.",
        "tldr_zh": "本文提出了一种名为routewinformer的新型基于窗口的transformer，用于图像修复。它采用route-windows注意力模块和多尺度结构正则化，在各种数据集上实现了最先进的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "TraveLLaMA: Facilitating Multi-modal Large Language Models to Understand Urban Scenes and Provide Travel Assistance",
        "summary": "Tourism and travel planning increasingly rely on digital assistance, yet\nexisting multimodal AI systems often lack specialized knowledge and contextual\nunderstanding of urban environments. We present TraveLLaMA, a specialized\nmultimodal language model designed for urban scene understanding and travel\nassistance. Our work addresses the fundamental challenge of developing\npractical AI travel assistants through a novel large-scale dataset of 220k\nquestion-answer pairs. This comprehensive dataset uniquely combines 130k text\nQA pairs meticulously curated from authentic travel forums with GPT-enhanced\nresponses, alongside 90k vision-language QA pairs specifically focused on map\nunderstanding and scene comprehension. Through extensive fine-tuning\nexperiments on state-of-the-art vision-language models (LLaVA, Qwen-VL,\nShikra), we demonstrate significant performance improvements ranging from\n6.5\\%-9.4\\% in both pure text travel understanding and visual question\nanswering tasks. Our model exhibits exceptional capabilities in providing\ncontextual travel recommendations, interpreting map locations, and\nunderstanding place-specific imagery while offering practical information such\nas operating hours and visitor reviews. Comparative evaluations show TraveLLaMA\nsignificantly outperforms general-purpose models in travel-specific tasks,\nestablishing a new benchmark for multi-modal travel assistance systems.",
        "url": "http://arxiv.org/abs/2504.16505v1",
        "published_date": "2025-04-23T08:32:25+00:00",
        "updated_date": "2025-04-23T08:32:25+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Meng Chu",
            "Yukang Chen",
            "Haokun Gui",
            "Shaozuo Yu",
            "Yi Wang",
            "Jiaya Jia"
        ],
        "tldr": "travellama is a specialized multimodal language model for travel assistance, trained on a large-scale dataset and demonstrating improvements in urban scene understanding and travel recommendations.",
        "tldr_zh": "travellama 是一种专门用于旅行辅助的多模态语言模型，它在一个大型数据集上进行训练，并在城市场景理解和旅行建议方面展现出了提升。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "RGB-D Video Object Segmentation via Enhanced Multi-store Feature Memory",
        "summary": "The RGB-Depth (RGB-D) Video Object Segmentation (VOS) aims to integrate the\nfine-grained texture information of RGB with the spatial geometric clues of\ndepth modality, boosting the performance of segmentation. However,\noff-the-shelf RGB-D segmentation methods fail to fully explore cross-modal\ninformation and suffer from object drift during long-term prediction. In this\npaper, we propose a novel RGB-D VOS method via multi-store feature memory for\nrobust segmentation. Specifically, we design the hierarchical modality\nselection and fusion, which adaptively combines features from both modalities.\nAdditionally, we develop a segmentation refinement module that effectively\nutilizes the Segmentation Anything Model (SAM) to refine the segmentation mask,\nensuring more reliable results as memory to guide subsequent segmentation\ntasks. By leveraging spatio-temporal embedding and modality embedding, mixed\nprompts and fused images are fed into SAM to unleash its potential in RGB-D\nVOS. Experimental results show that the proposed method achieves\nstate-of-the-art performance on the latest RGB-D VOS benchmark.",
        "url": "http://arxiv.org/abs/2504.16471v1",
        "published_date": "2025-04-23T07:31:37+00:00",
        "updated_date": "2025-04-23T07:31:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Boyue Xu",
            "Ruichao Hou",
            "Tongwei Ren",
            "Gangshan Wu"
        ],
        "tldr": "this paper presents a new rgb-d video object segmentation method using a multi-store feature memory and sam for robust segmentation, achieving state-of-the-art performance.",
        "tldr_zh": "本文提出了一种新的rgb-d视频对象分割方法，该方法使用多存储特征记忆和sam进行稳健分割，并实现了最先进的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "Automating tumor-infiltrating lymphocyte assessment in breast cancer histopathology images using QuPath: a transparent and accessible machine learning pipeline",
        "summary": "In this study, we built an end-to-end tumor-infiltrating lymphocytes (TILs)\nassessment pipeline within QuPath, demonstrating the potential of easily\naccessible tools to perform complex tasks in a fully automatic fashion. First,\nwe trained a pixel classifier to segment tumor, tumor-associated stroma, and\nother tissue compartments in breast cancer H&E-stained whole-slide images (WSI)\nto isolate tumor-associated stroma for subsequent analysis. Next, we applied a\npre-trained StarDist deep learning model in QuPath for cell detection and used\nthe extracted cell features to train a binary classifier distinguishing TILs\nfrom other cells. To evaluate our TILs assessment pipeline, we calculated the\nTIL density in each WSI and categorized them as low, medium, or high TIL\nlevels. Our pipeline was evaluated against pathologist-assigned TIL scores,\nachieving a Cohen's kappa of 0.71 on the external test set, corroborating\nprevious research findings. These results confirm that existing software can\noffer a practical solution for the assessment of TILs in H&E-stained WSIs of\nbreast cancer.",
        "url": "http://arxiv.org/abs/2504.16979v1",
        "published_date": "2025-04-23T17:54:59+00:00",
        "updated_date": "2025-04-23T17:54:59+00:00",
        "categories": [
            "q-bio.QM",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Masoud Tafavvoghi",
            "Lars Ailo Bongo",
            "André Berli Delgado",
            "Nikita Shvetsov",
            "Anders Sildnes",
            "Line Moi",
            "Lill-Tove Rasmussen Busund",
            "Kajsa Møllersen"
        ],
        "tldr": "this paper presents an automated pipeline within qupath for assessing tumor-infiltrating lymphocytes (tils) in breast cancer histopathology images, achieving a cohen's kappa of 0.71 against pathologist scores.",
        "tldr_zh": "本文介绍了一个在qupath中自动评估乳腺癌组织病理图像中肿瘤浸润淋巴细胞（tils）的流水线，与病理学家评分相比，cohen's kappa值为0.71。",
        "relevance_score": 2,
        "novelty_claim_score": 5,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    },
    {
        "title": "Federated EndoViT: Pretraining Vision Transformers via Federated Learning on Endoscopic Image Collections",
        "summary": "Purpose: In this study, we investigate the training of foundation models\nusing federated learning to address data-sharing limitations and enable\ncollaborative model training without data transfer for minimally invasive\nsurgery. Methods: Inspired by the EndoViT study, we adapt the Masked\nAutoencoder for federated learning, enhancing it with adaptive Sharpness-Aware\nMinimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is\npretrained on the Endo700k dataset collection and later fine-tuned and\nevaluated for tasks such as Semantic Segmentation, Action Triplet Recognition,\nand Surgical Phase Recognition. Results: Our findings demonstrate that\nintegrating adaptive FedSAM into the federated MAE approach improves\npretraining, leading to a reduction in reconstruction loss per patch. The\napplication of FL-EndoViT in surgical downstream tasks results in performance\ncomparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over\nCEN-EndoViT in surgical scene segmentation when data is limited and in action\ntriplet recognition when large datasets are used. Conclusion: These findings\nhighlight the potential of federated learning for privacy-preserving training\nof surgical foundation models, offering a robust and generalizable solution for\nsurgical data science. Effective collaboration requires adapting federated\nlearning methods, such as the integration of FedSAM, which can accommodate the\ninherent data heterogeneity across institutions. In future, exploring FL in\nvideo-based models may enhance these capabilities by incorporating\nspatiotemporal dynamics crucial for real-world surgical environments.",
        "url": "http://arxiv.org/abs/2504.16612v1",
        "published_date": "2025-04-23T10:54:32+00:00",
        "updated_date": "2025-04-23T10:54:32+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Max Kirchner",
            "Alexander C. Jenke",
            "Sebastian Bodenstedt",
            "Fiona R. Kolbinger",
            "Oliver Saldanha",
            "Jakob N. Kather",
            "Martin Wagner",
            "Stefanie Speidel"
        ],
        "tldr": "the paper introduces federated endovit, a federated learning approach using masked autoencoders, fedsam, and swa for pretraining vision transformers on endoscopic images. it shows comparable or better performance than centralized training in surgical tasks, highlighting the potential for privacy-preserving surgical foundation models.",
        "tldr_zh": "该论文介绍了federated endovit，一种使用掩码自编码器、fedsam和swa的联邦学习方法，用于在内窥镜图像上预训练视觉transformer。它表明在外科任务中，该方法与集中式训练相比具有可比较或更好的性能，突出了保护隐私的外科基础模型的潜力。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    },
    {
        "title": "Federated Learning of Low-Rank One-Shot Image Detection Models in Edge Devices with Scalable Accuracy and Compute Complexity",
        "summary": "This paper introduces a novel federated learning framework termed LoRa-FL\ndesigned for training low-rank one-shot image detection models deployed on edge\ndevices. By incorporating low-rank adaptation techniques into one-shot\ndetection architectures, our method significantly reduces both computational\nand communication overhead while maintaining scalable accuracy. The proposed\nframework leverages federated learning to collaboratively train lightweight\nimage recognition models, enabling rapid adaptation and efficient deployment\nacross heterogeneous, resource-constrained devices. Experimental evaluations on\nthe MNIST and CIFAR10 benchmark datasets, both in an\nindependent-and-identically-distributed (IID) and non-IID setting, demonstrate\nthat our approach achieves competitive detection performance while\nsignificantly reducing communication bandwidth and compute complexity. This\nmakes it a promising solution for adaptively reducing the communication and\ncompute power overheads, while not sacrificing model accuracy.",
        "url": "http://arxiv.org/abs/2504.16515v1",
        "published_date": "2025-04-23T08:40:44+00:00",
        "updated_date": "2025-04-23T08:40:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Abdul Hannaan",
            "Zubair Shah",
            "Aiman Erbad",
            "Amr Mohamed",
            "Ali Safa"
        ],
        "tldr": "the paper introduces lora-fl, a federated learning framework employing low-rank adaptation to train lightweight one-shot image detection models on edge devices, achieving scalable accuracy with reduced communication and computation overhead.",
        "tldr_zh": "该论文介绍了 lora-fl，一个联邦学习框架，它采用低秩适应来训练边缘设备上的轻量级单次图像检测模型，以可扩展的精度和降低的通信和计算开销。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    },
    {
        "title": "CLPSTNet: A Progressive Multi-Scale Convolutional Steganography Model Integrating Curriculum Learning",
        "summary": "In recent years, a large number of works have introduced Convolutional Neural\nNetworks (CNNs) into image steganography, which transform traditional\nsteganography methods such as hand-crafted features and prior knowledge design\ninto steganography methods that neural networks autonomically learn information\nembedding. However, due to the inherent complexity of digital images, issues of\ninvisibility and security persist when using CNN models for information\nembedding. In this paper, we propose Curriculum Learning Progressive Steganophy\nNetwork (CLPSTNet). The network consists of multiple progressive multi-scale\nconvolutional modules that integrate Inception structures and dilated\nconvolutions. The module contains multiple branching pathways, starting from a\nsmaller convolutional kernel and dilatation rate, extracting the basic, local\nfeature information from the feature map, and gradually expanding to the\nconvolution with a larger convolutional kernel and dilatation rate for\nperceiving the feature information of a larger receptive field, so as to\nrealize the multi-scale feature extraction from shallow to deep, and from fine\nto coarse, allowing the shallow secret information features to be refined in\ndifferent fusion stages. The experimental results show that the proposed\nCLPSTNet not only has high PSNR , SSIM metrics and decoding accuracy on three\nlarge public datasets, ALASKA2, VOC2012 and ImageNet, but also the\nsteganographic images generated by CLPSTNet have low steganalysis scores.You\ncan find our code at\n\\href{https://github.com/chaos-boops/CLPSTNet}{https://github.com/chaos-boops/CLPSTNet}.",
        "url": "http://arxiv.org/abs/2504.16364v1",
        "published_date": "2025-04-23T02:34:25+00:00",
        "updated_date": "2025-04-23T02:34:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CR"
        ],
        "authors": [
            "Fengchun Liu",
            "Tong Zhang",
            "Chunying Zhang"
        ],
        "tldr": "the paper introduces clpstnet, a novel convolutional steganography network that uses curriculum learning and multi-scale feature extraction to improve invisibility and security in image steganography. it claims state-of-the-art performance on multiple datasets.",
        "tldr_zh": "该论文介绍了一种新的卷积隐写网络clpstnet，它利用课程学习和多尺度特征提取来提高图像隐写的不可见性和安全性。该研究声称在多个数据集上实现了最先进的性能。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 3
    }
]