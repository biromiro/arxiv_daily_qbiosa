[
    {
        "title": "Survey of Video Diffusion Models: Foundations, Implementations, and Applications",
        "summary": "Recent advances in diffusion models have revolutionized video generation,\noffering superior temporal consistency and visual quality compared to\ntraditional generative adversarial networks-based approaches. While this\nemerging field shows tremendous promise in applications, it faces significant\nchallenges in motion consistency, computational efficiency, and ethical\nconsiderations. This survey provides a comprehensive review of diffusion-based\nvideo generation, examining its evolution, technical foundations, and practical\napplications. We present a systematic taxonomy of current methodologies,\nanalyze architectural innovations and optimization strategies, and investigate\napplications across low-level vision tasks such as denoising and\nsuper-resolution. Additionally, we explore the synergies between diffusionbased\nvideo generation and related domains, including video representation learning,\nquestion answering, and retrieval. Compared to the existing surveys (Lei et\nal., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which\nfocus on specific aspects of video generation, such as human video synthesis\n(Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our\nwork provides a broader, more updated, and more fine-grained perspective on\ndiffusion-based approaches with a special section for evaluation metrics,\nindustry solutions, and training engineering techniques in video generation.\nThis survey serves as a foundational resource for researchers and practitioners\nworking at the intersection of diffusion models and video generation, providing\ninsights into both the theoretical frameworks and practical implementations\nthat drive this rapidly evolving field. A structured list of related works\ninvolved in this survey is also available on\nhttps://github.com/Eyeline-Research/Survey-Video-Diffusion.",
        "url": "http://arxiv.org/abs/2504.16081v1",
        "published_date": "2025-04-22T17:59:17+00:00",
        "updated_date": "2025-04-22T17:59:17+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Yimu Wang",
            "Xuye Liu",
            "Wei Pang",
            "Li Ma",
            "Shuai Yuan",
            "Paul Debevec",
            "Ning Yu"
        ],
        "tldr": "this survey paper provides a comprehensive overview of video diffusion models, covering their foundations, implementations, and applications, while also addressing challenges and ethical considerations. it distinguishes itself from existing surveys by offering a broader and more fine-grained perspective, with a special focus on evaluation metrics, industry solutions, and training engineering techniques.",
        "tldr_zh": "这篇综述论文全面概述了视频扩散模型，涵盖了其基础、实现和应用，同时讨论了挑战和伦理考量。它通过提供更广泛、更细致的视角，并特别关注评估指标、行业解决方案和训练工程技术，从而区别于现有的同类综述。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning",
        "summary": "Recent text-to-image diffusion models achieve impressive visual quality\nthrough extensive scaling of training data and model parameters, yet they often\nstruggle with complex scenes and fine-grained details. Inspired by the\nself-reflection capabilities emergent in large language models, we propose\nReflectionFlow, an inference-time framework enabling diffusion models to\niteratively reflect upon and refine their outputs. ReflectionFlow introduces\nthree complementary inference-time scaling axes: (1) noise-level scaling to\noptimize latent initialization; (2) prompt-level scaling for precise semantic\nguidance; and most notably, (3) reflection-level scaling, which explicitly\nprovides actionable reflections to iteratively assess and correct previous\ngenerations. To facilitate reflection-level scaling, we construct GenRef, a\nlarge-scale dataset comprising 1 million triplets, each containing a\nreflection, a flawed image, and an enhanced image. Leveraging this dataset, we\nefficiently perform reflection tuning on state-of-the-art diffusion\ntransformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified\nframework. Experimental results show that ReflectionFlow significantly\noutperforms naive noise-level scaling methods, offering a scalable and\ncompute-efficient solution toward higher-quality image synthesis on challenging\ntasks.",
        "url": "http://arxiv.org/abs/2504.16080v1",
        "published_date": "2025-04-22T17:58:07+00:00",
        "updated_date": "2025-04-22T17:58:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Le Zhuo",
            "Liangbing Zhao",
            "Sayak Paul",
            "Yue Liao",
            "Renrui Zhang",
            "Yi Xin",
            "Peng Gao",
            "Mohamed Elhoseiny",
            "Hongsheng Li"
        ],
        "tldr": "the paper introduces reflectionflow, an inference-time optimization framework for text-to-image diffusion models, leveraging self-reflection and a tuned diffusion transformer on a large-scale dataset to enhance image quality, especially for complex scenes.",
        "tldr_zh": "该论文介绍了reflectionflow，一个用于文本到图像扩散模型的推理时优化框架，它利用自反思和一个在大型数据集上调整的扩散转换器来提高图像质量，尤其是在复杂场景中。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Describe Anything: Detailed Localized Image and Video Captioning",
        "summary": "Generating detailed and accurate descriptions for specific regions in images\nand videos remains a fundamental challenge for vision-language models. We\nintroduce the Describe Anything Model (DAM), a model designed for detailed\nlocalized captioning (DLC). DAM preserves both local details and global context\nthrough two key innovations: a focal prompt, which ensures high-resolution\nencoding of targeted regions, and a localized vision backbone, which integrates\nprecise localization with its broader context. To tackle the scarcity of\nhigh-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data\nPipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and\nexpands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark\ndesigned to evaluate DLC without relying on reference captions. DAM sets new\nstate-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and\ndetailed multi-sentence localized image and video captioning.",
        "url": "http://arxiv.org/abs/2504.16072v1",
        "published_date": "2025-04-22T17:51:41+00:00",
        "updated_date": "2025-04-22T17:51:41+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Long Lian",
            "Yifan Ding",
            "Yunhao Ge",
            "Sifei Liu",
            "Hanzi Mao",
            "Boyi Li",
            "Marco Pavone",
            "Ming-Yu Liu",
            "Trevor Darrell",
            "Adam Yala",
            "Yin Cui"
        ],
        "tldr": "the paper introduces the describe anything model (dam) for detailed localized image and video captioning, utilizing a focal prompt and localized vision backbone, and a semi-supervised learning pipeline (dlc-sdp) to address data scarcity. dam achieves state-of-the-art results on multiple benchmarks.",
        "tldr_zh": "该论文介绍了 describe anything model (dam)，用于生成图像和视频的详细局部描述，利用焦点提示和局部视觉骨干网络，以及半监督学习管道 (dlc-sdp) 来解决数据稀缺问题。dam 在多个基准测试中取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis",
        "summary": "Latent diffusion models (LDMs) dominate high-quality image generation, yet\nintegrating representation learning with generative modeling remains a\nchallenge. We introduce a novel generative image modeling framework that\nseamlessly bridges this gap by leveraging a diffusion model to jointly model\nlow-level image latents (from a variational autoencoder) and high-level\nsemantic features (from a pretrained self-supervised encoder like DINO). Our\nlatent-semantic diffusion approach learns to generate coherent image-feature\npairs from pure noise, significantly enhancing both generative quality and\ntraining efficiency, all while requiring only minimal modifications to standard\nDiffusion Transformer architectures. By eliminating the need for complex\ndistillation objectives, our unified design simplifies training and unlocks a\npowerful new inference strategy: Representation Guidance, which leverages\nlearned semantics to steer and refine image generation. Evaluated in both\nconditional and unconditional settings, our method delivers substantial\nimprovements in image quality and training convergence speed, establishing a\nnew direction for representation-aware generative modeling.",
        "url": "http://arxiv.org/abs/2504.16064v1",
        "published_date": "2025-04-22T17:41:42+00:00",
        "updated_date": "2025-04-22T17:41:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Theodoros Kouzelis",
            "Efstathios Karypidis",
            "Ioannis Kakogeorgiou",
            "Spyros Gidaris",
            "Nikos Komodakis"
        ],
        "tldr": "this paper introduces a latent-semantic diffusion model that jointly models image latents and semantic features using a diffusion model to improve image generation quality and training efficiency. it uses representation guidance for image refinement.",
        "tldr_zh": "该论文介绍了一种潜在语义扩散模型，该模型使用扩散模型联合建模图像潜在变量和语义特征，以提高图像生成质量和训练效率。它使用表征引导进行图像细化。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Temporal Consistency in Diffusion-Based Video Editing with Adaptor Modules: A Theoretical Framework",
        "summary": "Adapter-based methods are commonly used to enhance model performance with\nminimal additional complexity, especially in video editing tasks that require\nframe-to-frame consistency. By inserting small, learnable modules into\npretrained diffusion models, these adapters can maintain temporal coherence\nwithout extensive retraining. Approaches that incorporate prompt learning with\nboth shared and frame-specific tokens are particularly effective in preserving\ncontinuity across frames at low training cost. In this work, we want to provide\na general theoretical framework for adapters that maintain frame consistency in\nDDIM-based models under a temporal consistency loss. First, we prove that the\ntemporal consistency objective is differentiable under bounded feature norms,\nand we establish a Lipschitz bound on its gradient. Second, we show that\ngradient descent on this objective decreases the loss monotonically and\nconverges to a local minimum if the learning rate is within an appropriate\nrange. Finally, we analyze the stability of modules in the DDIM inversion\nprocedure, showing that the associated error remains controlled. These\ntheoretical findings will reinforce the reliability of diffusion-based video\nediting methods that rely on adapter strategies and provide theoretical\ninsights in video generation tasks.",
        "url": "http://arxiv.org/abs/2504.16016v1",
        "published_date": "2025-04-22T16:28:35+00:00",
        "updated_date": "2025-04-22T16:28:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinyuan Song",
            "Yangfan He",
            "Sida Li",
            "Jianhui Wang",
            "Hongyang He",
            "Xinhang Yuan",
            "Ruoyu Wang",
            "Jiaqi Chen",
            "Keqin Li",
            "Kuan Lu",
            "Menghao Huo",
            "Binxu Li",
            "Pei Liu"
        ],
        "tldr": "this paper provides a theoretical framework for adapter-based diffusion models that maintain temporal consistency in video editing, proving differentiability, convergence, and stability.",
        "tldr_zh": "本文为基于适配器的扩散模型提供了一个理论框架，该框架在视频编辑中保持时间一致性，证明了可微性、收敛性和稳定性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "FreeGraftor: Training-Free Cross-Image Feature Grafting for Subject-Driven Text-to-Image Generation",
        "summary": "Subject-driven image generation aims to synthesize novel scenes that\nfaithfully preserve subject identity from reference images while adhering to\ntextual guidance, yet existing methods struggle with a critical trade-off\nbetween fidelity and efficiency. Tuning-based approaches rely on time-consuming\nand resource-intensive subject-specific optimization, while zero-shot methods\nfail to maintain adequate subject consistency. In this work, we propose\nFreeGraftor, a training-free framework that addresses these limitations through\ncross-image feature grafting. Specifically, FreeGraftor employs semantic\nmatching and position-constrained attention fusion to transfer visual details\nfrom reference subjects to the generated image. Additionally, our framework\nincorporates a novel noise initialization strategy to preserve geometry priors\nof reference subjects for robust feature matching. Extensive qualitative and\nquantitative experiments demonstrate that our method enables precise subject\nidentity transfer while maintaining text-aligned scene synthesis. Without\nrequiring model fine-tuning or additional training, FreeGraftor significantly\noutperforms existing zero-shot and training-free approaches in both subject\nfidelity and text alignment. Furthermore, our framework can seamlessly extend\nto multi-subject generation, making it practical for real-world deployment. Our\ncode is available at https://github.com/Nihukat/FreeGraftor.",
        "url": "http://arxiv.org/abs/2504.15958v1",
        "published_date": "2025-04-22T14:55:23+00:00",
        "updated_date": "2025-04-22T14:55:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zebin Yao",
            "Lei Ren",
            "Huixing Jiang",
            "Chen Wei",
            "Xiaojie Wang",
            "Ruifan Li",
            "Fangxiang Feng"
        ],
        "tldr": "freegraftor is a training-free method for subject-driven text-to-image generation that uses cross-image feature grafting to improve subject fidelity and text alignment without fine-tuning. it also preserves the geometry prior of the reference subject.",
        "tldr_zh": "freegraftor是一种无需训练的主题驱动文本到图像生成方法，它使用跨图像特征嫁接来提高主题保真度和文本对齐，而无需微调。它还保留了参考对象的几何先验。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reasoning Physical Video Generation with Diffusion Timestep Tokens via Reinforcement Learning",
        "summary": "Despite recent progress in video generation, producing videos that adhere to\nphysical laws remains a significant challenge. Traditional diffusion-based\nmethods struggle to extrapolate to unseen physical conditions (eg, velocity)\ndue to their reliance on data-driven approximations. To address this, we\npropose to integrate symbolic reasoning and reinforcement learning to enforce\nphysical consistency in video generation. We first introduce the Diffusion\nTimestep Tokenizer (DDT), which learns discrete, recursive visual tokens by\nrecovering visual attributes lost during the diffusion process. The recursive\nvisual tokens enable symbolic reasoning by a large language model. Based on it,\nwe propose the Phys-AR framework, which consists of two stages: The first stage\nuses supervised fine-tuning to transfer symbolic knowledge, while the second\nstage applies reinforcement learning to optimize the model's reasoning\nabilities through reward functions based on physical conditions. Our approach\nallows the model to dynamically adjust and improve the physical properties of\ngenerated videos, ensuring adherence to physical laws. Experimental results\ndemonstrate that PhysAR can generate videos that are physically consistent.",
        "url": "http://arxiv.org/abs/2504.15932v1",
        "published_date": "2025-04-22T14:20:59+00:00",
        "updated_date": "2025-04-22T14:20:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wang Lin",
            "Liyu Jia",
            "Wentao Hu",
            "Kaihang Pan",
            "Zhongqi Yue",
            "Wei Zhao",
            "Jingyuan Chen",
            "Fei Wu",
            "Hanwang Zhang"
        ],
        "tldr": "the paper introduces phys-ar, a framework combining diffusion models, symbolic reasoning (via llms), and reinforcement learning to generate physically consistent videos. it uses diffusion timestep tokens (ddt) to incorporate attributes and improve on data-driven limitations of diffusion models.",
        "tldr_zh": "该论文介绍了一种名为phys-ar的框架，它结合了扩散模型、符号推理（通过llm）和强化学习来生成符合物理规律的视频。它使用扩散时间步长令牌（ddt）来整合属性，并改进了扩散模型在数据驱动方面的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Satellite to GroundScape -- Large-scale Consistent Ground View Generation from Satellite Views",
        "summary": "Generating consistent ground-view images from satellite imagery is\nchallenging, primarily due to the large discrepancies in viewing angles and\nresolution between satellite and ground-level domains. Previous efforts mainly\nconcentrated on single-view generation, often resulting in inconsistencies\nacross neighboring ground views. In this work, we propose a novel cross-view\nsynthesis approach designed to overcome these challenges by ensuring\nconsistency across ground-view images generated from satellite views. Our\nmethod, based on a fixed latent diffusion model, introduces two conditioning\nmodules: satellite-guided denoising, which extracts high-level scene layout to\nguide the denoising process, and satellite-temporal denoising, which captures\ncamera motion to maintain consistency across multiple generated views. We\nfurther contribute a large-scale satellite-ground dataset containing over\n100,000 perspective pairs to facilitate extensive ground scene or video\ngeneration. Experimental results demonstrate that our approach outperforms\nexisting methods on perceptual and temporal metrics, achieving high\nphotorealism and consistency in multi-view outputs.",
        "url": "http://arxiv.org/abs/2504.15786v1",
        "published_date": "2025-04-22T10:58:42+00:00",
        "updated_date": "2025-04-22T10:58:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ningli Xu",
            "Rongjun Qin"
        ],
        "tldr": "this paper introduces a novel cross-view synthesis approach using a diffusion model with satellite-guided and satellite-temporal denoising to generate consistent ground-view images from satellite imagery, accompanied by a new large-scale dataset.",
        "tldr_zh": "该论文提出了一种新颖的跨视角合成方法，使用扩散模型和卫星引导及卫星时间去噪技术，从卫星图像生成一致的地面图像，并伴随一个新的大规模数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Structure-Preserving Zero-Shot Image Editing via Stage-Wise Latent Injection in Diffusion Models",
        "summary": "We propose a diffusion-based framework for zero-shot image editing that\nunifies text-guided and reference-guided approaches without requiring\nfine-tuning. Our method leverages diffusion inversion and timestep-specific\nnull-text embeddings to preserve the structural integrity of the source image.\nBy introducing a stage-wise latent injection strategy-shape injection in early\nsteps and attribute injection in later steps-we enable precise, fine-grained\nmodifications while maintaining global consistency. Cross-attention with\nreference latents facilitates semantic alignment between the source and\nreference. Extensive experiments across expression transfer, texture\ntransformation, and style infusion demonstrate state-of-the-art performance,\nconfirming the method's scalability and adaptability to diverse image editing\nscenarios.",
        "url": "http://arxiv.org/abs/2504.15723v1",
        "published_date": "2025-04-22T09:18:16+00:00",
        "updated_date": "2025-04-22T09:18:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dasol Jeong",
            "Donggoo Kang",
            "Jiwon Park",
            "Hyebean Lee",
            "Joonki Paik"
        ],
        "tldr": "this paper presents a diffusion-based zero-shot image editing framework that combines text-guided and reference-guided approaches using stage-wise latent injection to preserve structural integrity while modifying image attributes.",
        "tldr_zh": "本文提出了一种基于扩散模型的零样本图像编辑框架，该框架结合了文本引导和参考引导方法，通过阶段式潜在注入来保持结构完整性，同时修改图像属性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vidi: Large Multimodal Models for Video Understanding and Editing",
        "summary": "Humans naturally share information with those they are connected to, and\nvideo has become one of the dominant mediums for communication and expression\non the Internet. To support the creation of high-quality large-scale video\ncontent, a modern pipeline requires a comprehensive understanding of both the\nraw input materials (e.g., the unedited footage captured by cameras) and the\nediting components (e.g., visual effects). In video editing scenarios, models\nmust process multiple modalities (e.g., vision, audio, text) with strong\nbackground knowledge and handle flexible input lengths (e.g., hour-long raw\nvideos), which poses significant challenges for traditional models. In this\nreport, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a\nwide range of video understand editing scenarios. The first release focuses on\ntemporal retrieval, i.e., identifying the time ranges within the input videos\ncorresponding to a given text query, which plays a critical role in intelligent\nediting. The model is capable of processing hour-long videos with strong\ntemporal understanding capability, e.g., retrieve time ranges for certain\nqueries. To support a comprehensive evaluation in real-world scenarios, we also\npresent the VUE-TR benchmark, which introduces five key advancements. 1) Video\nduration: significantly longer than videos of existing temporal retrival\ndatasets, 2) Audio support: includes audio-based queries, 3) Query format:\ndiverse query lengths/formats, 4) Annotation quality: ground-truth time ranges\nare manually annotated. 5) Evaluation metric: a refined IoU metric to support\nevaluation over multiple time ranges. Remarkably, Vidi significantly\noutperforms leading proprietary models, e.g., GPT-4o and Gemini, on the\ntemporal retrieval task, indicating its superiority in video editing scenarios.",
        "url": "http://arxiv.org/abs/2504.15681v2",
        "published_date": "2025-04-22T08:04:45+00:00",
        "updated_date": "2025-04-24T05:36:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Vidi Team",
            "Celong Liu",
            "Chia-Wen Kuo",
            "Dawei Du",
            "Fan Chen",
            "Guang Chen",
            "Jiamin Yuan",
            "Lingxi Zhang",
            "Lu Guo",
            "Lusha Li",
            "Longyin Wen",
            "Qingyu Chen",
            "Rachel Deng",
            "Sijie Zhu",
            "Stuart Siew",
            "Tong Jin",
            "Wei Lu",
            "Wen Zhong",
            "Xiaohui Shen",
            "Xin Gu",
            "Xing Mei",
            "Xueqiong Qu"
        ],
        "tldr": "the paper introduces vidi, a family of large multimodal models (lmms) designed for video understanding and editing, specifically addressing temporal retrieval in long videos and demonstrating superior performance compared to existing models like gpt-4o and gemini. it also introduces vue-tr, a new benchmark for video temporal retrieval.",
        "tldr_zh": "该论文介绍了vidi，一种用于视频理解和编辑的大型多模态模型（lmms），专门解决长视频中的时序检索问题，并展示了优于现有模型（如gpt-4o和gemini）的性能。同时，还介绍了一个新的视频时序检索基准 vue-tr。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DiTPainter: Efficient Video Inpainting with Diffusion Transformers",
        "summary": "Many existing video inpainting algorithms utilize optical flows to construct\nthe corresponding maps and then propagate pixels from adjacent frames to\nmissing areas by mapping. Despite the effectiveness of the propagation\nmechanism, they might encounter blurry and inconsistencies when dealing with\ninaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT)\nhas emerged as a revolutionary technique for video generation tasks. However,\npretrained DiT models for video generation all contain a large amount of\nparameters, which makes it very time consuming to apply to video inpainting\ntasks. In this paper, we present DiTPainter, an end-to-end video inpainting\nmodel based on Diffusion Transformer (DiT). DiTPainter uses an efficient\ntransformer network designed for video inpainting, which is trained from\nscratch instead of initializing from any large pretrained models. DiTPainter\ncan address videos with arbitrary lengths and can be applied to video\ndecaptioning and video completion tasks with an acceptable time cost.\nExperiments show that DiTPainter outperforms existing video inpainting\nalgorithms with higher quality and better spatial-temporal consistency.",
        "url": "http://arxiv.org/abs/2504.15661v1",
        "published_date": "2025-04-22T07:36:45+00:00",
        "updated_date": "2025-04-22T07:36:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xian Wu",
            "Chang Liu"
        ],
        "tldr": "ditpainter is a new diffusion transformer based video inpainting model that's trained from scratch, achieving improved quality and consistency compared to existing methods while addressing limitations of optical flow-based approaches.",
        "tldr_zh": "ditpainter是一种新的基于diffusion transformer的视频修复模型，该模型从头开始训练，与现有方法相比，提高了质量和一致性，同时解决了基于光流方法存在的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "AdaViP: Aligning Multi-modal LLMs via Adaptive Vision-enhanced Preference Optimization",
        "summary": "Preference alignment through Direct Preference Optimization (DPO) has\ndemonstrated significant effectiveness in aligning multimodal large language\nmodels (MLLMs) with human preferences. However, existing methods focus\nprimarily on language preferences while neglecting the critical visual context.\nIn this paper, we propose an Adaptive Vision-enhanced Preference optimization\n(AdaViP) that addresses these limitations through two key innovations: (1)\nvision-based preference pair construction, which integrates multiple visual\nfoundation models to strategically remove key visual elements from the image,\nenhancing MLLMs' sensitivity to visual details; and (2) adaptive preference\noptimization that dynamically balances vision- and language-based preferences\nfor more accurate alignment. Extensive evaluations across different benchmarks\ndemonstrate our effectiveness. Notably, our AdaViP-7B achieves 93.7% and 96.4%\nreductions in response-level and mentioned-level hallucination respectively on\nthe Object HalBench, significantly outperforming current state-of-the-art\nmethods.",
        "url": "http://arxiv.org/abs/2504.15619v1",
        "published_date": "2025-04-22T06:19:38+00:00",
        "updated_date": "2025-04-22T06:19:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinda Lu",
            "Jinghan Li",
            "Yuan Gao",
            "Junkang Wu",
            "Jiancan Wu",
            "Xiang Wang",
            "Xiangnan He"
        ],
        "tldr": "adavip enhances multimodal llm alignment by adaptively balancing vision and language preferences and strategically removing visual elements to improve sensitivity to visual details, leading to significant reductions in object hallucination.",
        "tldr_zh": "adavip通过自适应地平衡视觉和语言偏好，并策略性地移除视觉元素以提高对视觉细节的敏感度，从而增强多模态llm的对齐，显著减少了对象幻觉。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Emergence and Evolution of Interpretable Concepts in Diffusion Models",
        "summary": "Diffusion models have become the go-to method for text-to-image generation,\nproducing high-quality images from noise through a process called reverse\ndiffusion. Understanding the dynamics of the reverse diffusion process is\ncrucial in steering the generation and achieving high sample quality. However,\nthe inner workings of diffusion models is still largely a mystery due to their\nblack-box nature and complex, multi-step generation process. Mechanistic\nInterpretability (MI) techniques, such as Sparse Autoencoders (SAEs), aim at\nuncovering the operating principles of models through granular analysis of\ntheir internal representations. These MI techniques have been successful in\nunderstanding and steering the behavior of large language models at scale.\nHowever, the great potential of SAEs has not yet been applied toward gaining\ninsight into the intricate generative process of diffusion models. In this\nwork, we leverage the SAE framework to probe the inner workings of a popular\ntext-to-image diffusion model, and uncover a variety of human-interpretable\nconcepts in its activations. Interestingly, we find that even before the first\nreverse diffusion step is completed, the final composition of the scene can be\npredicted surprisingly well by looking at the spatial distribution of activated\nconcepts. Moreover, going beyond correlational analysis, we show that the\ndiscovered concepts have a causal effect on the model output and can be\nleveraged to steer the generative process. We design intervention techniques\naimed at manipulating image composition and style, and demonstrate that (1) in\nearly stages of diffusion image composition can be effectively controlled, (2)\nin the middle stages of diffusion image composition is finalized, however\nstylistic interventions are effective, and (3) in the final stages of diffusion\nonly minor textural details are subject to change.",
        "url": "http://arxiv.org/abs/2504.15473v1",
        "published_date": "2025-04-21T22:48:37+00:00",
        "updated_date": "2025-04-21T22:48:37+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "eess.IV",
            "I.2.6; I.2.10"
        ],
        "authors": [
            "Berk Tinaz",
            "Zalan Fabian",
            "Mahdi Soltanolkotabi"
        ],
        "tldr": "this paper applies mechanistic interpretability (mi) techniques, specifically sparse autoencoders (saes), to diffusion models, revealing interpretable concepts and demonstrating their causal effect on image generation, enabling controlled steering of the generative process.",
        "tldr_zh": "本文将机制可解释性（mi）技术，特别是稀疏自编码器（sae），应用于扩散模型，揭示了可解释的概念，并展示了它们对图像生成的因果效应，从而能够控制生成过程。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Manifold Induced Biases for Zero-shot and Few-shot Detection of Generated Images",
        "summary": "Distinguishing between real and AI-generated images, commonly referred to as\n'image detection', presents a timely and significant challenge. Despite\nextensive research in the (semi-)supervised regime, zero-shot and few-shot\nsolutions have only recently emerged as promising alternatives. Their main\nadvantage is in alleviating the ongoing data maintenance, which quickly becomes\noutdated due to advances in generative technologies. We identify two main gaps:\n(1) a lack of theoretical grounding for the methods, and (2) significant room\nfor performance improvements in zero-shot and few-shot regimes. Our approach is\nfounded on understanding and quantifying the biases inherent in generated\ncontent, where we use these quantities as criteria for characterizing generated\nimages. Specifically, we explore the biases of the implicit probability\nmanifold, captured by a pre-trained diffusion model. Through score-function\nanalysis, we approximate the curvature, gradient, and bias towards points on\nthe probability manifold, establishing criteria for detection in the zero-shot\nregime. We further extend our contribution to the few-shot setting by employing\na mixture-of-experts methodology. Empirical results across 20 generative models\ndemonstrate that our method outperforms current approaches in both zero-shot\nand few-shot settings. This work advances the theoretical understanding and\npractical usage of generated content biases through the lens of manifold\nanalysis.",
        "url": "http://arxiv.org/abs/2504.15470v1",
        "published_date": "2025-04-21T22:39:24+00:00",
        "updated_date": "2025-04-21T22:39:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jonathan Brokman",
            "Amit Giloni",
            "Omer Hofman",
            "Roman Vainshtein",
            "Hisashi Kojima",
            "Guy Gilboa"
        ],
        "tldr": "this paper introduces a zero-shot and few-shot method for detecting ai-generated images by analyzing biases in the implicit probability manifold captured by pre-trained diffusion models, achieving superior performance compared to existing methods.",
        "tldr_zh": "本文提出了一种零样本和少样本方法，通过分析预训练扩散模型捕获的隐式概率流形中的偏差来检测ai生成的图像，与现有方法相比，实现了更优越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Physics Driven Image Simulation from Commercial Satellite Imagery",
        "summary": "Physics driven image simulation allows for the modeling and creation of\nrealistic imagery beyond what is afforded by typical rendering pipelines. We\naim to automatically generate a physically realistic scene for simulation of a\ngiven region using satellite imagery to model the scene geometry, drive\nmaterial estimates, and populate the scene with dynamic elements. We present\nautomated techniques to utilize satellite imagery throughout the simulated\nscene to expedite scene construction and decrease manual overhead. Our\ntechnique does not use lidar, enabling simulations that could not be\nconstructed previously. To develop a 3D scene, we model the various components\nof the real location, addressing the terrain, modelling man-made structures,\nand populating the scene with smaller elements such as vegetation and vehicles.\nTo create the scene we begin with a Digital Surface Model, which serves as the\nbasis for scene geometry, and allows us to reason about the real location in a\ncommon 3D frame of reference. These simulated scenes can provide increased\nfidelity with less manual intervention for novel locations on earth, and can\nfacilitate algorithm development, and processing pipelines for imagery ranging\nfrom UV to LWIR $(200nm-20\\mu m)$.",
        "url": "http://arxiv.org/abs/2504.15378v1",
        "published_date": "2025-04-21T18:38:00+00:00",
        "updated_date": "2025-04-21T18:38:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Scott Sorensen",
            "Wayne Treible",
            "Robert Wagner",
            "Andrew D. Gilliam",
            "Todd Rovito",
            "Joseph L. Mundy"
        ],
        "tldr": "this paper presents an automated method for generating physically realistic 3d scenes from satellite imagery, without relying on lidar, for applications such as algorithm development and imagery processing across a wide spectrum of wavelengths.",
        "tldr_zh": "本文提出了一种利用卫星图像自动生成物理上逼真的3d场景的方法，无需依赖激光雷达，可用于算法开发和图像处理等应用，并涵盖广泛的波长范围。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention",
        "summary": "The integration of long-context capabilities with visual understanding\nunlocks unprecedented potential for Vision Language Models (VLMs). However, the\nquadratic attention complexity during the pre-filling phase remains a\nsignificant obstacle to real-world deployment. To overcome this limitation, we\nintroduce MMInference (Multimodality Million tokens Inference), a dynamic\nsparse attention method that accelerates the prefilling stage for long-context\nmulti-modal inputs. First, our analysis reveals that the temporal and spatial\nlocality of video input leads to a unique sparse pattern, the Grid pattern.\nSimultaneously, VLMs exhibit markedly different sparse distributions across\ndifferent modalities. We introduce a permutation-based method to leverage the\nunique Grid pattern and handle modality boundary issues. By offline search the\noptimal sparse patterns for each head, MMInference constructs the sparse\ndistribution dynamically based on the input. We also provide optimized GPU\nkernels for efficient sparse computations. Notably, MMInference integrates\nseamlessly into existing VLM pipelines without any model modifications or\nfine-tuning. Experiments on multi-modal benchmarks-including Video QA,\nCaptioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art\nlong-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that\nMMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while\nmaintaining accuracy. Our code is available at https://aka.ms/MMInference.",
        "url": "http://arxiv.org/abs/2504.16083v1",
        "published_date": "2025-04-22T17:59:51+00:00",
        "updated_date": "2025-04-22T17:59:51+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yucheng Li",
            "Huiqiang Jiang",
            "Chengruidong Zhang",
            "Qianhui Wu",
            "Xufang Luo",
            "Surin Ahn",
            "Amir H. Abdi",
            "Dongsheng Li",
            "Jianfeng Gao",
            "Yuqing Yang",
            "Lili Qiu"
        ],
        "tldr": "the paper introduces mminference, a modality-aware sparse attention method that accelerates the pre-filling stage for long-context vlms by leveraging the temporal and spatial locality of video input without model modification, achieving up to 8.3x speedup at 1m tokens while maintaining accuracy.",
        "tldr_zh": "本文介绍了mminference，一种模态感知的稀疏注意力方法，通过利用视频输入的时间和空间局部性来加速长上下文vlms的预填充阶段，无需模型修改，在1m tokens的情况下实现了高达8.3倍的加速，同时保持了准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting",
        "summary": "We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a\nsystem to summarise hour long videos with no-supervision. Most existing video\nunderstanding models work well on short videos of pre-segmented events, yet\nthey struggle to summarise longer videos where relevant events are sparsely\ndistributed and not pre-segmented. Moreover, long-form video understanding\noften relies on supervised hierarchical training that needs extensive\nannotations which are costly, slow and prone to inconsistency. With ViSMaP we\nbridge the gap between short videos (where annotated data is plentiful) and\nlong ones (where it's not). We rely on LLMs to create optimised\npseudo-summaries of long videos using segment descriptions from short ones.\nThese pseudo-summaries are used as training data for a model that generates\nlong-form video summaries, bypassing the need for expensive annotations of long\nvideos. Specifically, we adopt a meta-prompting strategy to iteratively\ngenerate and refine creating pseudo-summaries of long videos. The strategy\nleverages short clip descriptions obtained from a supervised short video model\nto guide the summary. Each iteration uses three LLMs working in sequence: one\nto generate the pseudo-summary from clip descriptions, another to evaluate it,\nand a third to optimise the prompt of the generator. This iteration is\nnecessary because the quality of the pseudo-summaries is highly dependent on\nthe generator prompt, and varies widely among videos. We evaluate our summaries\nextensively on multiple datasets; our results show that ViSMaP achieves\nperformance comparable to fully supervised state-of-the-art models while\ngeneralising across domains without sacrificing performance. Code will be\nreleased upon publication.",
        "url": "http://arxiv.org/abs/2504.15921v1",
        "published_date": "2025-04-22T14:06:01+00:00",
        "updated_date": "2025-04-22T14:06:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jian Hu",
            "Dimitrios Korkinof",
            "Shaogang Gong",
            "Mariano Beguerisse-Diaz"
        ],
        "tldr": "the paper introduces vismap, an unsupervised video summarization system using meta-prompting and llms to generate pseudo-summaries of long videos from short clip descriptions, achieving performance comparable to supervised methods.",
        "tldr_zh": "该论文介绍了vismap，一种无监督视频摘要系统，它使用元提示和大型语言模型从短视频片段描述生成长视频的伪摘要，性能与监督方法相当。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VLM-based Prompts as the Optimal Assistant for Unpaired Histopathology Virtual Staining",
        "summary": "In histopathology, tissue sections are typically stained using common H&E\nstaining or special stains (MAS, PAS, PASM, etc.) to clearly visualize specific\ntissue structures. The rapid advancement of deep learning offers an effective\nsolution for generating virtually stained images, significantly reducing the\ntime and labor costs associated with traditional histochemical staining.\nHowever, a new challenge arises in separating the fundamental visual\ncharacteristics of tissue sections from the visual differences induced by\nstaining agents. Additionally, virtual staining often overlooks essential\npathological knowledge and the physical properties of staining, resulting in\nonly style-level transfer. To address these issues, we introduce, for the first\ntime in virtual staining tasks, a pathological vision-language large model\n(VLM) as an auxiliary tool. We integrate contrastive learnable prompts,\nfoundational concept anchors for tissue sections, and staining-specific concept\nanchors to leverage the extensive knowledge of the pathological VLM. This\napproach is designed to describe, frame, and enhance the direction of virtual\nstaining. Furthermore, we have developed a data augmentation method based on\nthe constraints of the VLM. This method utilizes the VLM's powerful image\ninterpretation capabilities to further integrate image style and structural\ninformation, proving beneficial in high-precision pathological diagnostics.\nExtensive evaluations on publicly available multi-domain unpaired staining\ndatasets demonstrate that our method can generate highly realistic images and\nenhance the accuracy of downstream tasks, such as glomerular detection and\nsegmentation. Our code is available at:\nhttps://github.com/CZZZZZZZZZZZZZZZZZ/VPGAN-HARBOR",
        "url": "http://arxiv.org/abs/2504.15545v1",
        "published_date": "2025-04-22T02:46:13+00:00",
        "updated_date": "2025-04-22T02:46:13+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Zizhi Chen",
            "Xinyu Zhang",
            "Minghao Han",
            "Yizhou Liu",
            "Ziyun Qian",
            "Weifeng Zhang",
            "Xukun Zhang",
            "Jingwei Wei",
            "Lihua Zhang"
        ],
        "tldr": "this paper introduces a novel virtual staining method for histopathology images using a vision-language large model (vlm) with contrastive learnable prompts and vlm-based data augmentation, achieving realistic image generation and improved downstream task performance.",
        "tldr_zh": "本文提出了一种新颖的组织病理学图像虚拟染色方法，该方法使用视觉-语言大模型（vlm），结合对比学习提示和基于vlm的数据增强，实现了逼真的图像生成和下游任务性能的提升。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "InstaRevive: One-Step Image Enhancement via Dynamic Score Matching",
        "summary": "Image enhancement finds wide-ranging applications in real-world scenarios due\nto complex environments and the inherent limitations of imaging devices. Recent\ndiffusion-based methods yield promising outcomes but necessitate prolonged and\ncomputationally intensive iterative sampling. In response, we propose\nInstaRevive, a straightforward yet powerful image enhancement framework that\nemploys score-based diffusion distillation to harness potent generative\ncapability and minimize the sampling steps. To fully exploit the potential of\nthe pre-trained diffusion model, we devise a practical and effective diffusion\ndistillation pipeline using dynamic control to address inaccuracies in updating\ndirection during score matching. Our control strategy enables a dynamic\ndiffusing scope, facilitating precise learning of denoising trajectories within\nthe diffusion model and ensuring accurate distribution matching gradients\nduring training. Additionally, to enrich guidance for the generative power, we\nincorporate textual prompts via image captioning as auxiliary conditions,\nfostering further exploration of the diffusion model. Extensive experiments\nsubstantiate the efficacy of our framework across a diverse array of\nchallenging tasks and datasets, unveiling the compelling efficacy and\nefficiency of InstaRevive in delivering high-quality and visually appealing\nresults. Code is available at https://github.com/EternalEvan/InstaRevive.",
        "url": "http://arxiv.org/abs/2504.15513v1",
        "published_date": "2025-04-22T01:19:53+00:00",
        "updated_date": "2025-04-22T01:19:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yixuan Zhu",
            "Haolin Wang",
            "Ao Li",
            "Wenliang Zhao",
            "Yansong Tang",
            "Jingxuan Niu",
            "Lei Chen",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "tldr": "the paper introduces instarevive, a one-step image enhancement framework using score-based diffusion distillation with dynamic control and textual prompts to achieve efficient and high-quality results.",
        "tldr_zh": "本文介绍了一种名为instarevive的图像增强框架，该框架采用基于分数的扩散蒸馏，结合动态控制和文本提示，实现高效高质量的图像增强。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MirrorVerse: Pushing Diffusion Models to Realistically Reflect the World",
        "summary": "Diffusion models have become central to various image editing tasks, yet they\noften fail to fully adhere to physical laws, particularly with effects like\nshadows, reflections, and occlusions. In this work, we address the challenge of\ngenerating photorealistic mirror reflections using diffusion-based generative\nmodels. Despite extensive training data, existing diffusion models frequently\noverlook the nuanced details crucial to authentic mirror reflections. Recent\napproaches have attempted to resolve this by creating synhetic datasets and\nframing reflection generation as an inpainting task; however, they struggle to\ngeneralize across different object orientations and positions relative to the\nmirror. Our method overcomes these limitations by introducing key augmentations\ninto the synthetic data pipeline: (1) random object positioning, (2) randomized\nrotations, and (3) grounding of objects, significantly enhancing generalization\nacross poses and placements. To further address spatial relationships and\nocclusions in scenes with multiple objects, we implement a strategy to pair\nobjects during dataset generation, resulting in a dataset robust enough to\nhandle these complex scenarios. Achieving generalization to real-world scenes\nremains a challenge, so we introduce a three-stage training curriculum to\ndevelop the MirrorFusion 2.0 model to improve real-world performance. We\nprovide extensive qualitative and quantitative evaluations to support our\napproach. The project page is available at: https://mirror-verse.github.io/.",
        "url": "http://arxiv.org/abs/2504.15397v1",
        "published_date": "2025-04-21T19:01:02+00:00",
        "updated_date": "2025-04-21T19:01:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ankit Dhiman",
            "Manan Shah",
            "R Venkatesh Babu"
        ],
        "tldr": "the paper introduces mirrorverse, a diffusion model trained with a novel synthetic data generation pipeline and a three-stage training curriculum to improve the realism of mirror reflections, addressing limitations of existing models in handling object orientations, positions, and occlusions in complex scenes.",
        "tldr_zh": "该论文介绍了mirrorverse，一个扩散模型，通过新颖的合成数据生成流程和三阶段训练课程进行训练，以提高镜面反射的真实感，解决了现有模型在处理复杂场景中物体方向、位置和遮挡方面的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FaceInsight: A Multimodal Large Language Model for Face Perception",
        "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nstrong capabilities in understanding general visual content. However, these\ngeneral-domain MLLMs perform poorly in face perception tasks, often producing\ninaccurate or misleading responses to face-specific queries. To address this\ngap, we propose FaceInsight, the versatile face perception MLLM that provides\nfine-grained facial information. Our approach introduces visual-textual\nalignment of facial knowledge to model both uncertain dependencies and\ndeterministic relationships among facial information, mitigating the\nlimitations of language-driven reasoning. Additionally, we incorporate face\nsegmentation maps as an auxiliary perceptual modality, enriching the visual\ninput with localized structural cues to enhance semantic understanding.\nComprehensive experiments and analyses across three face perception tasks\ndemonstrate that FaceInsight consistently outperforms nine compared MLLMs under\nboth training-free and fine-tuned settings.",
        "url": "http://arxiv.org/abs/2504.15624v1",
        "published_date": "2025-04-22T06:31:57+00:00",
        "updated_date": "2025-04-22T06:31:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingzhi Li",
            "Changjiang Luo",
            "Ruoyu Chen",
            "Hua Zhang",
            "Wenqi Ren",
            "Jianhou Gan",
            "Xiaochun Cao"
        ],
        "tldr": "faceinsight, a new multimodal large language model, is proposed to address the poor performance of general mllms in face perception tasks by incorporating visual-textual alignment of facial knowledge and face segmentation maps as an auxiliary modality, demonstrating superior performance in experiments.",
        "tldr_zh": "faceinsight是一种新型多模态大型语言模型，旨在解决通用 mllm 在面部感知任务中的不良表现，通过结合面部知识的视觉-文本对齐和面部分割图作为辅助模态，实验表明该模型具有优越的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "MVQA: Mamba with Unified Sampling for Efficient Video Quality Assessment",
        "summary": "The rapid growth of long-duration, high-definition videos has made efficient\nvideo quality assessment (VQA) a critical challenge. Existing research\ntypically tackles this problem through two main strategies: reducing model\nparameters and resampling inputs. However, light-weight Convolution Neural\nNetworks (CNN) and Transformers often struggle to balance efficiency with high\nperformance due to the requirement of long-range modeling capabilities.\nRecently, the state-space model, particularly Mamba, has emerged as a promising\nalternative, offering linear complexity with respect to sequence length.\nMeanwhile, efficient VQA heavily depends on resampling long sequences to\nminimize computational costs, yet current resampling methods are often weak in\npreserving essential semantic information. In this work, we present MVQA, a\nMamba-based model designed for efficient VQA along with a novel Unified\nSemantic and Distortion Sampling (USDS) approach. USDS combines semantic patch\nsampling from low-resolution videos and distortion patch sampling from\noriginal-resolution videos. The former captures semantically dense regions,\nwhile the latter retains critical distortion details. To prevent computation\nincrease from dual inputs, we propose a fusion mechanism using pre-defined\nmasks, enabling a unified sampling strategy that captures both semantic and\nquality information without additional computational burden. Experiments show\nthat the proposed MVQA, equipped with USDS, achieve comparable performance to\nstate-of-the-art methods while being $2\\times$ as fast and requiring only $1/5$\nGPU memory.",
        "url": "http://arxiv.org/abs/2504.16003v1",
        "published_date": "2025-04-22T16:08:23+00:00",
        "updated_date": "2025-04-22T16:08:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yachun Mi",
            "Yu Li",
            "Weicheng Meng",
            "Chaofeng Chen",
            "Chen Hui",
            "Shaohui Liu"
        ],
        "tldr": "the paper introduces mvqa, a mamba-based video quality assessment model with a unified semantic and distortion sampling strategy (usds) for improved efficiency and performance.",
        "tldr_zh": "这篇论文介绍了mvqa，一个基于mamba的视频质量评估模型，采用统一的语义和失真采样策略(usds)，旨在提高效率和性能。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "AffordanceSAM: Segment Anything Once More in Affordance Grounding",
        "summary": "Improving the generalization ability of an affordance grounding model to\nrecognize regions for unseen objects and affordance functions is crucial for\nreal-world application. However, current models are still far away from such\nstandards. To address this problem, we introduce AffordanceSAM, an effective\napproach that extends SAM's generalization capacity to the domain of affordance\ngrounding. For the purpose of thoroughly transferring SAM's robust performance\nin segmentation to affordance, we initially propose an affordance-adaption\nmodule in order to help modify SAM's segmentation output to be adapted to the\nspecific functional regions required for affordance grounding. We concurrently\nmake a coarse-to-fine training recipe to make SAM first be aware of affordance\nobjects and actions coarsely, and then be able to generate affordance heatmaps\nfinely. Both quantitative and qualitative experiments show the strong\ngeneralization capacity of our AffordanceSAM, which not only surpasses previous\nmethods under AGD20K benchmark but also shows evidence to handle the task with\nnovel objects and affordance functions.",
        "url": "http://arxiv.org/abs/2504.15650v1",
        "published_date": "2025-04-22T07:16:56+00:00",
        "updated_date": "2025-04-22T07:16:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dengyang Jiang",
            "Mengmeng Wang",
            "Teli Ma",
            "Hengzhuang Li",
            "Yong liu",
            "Guang Dai",
            "Lei Zhang"
        ],
        "tldr": "the paper introduces affordancesam, an approach that extends the segment anything model (sam) for improved affordance grounding, demonstrating strong generalization on unseen objects and affordance functions.",
        "tldr_zh": "该论文介绍了affordancesam，该方法扩展了segment anything model (sam)，以改进可供性接地，并在未见过的对象和可供性函数上表现出强大的泛化能力。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "RepNet-VSR: Reparameterizable Architecture for High-Fidelity Video Super-Resolution",
        "summary": "As a fundamental challenge in visual computing, video super-resolution (VSR)\nfocuses on reconstructing highdefinition video sequences from their degraded\nlowresolution counterparts. While deep convolutional neural networks have\ndemonstrated state-of-the-art performance in spatial-temporal super-resolution\ntasks, their computationally intensive nature poses significant deployment\nchallenges for resource-constrained edge devices, particularly in real-time\nmobile video processing scenarios where power efficiency and latency\nconstraints coexist. In this work, we propose a Reparameterizable Architecture\nfor High Fidelity Video Super Resolution method, named RepNet-VSR, for\nreal-time 4x video super-resolution. On the REDS validation set, the proposed\nmodel achieves 27.79 dB PSNR when processing 180p to 720p frames in 103 ms per\n10 frames on a MediaTek Dimensity NPU. The competition results demonstrate an\nexcellent balance between restoration quality and deployment efficiency. The\nproposed method scores higher than the previous champion algorithm of MAI video\nsuper-resolution challenge.",
        "url": "http://arxiv.org/abs/2504.15649v1",
        "published_date": "2025-04-22T07:15:07+00:00",
        "updated_date": "2025-04-22T07:15:07+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Biao Wu",
            "Diankai Zhang",
            "Shaoli Liu",
            "Si Gao",
            "Chengjian Zheng",
            "Ning Wang"
        ],
        "tldr": "repnet-vsr presents a reparameterizable architecture for real-time 4x video super-resolution, achieving a good balance between restoration quality and deployment efficiency on edge devices.",
        "tldr_zh": "repnet-vsr提出了一种可重参数化的架构，用于实时4倍视频超分辨率，实现了边缘设备上恢复质量和部署效率之间的良好平衡。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "Unifying Image Counterfactuals and Feature Attributions with Latent-Space Adversarial Attacks",
        "summary": "Counterfactuals are a popular framework for interpreting machine learning\npredictions. These what if explanations are notoriously challenging to create\nfor computer vision models: standard gradient-based methods are prone to\nproduce adversarial examples, in which imperceptible modifications to image\npixels provoke large changes in predictions. We introduce a new,\neasy-to-implement framework for counterfactual images that can flexibly adapt\nto contemporary advances in generative modeling. Our method, Counterfactual\nAttacks, resembles an adversarial attack on the representation of the image\nalong a low-dimensional manifold. In addition, given an auxiliary dataset of\nimage descriptors, we show how to accompany counterfactuals with feature\nattribution that quantify the changes between the original and counterfactual\nimages. These importance scores can be aggregated into global counterfactual\nexplanations that highlight the overall features driving model predictions.\nWhile this unification is possible for any counterfactual method, it has\nparticular computational efficiency for ours. We demonstrate the efficacy of\nour approach with the MNIST and CelebA datasets.",
        "url": "http://arxiv.org/abs/2504.15479v1",
        "published_date": "2025-04-21T23:09:30+00:00",
        "updated_date": "2025-04-21T23:09:30+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Jeremy Goldwasser",
            "Giles Hooker"
        ],
        "tldr": "the paper introduces a new method, counterfactual attacks, for generating counterfactual images using latent-space adversarial attacks and demonstrates its ability to provide feature attribution, unifying counterfactual explanations for computer vision models.",
        "tldr_zh": "该论文介绍了一种名为“反事实攻击”的新方法，该方法使用潜在空间对抗性攻击生成反事实图像，并展示了其提供特征归因的能力，从而统一了针对计算机视觉模型的反事实解释。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception",
        "summary": "Recent reasoning models through test-time scaling have demonstrated that long\nchain-of-thoughts can unlock substantial performance boosts in hard reasoning\ntasks such as math and code. However, the benefit of such long thoughts for\nsystem-2 reasoning is relatively less explored in other domains such as\nperceptual tasks where shallower, system-1 reasoning seems sufficient. In this\npaper, we introduce LongPerceptualThoughts, a new synthetic dataset with 30K\nlong-thought traces for perceptual tasks. The key challenges in synthesizing\nelaborate reasoning thoughts for perceptual tasks are that off-the-shelf models\nare not yet equipped with such thinking behavior and that it is not\nstraightforward to build a reliable process verifier for perceptual tasks.\nThus, we propose a novel three-stage data synthesis framework that first\nsynthesizes verifiable multiple-choice questions from dense image descriptions,\nthen extracts simple CoTs from VLMs for those verifiable problems, and finally\nexpands those simple thoughts to elaborate long thoughts via frontier reasoning\nmodels. In controlled experiments with a strong instruction-tuned 7B model, we\ndemonstrate notable improvements over existing visual reasoning data-generation\nmethods. Our model, trained on the generated dataset, achieves an average +3.4\npoints improvement over 5 vision-centric benchmarks, including +11.8 points on\nV$^*$ Bench. Notably, despite being tuned for vision tasks, it also improves\nperformance on the text reasoning benchmark, MMLU-Pro, by +2 points.",
        "url": "http://arxiv.org/abs/2504.15362v1",
        "published_date": "2025-04-21T18:10:38+00:00",
        "updated_date": "2025-04-21T18:10:38+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Yuan-Hong Liao",
            "Sven Elflein",
            "Liu He",
            "Laura Leal-Taixé",
            "Yejin Choi",
            "Sanja Fidler",
            "David Acuna"
        ],
        "tldr": "the paper introduces longperceptualthoughts, a synthetic dataset with long-reasoning traces for perceptual tasks, and demonstrates its effectiveness in improving performance on vision and text reasoning benchmarks using a three-stage data synthesis framework.",
        "tldr_zh": "该论文介绍了 longperceptualthoughts，一个带有长推理轨迹的感知任务合成数据集，并展示了其通过三阶段数据合成框架提高视觉和文本推理基准性能的有效性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "Classification of Firn Data via Topological Features",
        "summary": "In this paper we evaluate the performance of topological features for\ngeneralizable and robust classification of firn image data, with the broader\ngoal of understanding the advantages, pitfalls, and trade-offs in topological\nfeaturization. Firn refers to layers of granular snow within glaciers that\nhaven't been compressed into ice. This compactification process imposes\ndistinct topological and geometric structure on firn that varies with depth\nwithin the firn column, making topological data analysis (TDA) a natural choice\nfor understanding the connection between depth and structure. We use two\nclasses of topological features, sublevel set features and distance transform\nfeatures, together with persistence curves, to predict sample depth from\nmicroCT images. A range of challenging training-test scenarios reveals that no\none choice of method dominates in all categories, and uncoveres a web of\ntrade-offs between accuracy, interpretability, and generalizability.",
        "url": "http://arxiv.org/abs/2504.16150v1",
        "published_date": "2025-04-22T14:33:33+00:00",
        "updated_date": "2025-04-22T14:33:33+00:00",
        "categories": [
            "cs.CV",
            "math.AT",
            "55N31, 68T45"
        ],
        "authors": [
            "Sarah Day",
            "Jesse Dimino",
            "Matt Jester",
            "Kaitlin Keegan",
            "Thomas Weighill"
        ],
        "tldr": "this paper explores using topological data analysis (tda) techniques, specifically sublevel set features, distance transform features, and persistence curves, to classify firn image data and predict sample depth. it highlights trade-offs between accuracy, interpretability, and generalizability across different methods.",
        "tldr_zh": "本文探讨了使用拓扑数据分析 (tda) 技术，特别是亚水平集特征、距离变换特征和持久性曲线，来分类粒雪图像数据并预测样本深度。它强调了不同方法在准确性、可解释性和泛化能力之间的权衡。",
        "relevance_score": 2,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4
    }
]