[
    {
        "title": "LoViC: Efficient Long Video Generation with Context Compression",
        "summary": "Despite recent advances in diffusion transformers (DiTs) for text-to-video\ngeneration, scaling to long-duration content remains challenging due to the\nquadratic complexity of self-attention. While prior efforts -- such as sparse\nattention and temporally autoregressive models -- offer partial relief, they\noften compromise temporal coherence or scalability. We introduce LoViC, a\nDiT-based framework trained on million-scale open-domain videos, designed to\nproduce long, coherent videos through a segment-wise generation process. At the\ncore of our approach is FlexFormer, an expressive autoencoder that jointly\ncompresses video and text into unified latent representations. It supports\nvariable-length inputs with linearly adjustable compression rates, enabled by a\nsingle query token design based on the Q-Former architecture. Additionally, by\nencoding temporal context through position-aware mechanisms, our model\nseamlessly supports prediction, retradiction, interpolation, and multi-shot\ngeneration within a unified paradigm. Extensive experiments across diverse\ntasks validate the effectiveness and versatility of our approach.",
        "url": "http://arxiv.org/abs/2507.12952v1",
        "published_date": "2025-07-17T09:46:43+00:00",
        "updated_date": "2025-07-17T09:46:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaxiu Jiang",
            "Wenbo Li",
            "Jingjing Ren",
            "Yuping Qiu",
            "Yong Guo",
            "Xiaogang Xu",
            "Han Wu",
            "Wangmeng Zuo"
        ],
        "tldr": "LoViC introduces a DiT-based framework with FlexFormer, an autoencoder that efficiently compresses video and text into unified latent representations for generating long, coherent videos.",
        "tldr_zh": "LoViC 引入了一个基于 DiT 的框架，其中 FlexFormer 是一种自动编码器，可将视频和文本有效地压缩为统一的潜在表示，以生成长而连贯的视频。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Taming Diffusion Transformer for Real-Time Mobile Video Generation",
        "summary": "Diffusion Transformers (DiT) have shown strong performance in video\ngeneration tasks, but their high computational cost makes them impractical for\nresource-constrained devices like smartphones, and real-time generation is even\nmore challenging. In this work, we propose a series of novel optimizations to\nsignificantly accelerate video generation and enable real-time performance on\nmobile platforms. First, we employ a highly compressed variational autoencoder\n(VAE) to reduce the dimensionality of the input data without sacrificing visual\nquality. Second, we introduce a KD-guided, sensitivity-aware tri-level pruning\nstrategy to shrink the model size to suit mobile platform while preserving\ncritical performance characteristics. Third, we develop an adversarial step\ndistillation technique tailored for DiT, which allows us to reduce the number\nof inference steps to four. Combined, these optimizations enable our model to\nachieve over 10 frames per second (FPS) generation on an iPhone 16 Pro Max,\ndemonstrating the feasibility of real-time, high-quality video generation on\nmobile devices.",
        "url": "http://arxiv.org/abs/2507.13343v1",
        "published_date": "2025-07-17T17:59:10+00:00",
        "updated_date": "2025-07-17T17:59:10+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Yushu Wu",
            "Yanyu Li",
            "Anil Kag",
            "Ivan Skorokhodov",
            "Willi Menapace",
            "Ke Ma",
            "Arpit Sahni",
            "Ju Hu",
            "Aliaksandr Siarohin",
            "Dhritiman Sagar",
            "Yanzhi Wang",
            "Sergey Tulyakov"
        ],
        "tldr": "This paper introduces optimizations for Diffusion Transformers (DiT) to achieve real-time video generation on mobile devices, using VAE compression, pruning, and distillation techniques.",
        "tldr_zh": "本文介绍了一系列针对扩散Transformer (DiT)的优化，旨在实现移动设备上的实时视频生成，方法包括VAE压缩、剪枝和蒸馏技术。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Resurrect Mask AutoRegressive Modeling for Efficient and Scalable Image Generation",
        "summary": "AutoRegressive (AR) models have made notable progress in image generation,\nwith Masked AutoRegressive (MAR) models gaining attention for their efficient\nparallel decoding. However, MAR models have traditionally underperformed when\ncompared to standard AR models. This study refines the MAR architecture to\nimprove image generation quality. We begin by evaluating various image\ntokenizers to identify the most effective one. Subsequently, we introduce an\nimproved Bidirectional LLaMA architecture by replacing causal attention with\nbidirectional attention and incorporating 2D RoPE, which together form our\nadvanced model, MaskGIL. Scaled from 111M to 1.4B parameters, MaskGIL achieves\na FID score of 3.71, matching state-of-the-art AR models in the ImageNet\n256x256 benchmark, while requiring only 8 inference steps compared to the 256\nsteps of AR models. Furthermore, we develop a text-driven MaskGIL model with\n775M parameters for generating images from text at various resolutions. Beyond\nimage generation, MaskGIL extends to accelerate AR-based generation and enable\nreal-time speech-to-image conversion. Our codes and models are available at\nhttps://github.com/synbol/MaskGIL.",
        "url": "http://arxiv.org/abs/2507.13032v1",
        "published_date": "2025-07-17T12:02:38+00:00",
        "updated_date": "2025-07-17T12:02:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yi Xin",
            "Le Zhuo",
            "Qi Qin",
            "Siqi Luo",
            "Yuewen Cao",
            "Bin Fu",
            "Yangfan He",
            "Hongsheng Li",
            "Guangtao Zhai",
            "Xiaohong Liu",
            "Peng Gao"
        ],
        "tldr": "This paper introduces MaskGIL, an improved Masked AutoRegressive model that achieves state-of-the-art image generation quality with significantly fewer inference steps compared to standard AR models and demonstrates its applicability to text-to-image and speech-to-image tasks.",
        "tldr_zh": "该论文介绍了一种改进的Masked AutoRegressive模型MaskGIL，它以显著少于传统AR模型的推理步骤实现了最先进的图像生成质量，并展示了其在文本到图像和语音到图像任务中的适用性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models",
        "summary": "This paper focuses on monolithic Multimodal Large Language Models (MLLMs),\nwhich integrate visual encoding and language decoding into a single model.\nExisting structures and pre-training strategies for monolithic MLLMs often\nsuffer from unstable optimization and catastrophic forgetting. To address these\nchallenges, our key idea is to embed a new visual parameter space into a\npre-trained LLM, enabling stable learning of visual knowledge from noisy data\nvia delta tuning. Based on this principle, we first introduce Mono-InternVL, an\nadvanced monolithic MLLM that incorporates a set of visual experts through a\nmultimodal mixture-of-experts architecture. In addition, we design an\ninnovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize\nits visual capabilities via progressive learning. Mono-InternVL achieves\ncompetitive performance against existing MLLMs but also leads to relatively\nexpensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper\nand stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++\nintroduces additional visual attention experts to Mono-InternVL-1.5 and\nre-organizes the pre-training process in an efficient manner. During inference,\nit includes a fused CUDA kernel to speed up its MoE operations. With these\ndesigns, Mono-InternVL-1.5 significantly reduces training and inference costs,\nwhile still maintaining competitive performance with Mono-InternVL. To evaluate\nour approach, we conduct extensive experiments across 15 benchmarks. Results\ndemonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out\nof 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared\nto its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves\nsimilar multimodal performance while reducing first-token latency by up to 69%.\nCode and models are released at https://github.com/OpenGVLab/Mono-InternVL.",
        "url": "http://arxiv.org/abs/2507.12566v1",
        "published_date": "2025-07-16T18:31:23+00:00",
        "updated_date": "2025-07-16T18:31:23+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Gen Luo",
            "Wenhan Dou",
            "Wenhao Li",
            "Zhaokai Wang",
            "Xue Yang",
            "Changyao Tian",
            "Hao Li",
            "Weiyun Wang",
            "Wenhai Wang",
            "Xizhou Zhu",
            "Yu Qiao",
            "Jifeng Dai"
        ],
        "tldr": "The paper introduces Mono-InternVL-1.5, a monolithic multimodal large language model enhancing visual knowledge learning and reducing training/inference costs through a multimodal mixture-of-experts architecture and an improved endogenous visual pre-training method, achieving competitive performance with reduced latency.",
        "tldr_zh": "本文介绍了Mono-InternVL-1.5，一种单体多模态大型语言模型，通过多模态混合专家架构和改进的内生视觉预训练方法，增强了视觉知识学习并降低了训练/推理成本，在降低延迟的同时实现了有竞争力的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]