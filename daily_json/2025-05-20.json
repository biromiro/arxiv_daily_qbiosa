[
    {
        "title": "Mean Flows for One-step Generative Modeling",
        "summary": "We propose a principled and effective framework for one-step generative\nmodeling. We introduce the notion of average velocity to characterize flow\nfields, in contrast to instantaneous velocity modeled by Flow Matching methods.\nA well-defined identity between average and instantaneous velocities is derived\nand used to guide neural network training. Our method, termed the MeanFlow\nmodel, is self-contained and requires no pre-training, distillation, or\ncurriculum learning. MeanFlow demonstrates strong empirical performance: it\nachieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet\n256x256 trained from scratch, significantly outperforming previous\nstate-of-the-art one-step diffusion/flow models. Our study substantially\nnarrows the gap between one-step diffusion/flow models and their multi-step\npredecessors, and we hope it will motivate future research to revisit the\nfoundations of these powerful models.",
        "url": "http://arxiv.org/abs/2505.13447v1",
        "published_date": "2025-05-19T17:59:42+00:00",
        "updated_date": "2025-05-19T17:59:42+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Zhengyang Geng",
            "Mingyang Deng",
            "Xingjian Bai",
            "J. Zico Kolter",
            "Kaiming He"
        ],
        "tldr": "The paper introduces MeanFlow, a novel one-step generative modeling framework that achieves state-of-the-art FID scores on ImageNet 256x256 without pre-training or curriculum learning, significantly closing the gap with multi-step methods.",
        "tldr_zh": "该论文介绍了一种新的单步生成模型框架MeanFlow，它在ImageNet 256x256上实现了最先进的FID分数，无需预训练或课程学习，显著缩小了与多步方法的差距。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "MAGI-1: Autoregressive Video Generation at Scale",
        "summary": "We present MAGI-1, a world model that generates videos by autoregressively\npredicting a sequence of video chunks, defined as fixed-length segments of\nconsecutive frames. Trained to denoise per-chunk noise that increases\nmonotonically over time, MAGI-1 enables causal temporal modeling and naturally\nsupports streaming generation. It achieves strong performance on image-to-video\n(I2V) tasks conditioned on text instructions, providing high temporal\nconsistency and scalability, which are made possible by several algorithmic\ninnovations and a dedicated infrastructure stack. MAGI-1 facilitates\ncontrollable generation via chunk-wise prompting and supports real-time,\nmemory-efficient deployment by maintaining constant peak inference cost,\nregardless of video length. The largest variant of MAGI-1 comprises 24 billion\nparameters and supports context lengths of up to 4 million tokens,\ndemonstrating the scalability and robustness of our approach. The code and\nmodels are available at https://github.com/SandAI-org/MAGI-1 and\nhttps://github.com/SandAI-org/MagiAttention. The product can be accessed at\nhttps://sand.ai.",
        "url": "http://arxiv.org/abs/2505.13211v1",
        "published_date": "2025-05-19T14:58:50+00:00",
        "updated_date": "2025-05-19T14:58:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sand. ai",
            "Hansi Teng",
            "Hongyu Jia",
            "Lei Sun",
            "Lingzhi Li",
            "Maolin Li",
            "Mingqiu Tang",
            "Shuai Han",
            "Tianning Zhang",
            "W. Q. Zhang",
            "Weifeng Luo",
            "Xiaoyang Kang",
            "Yuchen Sun",
            "Yue Cao",
            "Yunpeng Huang",
            "Yutong Lin",
            "Yuxin Fang",
            "Zewei Tao",
            "Zheng Zhang",
            "Zhongshu Wang",
            "Zixun Liu",
            "Dai Shi",
            "Guoli Su",
            "Hanwen Sun",
            "Hong Pan",
            "Jie Wang",
            "Jiexin Sheng",
            "Min Cui",
            "Min Hu",
            "Ming Yan",
            "Shucheng Yin",
            "Siran Zhang",
            "Tingting Liu",
            "Xianping Yin",
            "Xiaoyu Yang",
            "Xin Song",
            "Xuan Hu",
            "Yankai Zhang",
            "Yuqiao Li"
        ],
        "tldr": "MAGI-1 is a 24B parameter autoregressive video generation model that uses denoising and chunk-wise processing to achieve scalable, temporally consistent video generation with real-time, memory-efficient deployment.",
        "tldr_zh": "MAGI-1是一个拥有240亿参数的自回归视频生成模型，它采用去噪和分块处理方法，实现了可扩展、时间上一致的视频生成，并支持实时、内存高效的部署。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Faster Video Diffusion with Trainable Sparse Attention",
        "summary": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D\nattention, even though most of the attention mass concentrates on a small\nsubset of positions. We turn this observation into VSA, a trainable,\nhardware-efficient sparse attention that replaces full attention at \\emph{both}\ntraining and inference. In VSA, a lightweight coarse stage pools tokens into\ntiles and identifies high-weight \\emph{critical tokens}; a fine stage computes\ntoken-level attention only inside those tiles subjecting to block computing\nlayout to ensure hard efficiency. This leads to a single differentiable kernel\nthat trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of\nFlashAttention3 MFU. We perform a large sweep of ablation studies and\nscaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA\nreaches a Pareto point that cuts training FLOPS by 2.53$\\times$ with no drop in\ndiffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention\ntime by 6$\\times$ and lowers end-to-end generation time from 31s to 18s with\ncomparable quality. These results establish trainable sparse attention as a\npractical alternative to full attention and a key enabler for further scaling\nof video diffusion models.",
        "url": "http://arxiv.org/abs/2505.13389v1",
        "published_date": "2025-05-19T17:30:13+00:00",
        "updated_date": "2025-05-19T17:30:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peiyuan Zhang",
            "Haofeng Huang",
            "Yongqi Chen",
            "Will Lin",
            "Zhengzhong Liu",
            "Ion Stoica",
            "Eric P. Xing",
            "Hao Zhang"
        ],
        "tldr": "The paper introduces VSA, a trainable sparse attention mechanism for video diffusion transformers that significantly reduces training FLOPs and inference time while maintaining comparable quality, making it a practical alternative to full attention.",
        "tldr_zh": "该论文介绍了一种可训练的稀疏注意力机制VSA，用于视频扩散Transformer，它在保持相当质量的同时，显著降低了训练FLOPs和推理时间，使其成为完全注意力的实用替代方案。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Swin DiT: Diffusion Transformer using Pseudo Shifted Windows",
        "summary": "Diffusion Transformers (DiTs) achieve remarkable performance within the\ndomain of image generation through the incorporation of the transformer\narchitecture. Conventionally, DiTs are constructed by stacking serial isotropic\nglobal information modeling transformers, which face significant computational\ncost when processing high-resolution images. We empirically analyze that latent\nspace image generation does not exhibit a strong dependence on global\ninformation as traditionally assumed. Most of the layers in the model\ndemonstrate redundancy in global computation. In addition, conventional\nattention mechanisms exhibit low-frequency inertia issues. To address these\nissues, we propose \\textbf{P}seudo \\textbf{S}hifted \\textbf{W}indow\n\\textbf{A}ttention (PSWA), which fundamentally mitigates global model\nredundancy. PSWA achieves intermediate global-local information interaction\nthrough window attention, while employing a high-frequency bridging branch to\nsimulate shifted window operations, supplementing appropriate global and\nhigh-frequency information. Furthermore, we propose the Progressive Coverage\nChannel Allocation(PCCA) strategy that captures high-order attention similarity\nwithout additional computational cost. Building upon all of them, we propose a\nseries of Pseudo \\textbf{S}hifted \\textbf{Win}dow DiTs (\\textbf{Swin DiT}),\naccompanied by extensive experiments demonstrating their superior performance.\nFor example, our proposed Swin-DiT-L achieves a 54%$\\uparrow$ FID improvement\nover DiT-XL/2 while requiring less computational.\nhttps://github.com/wujiafu007/Swin-DiT",
        "url": "http://arxiv.org/abs/2505.13219v1",
        "published_date": "2025-05-19T15:02:33+00:00",
        "updated_date": "2025-05-19T15:02:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiafu Wu",
            "Yabiao Wang",
            "Jian Li",
            "Jinlong Peng",
            "Yun Cao",
            "Chengjie Wang",
            "Jiangning Zhang"
        ],
        "tldr": "The paper introduces Swin DiT, a Diffusion Transformer that uses a Pseudo Shifted Window Attention mechanism to improve image generation performance and reduce computational cost compared to standard DiTs, achieving significant FID improvements.",
        "tldr_zh": "该论文介绍了Swin DiT，一种使用伪移位窗口注意力机制的扩散Transformer，与标准的DiT相比，提高了图像生成性能并降低了计算成本，实现了显著的FID改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MVAR: Visual Autoregressive Modeling with Scale and Spatial Markovian Conditioning",
        "summary": "Essential to visual generation is efficient modeling of visual data priors.\nConventional next-token prediction methods define the process as learning the\nconditional probability distribution of successive tokens. Recently, next-scale\nprediction methods redefine the process to learn the distribution over\nmulti-scale representations, significantly reducing generation latency.\nHowever, these methods condition each scale on all previous scales and require\neach token to consider all preceding tokens, exhibiting scale and spatial\nredundancy. To better model the distribution by mitigating redundancy, we\npropose Markovian Visual AutoRegressive modeling (MVAR), a novel autoregressive\nframework that introduces scale and spatial Markov assumptions to reduce the\ncomplexity of conditional probability modeling. Specifically, we introduce a\nscale-Markov trajectory that only takes as input the features of adjacent\npreceding scale for next-scale prediction, enabling the adoption of a parallel\ntraining strategy that significantly reduces GPU memory consumption.\nFurthermore, we propose spatial-Markov attention, which restricts the attention\nof each token to a localized neighborhood of size k at corresponding positions\non adjacent scales, rather than attending to every token across these scales,\nfor the pursuit of reduced modeling complexity. Building on these improvements,\nwe reduce the computational complexity of attention calculation from O(N^2) to\nO(Nk), enabling training with just eight NVIDIA RTX 4090 GPUs and eliminating\nthe need for KV cache during inference. Extensive experiments on ImageNet\ndemonstrate that MVAR achieves comparable or superior performance with both\nsmall model trained from scratch and large fine-tuned models, while reducing\nthe average GPU memory footprint by 3.0x.",
        "url": "http://arxiv.org/abs/2505.12742v1",
        "published_date": "2025-05-19T05:56:44+00:00",
        "updated_date": "2025-05-19T05:56:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinhua Zhang",
            "Wei Long",
            "Minghao Han",
            "Weiyi You",
            "Shuhang Gu"
        ],
        "tldr": "The paper introduces MVAR, a novel autoregressive framework for visual generation that uses scale and spatial Markov assumptions to reduce computational complexity and GPU memory consumption, achieving comparable or superior performance on ImageNet with reduced resources.",
        "tldr_zh": "该论文介绍了一种名为 MVAR 的新型自回归框架，用于视觉生成。该框架利用尺度和空间马尔可夫假设来降低计算复杂度和 GPU 内存消耗，并在 ImageNet 上实现了可比或更优越的性能，同时减少了资源需求。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Few-Step Diffusion via Score identity Distillation",
        "summary": "Diffusion distillation has emerged as a promising strategy for accelerating\ntext-to-image (T2I) diffusion models by distilling a pretrained score network\ninto a one- or few-step generator. While existing methods have made notable\nprogress, they often rely on real or teacher-synthesized images to perform well\nwhen distilling high-resolution T2I diffusion models such as Stable Diffusion\nXL (SDXL), and their use of classifier-free guidance (CFG) introduces a\npersistent trade-off between text-image alignment and generation diversity. We\naddress these challenges by optimizing Score identity Distillation (SiD) -- a\ndata-free, one-step distillation framework -- for few-step generation. Backed\nby theoretical analysis that justifies matching a uniform mixture of outputs\nfrom all generation steps to the data distribution, our few-step distillation\nalgorithm avoids step-specific networks and integrates seamlessly into existing\npipelines, achieving state-of-the-art performance on SDXL at 1024x1024\nresolution. To mitigate the alignment-diversity trade-off when real text-image\npairs are available, we introduce a Diffusion GAN-based adversarial loss\napplied to the uniform mixture and propose two new guidance strategies:\nZero-CFG, which disables CFG in the teacher and removes text conditioning in\nthe fake score network, and Anti-CFG, which applies negative CFG in the fake\nscore network. This flexible setup improves diversity without sacrificing\nalignment. Comprehensive experiments on SD1.5 and SDXL demonstrate\nstate-of-the-art performance in both one-step and few-step generation settings,\nalong with robustness to the absence of real images. Our efficient PyTorch\nimplementation, along with the resulting one- and few-step distilled\ngenerators, will be released publicly as a separate branch at\nhttps://github.com/mingyuanzhou/SiD-LSG.",
        "url": "http://arxiv.org/abs/2505.12674v1",
        "published_date": "2025-05-19T03:45:16+00:00",
        "updated_date": "2025-05-19T03:45:16+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "authors": [
            "Mingyuan Zhou",
            "Yi Gu",
            "Zhendong Wang"
        ],
        "tldr": "This paper introduces Score identity Distillation (SiD), a data-free, one-step distillation framework that achieves state-of-the-art performance on Stable Diffusion XL for high-resolution text-to-image generation, addressing the alignment-diversity trade-off and robustness to the absence of real images.",
        "tldr_zh": "本文介绍了一种名为Score identity Distillation (SiD)的无数据单步蒸馏框架，该框架在 Stable Diffusion XL 上实现了最先进的高分辨率文本到图像生成性能，解决了对齐-多样性权衡问题，并提高了在缺乏真实图像时的鲁棒性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Accelerate TarFlow Sampling with GS-Jacobi Iteration",
        "summary": "Image generation models have achieved widespread applications. As an\ninstance, the TarFlow model combines the transformer architecture with\nNormalizing Flow models, achieving state-of-the-art results on multiple\nbenchmarks. However, due to the causal form of attention requiring sequential\ncomputation, TarFlow's sampling process is extremely slow. In this paper, we\ndemonstrate that through a series of optimization strategies, TarFlow sampling\ncan be greatly accelerated by using the Gauss-Seidel-Jacobi (abbreviated as\nGS-Jacobi) iteration method. Specifically, we find that blocks in the TarFlow\nmodel have varying importance: a small number of blocks play a major role in\nimage generation tasks, while other blocks contribute relatively little; some\nblocks are sensitive to initial values and prone to numerical overflow, while\nothers are relatively robust. Based on these two characteristics, we propose\nthe Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM): CRM\nis used to identify whether a TarFlow block is \"simple\" (converges in few\niterations) or \"tough\" (requires more iterations); IGM is used to evaluate\nwhether the initial value of the iteration is good. Experiments on four TarFlow\nmodels demonstrate that GS-Jacobi sampling can significantly enhance sampling\nefficiency while maintaining the quality of generated images (measured by FID),\nachieving speed-ups of 4.53x in Img128cond, 5.32x in AFHQ, 2.96x in\nImg64uncond, and 2.51x in Img64cond without degrading FID scores or sample\nquality. Code and checkpoints are accessible on\nhttps://github.com/encoreus/GS-Jacobi_for_TarFlow",
        "url": "http://arxiv.org/abs/2505.12849v1",
        "published_date": "2025-05-19T08:35:44+00:00",
        "updated_date": "2025-05-19T08:35:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ben Liu",
            "Zhen Qin"
        ],
        "tldr": "This paper accelerates TarFlow image generation, a transformer-based normalizing flow model, using a GS-Jacobi iteration approach, achieving significant speed-ups without compromising image quality.",
        "tldr_zh": "该论文使用GS-Jacobi迭代方法加速了基于Transformer的归一化流模型TarFlow的图像生成，在不影响图像质量的前提下，实现了显著的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]