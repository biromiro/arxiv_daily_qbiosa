[
    {
        "title": "OmniV2V: Versatile Video Generation and Editing via Dynamic Content Manipulation",
        "summary": "The emergence of Diffusion Transformers (DiT) has brought significant\nadvancements to video generation, especially in text-to-video and\nimage-to-video tasks. Although video generation is widely applied in various\nfields, most existing models are limited to single scenarios and cannot perform\ndiverse video generation and editing through dynamic content manipulation. We\npropose OmniV2V, a video model capable of generating and editing videos across\ndifferent scenarios based on various operations, including: object movement,\nobject addition, mask-guided video edit, try-on, inpainting, outpainting, human\nanimation, and controllable character video synthesis. We explore a unified\ndynamic content manipulation injection module, which effectively integrates the\nrequirements of the above tasks. In addition, we design a visual-text\ninstruction module based on LLaVA, enabling the model to effectively understand\nthe correspondence between visual content and instructions. Furthermore, we\nbuild a comprehensive multi-task data processing system. Since there is data\noverlap among various tasks, this system can efficiently provide data\naugmentation. Using this system, we construct a multi-type, multi-scenario\nOmniV2V dataset and its corresponding OmniV2V-Test benchmark. Extensive\nexperiments show that OmniV2V works as well as, and sometimes better than, the\nbest existing open-source and commercial models for many video generation and\nediting tasks.",
        "url": "http://arxiv.org/abs/2506.01801v1",
        "published_date": "2025-06-02T15:42:06+00:00",
        "updated_date": "2025-06-02T15:42:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sen Liang",
            "Zhentao Yu",
            "Zhengguang Zhou",
            "Teng Hu",
            "Hongmei Wang",
            "Yi Chen",
            "Qin Lin",
            "Yuan Zhou",
            "Xin Li",
            "Qinglin Lu",
            "Zhibo Chen"
        ],
        "tldr": "OmniV2V is a versatile video generation and editing model that supports various dynamic content manipulation tasks through a unified module and visual-text instruction module, outperforming existing models in many tasks.",
        "tldr_zh": "OmniV2V是一个通用的视频生成和编辑模型，它通过统一的模块和视觉文本指令模块支持各种动态内容操作任务，在许多任务中优于现有的模型。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Many-for-Many: Unify the Training of Multiple Video and Image Generation and Manipulation Tasks",
        "summary": "Diffusion models have shown impressive performance in many visual generation\nand manipulation tasks. Many existing methods focus on training a model for a\nspecific task, especially, text-to-video (T2V) generation, while many other\nworks focus on finetuning the pretrained T2V model for image-to-video (I2V),\nvideo-to-video (V2V), image and video manipulation tasks, etc. However,\ntraining a strong T2V foundation model requires a large amount of high-quality\nannotations, which is very costly. In addition, many existing models can\nperform only one or several tasks. In this work, we introduce a unified\nframework, namely many-for-many, which leverages the available training data\nfrom many different visual generation and manipulation tasks to train a single\nmodel for those different tasks. Specifically, we design a lightweight adapter\nto unify the different conditions in different tasks, then employ a joint\nimage-video learning strategy to progressively train the model from scratch.\nOur joint learning leads to a unified visual generation and manipulation model\nwith improved video generation performance. In addition, we introduce depth\nmaps as a condition to help our model better perceive the 3D space in visual\ngeneration. Two versions of our model are trained with different model sizes\n(8B and 2B), each of which can perform more than 10 different tasks. In\nparticular, our 8B model demonstrates highly competitive performance in video\ngeneration tasks compared to open-source and even commercial engines. Our\nmodels and source codes are available at https://github.com/leeruibin/MfM.git.",
        "url": "http://arxiv.org/abs/2506.01758v1",
        "published_date": "2025-06-02T15:05:44+00:00",
        "updated_date": "2025-06-02T15:05:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tao Yang",
            "Ruibin Li",
            "Yangming Shi",
            "Yuqi Zhang",
            "Qide Dong",
            "Haoran Cheng",
            "Weiguo Feng",
            "Shilei Wen",
            "Bingyue Peng",
            "Lei Zhang"
        ],
        "tldr": "The paper presents a unified framework called 'many-for-many' that trains a single diffusion model to perform multiple visual generation and manipulation tasks, including video generation, using a joint image-video learning strategy and depth map conditioning.",
        "tldr_zh": "该论文提出了一个名为'many-for-many'的统一框架，通过联合图像-视频学习策略和深度图条件，训练单个扩散模型来执行多个视觉生成和操控任务，包括视频生成。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Dual-Process Image Generation",
        "summary": "Prior methods for controlling image generation are limited in their ability\nto be taught new tasks. In contrast, vision-language models, or VLMs, can learn\ntasks in-context and produce the correct outputs for a given input. We propose\na dual-process distillation scheme that allows feed-forward image generators to\nlearn new tasks from deliberative VLMs. Our scheme uses a VLM to rate the\ngenerated images and backpropagates this gradient to update the weights of the\nimage generator. Our general framework enables a wide variety of new control\ntasks through the same text-and-image based interface. We showcase a handful of\napplications of this technique for different types of control signals, such as\ncommonsense inferences and visual prompts. With our method, users can implement\nmultimodal controls for properties such as color palette, line weight, horizon\nposition, and relative depth within a matter of minutes. Project page:\nhttps://dual-process.github.io.",
        "url": "http://arxiv.org/abs/2506.01955v1",
        "published_date": "2025-06-02T17:59:56+00:00",
        "updated_date": "2025-06-02T17:59:56+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Grace Luo",
            "Jonathan Granskog",
            "Aleksander Holynski",
            "Trevor Darrell"
        ],
        "tldr": "This paper introduces a dual-process distillation method that enables feed-forward image generators to learn new control tasks from vision-language models via VLM-rated image feedback, allowing for rapid implementation of multimodal controls.",
        "tldr_zh": "该论文提出了一种双过程蒸馏方法，通过VLM评估图像的反馈，使前馈图像生成器能够从视觉语言模型中学习新的控制任务，从而能够快速实现多模态控制。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Image Generation from Contextually-Contradictory Prompts",
        "summary": "Text-to-image diffusion models excel at generating high-quality, diverse\nimages from natural language prompts. However, they often fail to produce\nsemantically accurate results when the prompt contains concept combinations\nthat contradict their learned priors. We define this failure mode as contextual\ncontradiction, where one concept implicitly negates another due to entangled\nassociations learned during training. To address this, we propose a stage-aware\nprompt decomposition framework that guides the denoising process using a\nsequence of proxy prompts. Each proxy prompt is constructed to match the\nsemantic content expected to emerge at a specific stage of denoising, while\nensuring contextual coherence. To construct these proxy prompts, we leverage a\nlarge language model (LLM) to analyze the target prompt, identify\ncontradictions, and generate alternative expressions that preserve the original\nintent while resolving contextual conflicts. By aligning prompt information\nwith the denoising progression, our method enables fine-grained semantic\ncontrol and accurate image generation in the presence of contextual\ncontradictions. Experiments across a variety of challenging prompts show\nsubstantial improvements in alignment to the textual prompt.",
        "url": "http://arxiv.org/abs/2506.01929v1",
        "published_date": "2025-06-02T17:48:12+00:00",
        "updated_date": "2025-06-02T17:48:12+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Saar Huberman",
            "Or Patashnik",
            "Omer Dahary",
            "Ron Mokady",
            "Daniel Cohen-Or"
        ],
        "tldr": "The paper introduces a stage-aware prompt decomposition framework using LLMs to resolve contextual contradictions in text-to-image diffusion models, improving semantic accuracy in generated images.",
        "tldr_zh": "该论文介绍了一种阶段感知的 prompt 分解框架，使用大型语言模型来解决文本到图像扩散模型中的上下文矛盾，从而提高生成图像的语义准确性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Cycle Consistency as Reward: Learning Image-Text Alignment without Human Preferences",
        "summary": "Learning alignment between language and vision is a fundamental challenge,\nespecially as multimodal data becomes increasingly detailed and complex.\nExisting methods often rely on collecting human or AI preferences, which can be\ncostly and time-intensive. We propose an alternative approach that leverages\ncycle consistency as a supervisory signal. Given an image and generated text,\nwe map the text back to image space using a text-to-image model and compute the\nsimilarity between the original image and its reconstruction. Analogously, for\ntext-to-image generation, we measure the textual similarity between an input\ncaption and its reconstruction through the cycle. We use the cycle consistency\nscore to rank candidates and construct a preference dataset of 866K comparison\npairs. The reward model trained on our dataset outperforms state-of-the-art\nalignment metrics on detailed captioning, with superior inference-time\nscalability when used as a verifier for Best-of-N sampling. Furthermore,\nperforming DPO and Diffusion DPO using our dataset enhances performance across\na wide range of vision-language tasks and text-to-image generation. Our\ndataset, model, and code are at https://cyclereward.github.io",
        "url": "http://arxiv.org/abs/2506.02095v1",
        "published_date": "2025-06-02T17:42:58+00:00",
        "updated_date": "2025-06-02T17:42:58+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Hyojin Bahng",
            "Caroline Chan",
            "Fredo Durand",
            "Phillip Isola"
        ],
        "tldr": "This paper introduces a novel method for learning image-text alignment by using cycle consistency as a reward signal, avoiding the need for costly human preferences and achieving state-of-the-art performance on detailed captioning and other vision-language tasks.",
        "tldr_zh": "本文介绍了一种新颖的图像-文本对齐学习方法，通过使用循环一致性作为奖励信号，避免了对昂贵的人工偏好的需求，并在详细的图像描述和其他视觉-语言任务上实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unlocking Aha Moments via Reinforcement Learning: Advancing Collaborative Visual Comprehension and Generation",
        "summary": "Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify\nvisual comprehension and generation. However, these two capabilities remain\nlargely independent, as if they are two separate functions encapsulated within\nthe same model. Consequently, visual comprehension does not enhance visual\ngeneration, and the reasoning mechanisms of LLMs have not been fully integrated\nto revolutionize image generation. In this paper, we propose to enable the\ncollaborative co-evolution of visual comprehension and generation, advancing\nimage generation into an iterative introspective process. We introduce a\ntwo-stage training approach: supervised fine-tuning teaches the MLLM with the\nfoundational ability to generate genuine CoT for visual generation, while\nreinforcement learning activates its full potential via an\nexploration-exploitation trade-off. Ultimately, we unlock the Aha moment in\nvisual generation, advancing MLLMs from text-to-image tasks to unified image\ngeneration. Extensive experiments demonstrate that our model not only excels in\ntext-to-image generation and image editing, but also functions as a superior\nimage semantic evaluator with enhanced visual comprehension capabilities.\nProject Page: https://janus-pro-r1.github.io.",
        "url": "http://arxiv.org/abs/2506.01480v1",
        "published_date": "2025-06-02T09:39:28+00:00",
        "updated_date": "2025-06-02T09:39:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaihang Pan",
            "Yang Wu",
            "Wendong Bu",
            "Kai Shen",
            "Juncheng Li",
            "Yingting Wang",
            "Yunfei Li",
            "Siliang Tang",
            "Jun Xiao",
            "Fei Wu",
            "Hang Zhao",
            "Yueting Zhuang"
        ],
        "tldr": "This paper introduces a two-stage training approach combining supervised fine-tuning and reinforcement learning to enable collaborative co-evolution of visual comprehension and generation in MLLMs, leading to improved text-to-image generation, image editing, and semantic evaluation.",
        "tldr_zh": "本文提出了一种结合监督微调和强化学习的两阶段训练方法，旨在实现MLLM中视觉理解和生成的协同进化，从而改进文本到图像的生成、图像编辑和语义评估。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DiffuseSlide: Training-Free High Frame Rate Video Generation Diffusion",
        "summary": "Recent advancements in diffusion models have revolutionized video generation,\nenabling the creation of high-quality, temporally consistent videos. However,\ngenerating high frame-rate (FPS) videos remains a significant challenge due to\nissues such as flickering and degradation in long sequences, particularly in\nfast-motion scenarios. Existing methods often suffer from computational\ninefficiencies and limitations in maintaining video quality over extended\nframes. In this paper, we present a novel, training-free approach for high FPS\nvideo generation using pre-trained diffusion models. Our method, DiffuseSlide,\nintroduces a new pipeline that leverages key frames from low FPS videos and\napplies innovative techniques, including noise re-injection and sliding window\nlatent denoising, to achieve smooth, consistent video outputs without the need\nfor additional fine-tuning. Through extensive experiments, we demonstrate that\nour approach significantly improves video quality, offering enhanced temporal\ncoherence and spatial fidelity. The proposed method is not only computationally\nefficient but also adaptable to various video generation tasks, making it ideal\nfor applications such as virtual reality, video games, and high-quality content\ncreation.",
        "url": "http://arxiv.org/abs/2506.01454v1",
        "published_date": "2025-06-02T09:12:41+00:00",
        "updated_date": "2025-06-02T09:12:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Geunmin Hwang",
            "Hyun-kyu Ko",
            "Younghyun Kim",
            "Seungryong Lee",
            "Eunbyung Park"
        ],
        "tldr": "DiffuseSlide introduces a training-free approach to generate high frame-rate videos from pre-trained diffusion models using noise re-injection and sliding window latent denoising, improving temporal coherence and spatial fidelity.",
        "tldr_zh": "DiffuseSlide 提出了一种免训练的方法，利用预训练的扩散模型，通过噪声重注入和滑动窗口潜在去噪来生成高帧率视频，从而提高时间连贯性和空间保真度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Playing with Transformer at 30+ FPS via Next-Frame Diffusion",
        "summary": "Autoregressive video models offer distinct advantages over bidirectional\ndiffusion models in creating interactive video content and supporting streaming\napplications with arbitrary duration. In this work, we present Next-Frame\nDiffusion (NFD), an autoregressive diffusion transformer that incorporates\nblock-wise causal attention, enabling iterative sampling and efficient\ninference via parallel token generation within each frame. Nonetheless,\nachieving real-time video generation remains a significant challenge for such\nmodels, primarily due to the high computational cost associated with diffusion\nsampling and the hardware inefficiencies inherent to autoregressive generation.\nTo address this, we introduce two innovations: (1) We extend consistency\ndistillation to the video domain and adapt it specifically for video models,\nenabling efficient inference with few sampling steps; (2) To fully leverage\nparallel computation, motivated by the observation that adjacent frames often\nshare the identical action input, we propose speculative sampling. In this\napproach, the model generates next few frames using current action input, and\ndiscard speculatively generated frames if the input action differs. Experiments\non a large-scale action-conditioned video generation benchmark demonstrate that\nNFD beats autoregressive baselines in terms of both visual quality and sampling\nefficiency. We, for the first time, achieves autoregressive video generation at\nover 30 Frames Per Second (FPS) on an A100 GPU using a 310M model.",
        "url": "http://arxiv.org/abs/2506.01380v1",
        "published_date": "2025-06-02T07:16:01+00:00",
        "updated_date": "2025-06-02T07:16:01+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xinle Cheng",
            "Tianyu He",
            "Jiayi Xu",
            "Junliang Guo",
            "Di He",
            "Jiang Bian"
        ],
        "tldr": "The paper introduces Next-Frame Diffusion (NFD), an autoregressive diffusion transformer for efficient action-conditioned video generation, achieving real-time performance (30+ FPS) through consistency distillation and speculative sampling.",
        "tldr_zh": "该论文介绍了下一帧扩散（NFD），一种用于高效动作条件视频生成的自回归扩散Transformer，通过一致性蒸馏和推测采样实现了实时性能（30+ FPS）。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]