[
    {
        "title": "X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again",
        "summary": "Numerous efforts have been made to extend the ``next token prediction''\nparadigm to visual contents, aiming to create a unified approach for both image\ngeneration and understanding. Nevertheless, attempts to generate images through\nautoregressive modeling with discrete tokens have been plagued by issues such\nas low visual fidelity, distorted outputs, and failure to adhere to complex\ninstructions when rendering intricate details. These shortcomings are likely\nattributed to cumulative errors during autoregressive inference or information\nloss incurred during the discretization process. Probably due to this\nchallenge, recent research has increasingly shifted toward jointly training\nimage generation with diffusion objectives and language generation with\nautoregressive objectives, moving away from unified modeling approaches. In\nthis work, we demonstrate that reinforcement learning can effectively mitigate\nartifacts and largely enhance the generation quality of a discrete\nautoregressive modeling method, thereby enabling seamless integration of image\nand language generation. Our framework comprises a semantic image tokenizer, a\nunified autoregressive model for both language and images, and an offline\ndiffusion decoder for image generation, termed X-Omni. X-Omni achieves\nstate-of-the-art performance in image generation tasks using a 7B language\nmodel, producing images with high aesthetic quality while exhibiting strong\ncapabilities in following instructions and rendering long texts.",
        "url": "http://arxiv.org/abs/2507.22058v1",
        "published_date": "2025-07-29T17:59:04+00:00",
        "updated_date": "2025-07-29T17:59:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zigang Geng",
            "Yibing Wang",
            "Yeyao Ma",
            "Chen Li",
            "Yongming Rao",
            "Shuyang Gu",
            "Zhao Zhong",
            "Qinglin Lu",
            "Han Hu",
            "Xiaosong Zhang",
            "Linus",
            "Di Wang",
            "Jie Jiang"
        ],
        "tldr": "The paper introduces X-Omni, a framework that uses reinforcement learning to improve the image generation quality of discrete autoregressive models, enabling seamless integration of image and language generation and achieving state-of-the-art performance.",
        "tldr_zh": "本文介绍了一个名为X-Omni的框架，该框架使用强化学习来提高离散自回归模型的图像生成质量，从而实现图像和语言生成的无缝集成，并达到最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Aether Weaver: Multimodal Affective Narrative Co-Generation with Dynamic Scene Graphs",
        "summary": "We introduce Aether Weaver, a novel, integrated framework for multimodal\nnarrative co-generation that overcomes limitations of sequential text-to-visual\npipelines. Our system concurrently synthesizes textual narratives, dynamic\nscene graph representations, visual scenes, and affective soundscapes, driven\nby a tightly integrated, co-generation mechanism. At its core, the Narrator, a\nlarge language model, generates narrative text and multimodal prompts, while\nthe Director acts as a dynamic scene graph manager, and analyzes the text to\nbuild and maintain a structured representation of the story's world, ensuring\nspatio-temporal and relational consistency for visual rendering and subsequent\nnarrative generation. Additionally, a Narrative Arc Controller guides the\nhigh-level story structure, influencing multimodal affective consistency,\nfurther complemented by an Affective Tone Mapper that ensures congruent\nemotional expression across all modalities. Through qualitative evaluations on\na diverse set of narrative prompts encompassing various genres, we demonstrate\nthat Aether Weaver significantly enhances narrative depth, visual fidelity, and\nemotional resonance compared to cascaded baseline approaches. This integrated\nframework provides a robust platform for rapid creative prototyping and\nimmersive storytelling experiences.",
        "url": "http://arxiv.org/abs/2507.21893v1",
        "published_date": "2025-07-29T15:01:31+00:00",
        "updated_date": "2025-07-29T15:01:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Saeed Ghorbani"
        ],
        "tldr": "The paper introduces Aether Weaver, a co-generation framework that simultaneously synthesizes text, scene graphs, visuals, and soundscapes for enhanced narrative generation with improved consistency and emotional resonance.",
        "tldr_zh": "该论文介绍了 Aether Weaver，一个协同生成框架，可以同步合成文本、场景图、视觉效果和声景，从而增强叙事生成能力，并提高一致性和情感共鸣。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE",
        "summary": "Although GRPO substantially enhances flow matching models in human preference\nalignment of image generation, methods such as FlowGRPO still exhibit\ninefficiency due to the necessity of sampling and optimizing over all denoising\nsteps specified by the Markov Decision Process (MDP). In this paper, we propose\n$\\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed\nsampling strategies through the integration of stochastic differential\nequations (SDE) and ordinary differential equations (ODE). This streamlines the\noptimization process within the MDP to improve efficiency and boost\nperformance. Specifically, MixGRPO introduces a sliding window mechanism, using\nSDE sampling and GRPO-guided optimization only within the window, while\napplying ODE sampling outside. This design confines sampling randomness to the\ntime-steps within the window, thereby reducing the optimization overhead, and\nallowing for more focused gradient updates to accelerate convergence.\nAdditionally, as time-steps beyond the sliding window are not involved in\noptimization, higher-order solvers are supported for sampling. So we present a\nfaster variant, termed $\\textbf{MixGRPO-Flash}$, which further improves\ntraining efficiency while achieving comparable performance. MixGRPO exhibits\nsubstantial gains across multiple dimensions of human preference alignment,\noutperforming DanceGRPO in both effectiveness and efficiency, with nearly 50%\nlower training time. Notably, MixGRPO-Flash further reduces training time by\n71%. Codes and models are available at\n$\\href{https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$.",
        "url": "http://arxiv.org/abs/2507.21802v1",
        "published_date": "2025-07-29T13:40:09+00:00",
        "updated_date": "2025-07-29T13:40:09+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Junzhe Li",
            "Yutao Cui",
            "Tao Huang",
            "Yinping Ma",
            "Chun Fan",
            "Miles Yang",
            "Zhao Zhong"
        ],
        "tldr": "The paper introduces MixGRPO and MixGRPO-Flash, which improve the efficiency of flow-based GRPO for preference alignment in image generation by integrating ODE and SDE sampling with a sliding window mechanism, resulting in faster training times.",
        "tldr_zh": "本文介绍了MixGRPO和MixGRPO-Flash，通过结合常微分方程（ODE）和随机微分方程（SDE）采样，并采用滑动窗口机制，提高了基于流的GRPO在图像生成偏好对齐方面的效率，从而缩短了训练时间。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MAGE: Multimodal Alignment and Generation Enhancement via Bridging Visual and Semantic Spaces",
        "summary": "In the latest advancements in multimodal learning, effectively addressing the\nspatial and semantic losses of visual data after encoding remains a critical\nchallenge. This is because the performance of large multimodal models is\npositively correlated with the coupling between visual encoders and large\nlanguage models. Existing approaches often face issues such as vector gaps or\nsemantic disparities, resulting in information loss during the propagation\nprocess. To address these issues, we propose MAGE (Multimodal Alignment and\nGeneration Enhancement), a novel framework that bridges the semantic spaces of\nvision and text through an innovative alignment mechanism. By introducing the\nIntelligent Alignment Network (IAN), MAGE achieves dimensional and semantic\nalignment. To reduce the gap between synonymous heterogeneous data, we employ a\ntraining strategy that combines cross-entropy and mean squared error,\nsignificantly enhancing the alignment effect. Moreover, to enhance MAGE's\n\"Any-to-Any\" capability, we developed a fine-tuning dataset for multimodal\ntool-calling instructions to expand the model's output capability boundaries.\nFinally, our proposed multimodal large model architecture, MAGE, achieved\nsignificantly better performance compared to similar works across various\nevaluation benchmarks, including MME, MMBench, and SEED. Complete code and\nappendix are available at: https://github.com/GTCOM-NLP/MAGE.",
        "url": "http://arxiv.org/abs/2507.21741v1",
        "published_date": "2025-07-29T12:17:46+00:00",
        "updated_date": "2025-07-29T12:17:46+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Shaojun E",
            "Yuchen Yang",
            "Jiaheng Wu",
            "Yan Zhang",
            "Tiejun Zhao",
            "Ziyan Chen"
        ],
        "tldr": "The paper introduces MAGE, a multimodal framework that enhances alignment between visual and language models using an Intelligent Alignment Network (IAN) and a combined cross-entropy and MSE training strategy. MAGE demonstrates improved performance on multimodal benchmarks and expands output capabilities with a fine-tuning dataset.",
        "tldr_zh": "该论文介绍了MAGE，一个多模态框架，通过智能对齐网络 (IAN) 和结合交叉熵与均方误差的训练策略来增强视觉和语言模型之间的对齐。MAGE在多模态基准测试中表现出改进的性能，并通过微调数据集扩展了输出能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]