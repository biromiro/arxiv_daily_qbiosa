[
    {
        "title": "Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think",
        "summary": "REPA and its variants effectively mitigate training challenges in diffusion\nmodels by incorporating external visual representations from pretrained models,\nthrough alignment between the noisy hidden projections of denoising networks\nand foundational clean image representations. We argue that the external\nalignment, which is absent during the entire denoising inference process, falls\nshort of fully harnessing the potential of discriminative representations. In\nthis work, we propose a straightforward method called Representation\nEntanglement for Generation (REG), which entangles low-level image latents with\na single high-level class token from pretrained foundation models for\ndenoising. REG acquires the capability to produce coherent image-class pairs\ndirectly from pure noise, substantially improving both generation quality and\ntraining efficiency. This is accomplished with negligible additional inference\noverhead, requiring only one single additional token for denoising (<0.5\\%\nincrease in FLOPs and latency). The inference process concurrently reconstructs\nboth image latents and their corresponding global semantics, where the acquired\nsemantic knowledge actively guides and enhances the image generation process.\nOn ImageNet 256$\\times$256, SiT-XL/2 + REG demonstrates remarkable convergence\nacceleration, achieving $\\textbf{63}\\times$ and $\\textbf{23}\\times$ faster\ntraining than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively,\nSiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA\ntrained for 4M iterations ($\\textbf{10}\\times$ longer). Code is available at:\nhttps://github.com/Martinser/REG.",
        "url": "http://arxiv.org/abs/2507.01467v1",
        "published_date": "2025-07-02T08:29:18+00:00",
        "updated_date": "2025-07-02T08:29:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ge Wu",
            "Shen Zhang",
            "Ruijing Shi",
            "Shanghua Gao",
            "Zhenyuan Chen",
            "Lei Wang",
            "Zhaowei Chen",
            "Hongcheng Gao",
            "Yao Tang",
            "Jian Yang",
            "Ming-Ming Cheng",
            "Xiang Li"
        ],
        "tldr": "The paper introduces Representation Entanglement for Generation (REG), a method that entangles low-level image latents with a high-level class token, leading to significantly faster and higher quality image generation compared to previous approaches like REPA.",
        "tldr_zh": "该论文介绍了表征纠缠生成（REG）方法，该方法将低级图像潜在空间与高级类别令牌关联，与之前的REPA等方法相比，显著提高了图像生成的效率和质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation",
        "summary": "We present Locality-aware Parallel Decoding (LPD) to accelerate\nautoregressive image generation. Traditional autoregressive image generation\nrelies on next-patch prediction, a memory-bound process that leads to high\nlatency. Existing works have tried to parallelize next-patch prediction by\nshifting to multi-patch prediction to accelerate the process, but only achieved\nlimited parallelization. To achieve high parallelization while maintaining\ngeneration quality, we introduce two key techniques: (1) Flexible Parallelized\nAutoregressive Modeling, a novel architecture that enables arbitrary generation\nordering and degrees of parallelization. It uses learnable position query\ntokens to guide generation at target positions while ensuring mutual visibility\namong concurrently generated tokens for consistent parallel decoding. (2)\nLocality-aware Generation Ordering, a novel schedule that forms groups to\nminimize intra-group dependencies and maximize contextual support, enhancing\ngeneration quality. With these designs, we reduce the generation steps from 256\nto 20 (256$\\times$256 res.) and 1024 to 48 (512$\\times$512 res.) without\ncompromising quality on the ImageNet class-conditional generation, and\nachieving at least 3.4$\\times$ lower latency than previous parallelized\nautoregressive models.",
        "url": "http://arxiv.org/abs/2507.01957v1",
        "published_date": "2025-07-02T17:59:23+00:00",
        "updated_date": "2025-07-02T17:59:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhuoyang Zhang",
            "Luke J. Huang",
            "Chengyue Wu",
            "Shang Yang",
            "Kelly Peng",
            "Yao Lu",
            "Song Han"
        ],
        "tldr": "This paper introduces Locality-aware Parallel Decoding (LPD) to accelerate autoregressive image generation by parallelizing patch prediction while maintaining quality, achieving significant speedup and reducing generation steps.",
        "tldr_zh": "本文介绍了局部感知并行解码（LPD），通过并行化图像块预测来加速自回归图像生成，同时保持图像质量，实现了显著的加速并减少了生成步骤。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis",
        "summary": "Recent advances in large language models (LLMs) have spurred interests in\nencoding images as discrete tokens and leveraging autoregressive (AR)\nframeworks for visual generation. However, the quantization process in AR-based\nvisual generation models inherently introduces information loss that degrades\nimage fidelity. To mitigate this limitation, recent studies have explored to\nautoregressively predict continuous tokens. Unlike discrete tokens that reside\nin a structured and bounded space, continuous representations exist in an\nunbounded, high-dimensional space, making density estimation more challenging\nand increasing the risk of generating out-of-distribution artifacts. Based on\nthe above findings, this work introduces DisCon (Discrete-Conditioned\nContinuous Autoregressive Model), a novel framework that reinterprets discrete\ntokens as conditional signals rather than generation targets. By modeling the\nconditional probability of continuous representations conditioned on discrete\ntokens, DisCon circumvents the optimization challenges of continuous token\nmodeling while avoiding the information loss caused by quantization. DisCon\nachieves a gFID score of 1.38 on ImageNet 256$\\times$256 generation,\noutperforming state-of-the-art autoregressive approaches by a clear margin.",
        "url": "http://arxiv.org/abs/2507.01756v1",
        "published_date": "2025-07-02T14:33:52+00:00",
        "updated_date": "2025-07-02T14:33:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peng Zheng",
            "Junke Wang",
            "Yi Chang",
            "Yizhou Yu",
            "Rui Ma",
            "Zuxuan Wu"
        ],
        "tldr": "The paper introduces DisCon, a novel autoregressive image generation framework that uses discrete tokens as conditional signals for continuous representation generation, achieving state-of-the-art gFID on ImageNet generation.",
        "tldr_zh": "该论文介绍了一种名为DisCon的新型自回归图像生成框架，该框架使用离散令牌作为连续表示生成的条件信号，并在ImageNet生成上实现了最先进的gFID。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective",
        "summary": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation.",
        "url": "http://arxiv.org/abs/2507.01652v1",
        "published_date": "2025-07-02T12:27:06+00:00",
        "updated_date": "2025-07-02T12:27:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Yuxin Mao",
            "Zhen Qin",
            "Jinxing Zhou",
            "Hui Deng",
            "Xuyang Shen",
            "Bin Fan",
            "Jing Zhang",
            "Yiran Zhong",
            "Yuchao Dai"
        ],
        "tldr": "This paper introduces LASADGen, an autoregressive image generator using a novel linear attention mechanism (LASAD) that incorporates spatial awareness, achieving state-of-the-art image generation performance on ImageNet with linear complexity.",
        "tldr_zh": "本文介绍了 LASADGen，一种使用新型线性注意力机制 (LASAD) 的自回归图像生成器，该机制结合了空间感知，在 ImageNet 上实现了最先进的图像生成性能和线性复杂性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]