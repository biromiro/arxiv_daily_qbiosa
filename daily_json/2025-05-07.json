[
    {
        "title": "FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios",
        "summary": "Action customization involves generating videos where the subject performs\nactions dictated by input control signals. Current methods use pose-guided or\nglobal motion customization but are limited by strict constraints on spatial\nstructure, such as layout, skeleton, and viewpoint consistency, reducing\nadaptability across diverse subjects and scenarios. To overcome these\nlimitations, we propose FlexiAct, which transfers actions from a reference\nvideo to an arbitrary target image. Unlike existing methods, FlexiAct allows\nfor variations in layout, viewpoint, and skeletal structure between the subject\nof the reference video and the target image, while maintaining identity\nconsistency. Achieving this requires precise action control, spatial structure\nadaptation, and consistency preservation. To this end, we introduce RefAdapter,\na lightweight image-conditioned adapter that excels in spatial adaptation and\nconsistency preservation, surpassing existing methods in balancing appearance\nconsistency and structural flexibility. Additionally, based on our\nobservations, the denoising process exhibits varying levels of attention to\nmotion (low frequency) and appearance details (high frequency) at different\ntimesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike\nexisting methods that rely on separate spatial-temporal architectures, directly\nachieves action extraction during the denoising process. Experiments\ndemonstrate that our method effectively transfers actions to subjects with\ndiverse layouts, skeletons, and viewpoints. We release our code and model\nweights to support further research at\nhttps://shiyi-zh0408.github.io/projectpages/FlexiAct/",
        "url": "http://arxiv.org/abs/2505.03730v1",
        "published_date": "2025-05-06T17:58:02+00:00",
        "updated_date": "2025-05-06T17:58:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Shiyi Zhang",
            "Junhao Zhuang",
            "Zhaoyang Zhang",
            "Ying Shan",
            "Yansong Tang"
        ],
        "tldr": "flexiact proposes a novel approach for action customization in videos, enabling action transfer from a reference video to a target image with variations in layout, viewpoint, and skeletal structure, while maintaining identity consistency, using a frequency-aware action extraction module and a spatial adaptation adapter.",
        "tldr_zh": "flexiact 提出了一种新颖的视频动作定制方法，能够将参考视频中的动作转移到目标图像中，允许布局、视角和骨骼结构的变化，同时保持身份一致性，该方法使用了一个频率感知动作提取模块和一个空间适配器。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Distribution-Conditional Generation: From Class Distribution to Creative Generation",
        "summary": "Text-to-image (T2I) diffusion models are effective at producing semantically\naligned images, but their reliance on training data distributions limits their\nability to synthesize truly novel, out-of-distribution concepts. Existing\nmethods typically enhance creativity by combining pairs of known concepts,\nyielding compositions that, while out-of-distribution, remain linguistically\ndescribable and bounded within the existing semantic space. Inspired by the\nsoft probabilistic outputs of classifiers on ambiguous inputs, we propose\nDistribution-Conditional Generation, a novel formulation that models creativity\nas image synthesis conditioned on class distributions, enabling semantically\nunconstrained creative generation. Building on this, we propose DisTok, an\nencoder-decoder framework that maps class distributions into a latent space and\ndecodes them into tokens of creative concept. DisTok maintains a dynamic\nconcept pool and iteratively sampling and fusing concept pairs, enabling the\ngeneration of tokens aligned with increasingly complex class distributions. To\nenforce distributional consistency, latent vectors sampled from a Gaussian\nprior are decoded into tokens and rendered into images, whose class\ndistributions-predicted by a vision-language model-supervise the alignment\nbetween input distributions and the visual semantics of generated tokens. The\nresulting tokens are added to the concept pool for subsequent composition.\nExtensive experiments demonstrate that DisTok, by unifying\ndistribution-conditioned fusion and sampling-based synthesis, enables efficient\nand flexible token-level generation, achieving state-of-the-art performance\nwith superior text-image alignment and human preference scores.",
        "url": "http://arxiv.org/abs/2505.03667v1",
        "published_date": "2025-05-06T16:07:12+00:00",
        "updated_date": "2025-05-06T16:07:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fu Feng",
            "Yucheng Xie",
            "Xu Yang",
            "Jing Wang",
            "Xin Geng"
        ],
        "tldr": "this paper introduces distribution-conditional generation and the distok framework, which uses class distributions to guide text-to-image generation for more creative and out-of-distribution image synthesis, achieving state-of-the-art performance.",
        "tldr_zh": "本文介绍了distribution-conditional generation和distok框架，该框架使用类分布来指导文本到图像的生成，从而实现更具创造性和超出分布范围的图像合成，并达到最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps from T1-Weighted MRI using CycleGAN Models",
        "summary": "Fractional anisotropy (FA) and directionally encoded colour (DEC) maps are\nessential for evaluating white matter integrity and structural connectivity in\nneuroimaging. However, the spatial misalignment between FA maps and\ntractography atlases hinders their effective integration into predictive\nmodels. To address this issue, we propose a CycleGAN based approach for\ngenerating FA maps directly from T1-weighted MRI scans, representing the first\napplication of this technique to both healthy and tumour-affected tissues. Our\nmodel, trained on unpaired data, produces high fidelity maps, which have been\nrigorously evaluated using Structural Similarity Index (SSIM) and Peak\nSignal-to-Noise Ratio (PSNR), demonstrating particularly robust performance in\ntumour regions. Radiological assessments further underscore the model's\npotential to enhance clinical workflows by providing an AI-driven alternative\nthat reduces the necessity for additional scans.",
        "url": "http://arxiv.org/abs/2505.03662v1",
        "published_date": "2025-05-06T16:05:22+00:00",
        "updated_date": "2025-05-06T16:05:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "68U10"
        ],
        "authors": [
            "Xin Du",
            "Francesca M. Cozzi",
            "Rajesh Jena"
        ],
        "tldr": "this paper introduces a cyclegan model to generate 3d fa maps from t1-weighted mri scans, particularly focusing on improving spatial alignment for brain tumor imaging applications.",
        "tldr_zh": "本文提出了一种基于cyclegan的模型，用于从t1加权mri扫描生成3d fa图，特别关注于改善脑肿瘤成像应用中的空间对齐。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bounding Box-Guided Diffusion for Synthesizing Industrial Images and Segmentation Map",
        "summary": "Synthetic dataset generation in Computer Vision, particularly for industrial\napplications, is still underexplored. Industrial defect segmentation, for\ninstance, requires highly accurate labels, yet acquiring such data is costly\nand time-consuming. To address this challenge, we propose a novel\ndiffusion-based pipeline for generating high-fidelity industrial datasets with\nminimal supervision. Our approach conditions the diffusion model on enriched\nbounding box representations to produce precise segmentation masks, ensuring\nrealistic and accurately localized defect synthesis. Compared to existing\nlayout-conditioned generative methods, our approach improves defect consistency\nand spatial accuracy. We introduce two quantitative metrics to evaluate the\neffectiveness of our method and assess its impact on a downstream segmentation\ntask trained on real and synthetic data. Our results demonstrate that\ndiffusion-based synthesis can bridge the gap between artificial and real-world\nindustrial data, fostering more reliable and cost-efficient segmentation\nmodels. The code is publicly available at\nhttps://github.com/covisionlab/diffusion_labeling.",
        "url": "http://arxiv.org/abs/2505.03623v1",
        "published_date": "2025-05-06T15:21:36+00:00",
        "updated_date": "2025-05-06T15:21:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Alessandro Simoni",
            "Francesco Pelosin"
        ],
        "tldr": "this paper introduces a diffusion-based pipeline for generating synthetic industrial images and segmentation maps using bounding box conditioning, improving defect consistency and spatial accuracy for defect segmentation tasks.",
        "tldr_zh": "本文介绍了一种基于扩散的图像生成流程，通过边界框条件约束来合成工业图像和分割图，从而提高缺陷分割任务的缺陷一致性和空间精度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model",
        "summary": "Audio-driven human animation technology is widely used in human-computer\ninteraction, and the emergence of diffusion models has further advanced its\ndevelopment. Currently, most methods rely on multi-stage generation and\nintermediate representations, resulting in long inference time and issues with\ngeneration quality in specific foreground regions and audio-motion consistency.\nThese shortcomings are primarily due to the lack of localized fine-grained\nsupervised guidance. To address above challenges, we propose PAHA, an\nend-to-end audio-driven upper-body human animation framework with diffusion\nmodel. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts\nConsistency Enhancement (PCE). PAR dynamically adjusts regional training loss\nweights based on pose confidence scores, effectively improving visual quality.\nPCE constructs and trains diffusion-based regional audio-visual classifiers to\nimprove the consistency of motion and co-speech audio. Afterwards, we design\ntwo novel inference guidance methods for the foregoing classifiers, Sequential\nGuidance (SG) and Differential Guidance (DG), to balance efficiency and quality\nrespectively. Additionally, we build CNAS, the first public Chinese News Anchor\nSpeech dataset, to advance research and validation in this field. Extensive\nexperimental results and user studies demonstrate that PAHA significantly\noutperforms existing methods in audio-motion alignment and video-related\nevaluations. The codes and CNAS dataset will be released upon acceptance.",
        "url": "http://arxiv.org/abs/2505.03603v1",
        "published_date": "2025-05-06T15:03:58+00:00",
        "updated_date": "2025-05-06T15:03:58+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Y. B. Wang",
            "S. Z. Zhou",
            "J. F. Wu",
            "T. Hu",
            "J. N. Zhang",
            "Y. Liu"
        ],
        "tldr": "the paper introduces paha, an end-to-end diffusion-based framework for audio-driven upper-body human animation that addresses limitations of existing methods by incorporating parts-aware re-weighting and consistency enhancement, along with a new chinese news anchor dataset (cnas).",
        "tldr_zh": "该论文介绍了一种名为paha的端到端扩散模型框架，用于音频驱动的上半身人体动画。该框架通过引入部件感知重新加权和一致性增强技术，解决了现有方法的局限性，并提供了一个新的中文新闻主播数据集(cnas)。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Real-Time Person Image Synthesis Using a Flow Matching Model",
        "summary": "Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images\nconditioned on a target pose and a source image. This task plays a key role in\nvarious real-world applications, such as sign language video generation, AR/VR,\ngaming, and live streaming. In these scenarios, real-time PGPIS is critical for\nproviding immediate visual feedback and maintaining user immersion.However,\nachieving real-time performance remains a significant challenge due to the\ncomplexity of synthesizing high-fidelity images from diverse and dynamic human\nposes. Recent diffusion-based methods have shown impressive image quality in\nPGPIS, but their slow sampling speeds hinder deployment in time-sensitive\napplications. This latency is particularly problematic in tasks like generating\nsign language videos during live broadcasts, where rapid image updates are\nrequired. Therefore, developing a fast and reliable PGPIS model is a crucial\nstep toward enabling real-time interactive systems. To address this challenge,\nwe propose a generative model based on flow matching (FM). Our approach enables\nfaster, more stable, and more efficient training and sampling. Furthermore, the\nproposed model supports conditional generation and can operate in latent space,\nmaking it especially suitable for real-time PGPIS applications where both speed\nand quality are critical. We evaluate our proposed method, Real-Time Person\nImage Synthesis Using a Flow Matching Model (RPFM), on the widely used\nDeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves\nnear-real-time sampling speeds while maintaining performance comparable to the\nstate-of-the-art models. Our methodology trades off a slight, acceptable\ndecrease in generated-image accuracy for over a twofold increase in generation\nspeed, thereby ensuring real-time performance.",
        "url": "http://arxiv.org/abs/2505.03562v1",
        "published_date": "2025-05-06T14:13:44+00:00",
        "updated_date": "2025-05-06T14:13:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jiwoo Jeong",
            "Kirok Kim",
            "Wooju Kim",
            "Nam-Joon Kim"
        ],
        "tldr": "this paper introduces a flow matching model (rpfm) for real-time pose-guided person image synthesis, achieving near-real-time performance with a slight trade-off in image quality compared to sota diffusion models.",
        "tldr_zh": "本文介绍了一种基于flow matching的模型（rpfm），用于实时姿势引导的人物图像合成，与sota扩散模型相比，在图像质量上略有牺牲，但实现了近实时的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI Synthesis: Advancing Pretraining and Clinical Applications",
        "summary": "Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for\ndiagnosing heart diseases and evaluating cardiac health. However, the limited\navailability of large-scale, high-quality CMR datasets poses a major challenge\nto the effective application of artificial intelligence (AI) in this domain.\nEven the amount of unlabeled data and the health status it covers are difficult\nto meet the needs of model pretraining, which hinders the performance of AI\nmodels on downstream tasks. In this study, we present Cardiac Phenotype-Guided\nCMR Generation (CPGG), a novel approach for generating diverse CMR data that\ncovers a wide spectrum of cardiac health status. The CPGG framework consists of\ntwo stages: in the first stage, a generative model is trained using cardiac\nphenotypes derived from CMR data; in the second stage, a masked autoregressive\ndiffusion model, conditioned on these phenotypes, generates high-fidelity CMR\ncine sequences that capture both structural and functional features of the\nheart in a fine-grained manner. We synthesized a massive amount of CMR to\nexpand the pretraining data. Experimental results show that CPGG generates\nhigh-quality synthetic CMR data, significantly improving performance on various\ndownstream tasks, including diagnosis and cardiac phenotypes prediction. These\ngains are demonstrated across both public and private datasets, highlighting\nthe effectiveness of our approach. Code is availabel at\nhttps://anonymous.4open.science/r/CPGG.",
        "url": "http://arxiv.org/abs/2505.03426v1",
        "published_date": "2025-05-06T11:06:41+00:00",
        "updated_date": "2025-05-06T11:06:41+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ziyu Li",
            "Yujian Hu",
            "Zhengyao Ding",
            "Yiheng Mao",
            "Haitao Li",
            "Fan Yi",
            "Hongkun Zhang",
            "Zhengxing Huang"
        ],
        "tldr": "this paper introduces cardiac phenotype-guided cmr generation (cpgg), a two-stage generative model that synthesizes high-fidelity cardiac mri data conditioned on cardiac phenotypes, demonstrating improved performance on downstream tasks after pretraining with the synthetic data.",
        "tldr_zh": "该论文介绍了一种名为心脏表型引导的cmr生成(cpgg)的两阶段生成模型，该模型在心脏表型的条件下合成高保真的心脏mri数据，并证明了使用合成数据进行预训练后，在下游任务中的性能得到了提高。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "EOPose : Exemplar-based object reposing using Generalized Pose Correspondences",
        "summary": "Reposing objects in images has a myriad of applications, especially for\ne-commerce where several variants of product images need to be produced\nquickly. In this work, we leverage the recent advances in unsupervised keypoint\ncorrespondence detection between different object images of the same class to\npropose an end-to-end framework for generic object reposing. Our method,\nEOPose, takes a target pose-guidance image as input and uses its keypoint\ncorrespondence with the source object image to warp and re-render the latter\ninto the target pose using a novel three-step approach. Unlike generative\napproaches, our method also preserves the fine-grained details of the object\nsuch as its exact colors, textures, and brand marks. We also prepare a new\ndataset of paired objects based on the Objaverse dataset to train and test our\nnetwork. EOPose produces high-quality reposing output as evidenced by different\nimage quality metrics (PSNR, SSIM and FID). Besides a description of the method\nand the dataset, the paper also includes detailed ablation and user studies to\nindicate the efficacy of the proposed method",
        "url": "http://arxiv.org/abs/2505.03394v1",
        "published_date": "2025-05-06T10:17:32+00:00",
        "updated_date": "2025-05-06T10:17:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sarthak Mehrotra",
            "Rishabh Jain",
            "Mayur Hemani",
            "Balaji Krishnamurthy",
            "Mausoom Sarkar"
        ],
        "tldr": "the paper introduces eopose, a novel end-to-end framework for object reposing using unsupervised keypoint correspondences, preserving fine-grained details and demonstrating high-quality results on a new objaverse-based dataset.",
        "tldr_zh": "该论文介绍了eopose，一种新颖的端到端物体姿态调整框架，它使用无监督的关键点对应关系，保留了精细的细节，并在一个新的基于objaverse的数据集上展示了高质量的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant",
        "summary": "Medical AI assistants support doctors in disease diagnosis, medical image\nanalysis, and report generation. However, they still face significant\nchallenges in clinical use, including limited accuracy with multimodal content\nand insufficient validation in real-world settings. We propose RCMed, a\nfull-stack AI assistant that improves multimodal alignment in both input and\noutput, enabling precise anatomical delineation, accurate localization, and\nreliable diagnosis through hierarchical vision-language grounding. A\nself-reinforcing correlation mechanism allows visual features to inform\nlanguage context, while language semantics guide pixel-wise attention, forming\na closed loop that refines both modalities. This correlation is enhanced by a\ncolor region description strategy, translating anatomical structures into\nsemantically rich text to learn shape-location-text relationships across\nscales. Trained on 20 million image-mask-description triplets, RCMed achieves\nstate-of-the-art precision in contextualizing irregular lesions and subtle\nanatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It\nachieved a 23.5% relative improvement in cell segmentation from microscopy\nimages over prior methods. RCMed's strong vision-language alignment enables\nexceptional generalization, with state-of-the-art performance in external\nvalidation across 20 clinically significant cancer types, including novel\ntasks. This work demonstrates how integrated multimodal models capture\nfine-grained patterns, enabling human-level interpretation in complex scenarios\nand advancing human-centric AI healthcare.",
        "url": "http://arxiv.org/abs/2505.03380v1",
        "published_date": "2025-05-06T10:00:08+00:00",
        "updated_date": "2025-05-06T10:00:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "eess.IV"
        ],
        "authors": [
            "Haonan Wang",
            "Jiaji Mao",
            "Lehan Wang",
            "Qixiang Zhang",
            "Marawan Elbatel",
            "Yi Qin",
            "Huijun Hu",
            "Baoxun Li",
            "Wenhui Deng",
            "Weifeng Qin",
            "Hongrui Li",
            "Jialin Liang",
            "Jun Shen",
            "Xiaomeng Li"
        ],
        "tldr": "the paper introduces rcmed, a novel medical ai assistant leveraging reinforced vision-language correlation and a color region description strategy to achieve state-of-the-art performance in medical image analysis and diagnosis across numerous clinical tasks and cancer types.",
        "tldr_zh": "该论文介绍了rcmed，一种新型医学ai助手，利用强化视觉-语言相关性和颜色区域描述策略，在医学图像分析和诊断方面实现了最先进的性能，涵盖了众多临床任务和癌症类型。",
        "relevance_score": 6,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for Scene Text Editing",
        "summary": "The task of scene text editing is to modify or add texts on images while\nmaintaining the fidelity of newly generated text and visual coherence with the\nbackground. Recent works based on latent diffusion models (LDM) show improved\ntext editing results, yet still face challenges and often generate inaccurate\nor unrecognizable characters, especially for non-Latin ones (\\eg, Chinese),\nwhich have complex glyph structures. To address these issues, we present\nFLUX-Text, a simple and advanced multilingual scene text editing framework\nbased on FLUX-Fill. Specifically, we carefully investigate glyph conditioning,\nconsidering both visual and textual modalities. To retain the original\ngenerative capabilities of FLUX-Fill while enhancing its understanding and\ngeneration of glyphs, we propose lightweight glyph and text embedding modules.\nOwning to the lightweight design, FLUX-Text is trained only with $100K$\ntraining examples compared to current popular methods trained with 2.9M ones.\nWith no bells and whistles, our method achieves state-of-the-art performance on\ntext editing tasks. Qualitative and quantitative experiments on the public\ndatasets demonstrate that our method surpasses previous works in text fidelity.",
        "url": "http://arxiv.org/abs/2505.03329v1",
        "published_date": "2025-05-06T08:56:28+00:00",
        "updated_date": "2025-05-06T08:56:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rui Lan",
            "Yancheng Bai",
            "Xu Duan",
            "Mingxing Li",
            "Lei Sun",
            "Xiangxiang Chu"
        ],
        "tldr": "flux-text is a new scene text editing framework based on flux-fill that uses lightweight glyph and text embedding modules to improve text generation accuracy with fewer training samples, especially for non-latin characters.",
        "tldr_zh": "flux-text是一种新的基于flux-fill的场景文本编辑框架，它采用轻量级的字形和文本嵌入模块，以更少的训练样本提高了文本生成精度，尤其是在非拉丁字符方面。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning",
        "summary": "Recent advances in multimodal Reward Models (RMs) have shown significant\npromise in delivering reward signals to align vision models with human\npreferences. However, current RMs are generally restricted to providing direct\nresponses or engaging in shallow reasoning processes with limited depth, often\nleading to inaccurate reward signals. We posit that incorporating explicit long\nchains of thought (CoT) into the reward reasoning process can significantly\nstrengthen their reliability and robustness. Furthermore, we believe that once\nRMs internalize CoT reasoning, their direct response accuracy can also be\nimproved through implicit reasoning capabilities. To this end, this paper\nproposes UnifiedReward-Think, the first unified multimodal CoT-based reward\nmodel, capable of multi-dimensional, step-by-step long-chain reasoning for both\nvisual understanding and generation reward tasks. Specifically, we adopt an\nexploration-driven reinforcement fine-tuning approach to elicit and incentivize\nthe model's latent complex reasoning ability: (1) We first use a small amount\nof image generation preference data to distill the reasoning process of GPT-4o,\nwhich is then used for the model's cold start to learn the format and structure\nof CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge\nand generalization capabilities, we prepare large-scale unified multimodal\npreference data to elicit the model's reasoning process across various vision\ntasks. During this phase, correct reasoning outputs are retained for rejection\nsampling to refine the model (3) while incorrect predicted samples are finally\nused for Group Relative Policy Optimization (GRPO) based reinforcement\nfine-tuning, enabling the model to explore diverse reasoning paths and optimize\nfor correct and robust solutions. Extensive experiments across various vision\nreward tasks demonstrate the superiority of our model.",
        "url": "http://arxiv.org/abs/2505.03318v1",
        "published_date": "2025-05-06T08:46:41+00:00",
        "updated_date": "2025-05-06T08:46:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yibin Wang",
            "Zhimin Li",
            "Yuhang Zang",
            "Chunyu Wang",
            "Qinglin Lu",
            "Cheng Jin",
            "Jiaqi Wang"
        ],
        "tldr": "this paper introduces unifiedreward-think, a novel multimodal chain-of-thought reward model fine-tuned with reinforcement learning for enhanced reasoning in visual understanding and generation tasks, demonstrating improved performance across various vision reward tasks.",
        "tldr_zh": "本文介绍了 unifiedreward-think，一种新型多模态链式思维奖励模型，通过强化学习进行微调，以增强视觉理解和生成任务中的推理能力，并在各种视觉奖励任务中表现出改进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PiCo: Enhancing Text-Image Alignment with Improved Noise Selection and Precise Mask Control in Diffusion Models",
        "summary": "Advanced diffusion models have made notable progress in text-to-image\ncompositional generation. However, it is still a challenge for existing models\nto achieve text-image alignment when confronted with complex text prompts. In\nthis work, we highlight two factors that affect this alignment: the quality of\nthe randomly initialized noise and the reliability of the generated controlling\nmask. We then propose PiCo (Pick-and-Control), a novel training-free approach\nwith two key components to tackle these two factors. First, we develop a noise\nselection module to assess the quality of the random noise and determine\nwhether the noise is suitable for the target text. A fast sampling strategy is\nutilized to ensure efficiency in the noise selection stage. Second, we\nintroduce a referring mask module to generate pixel-level masks and to\nprecisely modulate the cross-attention maps. The referring mask is applied to\nthe standard diffusion process to guide the reasonable interaction between text\nand image features. Extensive experiments have been conducted to verify the\neffectiveness of PiCo in liberating users from the tedious process of random\ngeneration and in enhancing the text-image alignment for diverse text\ndescriptions.",
        "url": "http://arxiv.org/abs/2505.03203v1",
        "published_date": "2025-05-06T05:38:13+00:00",
        "updated_date": "2025-05-06T05:38:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chang Xie",
            "Chenyi Zhuang",
            "Pan Gao"
        ],
        "tldr": "pico is a training-free approach that improves text-image alignment in diffusion models by selecting suitable noise and precisely modulating cross-attention maps with pixel-level masks. it aims to enhance alignment for complex text prompts.",
        "tldr_zh": "pico是一种无需训练的方法，通过选择合适的噪声和使用像素级掩码精确调制交叉注意力图，来改进扩散模型中的文本-图像对齐。它的目的是增强复杂文本提示的对齐效果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Not All Parameters Matter: Masking Diffusion Models for Enhancing Generation Ability",
        "summary": "The diffusion models, in early stages focus on constructing basic image\nstructures, while the refined details, including local features and textures,\nare generated in later stages. Thus the same network layers are forced to learn\nboth structural and textural information simultaneously, significantly\ndiffering from the traditional deep learning architectures (e.g., ResNet or\nGANs) which captures or generates the image semantic information at different\nlayers. This difference inspires us to explore the time-wise diffusion models.\nWe initially investigate the key contributions of the U-Net parameters to the\ndenoising process and identify that properly zeroing out certain parameters\n(including large parameters) contributes to denoising, substantially improving\nthe generation quality on the fly. Capitalizing on this discovery, we propose a\nsimple yet effective method-termed ``MaskUNet''- that enhances generation\nquality with negligible parameter numbers. Our method fully leverages timestep-\nand sample-dependent effective U-Net parameters. To optimize MaskUNet, we offer\ntwo fine-tuning strategies: a training-based approach and a training-free\napproach, including tailored networks and optimization functions. In zero-shot\ninference on the COCO dataset, MaskUNet achieves the best FID score and further\ndemonstrates its effectiveness in downstream task evaluations. Project page:\nhttps://gudaochangsheng.github.io/MaskUnet-Page/",
        "url": "http://arxiv.org/abs/2505.03097v1",
        "published_date": "2025-05-06T01:14:20+00:00",
        "updated_date": "2025-05-06T01:14:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lei Wang",
            "Senmao Li",
            "Fei Yang",
            "Jianye Wang",
            "Ziheng Zhang",
            "Yuhan Liu",
            "Yaxing Wang",
            "Jian Yang"
        ],
        "tldr": "the paper proposes maskunet, a method that improves diffusion model generation quality by selectively masking u-net parameters based on their contribution to the denoising process, leading to better fid scores.",
        "tldr_zh": "该论文提出了maskunet，一种通过根据u-net参数对去噪过程的贡献选择性地屏蔽它们来提高扩散模型生成质量的方法，从而获得更好的fid分数。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Generating Narrated Lecture Videos from Slides with Synchronized Highlights",
        "summary": "Turning static slides into engaging video lectures takes considerable time\nand effort, requiring presenters to record explanations and visually guide\ntheir audience through the material. We introduce an end-to-end system designed\nto automate this process entirely. Given a slide deck, this system synthesizes\na video lecture featuring AI-generated narration synchronized precisely with\ndynamic visual highlights. These highlights automatically draw attention to the\nspecific concept being discussed, much like an effective presenter would. The\ncore technical contribution is a novel highlight alignment module. This module\naccurately maps spoken phrases to locations on a given slide using diverse\nstrategies (e.g., Levenshtein distance, LLM-based semantic analysis) at\nselectable granularities (line or word level) and utilizes timestamp-providing\nText-to-Speech (TTS) for timing synchronization. We demonstrate the system's\neffectiveness through a technical evaluation using a manually annotated slide\ndataset with 1000 samples, finding that LLM-based alignment achieves high\nlocation accuracy (F1 > 92%), significantly outperforming simpler methods,\nespecially on complex, math-heavy content. Furthermore, the calculated\ngeneration cost averages under $1 per hour of video, offering potential savings\nof two orders of magnitude compared to conservative estimates of manual\nproduction costs. This combination of high accuracy and extremely low cost\npositions this approach as a practical and scalable tool for transforming\nstatic slides into effective, visually-guided video lectures.",
        "url": "http://arxiv.org/abs/2505.02966v1",
        "published_date": "2025-05-05T18:51:53+00:00",
        "updated_date": "2025-05-05T18:51:53+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Alexander Holmberg"
        ],
        "tldr": "the paper introduces an end-to-end system that automatically generates narrated lecture videos from slides with synchronized visual highlights, using llm-based alignment for high accuracy and low cost.",
        "tldr_zh": "该论文介绍了一个端到端系统，可以从幻灯片自动生成带有同步视觉高亮的讲解视频，利用基于llm的对齐方式实现高精度和低成本。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generating Synthetic Data via Augmentations for Improved Facial Resemblance in DreamBooth and InstantID",
        "summary": "The personalization of Stable Diffusion for generating professional portraits\nfrom amateur photographs is a burgeoning area, with applications in various\ndownstream contexts. This paper investigates the impact of augmentations on\nimproving facial resemblance when using two prominent personalization\ntechniques: DreamBooth and InstantID. Through a series of experiments with\ndiverse subject datasets, we assessed the effectiveness of various augmentation\nstrategies on the generated headshots' fidelity to the original subject. We\nintroduce FaceDistance, a wrapper around FaceNet, to rank the generations based\non facial similarity, which aided in our assessment. Ultimately, this research\nprovides insights into the role of augmentations in enhancing facial\nresemblance in SDXL-generated portraits, informing strategies for their\neffective deployment in downstream applications.",
        "url": "http://arxiv.org/abs/2505.03557v1",
        "published_date": "2025-05-06T14:11:02+00:00",
        "updated_date": "2025-05-06T14:11:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Koray Ulusan",
            "Benjamin Kiefer"
        ],
        "tldr": "this paper explores how different augmentation strategies affect facial resemblance in portraits generated by dreambooth and instantid, using facedistance (a facenet wrapper) to evaluate similarity.",
        "tldr_zh": "本文研究了不同增强策略如何影响dreambooth和instantid生成的肖像中的面部相似性，并使用facedistance（facenet包装器）来评估相似度。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Mitigating Image Captioning Hallucinations in Vision-Language Models",
        "summary": "Hallucinations in vision-language models (VLMs) hinder reliability and\nreal-world applicability, usually stemming from distribution shifts between\npretraining data and test samples. Existing solutions, such as retraining or\nfine-tuning on additional data, demand significant computational resources and\nlabor-intensive data collection, while ensemble-based methods incur additional\ncosts by introducing auxiliary VLMs. To address these challenges, we propose a\nnovel test-time adaptation framework using reinforcement learning to mitigate\nhallucinations during inference without retraining or any auxiliary VLMs. By\nupdating only the learnable parameters in the layer normalization of the\nlanguage model (approximately 0.003% of the model parameters), our method\nreduces distribution shifts between test samples and pretraining samples. A\nCLIP-based hallucination evaluation model is proposed to provide dual rewards\nto VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in\nhallucination rates on LLaVA and InstructBLIP, respectively. Our approach\noutperforms state-of-the-art baselines with a 68.3% improvement in\nhallucination mitigation, demonstrating its effectiveness.",
        "url": "http://arxiv.org/abs/2505.03420v1",
        "published_date": "2025-05-06T10:55:21+00:00",
        "updated_date": "2025-05-06T10:55:21+00:00",
        "categories": [
            "cs.MM",
            "cs.CV"
        ],
        "authors": [
            "Fei Zhao",
            "Chengcui Zhang",
            "Runlin Zhang",
            "Tianyang Wang",
            "Xi Li"
        ],
        "tldr": "this paper proposes a test-time adaptation framework using reinforcement learning to mitigate hallucinations in vision-language models, achieving significant reductions in hallucination rates without retraining or auxiliary models by only updating a tiny percentage of the model parameters.",
        "tldr_zh": "本文提出了一种使用强化学习的测试时自适应框架，以减轻视觉-语言模型中的幻觉。通过仅更新模型参数的一小部分，该方法无需重新训练或辅助模型即可显著降低幻觉率。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DDaTR: Dynamic Difference-aware Temporal Residual Network for Longitudinal Radiology Report Generation",
        "summary": "Radiology Report Generation (RRG) automates the creation of radiology reports\nfrom medical imaging, enhancing the efficiency of the reporting process.\nLongitudinal Radiology Report Generation (LRRG) extends RRG by incorporating\nthe ability to compare current and prior exams, facilitating the tracking of\ntemporal changes in clinical findings. Existing LRRG approaches only extract\nfeatures from prior and current images using a visual pre-trained encoder,\nwhich are then concatenated to generate the final report. However, these\nmethods struggle to effectively capture both spatial and temporal correlations\nduring the feature extraction process. Consequently, the extracted features\ninadequately capture the information of difference across exams and thus\nunderrepresent the expected progressions, leading to sub-optimal performance in\nLRRG. To address this, we develop a novel dynamic difference-aware temporal\nresidual network (DDaTR). In DDaTR, we introduce two modules at each stage of\nthe visual encoder to capture multi-level spatial correlations. The Dynamic\nFeature Alignment Module (DFAM) is designed to align prior features across\nmodalities for the integrity of prior clinical information. Prompted by the\nenriched prior features, the dynamic difference-aware module (DDAM) captures\nfavorable difference information by identifying relationships across exams.\nFurthermore, our DDaTR employs the dynamic residual network to unidirectionally\ntransmit longitudinal information, effectively modelling temporal correlations.\nExtensive experiments demonstrated superior performance over existing methods\non three benchmarks, proving its efficacy in both RRG and LRRG tasks.",
        "url": "http://arxiv.org/abs/2505.03401v1",
        "published_date": "2025-05-06T10:29:23+00:00",
        "updated_date": "2025-05-06T10:29:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shanshan Song",
            "Hui Tang",
            "Honglong Yang",
            "Xiaomeng Li"
        ],
        "tldr": "the paper introduces ddatr, a novel neural network architecture for longitudinal radiology report generation (lrrg) that enhances spatial and temporal correlation capture, leading to improved performance on existing benchmarks.",
        "tldr_zh": "该论文介绍了ddatr，一种用于纵向放射学报告生成(lrrg)的新型神经网络架构，它增强了空间和时间相关性的捕获，从而提高了现有基准上的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Interpretable Zero-shot Learning with Infinite Class Concepts",
        "summary": "Zero-shot learning (ZSL) aims to recognize unseen classes by aligning images\nwith intermediate class semantics, like human-annotated concepts or class\ndefinitions. An emerging alternative leverages Large-scale Language Models\n(LLMs) to automatically generate class documents. However, these methods often\nface challenges with transparency in the classification process and may suffer\nfrom the notorious hallucination problem in LLMs, resulting in non-visual class\nsemantics. This paper redefines class semantics in ZSL with a focus on\ntransferability and discriminability, introducing a novel framework called\nZero-shot Learning with Infinite Class Concepts (InfZSL). Our approach\nleverages the powerful capabilities of LLMs to dynamically generate an\nunlimited array of phrase-level class concepts. To address the hallucination\nchallenge, we introduce an entropy-based scoring process that incorporates a\n``goodness\" concept selection mechanism, ensuring that only the most\ntransferable and discriminative concepts are selected. Our InfZSL framework not\nonly demonstrates significant improvements on three popular benchmark datasets\nbut also generates highly interpretable, image-grounded concepts. Code will be\nreleased upon acceptance.",
        "url": "http://arxiv.org/abs/2505.03361v1",
        "published_date": "2025-05-06T09:30:30+00:00",
        "updated_date": "2025-05-06T09:30:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zihan Ye",
            "Shreyank N Gowda",
            "Shiming Chen",
            "Yaochu Jin",
            "Kaizhu Huang",
            "Xiaobo Jin"
        ],
        "tldr": "the paper introduces infzsl, a zero-shot learning framework that leverages llms to generate infinite class concepts, using entropy-based scoring to select transferable and discriminative concepts and improve interpretability and performance.",
        "tldr_zh": "本文介绍了一种名为infzsl的零样本学习框架，该框架利用llm生成无限的类别概念，并使用基于熵的评分来选择可转移的和可区分的概念，从而提高可解释性和性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Enhancing Glass Defect Detection with Diffusion Models: Addressing Imbalanced Datasets in Manufacturing Quality Control",
        "summary": "Visual defect detection in industrial glass manufacturing remains a critical\nchallenge due to the low frequency of defective products, leading to imbalanced\ndatasets that limit the performance of deep learning models and computer vision\nsystems. This paper presents a novel approach using Denoising Diffusion\nProbabilistic Models (DDPMs) to generate synthetic defective glass product\nimages for data augmentation, effectively addressing class imbalance issues in\nmanufacturing quality control and automated visual inspection. The methodology\nsignificantly enhances image classification performance of standard CNN\narchitectures (ResNet50V2, EfficientNetB0, and MobileNetV2) in detecting\nanomalies by increasing the minority class representation. Experimental results\ndemonstrate substantial improvements in key machine learning metrics,\nparticularly in recall for defective samples across all tested deep neural\nnetwork architectures while maintaining perfect precision. The most dramatic\nimprovement was observed in ResNet50V2's overall classification accuracy, which\nincreased from 78 percent to 93 percent when trained with the augmented data.\nThis work provides a scalable, cost-effective approach to enhancing automated\ndefect detection in glass manufacturing that can potentially be extended to\nother industrial quality assurance systems and industries with similar class\nimbalance challenges.",
        "url": "http://arxiv.org/abs/2505.03134v1",
        "published_date": "2025-05-06T03:16:56+00:00",
        "updated_date": "2025-05-06T03:16:56+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Sajjad Rezvani Boroujeni",
            "Hossein Abedi",
            "Tom Bush"
        ],
        "tldr": "this paper uses diffusion models to generate synthetic defective glass images to address class imbalance issues, significantly improving the performance of cnns in defect detection.",
        "tldr_zh": "该论文使用扩散模型生成合成的有缺陷玻璃图像，以解决类别不平衡问题，从而显著提高了卷积神经网络在缺陷检测中的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion",
        "summary": "Video frame interpolation (VFI) that leverages the bio-inspired event cameras\nas guidance has recently shown better performance and memory efficiency than\nthe frame-based methods, thanks to the event cameras' advantages, such as high\ntemporal resolution. A hurdle for event-based VFI is how to effectively deal\nwith non-linear motion, caused by the dynamic changes in motion direction and\nspeed within the scene. Existing methods either use events to estimate sparse\noptical flow or fuse events with image features to estimate dense optical flow.\nUnfortunately, motion errors often degrade the VFI quality as the continuous\nmotion cues from events do not align with the dense spatial information of\nimages in the temporal dimension. In this paper, we find that object motion is\ncontinuous in space, tracking local regions over continuous time enables more\naccurate identification of spatiotemporal feature correlations. In light of\nthis, we propose a novel continuous point tracking-based VFI framework, named\nTimeTracker. Specifically, we first design a Scene-Aware Region Segmentation\n(SARS) module to divide the scene into similar patches. Then, a Continuous\nTrajectory guided Motion Estimation (CTME) module is proposed to track the\ncontinuous motion trajectory of each patch through events. Finally,\nintermediate frames at any given time are generated through global motion\noptimization and frame refinement. Moreover, we collect a real-world dataset\nthat features fast non-linear motion. Extensive experiments show that our\nmethod outperforms prior arts in both motion estimation and frame interpolation\nquality.",
        "url": "http://arxiv.org/abs/2505.03116v1",
        "published_date": "2025-05-06T02:12:19+00:00",
        "updated_date": "2025-05-06T02:12:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoyue Liu",
            "Jinghan Xu",
            "Yi Chang",
            "Hanyu Zhou",
            "Haozhi Zhao",
            "Lin Wang",
            "Luxin Yan"
        ],
        "tldr": "the paper introduces timetracker, a novel video frame interpolation framework that utilizes event cameras and continuous point tracking to better handle non-linear motion, outperforming existing methods in motion estimation and frame interpolation quality.",
        "tldr_zh": "该论文介绍了timetracker，一种新颖的视频帧插值框架，利用事件相机和连续点跟踪来更好地处理非线性运动，在运动估计和帧插值质量方面优于现有方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Lesion-Aware Generative Artificial Intelligence for Virtual Contrast-Enhanced Mammography in Breast Cancer",
        "summary": "Contrast-Enhanced Spectral Mammography (CESM) is a dual-energy mammographic\ntechnique that improves lesion visibility through the administration of an\niodinated contrast agent. It acquires both a low-energy image, comparable to\nstandard mammography, and a high-energy image, which are then combined to\nproduce a dual-energy subtracted image highlighting lesion contrast\nenhancement. While CESM offers superior diagnostic accuracy compared to\nstandard mammography, its use entails higher radiation exposure and potential\nside effects associated with the contrast medium. To address these limitations,\nwe propose Seg-CycleGAN, a generative deep learning framework for Virtual\nContrast Enhancement in CESM. The model synthesizes high-fidelity dual-energy\nsubtracted images from low-energy images, leveraging lesion segmentation maps\nto guide the generative process and improve lesion reconstruction. Building\nupon the standard CycleGAN architecture, Seg-CycleGAN introduces localized loss\nterms focused on lesion areas, enhancing the synthesis of diagnostically\nrelevant regions. Experiments on the CESM@UCBM dataset demonstrate that\nSeg-CycleGAN outperforms the baseline in terms of PSNR and SSIM, while\nmaintaining competitive MSE and VIF. Qualitative evaluations further confirm\nimproved lesion fidelity in the generated images. These results suggest that\nsegmentation-aware generative models offer a viable pathway toward\ncontrast-free CESM alternatives.",
        "url": "http://arxiv.org/abs/2505.03018v1",
        "published_date": "2025-05-05T20:41:30+00:00",
        "updated_date": "2025-05-05T20:41:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Aurora Rofena",
            "Arianna Manchia",
            "Claudia Lucia Piccolo",
            "Bruno Beomonte Zobel",
            "Paolo Soda",
            "Valerio Guarrasi"
        ],
        "tldr": "this paper introduces seg-cyclegan, a lesion-segmentation aware generative model for synthesizing contrast-enhanced mammograms from standard mammograms, potentially reducing radiation exposure and contrast agent side effects.",
        "tldr_zh": "本文介绍了一种病灶分割感知的生成模型seg-cyclegan，用于从标准乳房x光照片合成对比增强的乳房x光照片，从而可能减少辐射暴露和造影剂副作用。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models",
        "summary": "Current self-supervised algorithms mostly rely on transformations such as\ndata augmentation and masking to learn visual representations. This is achieved\nby inducing invariance or equivariance with respect to these transformations\nafter encoding two views of an image. This dominant two-view paradigm can limit\nthe flexibility of learned representations for downstream adaptation by\ncreating performance trade-offs between invariance-related tasks such as image\nclassification and more fine-grained equivariance-related tasks. In this work,\nwe introduce \\emph{seq-JEPA}, a world modeling paradigm based on\njoint-embedding predictive architecture that leverages architectural inductive\nbiases to resolve this trade-off. Without requiring an additional equivariance\npredictor or loss term, seq-JEPA simultaneously learns two architecturally\nsegregated representations: one equivariant to the specified transformations\nand another invariant to them and suited for tasks such as classification. To\ndo so, our model processes a short sequence of different views (observations)\nof an input image. Each encoded view is concatenated with embeddings\ncorresponding to the relative transformation (action) producing the next\nobservation in the sequence. A transformer encoder outputs an aggregate\nrepresentation of this sequence, which is subsequently conditioned on the\naction leading to the next observation to predict its representation.\nEmpirically, seq-JEPA achieves strong performance on equivariant benchmarks and\nimage classification without sacrificing one for the other. Additionally, our\nframework excels at tasks that inherently require aggregating a sequence of\nobservations, such as path integration across actions and predictive learning\nacross eye movements.",
        "url": "http://arxiv.org/abs/2505.03176v1",
        "published_date": "2025-05-06T04:39:11+00:00",
        "updated_date": "2025-05-06T04:39:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Hafez Ghaemi",
            "Eilif Muller",
            "Shahab Bakhtiari"
        ],
        "tldr": "the paper introduces seq-jepa, a world modeling paradigm that learns both invariant and equivariant representations from sequential views of an image by incorporating architectural inductive biases, achieving strong performance on both invariance and equivariance tasks.",
        "tldr_zh": "该论文介绍了 seq-jepa，一种世界建模范式，通过结合架构归纳偏置，从图像的序列视图中学习不变和等变表示，从而在不变性和等变性任务上都取得了强大的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Path and Bone-Contour Regularized Unpaired MRI-to-CT Translation",
        "summary": "Accurate MRI-to-CT translation promises the integration of complementary\nimaging information without the need for additional imaging sessions. Given the\npractical challenges associated with acquiring paired MRI and CT scans, the\ndevelopment of robust methods capable of leveraging unpaired datasets is\nessential for advancing the MRI-to-CT translation. Current unpaired MRI-to-CT\ntranslation methods, which predominantly rely on cycle consistency and\ncontrastive learning frameworks, frequently encounter challenges in accurately\ntranslating anatomical features that are highly discernible on CT but less\ndistinguishable on MRI, such as bone structures. This limitation renders these\napproaches less suitable for applications in radiation therapy, where precise\nbone representation is essential for accurate treatment planning. To address\nthis challenge, we propose a path- and bone-contour regularized approach for\nunpaired MRI-to-CT translation. In our method, MRI and CT images are projected\nto a shared latent space, where the MRI-to-CT mapping is modeled as a\ncontinuous flow governed by neural ordinary differential equations. The optimal\nmapping is obtained by minimizing the transition path length of the flow. To\nenhance the accuracy of translated bone structures, we introduce a trainable\nneural network to generate bone contours from MRI and implement mechanisms to\ndirectly and indirectly encourage the model to focus on bone contours and their\nadjacent regions. Evaluations conducted on three datasets demonstrate that our\nmethod outperforms existing unpaired MRI-to-CT translation approaches,\nachieving lower overall error rates. Moreover, in a downstream bone\nsegmentation task, our approach exhibits superior performance in preserving the\nfidelity of bone structures. Our code is available at:\nhttps://github.com/kennysyp/PaBoT.",
        "url": "http://arxiv.org/abs/2505.03114v1",
        "published_date": "2025-05-06T02:08:35+00:00",
        "updated_date": "2025-05-06T02:08:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Teng Zhou",
            "Jax Luo",
            "Yuping Sun",
            "Yiheng Tan",
            "Shun Yao",
            "Nazim Haouchine",
            "Scott Raymond"
        ],
        "tldr": "this paper introduces a path- and bone-contour regularized unpaired mri-to-ct translation method using neural odes to improve bone structure accuracy, which is crucial for radiation therapy applications.",
        "tldr_zh": "本文提出了一种基于路径和骨骼轮廓正则化的无配对mri到ct的转换方法，该方法利用神经常微分方程来提高骨骼结构的准确性，这对于放射治疗应用至关重要。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 6
    },
    {
        "title": "Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets",
        "summary": "Instruction-Action (IA) data pairs are valuable for training robotic systems,\nespecially autonomous vehicles (AVs), but having humans manually annotate this\ndata is costly and time-inefficient. This paper explores the potential of using\nmobile application Global Positioning System (GPS) references and Natural\nLanguage Processing (NLP) to automatically generate large volumes of IA\ncommands and responses without having a human generate or retroactively tag the\ndata. In our pilot data collection, by driving to various destinations and\ncollecting voice instructions from GPS applications, we demonstrate a means to\ncollect and categorize the diverse sets of instructions, further accompanied by\nvideo data to form complete vision-language-action triads. We provide details\non our completely automated data collection prototype system, ADVLAT-Engine. We\ncharacterize collected GPS voice instructions into eight different\nclassifications, highlighting the breadth of commands and referentialities\navailable for curation from freely available mobile applications. Through\nresearch and exploration into the automation of IA data pairs using GPS\nreferences, the potential to increase the speed and volume at which\nhigh-quality IA datasets are created, while minimizing cost, can pave the way\nfor robust vision-language-action (VLA) models to serve tasks in\nvision-language navigation (VLN) and human-interactive autonomous systems.",
        "url": "http://arxiv.org/abs/2505.03174v1",
        "published_date": "2025-05-06T04:38:41+00:00",
        "updated_date": "2025-05-06T04:38:41+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Guillermo Roque",
            "Erika Maquiling",
            "Jose Giovanni Tapia Lopez",
            "Ross Greer"
        ],
        "tldr": "this paper presents an automated system (advlat-engine) that uses gps and nlp to generate instruction-action pairs for training autonomous vehicles, thereby reducing the cost and time associated with manual annotation.",
        "tldr_zh": "本文提出了一种自动系统（advlat-engine），它使用gps和nlp生成指令-行动对，用于训练自动驾驶汽车，从而降低与手动注释相关的成本和时间。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph",
        "summary": "Comprehending long videos remains a significant challenge for Large\nMulti-modal Models (LMMs). Current LMMs struggle to process even minutes to\nhours videos due to their lack of explicit memory and retrieval mechanisms. To\naddress this limitation, we propose RAVU (Retrieval Augmented Video\nUnderstanding), a novel framework for video understanding enhanced by retrieval\nwith compositional reasoning over a spatio-temporal graph. We construct a graph\nrepresentation of the video, capturing both spatial and temporal relationships\nbetween entities. This graph serves as a long-term memory, allowing us to track\nobjects and their actions across time. To answer complex queries, we decompose\nthe queries into a sequence of reasoning steps and execute these steps on the\ngraph, retrieving relevant key information. Our approach enables more accurate\nunderstanding of long videos, particularly for queries that require multi-hop\nreasoning and tracking objects across frames. Our approach demonstrate superior\nperformances with limited retrieved frames (5-10) compared with other SOTA\nmethods and baselines on two major video QA datasets, NExT-QA and EgoSchema.",
        "url": "http://arxiv.org/abs/2505.03173v1",
        "published_date": "2025-05-06T04:38:09+00:00",
        "updated_date": "2025-05-06T04:38:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sameer Malik",
            "Moyuru Yamada",
            "Ayush Singh",
            "Dishank Aggarwal"
        ],
        "tldr": "ravu is a retrieval-augmented framework for long video understanding that uses a spatio-temporal graph to capture relationships between entities, enabling multi-hop reasoning and improved performance on video qa tasks.",
        "tldr_zh": "ravu是一个检索增强的框架，用于长视频理解，它使用时空图来捕捉实体之间的关系，从而实现多跳推理，并在视频问答任务中提高性能。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "Completing Spatial Transcriptomics Data for Gene Expression Prediction Benchmarking",
        "summary": "Spatial Transcriptomics is a groundbreaking technology that integrates\nhistology images with spatially resolved gene expression profiles. Among the\nvarious Spatial Transcriptomics techniques available, Visium has emerged as the\nmost widely adopted. However, its accessibility is limited by high costs, the\nneed for specialized expertise, and slow clinical integration. Additionally,\ngene capture inefficiencies lead to significant dropout, corrupting acquired\ndata. To address these challenges, the deep learning community has explored the\ngene expression prediction task directly from histology images. Yet,\ninconsistencies in datasets, preprocessing, and training protocols hinder fair\ncomparisons between models. To bridge this gap, we introduce SpaRED, a\nsystematically curated database comprising 26 public datasets, providing a\nstandardized resource for model evaluation. We further propose SpaCKLE, a\nstate-of-the-art transformer-based gene expression completion model that\nreduces mean squared error by over 82.5% compared to existing approaches.\nFinally, we establish the SpaRED benchmark, evaluating eight state-of-the-art\nprediction models on both raw and SpaCKLE-completed data, demonstrating SpaCKLE\nsubstantially improves the results across all the gene expression prediction\nmodels. Altogether, our contributions constitute the most comprehensive\nbenchmark of gene expression prediction from histology images to date and a\nstepping stone for future research on Spatial Transcriptomics.",
        "url": "http://arxiv.org/abs/2505.02980v1",
        "published_date": "2025-05-05T19:17:29+00:00",
        "updated_date": "2025-05-05T19:17:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Daniela Ruiz",
            "Paula Cardenas",
            "Leonardo Manrique",
            "Daniela Vega",
            "Gabriel Mejia",
            "Pablo Arbelaez"
        ],
        "tldr": "this paper introduces spared, a curated database for spatial transcriptomics, and spackle, a transformer-based gene expression completion model, significantly improving gene expression prediction from histology images.",
        "tldr_zh": "该论文介绍了一个用于空间转录组学的精选数据库spared，以及一个基于transformer的基因表达补全模型spackle，显著提高了从组织学图像中预测基因表达的效果。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks",
        "summary": "Deep learning has substantially advanced the Single Image Super-Resolution\n(SISR). However, existing researches have predominantly focused on raw\nperformance gains, with little attention paid to quantifying the\ntransferability of architectural components. In this paper, we introduce the\nconcept of \"Universality\" and its associated definitions which extend the\ntraditional notion of \"Generalization\" to encompass the modules' ease of\ntransferability, thus revealing the relationships between module universality\nand model generalizability. Then we propose the Universality Assessment\nEquation (UAE), a metric for quantifying how readily a given module could be\ntransplanted across models. Guided by the UAE results of standard residual\nblocks and other plug-and-play modules, we further design two optimized\nmodules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB).\nThrough comprehensive experiments on natural-scene benchmarks, remote-sensing\ndatasets, extreme-industrial imagery and on-device deployments, we demonstrate\nthat networks embedded with the proposed plug-and-play modules outperform\nseveral state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or\nenabling a 71.3% reduction in parameters with negligible loss in reconstruction\nfidelity.",
        "url": "http://arxiv.org/abs/2505.03522v1",
        "published_date": "2025-05-06T13:35:59+00:00",
        "updated_date": "2025-05-06T13:35:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haotong Cheng",
            "Zhiqi Zhang",
            "Hao Li",
            "Xinshang Zhang"
        ],
        "tldr": "this paper introduces a \"universality\" metric to quantify the transferability of modules in single image super-resolution (sisr) and proposes cycle residual blocks (crb and dcrb) that achieve improved performance and parameter reduction.",
        "tldr_zh": "本文提出了一个\"通用性\"指标来量化单图像超分辨率（sisr）中模块的可迁移性，并提出了循环残差块（crb 和 dcrb），实现了更好的性能和参数减少。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    },
    {
        "title": "Reducing Annotation Burden in Physical Activity Research Using Vision-Language Models",
        "summary": "Introduction: Data from wearable devices collected in free-living settings,\nand labelled with physical activity behaviours compatible with health research,\nare essential for both validating existing wearable-based measurement\napproaches and developing novel machine learning approaches. One common way of\nobtaining these labels relies on laborious annotation of sequences of images\ncaptured by cameras worn by participants through the course of a day. Methods:\nWe compare the performance of three vision language models and two\ndiscriminative models on two free-living validation studies with 161 and 111\nparticipants, collected in Oxfordshire, United Kingdom and Sichuan, China,\nrespectively, using the Autographer (OMG Life, defunct) wearable camera.\nResults: We found that the best open-source vision-language model (VLM) and\nfine-tuned discriminative model (DM) achieved comparable performance when\npredicting sedentary behaviour from single images on unseen participants in the\nOxfordshire study; median F1-scores: VLM = 0.89 (0.84, 0.92), DM = 0.91 (0.86,\n0.95). Performance declined for light (VLM = 0.60 (0.56,0.67), DM = 0.70 (0.63,\n0.79)), and moderate-to-vigorous intensity physical activity (VLM = 0.66 (0.53,\n0.85); DM = 0.72 (0.58, 0.84)). When applied to the external Sichuan study,\nperformance fell across all intensity categories, with median Cohen's\nkappa-scores falling from 0.54 (0.49, 0.64) to 0.26 (0.15, 0.37) for the VLM,\nand from 0.67 (0.60, 0.74) to 0.19 (0.10, 0.30) for the DM. Conclusion: Freely\navailable computer vision models could help annotate sedentary behaviour,\ntypically the most prevalent activity of daily living, from wearable camera\nimages within similar populations to seen data, reducing the annotation burden.",
        "url": "http://arxiv.org/abs/2505.03374v1",
        "published_date": "2025-05-06T09:49:45+00:00",
        "updated_date": "2025-05-06T09:49:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Abram Schonfeldt",
            "Benjamin Maylor",
            "Xiaofang Chen",
            "Ronald Clark",
            "Aiden Doherty"
        ],
        "tldr": "this paper explores the use of vision-language models to automatically annotate images from wearable cameras for physical activity research, showing promising results for sedentary behavior but performance decline in other activity categories and across different populations.",
        "tldr_zh": "本文探讨了使用视觉-语言模型自动标注可穿戴相机图像以进行身体活动研究的方法，该方法在标注静态行为方面表现出前景，但在其他活动类别和不同人群中的表现有所下降。",
        "relevance_score": 2,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    },
    {
        "title": "DCS-ST for Classification of Breast Cancer Histopathology Images with Limited Annotations",
        "summary": "Deep learning methods have shown promise in classifying breast cancer\nhistopathology images, but their performance often declines with limited\nannotated data, a critical challenge in medical imaging due to the high cost\nand expertise required for annotations.",
        "url": "http://arxiv.org/abs/2505.03204v1",
        "published_date": "2025-05-06T05:38:17+00:00",
        "updated_date": "2025-05-06T05:38:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Liu Suxing",
            "Byungwon Min"
        ],
        "tldr": "the paper addresses the challenge of classifying breast cancer histopathology images with limited labeled data using deep learning, a common problem in medical image analysis due to annotation costs.",
        "tldr_zh": "该论文探讨了如何使用深度学习在有限标注数据的情况下对乳腺癌组织病理学图像进行分类，这是医学图像分析中一个常见问题，因为标注成本很高。",
        "relevance_score": 2,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 4
    },
    {
        "title": "Robust Fairness Vision-Language Learning for Medical Image Analysis",
        "summary": "The advent of Vision-Language Models (VLMs) in medical image analysis has the\npotential to help process multimodal inputs and increase performance over\ntraditional inference methods. However, when considering the domain in which\nthese models will be implemented, fairness and robustness are important to\nensure the model stays true for any patient. In this paper, we introduce a\nframework for ensuring robustness and fairness of VLM models. This framework\nmodifies the loss function at training by identifying and adjusting faulty\nimage-text pairs through a Dynamic Bad Pair Mining algorithm and also utilizing\nSinkhorn distance to ensure the loss distributions of protected groups do not\ndeviate from the total loss. Experimental testing of our framework shows up to\na 8.6\\% improvement when looking at equity-scaled AUC.",
        "url": "http://arxiv.org/abs/2505.03153v1",
        "published_date": "2025-05-06T03:59:25+00:00",
        "updated_date": "2025-05-06T03:59:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sparsh Bansal",
            "Mingyang Wu",
            "Xin Wang",
            "Shu Hu"
        ],
        "tldr": "this paper introduces a framework using dynamic bad pair mining and sinkhorn distance to improve fairness and robustness in vision-language models for medical image analysis, showing an 8.6% improvement in equity-scaled auc.",
        "tldr_zh": "本文介绍了一个框架，使用动态坏对挖掘算法（dynamic bad pair mining）和sinkhorn距离，来提高医学图像分析中视觉-语言模型的公平性和鲁棒性，在公平性缩放auc方面有8.6%的提升。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    }
]