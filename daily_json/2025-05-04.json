[
    {
        "title": "DualDiff: Dual-branch Diffusion Model for Autonomous Driving with Semantic Fusion",
        "summary": "Accurate and high-fidelity driving scene reconstruction relies on fully\nleveraging scene information as conditioning. However, existing approaches,\nwhich primarily use 3D bounding boxes and binary maps for foreground and\nbackground control, fall short in capturing the complexity of the scene and\nintegrating multi-modal information. In this paper, we propose DualDiff, a\ndual-branch conditional diffusion model designed to enhance multi-view driving\nscene generation. We introduce Occupancy Ray Sampling (ORS), a semantic-rich 3D\nrepresentation, alongside numerical driving scene representation, for\ncomprehensive foreground and background control. To improve cross-modal\ninformation integration, we propose a Semantic Fusion Attention (SFA) mechanism\nthat aligns and fuses features across modalities. Furthermore, we design a\nforeground-aware masked (FGM) loss to enhance the generation of tiny objects.\nDualDiff achieves state-of-the-art performance in FID score, as well as\nconsistently better results in downstream BEV segmentation and 3D object\ndetection tasks.",
        "url": "http://arxiv.org/abs/2505.01857v1",
        "published_date": "2025-05-03T16:20:01+00:00",
        "updated_date": "2025-05-03T16:20:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoteng Li",
            "Zhao Yang",
            "Zezhong Qian",
            "Gongpeng Zhao",
            "Yuqi Huang",
            "Jun Yu",
            "Huazheng Zhou",
            "Longjun Liu"
        ],
        "tldr": "the paper introduces dualdiff, a dual-branch diffusion model for autonomous driving scene generation, utilizing semantic-rich 3d representation and a semantic fusion attention mechanism to achieve state-of-the-art performance in scene reconstruction and downstream tasks.",
        "tldr_zh": "这篇论文介绍了一种名为dualdiff的双分支扩散模型，用于自动驾驶场景生成，它利用语义丰富的3d表示和语义融合注意力机制，在场景重建和下游任务中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "PhytoSynth: Leveraging Multi-modal Generative Models for Crop Disease Data Generation with Novel Benchmarking and Prompt Engineering Approach",
        "summary": "Collecting large-scale crop disease images in the field is labor-intensive\nand time-consuming. Generative models (GMs) offer an alternative by creating\nsynthetic samples that resemble real-world images. However, existing research\nprimarily relies on Generative Adversarial Networks (GANs)-based image-to-image\ntranslation and lack a comprehensive analysis of computational requirements in\nagriculture. Therefore, this research explores a multi-modal text-to-image\napproach for generating synthetic crop disease images and is the first to\nprovide computational benchmarking in this context. We trained three Stable\nDiffusion (SD) variants-SDXL, SD3.5M (medium), and SD3.5L (large)-and\nfine-tuned them using Dreambooth and Low-Rank Adaptation (LoRA) fine-tuning\ntechniques to enhance generalization. SD3.5M outperformed the others, with an\naverage memory usage of 18 GB, power consumption of 180 W, and total energy use\nof 1.02 kWh/500 images (0.002 kWh per image) during inference task. Our results\ndemonstrate SD3.5M's ability to generate 500 synthetic images from just 36\nin-field samples in 1.5 hours. We recommend SD3.5M for efficient crop disease\ndata generation.",
        "url": "http://arxiv.org/abs/2505.01823v1",
        "published_date": "2025-05-03T14:03:42+00:00",
        "updated_date": "2025-05-03T14:03:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.ET"
        ],
        "authors": [
            "Nitin Rai",
            "Arnold W. Schumann",
            "Nathan Boyd"
        ],
        "tldr": "this paper explores using multi-modal text-to-image stable diffusion models, particularly sd3.5m, to generate synthetic crop disease images, providing computational benchmarks for this application and demonstrating efficient generation from limited real-world samples.",
        "tldr_zh": "该论文探索使用多模态文本到图像的stable diffusion模型（特别是sd3.5m）来生成合成的作物病害图像，为该应用提供了计算基准，并展示了从有限的真实样本中高效生成图像的能力。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth",
        "summary": "Recent advancements in autonomous driving (AD) systems have highlighted the\npotential of world models in achieving robust and generalizable performance\nacross both ordinary and challenging driving conditions. However, a key\nchallenge remains: precise and flexible camera pose control, which is crucial\nfor accurate viewpoint transformation and realistic simulation of scene\ndynamics. In this paper, we introduce PosePilot, a lightweight yet powerful\nframework that significantly enhances camera pose controllability in generative\nworld models. Drawing inspiration from self-supervised depth estimation,\nPosePilot leverages structure-from-motion principles to establish a tight\ncoupling between camera pose and video generation. Specifically, we incorporate\nself-supervised depth and pose readouts, allowing the model to infer depth and\nrelative camera motion directly from video sequences. These outputs drive\npose-aware frame warping, guided by a photometric warping loss that enforces\ngeometric consistency across synthesized frames. To further refine camera pose\nestimation, we introduce a reverse warping step and a pose regression loss,\nimproving viewpoint precision and adaptability. Extensive experiments on\nautonomous driving and general-domain video datasets demonstrate that PosePilot\nsignificantly enhances structural understanding and motion reasoning in both\ndiffusion-based and auto-regressive world models. By steering camera pose with\nself-supervised depth, PosePilot sets a new benchmark for pose controllability,\nenabling physically consistent, reliable viewpoint synthesis in generative\nworld models.",
        "url": "http://arxiv.org/abs/2505.01729v1",
        "published_date": "2025-05-03T07:51:46+00:00",
        "updated_date": "2025-05-03T07:51:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bu Jin",
            "Weize Li",
            "Baihan Yang",
            "Zhenxin Zhu",
            "Junpeng Jiang",
            "Huan-ang Gao",
            "Haiyang Sun",
            "Kun Zhan",
            "Hengtong Hu",
            "Xueyang Zhang",
            "Peng Jia",
            "Hao Zhao"
        ],
        "tldr": "the paper introduces posepilot, a framework that enhances camera pose controllability in generative world models using self-supervised depth estimation, improving viewpoint synthesis and geometric consistency.",
        "tldr_zh": "该论文介绍了 posepilot，一个利用自监督深度估计来增强生成世界模型中相机姿态可控性的框架，从而改进了视点合成和几何一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RAGAR: Retrieval Augment Personalized Image Generation Guided by Recommendation",
        "summary": "Personalized image generation is crucial for improving the user experience,\nas it renders reference images into preferred ones according to user visual\npreferences. Although effective, existing methods face two main issues. First,\nexisting methods treat all items in the user historical sequence equally when\nextracting user preferences, overlooking the varying semantic similarities\nbetween historical items and the reference item. Disproportionately high\nweights for low-similarity items distort users' visual preferences for the\nreference item. Second, existing methods heavily rely on consistency between\ngenerated and reference images to optimize the generation, which leads to\nunderfitting user preferences and hinders personalization. To address these\nissues, we propose Retrieval Augment Personalized Image GenerAtion guided by\nRecommendation (RAGAR). Our approach uses a retrieval mechanism to assign\ndifferent weights to historical items according to their similarities to the\nreference item, thereby extracting more refined users' visual preferences for\nthe reference item. Then we introduce a novel rank task based on the\nmulti-modal ranking model to optimize the personalization of the generated\nimages instead of forcing depend on consistency. Extensive experiments and\nhuman evaluations on three real-world datasets demonstrate that RAGAR achieves\nsignificant improvements in both personalization and semantic metrics compared\nto five baselines.",
        "url": "http://arxiv.org/abs/2505.01657v1",
        "published_date": "2025-05-03T02:20:30+00:00",
        "updated_date": "2025-05-03T02:20:30+00:00",
        "categories": [
            "cs.IR",
            "cs.CV"
        ],
        "authors": [
            "Run Ling",
            "Wenji Wang",
            "Yuting Liu",
            "Guibing Guo",
            "Linying Jiang",
            "Xingwei Wang"
        ],
        "tldr": "the paper introduces ragar, a novel approach for personalized image generation that leverages retrieval mechanisms and a ranking task to better incorporate user preferences, addressing limitations of existing methods that equally weigh user history and rely heavily on consistency with reference images.",
        "tldr_zh": "该论文介绍了ragar，一种新颖的个性化图像生成方法，它利用检索机制和排序任务来更好地整合用户偏好，解决了现有方法同等对待用户历史记录并过度依赖与参考图像一致性的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos",
        "summary": "Web-based educational videos offer flexible learning opportunities and are\nbecoming increasingly popular. However, improving user engagement and knowledge\nretention remains a challenge. Automatically generated questions can activate\nlearners and support their knowledge acquisition. Further, they can help\nteachers and learners assess their understanding. While large language and\nvision-language models have been employed in various tasks, their application\nto question generation for educational videos remains underexplored. In this\npaper, we investigate the capabilities of current vision-language models for\ngenerating learning-oriented questions for educational video content. We assess\n(1) out-of-the-box models' performance; (2) fine-tuning effects on\ncontent-specific question generation; (3) the impact of different video\nmodalities on question quality; and (4) in a qualitative study, question\nrelevance, answerability, and difficulty levels of generated questions. Our\nfindings delineate the capabilities of current vision-language models,\nhighlighting the need for fine-tuning and addressing challenges in question\ndiversity and relevance. We identify requirements for future multimodal\ndatasets and outline promising research directions.",
        "url": "http://arxiv.org/abs/2505.01790v1",
        "published_date": "2025-05-03T11:37:31+00:00",
        "updated_date": "2025-05-03T11:37:31+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.MM"
        ],
        "authors": [
            "Markos Stamatakis",
            "Joshua Berger",
            "Christian Wartena",
            "Ralph Ewerth",
            "Anett Hoppe"
        ],
        "tldr": "this paper explores the use of vision-language models for automatically generating questions for educational videos, assessing their performance and identifying areas for improvement via fine-tuning and data collection.",
        "tldr_zh": "本文探讨了使用视觉-语言模型自动生成教育视频的问题，评估了它们的性能，并通过微调和数据收集确定了改进领域。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Vision and Intention Boost Large Language Model in Long-Term Action Anticipation",
        "summary": "Long-term action anticipation (LTA) aims to predict future actions over an\nextended period. Previous approaches primarily focus on learning exclusively\nfrom video data but lack prior knowledge. Recent researches leverage large\nlanguage models (LLMs) by utilizing text-based inputs which suffer severe\ninformation loss. To tackle these limitations single-modality methods face, we\npropose a novel Intention-Conditioned Vision-Language (ICVL) model in this\nstudy that fully leverages the rich semantic information of visual data and the\npowerful reasoning capabilities of LLMs. Considering intention as a high-level\nconcept guiding the evolution of actions, we first propose to employ a\nvision-language model (VLM) to infer behavioral intentions as comprehensive\ntextual features directly from video inputs. The inferred intentions are then\nfused with visual features through a multi-modality fusion strategy, resulting\nin intention-enhanced visual representations. These enhanced visual\nrepresentations, along with textual prompts, are fed into LLM for future action\nanticipation. Furthermore, we propose an effective example selection strategy\njointly considers visual and textual similarities, providing more relevant and\ninformative examples for in-context learning. Extensive experiments with\nstate-of-the-art performance on Ego4D, EPIC-Kitchens-55, and EGTEA GAZE+\ndatasets fully demonstrate the effectiveness and superiority of the proposed\nmethod.",
        "url": "http://arxiv.org/abs/2505.01713v1",
        "published_date": "2025-05-03T06:33:54+00:00",
        "updated_date": "2025-05-03T06:33:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Congqi Cao",
            "Lanshu Hu",
            "Yating Yu",
            "Yanning Zhang"
        ],
        "tldr": "the paper introduces an intention-conditioned vision-language (icvl) model for long-term action anticipation, leveraging visual data and llms with an intention-aware fusion strategy and example selection, demonstrating state-of-the-art results on multiple datasets.",
        "tldr_zh": "该论文介绍了一种用于长期行为预测的意图条件视觉语言 (icvl) 模型，该模型利用视觉数据和大型语言模型，采用意图感知融合策略和示例选择，并在多个数据集上展示了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Mitigating Group-Level Fairness Disparities in Federated Visual Language Models",
        "summary": "Visual language models (VLMs) have shown remarkable capabilities in\nmultimodal tasks but face challenges in maintaining fairness across demographic\ngroups, particularly when deployed in federated learning (FL) environments.\nThis paper addresses the critical issue of group fairness in federated VLMs by\nintroducing FVL-FP, a novel framework that combines FL with fair prompt tuning\ntechniques. We focus on mitigating demographic biases while preserving model\nperformance through three innovative components: (1) Cross-Layer Demographic\nFair Prompting (CDFP), which adjusts potentially biased embeddings through\ncounterfactual regularization; (2) Demographic Subspace Orthogonal Projection\n(DSOP), which removes demographic bias in image representations by mapping fair\nprompt text to group subspaces; and (3) Fair-aware Prompt Fusion (FPF), which\ndynamically balances client contributions based on both performance and\nfairness metrics. Extensive evaluations across four benchmark datasets\ndemonstrate that our approach reduces demographic disparity by an average of\n45\\% compared to standard FL approaches, while maintaining task performance\nwithin 6\\% of state-of-the-art results. FVL-FP effectively addresses the\nchallenges of non-IID data distributions in federated settings and introduces\nminimal computational overhead while providing significant fairness benefits.\nOur work presents a parameter-efficient solution to the critical challenge of\nensuring equitable performance across demographic groups in privacy-preserving\nmultimodal systems.",
        "url": "http://arxiv.org/abs/2505.01851v1",
        "published_date": "2025-05-03T16:09:52+00:00",
        "updated_date": "2025-05-03T16:09:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chaomeng Chen",
            "Zitong Yu",
            "Junhao Dong",
            "Sen Su",
            "Linlin Shen",
            "Shutao Xia",
            "Xiaochun Cao"
        ],
        "tldr": "this paper introduces fvl-fp, a novel federated learning framework that leverages fair prompt tuning to mitigate group-level fairness disparities in visual language models, achieving significant fairness improvements with minimal performance impact.",
        "tldr_zh": "该论文介绍了一种名为fvl-fp的新型联邦学习框架，该框架利用公平提示调优来减轻视觉语言模型中群体层面的公平性差距，在性能影响最小的情况下实现了显著的公平性改进。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Seeing Heat with Color -- RGB-Only Wildfire Temperature Inference from SAM-Guided Multimodal Distillation using Radiometric Ground Truth",
        "summary": "High-fidelity wildfire monitoring using Unmanned Aerial Vehicles (UAVs)\ntypically requires multimodal sensing - especially RGB and thermal imagery -\nwhich increases hardware cost and power consumption. This paper introduces\nSAM-TIFF, a novel teacher-student distillation framework for pixel-level\nwildfire temperature prediction and segmentation using RGB input only. A\nmultimodal teacher network trained on paired RGB-Thermal imagery and\nradiometric TIFF ground truth distills knowledge to a unimodal RGB student\nnetwork, enabling thermal-sensor-free inference. Segmentation supervision is\ngenerated using a hybrid approach of segment anything (SAM)-guided mask\ngeneration, and selection via TOPSIS, along with Canny edge detection and\nOtsu's thresholding pipeline for automatic point prompt selection. Our method\nis the first to perform per-pixel temperature regression from RGB UAV data,\ndemonstrating strong generalization on the recent FLAME 3 dataset. This work\nlays the foundation for lightweight, cost-effective UAV-based wildfire\nmonitoring systems without thermal sensors.",
        "url": "http://arxiv.org/abs/2505.01638v1",
        "published_date": "2025-05-03T00:23:11+00:00",
        "updated_date": "2025-05-03T00:23:11+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "I.4.6; I.4.8"
        ],
        "authors": [
            "Michael Marinaccio",
            "Fatemeh Afghah"
        ],
        "tldr": "this paper presents a teacher-student distillation framework (sam-tiff) to predict per-pixel wildfire temperatures from rgb imagery only, using sam-guided segmentation and radiometric ground truth, enabling cheaper uav-based wildfire monitoring.",
        "tldr_zh": "本文提出了一种师生蒸馏框架 (sam-tiff)，仅使用 rgb 图像预测每个像素的野火温度，利用 sam 引导的分割和辐射地面实况，从而实现更廉价的基于无人机的野火监测。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "Multimodal and Multiview Deep Fusion for Autonomous Marine Navigation",
        "summary": "We propose a cross attention transformer based method for multimodal sensor\nfusion to build a birds eye view of a vessels surroundings supporting safer\nautonomous marine navigation. The model deeply fuses multiview RGB and long\nwave infrared images with sparse LiDAR point clouds. Training also integrates X\nband radar and electronic chart data to inform predictions. The resulting view\nprovides a detailed reliable scene representation improving navigational\naccuracy and robustness. Real world sea trials confirm the methods\neffectiveness even in adverse weather and complex maritime settings.",
        "url": "http://arxiv.org/abs/2505.01615v1",
        "published_date": "2025-05-02T22:32:50+00:00",
        "updated_date": "2025-05-02T22:32:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Dimitrios Dagdilelis",
            "Panagiotis Grigoriadis",
            "Roberto Galeazzi"
        ],
        "tldr": "this paper introduces a cross-attention transformer-based method for multimodal sensor fusion to create a bird's-eye view for autonomous marine navigation, using rgb, infrared, lidar, radar, and electronic chart data. real-world sea trials demonstrate its effectiveness.",
        "tldr_zh": "本文提出了一种基于交叉注意力transformer的多模态传感器融合方法，利用rgb、红外、激光雷达、雷达和电子海图数据，为自主航海构建鸟瞰图。真实海试验证了其有效性。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 4
    }
]