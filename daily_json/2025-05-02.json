[
    {
        "title": "T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation",
        "summary": "Text-to-video generative models have made significant strides in recent\nyears, producing high-quality videos that excel in both aesthetic appeal and\naccurate instruction following, and have become central to digital art creation\nand user engagement online. Yet, despite these advancements, their ability to\nrespect fundamental physical laws remains largely untested: many outputs still\nviolate basic constraints such as rigid-body collisions, energy conservation,\nand gravitational dynamics, resulting in unrealistic or even misleading\ncontent. Existing physical-evaluation benchmarks typically rely on automatic,\npixel-level metrics applied to simplistic, life-scenario prompts, and thus\noverlook both human judgment and first-principles physics. To fill this gap, we\nintroduce \\textbf{T2VPhysBench}, a first-principled benchmark that\nsystematically evaluates whether state-of-the-art text-to-video systems, both\nopen-source and commercial, obey twelve core physical laws including Newtonian\nmechanics, conservation principles, and phenomenological effects. Our benchmark\nemploys a rigorous human evaluation protocol and includes three targeted\nstudies: (1) an overall compliance assessment showing that all models score\nbelow 0.60 on average in each law category; (2) a prompt-hint ablation\nrevealing that even detailed, law-specific hints fail to remedy physics\nviolations; and (3) a counterfactual robustness test demonstrating that models\noften generate videos that explicitly break physical rules when so instructed.\nThe results expose persistent limitations in current architectures and offer\nconcrete insights for guiding future research toward truly physics-aware video\ngeneration.",
        "url": "http://arxiv.org/abs/2505.00337v1",
        "published_date": "2025-05-01T06:34:55+00:00",
        "updated_date": "2025-05-01T06:34:55+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Xuyang Guo",
            "Jiayan Huo",
            "Zhenmei Shi",
            "Zhao Song",
            "Jiahao Zhang",
            "Jiale Zhao"
        ],
        "tldr": "the paper introduces t2vphysbench, a new benchmark to evaluate the physical consistency of text-to-video models, revealing their significant limitations in adhering to fundamental physical laws.",
        "tldr_zh": "该论文介绍了t2vphysbench，一个新的基准测试，用于评估文本到视频模型的物理一致性，揭示了它们在遵守基本物理定律方面的重大局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 10,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Controllable Weather Synthesis and Removal with Video Diffusion Models",
        "summary": "Generating realistic and controllable weather effects in videos is valuable\nfor many applications. Physics-based weather simulation requires precise\nreconstructions that are hard to scale to in-the-wild videos, while current\nvideo editing often lacks realism and control. In this work, we introduce\nWeatherWeaver, a video diffusion model that synthesizes diverse weather effects\n-- including rain, snow, fog, and clouds -- directly into any input video\nwithout the need for 3D modeling. Our model provides precise control over\nweather effect intensity and supports blending various weather types, ensuring\nboth realism and adaptability. To overcome the scarcity of paired training\ndata, we propose a novel data strategy combining synthetic videos, generative\nimage editing, and auto-labeled real-world videos. Extensive evaluations show\nthat our method outperforms state-of-the-art methods in weather simulation and\nremoval, providing high-quality, physically plausible, and\nscene-identity-preserving results over various real-world videos.",
        "url": "http://arxiv.org/abs/2505.00704v1",
        "published_date": "2025-05-01T17:59:57+00:00",
        "updated_date": "2025-05-01T17:59:57+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Chih-Hao Lin",
            "Zian Wang",
            "Ruofan Liang",
            "Yuxuan Zhang",
            "Sanja Fidler",
            "Shenlong Wang",
            "Zan Gojcic"
        ],
        "tldr": "the paper introduces weatherweaver, a video diffusion model for synthesizing and removing controllable weather effects in videos using a novel data strategy to address the lack of paired training data.",
        "tldr_zh": "该论文介绍了weatherweaver，一种视频扩散模型，用于合成和移除视频中可控的天气效果，并使用创新的数据策略来解决配对训练数据不足的问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT",
        "summary": "Recent advancements in large language models have demonstrated how\nchain-of-thought (CoT) and reinforcement learning (RL) can improve performance.\nHowever, applying such reasoning strategies to the visual generation domain\nremains largely unexplored. In this paper, we present T2I-R1, a novel\nreasoning-enhanced text-to-image generation model, powered by RL with a\nbi-level CoT reasoning process. Specifically, we identify two levels of CoT\nthat can be utilized to enhance different stages of generation: (1) the\nsemantic-level CoT for high-level planning of the prompt and (2) the\ntoken-level CoT for low-level pixel processing during patch-by-patch\ngeneration. To better coordinate these two levels of CoT, we introduce\nBiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes\nboth generation CoTs within the same training step. By applying our reasoning\nstrategies to the baseline model, Janus-Pro, we achieve superior performance\nwith 13% improvement on T2I-CompBench and 19% improvement on the WISE\nbenchmark, even surpassing the state-of-the-art model FLUX.1. Code is available\nat: https://github.com/CaraJ7/T2I-R1",
        "url": "http://arxiv.org/abs/2505.00703v1",
        "published_date": "2025-05-01T17:59:46+00:00",
        "updated_date": "2025-05-01T17:59:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Dongzhi Jiang",
            "Ziyu Guo",
            "Renrui Zhang",
            "Zhuofan Zong",
            "Hao Li",
            "Le Zhuo",
            "Shilin Yan",
            "Pheng-Ann Heng",
            "Hongsheng Li"
        ],
        "tldr": "the paper introduces t2i-r1, a novel text-to-image generation model that uses reinforcement learning with a bi-level chain-of-thought (cot) reasoning process to improve performance, achieving state-of-the-art results on t2i-compbench and wise benchmarks.",
        "tldr_zh": "该论文介绍了 t2i-r1, 一种新型的文本到图像生成模型，它使用强化学习和双层思维链（cot）推理过程来提高性能，并在 t2i-compbench 和 wise 基准测试中取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "KeySync: A Robust Approach for Leakage-free Lip Synchronization in High Resolution",
        "summary": "Lip synchronization, known as the task of aligning lip movements in an\nexisting video with new input audio, is typically framed as a simpler variant\nof audio-driven facial animation. However, as well as suffering from the usual\nissues in talking head generation (e.g., temporal consistency), lip\nsynchronization presents significant new challenges such as expression leakage\nfrom the input video and facial occlusions, which can severely impact\nreal-world applications like automated dubbing, but are often neglected in\nexisting works. To address these shortcomings, we present KeySync, a two-stage\nframework that succeeds in solving the issue of temporal consistency, while\nalso incorporating solutions for leakage and occlusions using a carefully\ndesigned masking strategy. We show that KeySync achieves state-of-the-art\nresults in lip reconstruction and cross-synchronization, improving visual\nquality and reducing expression leakage according to LipLeak, our novel leakage\nmetric. Furthermore, we demonstrate the effectiveness of our new masking\napproach in handling occlusions and validate our architectural choices through\nseveral ablation studies. Code and model weights can be found at\nhttps://antonibigata.github.io/KeySync.",
        "url": "http://arxiv.org/abs/2505.00497v1",
        "published_date": "2025-05-01T12:56:17+00:00",
        "updated_date": "2025-05-01T12:56:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Antoni Bigata",
            "Rodrigo Mira",
            "Stella Bounareli",
            "Michał Stypułkowski",
            "Konstantinos Vougioukas",
            "Stavros Petridis",
            "Maja Pantic"
        ],
        "tldr": "keysync is a two-stage framework that achieves state-of-the-art lip synchronization by addressing temporal consistency, expression leakage, and occlusions using a novel masking strategy and leakage metric.",
        "tldr_zh": "keysync是一个两阶段框架，通过使用一种新颖的掩蔽策略和泄漏指标，解决了时间一致性、表情泄漏和遮挡等问题，从而实现了最先进的唇部同步。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "JointDiT: Enhancing RGB-Depth Joint Modeling with Diffusion Transformers",
        "summary": "We present JointDiT, a diffusion transformer that models the joint\ndistribution of RGB and depth. By leveraging the architectural benefit and\noutstanding image prior of the state-of-the-art diffusion transformer, JointDiT\nnot only generates high-fidelity images but also produces geometrically\nplausible and accurate depth maps. This solid joint distribution modeling is\nachieved through two simple yet effective techniques that we propose, i.e.,\nadaptive scheduling weights, which depend on the noise levels of each modality,\nand the unbalanced timestep sampling strategy. With these techniques, we train\nour model across all noise levels for each modality, enabling JointDiT to\nnaturally handle various combinatorial generation tasks, including joint\ngeneration, depth estimation, and depth-conditioned image generation by simply\ncontrolling the timestep of each branch. JointDiT demonstrates outstanding\njoint generation performance. Furthermore, it achieves comparable results in\ndepth estimation and depth-conditioned image generation, suggesting that joint\ndistribution modeling can serve as a replaceable alternative to conditional\ngeneration. The project page is available at\nhttps://byungki-k.github.io/JointDiT/.",
        "url": "http://arxiv.org/abs/2505.00482v1",
        "published_date": "2025-05-01T12:21:23+00:00",
        "updated_date": "2025-05-01T12:21:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kwon Byung-Ki",
            "Qi Dai",
            "Lee Hyoseok",
            "Chong Luo",
            "Tae-Hyun Oh"
        ],
        "tldr": "jointdit is a diffusion transformer that jointly models rgb and depth information using adaptive scheduling weights and unbalanced timestep sampling, achieving strong performance in joint generation, depth estimation, and depth-conditioned image generation.",
        "tldr_zh": "jointdit是一个扩散transformer，它使用自适应调度权重和非平衡时间步采样，联合建模rgb和深度信息，在联合生成、深度估计和深度条件图像生成方面表现出色。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models",
        "summary": "The rapid evolution of social media has provided enhanced communication\nchannels for individuals to create online content, enabling them to express\ntheir thoughts and opinions. Multimodal memes, often utilized for playful or\nhumorous expressions with visual and textual elements, are sometimes misused to\ndisseminate hate speech against individuals or groups. While the detection of\nhateful memes is well-researched, developing effective methods to transform\nhateful content in memes remains a significant challenge. Leveraging the\npowerful generation and reasoning capabilities of Vision-Language Models\n(VLMs), we address the tasks of detecting and mitigating hateful content. This\npaper presents two key contributions: first, a definition-guided prompting\ntechnique for detecting hateful memes, and second, a unified framework for\nmitigating hateful content in memes, named UnHateMeme, which works by replacing\nhateful textual and/or visual components. With our definition-guided prompts,\nVLMs achieve impressive performance on hateful memes detection task.\nFurthermore, our UnHateMeme framework, integrated with VLMs, demonstrates a\nstrong capability to convert hateful memes into non-hateful forms that meet\nhuman-level criteria for hate speech and maintain multimodal coherence between\nimage and text. Through empirical experiments, we show the effectiveness of\nstate-of-the-art pretrained VLMs such as LLaVA, Gemini and GPT-4o on the\nproposed tasks, providing a comprehensive analysis of their respective\nstrengths and limitations for these tasks. This paper aims to shed light on\nimportant applications of VLMs for ensuring safe and respectful online\nenvironments.",
        "url": "http://arxiv.org/abs/2505.00150v1",
        "published_date": "2025-04-30T19:48:12+00:00",
        "updated_date": "2025-04-30T19:48:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Minh-Hao Van",
            "Xintao Wu"
        ],
        "tldr": "this paper introduces a method for detecting and mitigating hateful content in multimodal memes using vision-language models (vlms), including a definition-guided prompting technique for detection and a framework (unhatememe) for transforming hateful memes into non-hateful ones by replacing textual/visual components.",
        "tldr_zh": "本文介绍了一种使用视觉-语言模型（vlms）检测和缓解多模态表情包中仇恨内容的方法，包括用于检测的定义引导提示技术和一个框架（unhatememe），通过替换文本/视觉组件将仇恨表情包转换为非仇恨表情包。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Eye2Eye: A Simple Approach for Monocular-to-Stereo Video Synthesis",
        "summary": "The rising popularity of immersive visual experiences has increased interest\nin stereoscopic 3D video generation. Despite significant advances in video\nsynthesis, creating 3D videos remains challenging due to the relative scarcity\nof 3D video data. We propose a simple approach for transforming a text-to-video\ngenerator into a video-to-stereo generator. Given an input video, our framework\nautomatically produces the video frames from a shifted viewpoint, enabling a\ncompelling 3D effect. Prior and concurrent approaches for this task typically\noperate in multiple phases, first estimating video disparity or depth, then\nwarping the video accordingly to produce a second view, and finally inpainting\nthe disoccluded regions. This approach inherently fails when the scene involves\nspecular surfaces or transparent objects. In such cases, single-layer disparity\nestimation is insufficient, resulting in artifacts and incorrect pixel shifts\nduring warping. Our work bypasses these restrictions by directly synthesizing\nthe new viewpoint, avoiding any intermediate steps. This is achieved by\nleveraging a pre-trained video model's priors on geometry, object materials,\noptics, and semantics, without relying on external geometry models or manually\ndisentangling geometry from the synthesis process. We demonstrate the\nadvantages of our approach in complex, real-world scenarios featuring diverse\nobject materials and compositions. See videos on\nhttps://video-eye2eye.github.io",
        "url": "http://arxiv.org/abs/2505.00135v1",
        "published_date": "2025-04-30T19:06:09+00:00",
        "updated_date": "2025-04-30T19:06:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Michal Geyer",
            "Omer Tov",
            "Linyi Jin",
            "Richard Tucker",
            "Inbar Mosseri",
            "Tali Dekel",
            "Noah Snavely"
        ],
        "tldr": "the paper proposes a novel \"eye2eye\" approach for monocular-to-stereo video synthesis by directly synthesizing the second viewpoint using a pre-trained video model, bypassing explicit depth or disparity estimation and mitigating artifacts in complex scenes.",
        "tldr_zh": "该论文提出了一种名为 “eye2eye” 的新方法，用于从单目视频合成立体视频，它通过直接使用预训练的视频模型来合成第二个视角，绕过了显式的深度或视差估计，并减少了复杂场景中的伪影。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RayZer: A Self-supervised Large View Synthesis Model",
        "summary": "We present RayZer, a self-supervised multi-view 3D Vision model trained\nwithout any 3D supervision, i.e., camera poses and scene geometry, while\nexhibiting emerging 3D awareness. Concretely, RayZer takes unposed and\nuncalibrated images as input, recovers camera parameters, reconstructs a scene\nrepresentation, and synthesizes novel views. During training, RayZer relies\nsolely on its self-predicted camera poses to render target views, eliminating\nthe need for any ground-truth camera annotations and allowing RayZer to be\ntrained with 2D image supervision. The emerging 3D awareness of RayZer is\nattributed to two key factors. First, we design a self-supervised framework,\nwhich achieves 3D-aware auto-encoding of input images by disentangling camera\nand scene representations. Second, we design a transformer-based model in which\nthe only 3D prior is the ray structure, connecting camera, pixel, and scene\nsimultaneously. RayZer demonstrates comparable or even superior novel view\nsynthesis performance than ``oracle'' methods that rely on pose annotations in\nboth training and testing. Project: https://hwjiang1510.github.io/RayZer/",
        "url": "http://arxiv.org/abs/2505.00702v1",
        "published_date": "2025-05-01T17:59:34+00:00",
        "updated_date": "2025-05-01T17:59:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanwen Jiang",
            "Hao Tan",
            "Peng Wang",
            "Haian Jin",
            "Yue Zhao",
            "Sai Bi",
            "Kai Zhang",
            "Fujun Luan",
            "Kalyan Sunkavalli",
            "Qixing Huang",
            "Georgios Pavlakos"
        ],
        "tldr": "rayzer is a self-supervised model that synthesizes novel views from unposed images, recovering camera parameters and reconstructing scenes without 3d supervision, achieving comparable or better performance than methods requiring pose annotations.",
        "tldr_zh": "rayzer是一个自监督模型，可以从无姿态图像中合成新的视图，在没有3d监督的情况下恢复相机参数和重建场景，并且实现了与需要姿势注释的方法相比具有竞争力甚至更好的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "GuideSR: Rethinking Guidance for One-Step High-Fidelity Diffusion-Based Super-Resolution",
        "summary": "In this paper, we propose GuideSR, a novel single-step diffusion-based image\nsuper-resolution (SR) model specifically designed to enhance image fidelity.\nExisting diffusion-based SR approaches typically adapt pre-trained generative\nmodels to image restoration tasks by adding extra conditioning on a\nVAE-downsampled representation of the degraded input, which often compromises\nstructural fidelity. GuideSR addresses this limitation by introducing a\ndual-branch architecture comprising: (1) a Guidance Branch that preserves\nhigh-fidelity structures from the original-resolution degraded input, and (2) a\nDiffusion Branch, which a pre-trained latent diffusion model to enhance\nperceptual quality. Unlike conventional conditioning mechanisms, our Guidance\nBranch features a tailored structure for image restoration tasks, combining\nFull Resolution Blocks (FRBs) with channel attention and an Image Guidance\nNetwork (IGN) with guided attention. By embedding detailed structural\ninformation directly into the restoration pipeline, GuideSR produces sharper\nand more visually consistent results. Extensive experiments on benchmark\ndatasets demonstrate that GuideSR achieves state-of-the-art performance while\nmaintaining the low computational cost of single-step approaches, with up to\n1.39dB PSNR gain on challenging real-world datasets. Our approach consistently\noutperforms existing methods across various reference-based metrics including\nPSNR, SSIM, LPIPS, DISTS and FID, further representing a practical advancement\nfor real-world image restoration.",
        "url": "http://arxiv.org/abs/2505.00687v1",
        "published_date": "2025-05-01T17:48:25+00:00",
        "updated_date": "2025-05-01T17:48:25+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Aditya Arora",
            "Zhengzhong Tu",
            "Yufei Wang",
            "Ruizheng Bai",
            "Jian Wang",
            "Sizhuo Ma"
        ],
        "tldr": "guidesr introduces a novel single-step diffusion-based image super-resolution model that uses a dual-branch architecture to enhance image fidelity by preserving high-fidelity structures from the original-resolution degraded input and enhancing perceptual quality using a pre-trained latent diffusion model.",
        "tldr_zh": "guidesr 提出了一种新颖的单步扩散图像超分辨率模型，该模型使用双分支架构来增强图像保真度，通过保留原始分辨率降级输入中的高保真度结构，并使用预训练的潜在扩散模型来增强感知质量。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Efficient Neural Video Representation with Temporally Coherent Modulation",
        "summary": "Implicit neural representations (INR) has found successful applications\nacross diverse domains. To employ INR in real-life, it is important to speed up\ntraining. In the field of INR for video applications, the state-of-the-art\napproach employs grid-type parametric encoding and successfully achieves a\nfaster encoding speed in comparison to its predecessors. However, the grid\nusage, which does not consider the video's dynamic nature, leads to redundant\nuse of trainable parameters. As a result, it has significantly lower parameter\nefficiency and higher bitrate compared to NeRV-style methods that do not use a\nparametric encoding. To address the problem, we propose Neural Video\nrepresentation with Temporally coherent Modulation (NVTM), a novel framework\nthat can capture dynamic characteristics of video. By decomposing the\nspatio-temporal 3D video data into a set of 2D grids with flow information,\nNVTM enables learning video representation rapidly and uses parameter\nefficiently. Our framework enables to process temporally corresponding pixels\nat once, resulting in the fastest encoding speed for a reasonable video\nquality, especially when compared to the NeRV-style method, with a speed\nincrease of over 3 times. Also, it remarks an average of 1.54dB/0.019\nimprovements in PSNR/LPIPS on UVG (Dynamic) (even with 10% fewer parameters)\nand an average of 1.84dB/0.013 improvements in PSNR/LPIPS on MCL-JCV (Dynamic),\ncompared to previous grid-type works. By expanding this to compression tasks,\nwe demonstrate comparable performance to video compression standards (H.264,\nHEVC) and recent INR approaches for video compression. Additionally, we perform\nextensive experiments demonstrating the superior performance of our algorithm\nacross diverse tasks, encompassing super resolution, frame interpolation and\nvideo inpainting. Project page is https://sujiikim.github.io/NVTM/.",
        "url": "http://arxiv.org/abs/2505.00335v1",
        "published_date": "2025-05-01T06:20:42+00:00",
        "updated_date": "2025-05-01T06:20:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Seungjun Shin",
            "Suji Kim",
            "Dokwan Oh"
        ],
        "tldr": "the paper introduces a novel neural video representation framework (nvtm) that speeds up training and improves parameter efficiency by using temporally coherent modulation, outperforming existing methods in video representation tasks.",
        "tldr_zh": "本文介绍了一种新型神经视频表示框架（nvtm），它通过使用时间相干调制来加速训练并提高参数效率，在视频表示任务中优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Quaternion Wavelet-Conditioned Diffusion Models for Image Super-Resolution",
        "summary": "Image Super-Resolution is a fundamental problem in computer vision with broad\napplications spacing from medical imaging to satellite analysis. The ability to\nreconstruct high-resolution images from low-resolution inputs is crucial for\nenhancing downstream tasks such as object detection and segmentation. While\ndeep learning has significantly advanced SR, achieving high-quality\nreconstructions with fine-grained details and realistic textures remains\nchallenging, particularly at high upscaling factors. Recent approaches\nleveraging diffusion models have demonstrated promising results, yet they often\nstruggle to balance perceptual quality with structural fidelity. In this work,\nwe introduce ResQu a novel SR framework that integrates a quaternion wavelet\npreprocessing framework with latent diffusion models, incorporating a new\nquaternion wavelet- and time-aware encoder. Unlike prior methods that simply\napply wavelet transforms within diffusion models, our approach enhances the\nconditioning process by exploiting quaternion wavelet embeddings, which are\ndynamically integrated at different stages of denoising. Furthermore, we also\nleverage the generative priors of foundation models such as Stable Diffusion.\nExtensive experiments on domain-specific datasets demonstrate that our method\nachieves outstanding SR results, outperforming in many cases existing\napproaches in perceptual quality and standard evaluation metrics. The code will\nbe available after the revision process.",
        "url": "http://arxiv.org/abs/2505.00334v1",
        "published_date": "2025-05-01T06:17:33+00:00",
        "updated_date": "2025-05-01T06:17:33+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Luigi Sigillo",
            "Christian Bianchi",
            "Danilo Comminiello"
        ],
        "tldr": "the paper introduces resqu, a super-resolution framework using quaternion wavelet-conditioned diffusion models, leveraging stable diffusion priors to enhance perceptual quality and structural fidelity. it claims superior performance in domain-specific datasets.",
        "tldr_zh": "本文介绍了一种名为resqu的超分辨率框架，该框架采用四元数小波条件扩散模型，并利用stable diffusion先验来增强感知质量和结构保真度。作者声称该方法在特定领域的数据集上表现优异。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Multimodal Masked Autoencoder Pre-training for 3D MRI-Based Brain Tumor Analysis with Missing Modalities",
        "summary": "Multimodal magnetic resonance imaging (MRI) constitutes the first line of\ninvestigation for clinicians in the care of brain tumors, providing crucial\ninsights for surgery planning, treatment monitoring, and biomarker\nidentification. Pre-training on large datasets have been shown to help models\nlearn transferable representations and adapt with minimal labeled data. This\nbehavior is especially valuable in medical imaging, where annotations are often\nscarce. However, applying this paradigm to multimodal medical data introduces a\nchallenge: most existing approaches assume that all imaging modalities are\navailable during both pre-training and fine-tuning. In practice, missing\nmodalities often occur due to acquisition issues, specialist unavailability, or\nspecific experimental designs on small in-house datasets. Consequently, a\ncommon approach involves training a separate model for each desired modality\ncombination, making the process both resource-intensive and impractical for\nclinical use. Therefore, we introduce BM-MAE, a masked image modeling\npre-training strategy tailored for multimodal MRI data. The same pre-trained\nmodel seamlessly adapts to any combination of available modalities, extracting\nrich representations that capture both intra- and inter-modal information. This\nallows fine-tuning on any subset of modalities without requiring architectural\nchanges, while still benefiting from a model pre-trained on the full set of\nmodalities. Extensive experiments show that the proposed pre-training strategy\noutperforms or remains competitive with baselines that require separate\npre-training for each modality subset, while substantially surpassing training\nfrom scratch on several downstream tasks. Additionally, it can quickly and\nefficiently reconstruct missing modalities, highlighting its practical value.\nCode and trained models are available at: https://github.com/Lucas-rbnt/bmmae",
        "url": "http://arxiv.org/abs/2505.00568v1",
        "published_date": "2025-05-01T14:51:30+00:00",
        "updated_date": "2025-05-01T14:51:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Lucas Robinet",
            "Ahmad Berjaoui",
            "Elizabeth Cohen-Jonathan Moyal"
        ],
        "tldr": "this paper introduces bm-mae, a masked autoencoder pre-training strategy for 3d mri brain tumor analysis that addresses the challenge of missing modalities, allowing for seamless adaptation and improved performance compared to modality-specific pre-training.",
        "tldr_zh": "本文介绍了bm-mae，一种用于3d mri脑肿瘤分析的掩码自动编码器预训练策略，它解决了缺失模态的挑战，与模态特定的预训练相比，实现了无缝适应和性能提升。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "Neuroevolution of Self-Attention Over Proto-Objects",
        "summary": "Proto-objects - image regions that share common visual properties - offer a\npromising alternative to traditional attention mechanisms based on\nrectangular-shaped image patches in neural networks. Although previous work\ndemonstrated that evolving a patch-based hard-attention module alongside a\ncontroller network could achieve state-of-the-art performance in visual\nreinforcement learning tasks, our approach leverages image segmentation to work\nwith higher-level features. By operating on proto-objects rather than fixed\npatches, we significantly reduce the representational complexity: each image\ndecomposes into fewer proto-objects than regular patches, and each proto-object\ncan be efficiently encoded as a compact feature vector. This enables a\nsubstantially smaller self-attention module that processes richer semantic\ninformation. Our experiments demonstrate that this proto-object-based approach\nmatches or exceeds the state-of-the-art performance of patch-based\nimplementations with 62% less parameters and 2.6 times less training time.",
        "url": "http://arxiv.org/abs/2505.00186v1",
        "published_date": "2025-04-30T21:01:20+00:00",
        "updated_date": "2025-04-30T21:01:20+00:00",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Rafael C. Pinto",
            "Anderson R. Tavares"
        ],
        "tldr": "this paper introduces a neuroevolution approach to self-attention using proto-objects (image segments) instead of patches, resulting in a more efficient self-attention module with comparable or better performance.",
        "tldr_zh": "本文介绍了一种神经进化方法，通过使用原始对象（图像分割）代替补丁来实现自注意力机制，从而产生更高效的自注意力模块，并实现相当或更好的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "Uncertainty-Aware Multi-Expert Knowledge Distillation for Imbalanced Disease Grading",
        "summary": "Automatic disease image grading is a significant application of artificial\nintelligence for healthcare, enabling faster and more accurate patient\nassessments. However, domain shifts, which are exacerbated by data imbalance,\nintroduce bias into the model, posing deployment difficulties in clinical\napplications. To address the problem, we propose a novel\n\\textbf{U}ncertainty-aware \\textbf{M}ulti-experts \\textbf{K}nowledge\n\\textbf{D}istillation (UMKD) framework to transfer knowledge from multiple\nexpert models to a single student model. Specifically, to extract\ndiscriminative features, UMKD decouples task-agnostic and task-specific\nfeatures with shallow and compact feature alignment in the feature space. At\nthe output space, an uncertainty-aware decoupled distillation (UDD) mechanism\ndynamically adjusts knowledge transfer weights based on expert model\nuncertainties, ensuring robust and reliable distillation. Additionally, UMKD\nalso tackles the problems of model architecture heterogeneity and distribution\ndiscrepancies between source and target domains, which are inadequately tackled\nby previous KD approaches. Extensive experiments on histology prostate grading\n(\\textit{SICAPv2}) and fundus image grading (\\textit{APTOS}) demonstrate that\nUMKD achieves a new state-of-the-art in both source-imbalanced and\ntarget-imbalanced scenarios, offering a robust and practical solution for\nreal-world disease image grading.",
        "url": "http://arxiv.org/abs/2505.00592v1",
        "published_date": "2025-05-01T15:26:23+00:00",
        "updated_date": "2025-05-01T15:26:23+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Shuo Tong",
            "Shangde Gao",
            "Ke Liu",
            "Zihang Huang",
            "Hongxia Xu",
            "Haochao Ying",
            "Jian Wu"
        ],
        "tldr": "the paper introduces an uncertainty-aware multi-expert knowledge distillation (umkd) framework to address data imbalance and domain shift in disease image grading, achieving state-of-the-art results on histology and fundus image grading datasets.",
        "tldr_zh": "该论文介绍了一种不确定性感知的多专家知识蒸馏（umkd）框架，以解决疾病图像分级中的数据不平衡和领域转移问题，并在组织学和眼底图像分级数据集上取得了最新的成果。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 4
    },
    {
        "title": "Towards Lightweight Hyperspectral Image Super-Resolution with Depthwise Separable Dilated Convolutional Network",
        "summary": "Deep neural networks have demonstrated highly competitive performance in\nsuper-resolution (SR) for natural images by learning mappings from\nlow-resolution (LR) to high-resolution (HR) images. However, hyperspectral\nsuper-resolution remains an ill-posed problem due to the high spectral\ndimensionality of the data and the scarcity of available training samples.\nMoreover, existing methods often rely on large models with a high number of\nparameters or require the fusion with panchromatic or RGB images, both of which\nare often impractical in real-world scenarios. Inspired by the MobileNet\narchitecture, we introduce a lightweight depthwise separable dilated\nconvolutional network (DSDCN) to address the aforementioned challenges.\nSpecifically, our model leverages multiple depthwise separable convolutions,\nsimilar to the MobileNet architecture, and further incorporates a dilated\nconvolution fusion block to make the model more flexible for the extraction of\nboth spatial and spectral features. In addition, we propose a custom loss\nfunction that combines mean squared error (MSE), an L2 norm\nregularization-based constraint, and a spectral angle-based loss, ensuring the\npreservation of both spectral and spatial details. The proposed model achieves\nvery competitive performance on two publicly available hyperspectral datasets,\nmaking it well-suited for hyperspectral image super-resolution tasks. The\nsource codes are publicly available at:\n\\href{https://github.com/Usman1021/lightweight}{https://github.com/Usman1021/lightweight}.",
        "url": "http://arxiv.org/abs/2505.00374v1",
        "published_date": "2025-05-01T07:57:23+00:00",
        "updated_date": "2025-05-01T07:57:23+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Usman Muhammad",
            "Jorma Laaksonen",
            "Lyudmila Mihaylova"
        ],
        "tldr": "this paper introduces a lightweight depthwise separable dilated convolutional network (dsdcn) for hyperspectral image super-resolution, addressing limitations of existing methods in terms of model size and reliance on additional data sources.",
        "tldr_zh": "该论文介绍了一种轻量级的深度可分离扩张卷积网络(dsdcn)，用于高光谱图像超分辨率，解决了现有方法在模型大小和依赖额外数据源方面的局限性。",
        "relevance_score": 2,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4
    },
    {
        "title": "AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor Long-Term Medication Adherence and Care",
        "summary": "Chronic diseases, including diabetes, hypertension, asthma, HIV-AIDS,\nepilepsy, and tuberculosis, necessitate rigorous adherence to medication to\navert disease progression, manage symptoms, and decrease mortality rates.\nAdherence is frequently undermined by factors including patient behavior,\ncaregiver support, elevated medical costs, and insufficient healthcare\ninfrastructure. We propose AdCare-VLM, a specialized Video-LLaVA-based\nmultimodal large vision language model (LVLM) aimed at visual question\nanswering (VQA) concerning medication adherence through patient videos. We\nemploy a private dataset comprising 806 custom-annotated tuberculosis (TB)\nmedication monitoring videos, which have been labeled by clinical experts, to\nfine-tune the model for adherence pattern detection. We present LLM-TB-VQA, a\ndetailed medical adherence VQA dataset that encompasses positive, negative, and\nambiguous adherence cases. Our method identifies correlations between visual\nfeatures, such as the clear visibility of the patient's face, medication, water\nintake, and the act of ingestion, and their associated medical concepts in\ncaptions. This facilitates the integration of aligned visual-linguistic\nrepresentations and improves multimodal interactions. Experimental results\nindicate that our method surpasses parameter-efficient fine-tuning (PEFT)\nenabled VLM models, such as LLaVA-V1.5 and Chat-UniVi, with absolute\nimprovements ranging from 3.1% to 3.54% across pre-trained, regular, and\nlow-rank adaptation (LoRA) configurations. Comprehensive ablation studies and\nattention map visualizations substantiate our approach, enhancing\ninterpretability.",
        "url": "http://arxiv.org/abs/2505.00275v1",
        "published_date": "2025-05-01T03:48:12+00:00",
        "updated_date": "2025-05-01T03:48:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Md Asaduzzaman Jabin",
            "Hanqi Jiang",
            "Yiwei Li",
            "Patrick Kaggwa",
            "Eugene Douglass",
            "Juliet N. Sekandi",
            "Tianming Liu"
        ],
        "tldr": "the paper introduces adcare-vlm, a video-llava-based lvlm fine-tuned on a custom tb medication monitoring video dataset for vqa concerning medication adherence, showing improvements over existing peft-enabled vlm models.",
        "tldr_zh": "该论文介绍了adcare-vlm，一个基于video-llava的lvlm，通过在一个定制的结核病药物监测视频数据集上进行微调，用于关于药物依从性的vqa，并且相较于现有的peft驱动的vlm模型，性能有所提升。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    }
]