[
    {
        "title": "Dynamic View Synthesis as an Inverse Problem",
        "summary": "In this work, we address dynamic view synthesis from monocular videos as an\ninverse problem in a training-free setting. By redesigning the noise\ninitialization phase of a pre-trained video diffusion model, we enable\nhigh-fidelity dynamic view synthesis without any weight updates or auxiliary\nmodules. We begin by identifying a fundamental obstacle to deterministic\ninversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and\nresolve it by introducing a novel noise representation, termed K-order\nRecursive Noise Representation. We derive a closed form expression for this\nrepresentation, enabling precise and efficient alignment between the\nVAE-encoded and the DDIM inverted latents. To synthesize newly visible regions\nresulting from camera motion, we introduce Stochastic Latent Modulation, which\nperforms visibility aware sampling over the latent space to complete occluded\nregions. Comprehensive experiments demonstrate that dynamic view synthesis can\nbe effectively performed through structured latent manipulation in the noise\ninitialization phase.",
        "url": "http://arxiv.org/abs/2506.08004v1",
        "published_date": "2025-06-09T17:59:47+00:00",
        "updated_date": "2025-06-09T17:59:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hidir Yesiltepe",
            "Pinar Yanardag"
        ],
        "tldr": "This paper proposes a training-free approach for dynamic view synthesis from monocular videos by manipulating the noise initialization of a pre-trained video diffusion model. It introduces a novel noise representation and stochastic latent modulation for inverting and completing occluded regions.",
        "tldr_zh": "本文提出了一种从单目视频中进行动态视图合成的免训练方法，通过操纵预训练视频扩散模型的噪声初始化来实现。它引入了一种新颖的噪声表示和随机潜在调制，用于反演和补全遮挡区域。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation",
        "summary": "Recent progress in multimodal generation has increasingly combined\nautoregressive (AR) and diffusion-based approaches, leveraging their\ncomplementary strengths: AR models capture long-range dependencies and produce\nfluent, context-aware outputs, while diffusion models operate in continuous\nlatent spaces to refine high-fidelity visual details. However, existing hybrids\noften lack systematic guidance on how and why to allocate model capacity\nbetween these paradigms. In this work, we introduce MADFormer, a Mixed\nAutoregressive and Diffusion Transformer that serves as a testbed for analyzing\nAR-diffusion trade-offs. MADFormer partitions image generation into spatial\nblocks, using AR layers for one-pass global conditioning across blocks and\ndiffusion layers for iterative local refinement within each block. Through\ncontrolled experiments on FFHQ-1024 and ImageNet, we identify two key insights:\n(1) block-wise partitioning significantly improves performance on\nhigh-resolution images, and (2) vertically mixing AR and diffusion layers\nyields better quality-efficiency balances--improving FID by up to 75% under\nconstrained inference compute. Our findings offer practical design principles\nfor future hybrid generative models.",
        "url": "http://arxiv.org/abs/2506.07999v1",
        "published_date": "2025-06-09T17:59:01+00:00",
        "updated_date": "2025-06-09T17:59:01+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Junhao Chen",
            "Yulia Tsvetkov",
            "Xiaochuang Han"
        ],
        "tldr": "MADFormer is a hybrid autoregressive-diffusion Transformer architecture for image generation that partitions generation into spatial blocks, using AR for global conditioning and diffusion for local refinement. The paper offers insights on AR-diffusion trade-offs and design principles for future hybrid models.",
        "tldr_zh": "MADFormer是一种用于图像生成的混合自回归-扩散Transformer架构，它将生成过程划分为空间块，使用AR进行全局条件控制，使用扩散进行局部细化。该论文提供了关于AR-扩散权衡的见解，并为未来的混合模型提供了设计原则。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reinforcing Multimodal Understanding and Generation with Dual Self-rewards",
        "summary": "Building upon large language models (LLMs), recent large multimodal models\n(LMMs) unify cross-model understanding and generation into a single framework.\nHowever, LMMs still struggle to achieve accurate image-text alignment, prone to\ngenerating text responses contradicting the visual input or failing to follow\nthe text-to-image prompts. Current solutions require external supervision\n(e.g., human feedback or reward models) and only address unidirectional\ntasks-either understanding or generation. In this work, based on the\nobservation that understanding and generation are inverse dual tasks, we\nintroduce a self-supervised dual reward mechanism to reinforce the\nunderstanding and generation capabilities of LMMs. Specifically, we sample\nmultiple outputs for a given input in one task domain, then reverse the\ninput-output pairs to compute the dual likelihood of the model as self-rewards\nfor optimization. Extensive experimental results on visual understanding and\ngeneration benchmarks demonstrate that our method can effectively enhance the\nperformance of the model without any external supervision, especially achieving\nremarkable improvements in text-to-image tasks.",
        "url": "http://arxiv.org/abs/2506.07963v1",
        "published_date": "2025-06-09T17:38:45+00:00",
        "updated_date": "2025-06-09T17:38:45+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Jixiang Hong",
            "Yiran Zhang",
            "Guanzhong Wang",
            "Yi Liu",
            "Ji-Rong Wen",
            "Rui Yan"
        ],
        "tldr": "This paper introduces a self-supervised dual reward mechanism to improve the understanding and generation capabilities of large multimodal models (LMMs), especially in text-to-image tasks, without external supervision.",
        "tldr_zh": "该论文提出了一种自监督的 dual reward 机制，以提高大型多模态模型（LMM）的理解和生成能力，尤其是在文本到图像任务中，无需外部监督。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces",
        "summary": "Diffusion models have demonstrated remarkable performance in generating\nunimodal data across various tasks, including image, video, and text\ngeneration. On the contrary, the joint generation of multimodal data through\ndiffusion models is still in the early stages of exploration. Existing\napproaches heavily rely on external preprocessing protocols, such as tokenizers\nand variational autoencoders, to harmonize varied data representations into a\nunified, unimodal format. This process heavily demands the high accuracy of\nencoders and decoders, which can be problematic for applications with limited\ndata. To lift this restriction, we propose a novel framework for building\nmultimodal diffusion models on arbitrary state spaces, enabling native\ngeneration of coupled data across different modalities. By introducing an\ninnovative decoupled noise schedule for each modality, we enable both\nunconditional and modality-conditioned generation within a single model\nsimultaneously. We empirically validate our approach for text-image generation\nand mixed-type tabular data synthesis, demonstrating that it achieves\ncompetitive performance.",
        "url": "http://arxiv.org/abs/2506.07903v1",
        "published_date": "2025-06-09T16:20:20+00:00",
        "updated_date": "2025-06-09T16:20:20+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Kevin Rojas",
            "Yuchen Zhu",
            "Sichen Zhu",
            "Felix X. -F. Ye",
            "Molei Tao"
        ],
        "tldr": "This paper introduces a novel multimodal diffusion model framework that operates on arbitrary state spaces, eliminating reliance on external preprocessing methods like tokenizers and VAEs, showcasing competitive performance in text-image generation and mixed-type tabular data synthesis.",
        "tldr_zh": "本文提出了一种新型多模态扩散模型框架，该框架可以在任意状态空间上运行，无需依赖外部预处理方法（如tokenizer和VAE），并在文本-图像生成和混合类型表格数据合成中展示了良好的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement",
        "summary": "Despite recent advances in video generation, existing models still lack\nfine-grained controllability, especially for multi-subject customization with\nconsistent identity and interaction. In this paper, we propose PolyVivid, a\nmulti-subject video customization framework that enables flexible and\nidentity-consistent generation. To establish accurate correspondences between\nsubject images and textual entities, we design a VLLM-based text-image fusion\nmodule that embeds visual identities into the textual space for precise\ngrounding. To further enhance identity preservation and subject interaction, we\npropose a 3D-RoPE-based enhancement module that enables structured\nbidirectional fusion between text and image embeddings. Moreover, we develop an\nattention-inherited identity injection module to effectively inject fused\nidentity features into the video generation process, mitigating identity drift.\nFinally, we construct an MLLM-based data pipeline that combines MLLM-based\ngrounding, segmentation, and a clique-based subject consolidation strategy to\nproduce high-quality multi-subject data, effectively enhancing subject\ndistinction and reducing ambiguity in downstream video generation. Extensive\nexperiments demonstrate that PolyVivid achieves superior performance in\nidentity fidelity, video realism, and subject alignment, outperforming existing\nopen-source and commercial baselines.",
        "url": "http://arxiv.org/abs/2506.07848v1",
        "published_date": "2025-06-09T15:11:09+00:00",
        "updated_date": "2025-06-09T15:11:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Teng Hu",
            "Zhentao Yu",
            "Zhengguang Zhou",
            "Jiangning Zhang",
            "Yuan Zhou",
            "Qinglin Lu",
            "Ran Yi"
        ],
        "tldr": "PolyVivid introduces a novel framework for multi-subject video generation, enhancing identity consistency and subject interaction through VLLM-based fusion, 3D-RoPE enhancement, and attention-inherited identity injection. It also uses an MLLM-based data pipeline to produce high-quality training data.",
        "tldr_zh": "PolyVivid 提出了一个新颖的多主体视频生成框架，通过基于 VLLM 的融合、基于 3D-RoPE 的增强和注意力继承的身份注入，增强了身份一致性和主体交互。它还使用基于 MLLM 的数据管道来生成高质量的训练数据。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]