[
    {
        "title": "GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning",
        "summary": "Recent progress in diffusion models has greatly enhanced video generation\nquality, yet these models still require fine-tuning to improve specific\ndimensions like instance preservation, motion rationality, composition, and\nphysical plausibility. Existing fine-tuning approaches often rely on human\nannotations and large-scale computational resources, limiting their\npracticality. In this work, we propose GigaVideo-1, an efficient fine-tuning\nframework that advances video generation without additional human supervision.\nRather than injecting large volumes of high-quality data from external sources,\nGigaVideo-1 unlocks the latent potential of pre-trained video diffusion models\nthrough automatic feedback. Specifically, we focus on two key aspects of the\nfine-tuning process: data and optimization. To improve fine-tuning data, we\ndesign a prompt-driven data engine that constructs diverse, weakness-oriented\ntraining samples. On the optimization side, we introduce a reward-guided\ntraining strategy, which adaptively weights samples using feedback from\npre-trained vision-language models with a realism constraint. We evaluate\nGigaVideo-1 on the VBench-2.0 benchmark using Wan2.1 as the baseline across 17\nevaluation dimensions. Experiments show that GigaVideo-1 consistently improves\nperformance on almost all the dimensions with an average gain of about 4% using\nonly 4 GPU-hours. Requiring no manual annotations and minimal real data,\nGigaVideo-1 demonstrates both effectiveness and efficiency. Code, model, and\ndata will be publicly available.",
        "url": "http://arxiv.org/abs/2506.10639v1",
        "published_date": "2025-06-12T12:25:37+00:00",
        "updated_date": "2025-06-12T12:25:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoyi Bao",
            "Jindi Lv",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Xinze Chen",
            "YuKun Zhou",
            "Jiancheng Lv",
            "Xingang Wang",
            "Guan Huang"
        ],
        "tldr": "GigaVideo-1 is a fine-tuning framework for video generation that uses automatic feedback from pre-trained vision-language models to improve performance with only 4 GPU-hours and without human annotation.",
        "tldr_zh": "GigaVideo-1是一个视频生成微调框架，它使用来自预训练的视觉-语言模型的自动反馈，在仅需4 GPU-小时且无需人工标注的情况下提高性能。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "SpectralAR: Spectral Autoregressive Visual Generation",
        "summary": "Autoregressive visual generation has garnered increasing attention due to its\nscalability and compatibility with other modalities compared with diffusion\nmodels. Most existing methods construct visual sequences as spatial patches for\nautoregressive generation. However, image patches are inherently parallel,\ncontradicting the causal nature of autoregressive modeling. To address this, we\npropose a Spectral AutoRegressive (SpectralAR) visual generation framework,\nwhich realizes causality for visual sequences from the spectral perspective.\nSpecifically, we first transform an image into ordered spectral tokens with\nNested Spectral Tokenization, representing lower to higher frequency\ncomponents. We then perform autoregressive generation in a coarse-to-fine\nmanner with the sequences of spectral tokens. By considering different levels\nof detail in images, our SpectralAR achieves both sequence causality and token\nefficiency without bells and whistles. We conduct extensive experiments on\nImageNet-1K for image reconstruction and autoregressive generation, and\nSpectralAR achieves 3.02 gFID with only 64 tokens and 310M parameters. Project\npage: https://huang-yh.github.io/spectralar/.",
        "url": "http://arxiv.org/abs/2506.10962v1",
        "published_date": "2025-06-12T17:57:44+00:00",
        "updated_date": "2025-06-12T17:57:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Yuanhui Huang",
            "Weiliang Chen",
            "Wenzhao Zheng",
            "Yueqi Duan",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "tldr": "The paper introduces SpectralAR, a novel autoregressive visual generation framework that uses spectral tokens to achieve causality and efficiency in image generation, demonstrating promising results on ImageNet-1K. It addresses the problem of applying autoregressive modeling to inherently parallel image patches.",
        "tldr_zh": "该论文介绍了 SpectralAR，一种新颖的自回归视觉生成框架，它使用频谱令牌来实现图像生成中的因果关系和效率，并在 ImageNet-1K 上展示了有希望的结果。它解决了将自回归建模应用于固有并行图像块的问题。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "AIR: Zero-shot Generative Model Adaptation with Iterative Refinement",
        "summary": "Zero-shot generative model adaptation (ZSGM) aims to adapt a pre-trained\ngenerator to a target domain using only text guidance and without any samples\nfrom the target domain. Central to recent ZSGM approaches are directional loss\nwhich use the text guidance in the form of aligning the image offset with text\noffset in the embedding space of a vision-language model like CLIP. This is\nsimilar to the analogical reasoning in NLP where the offset between one pair of\nwords is used to identify a missing element in another pair by aligning the\noffset between these two pairs. However, a major limitation of existing ZSGM\nmethods is that the learning objective assumes the complete alignment between\nimage offset and text offset in the CLIP embedding space, resulting in quality\ndegrade in generated images. Our work makes two main contributions. Inspired by\nthe offset misalignment studies in NLP, as our first contribution, we perform\nan empirical study to analyze the misalignment between text offset and image\noffset in CLIP embedding space for various large publicly available datasets.\nOur important finding is that offset misalignment in CLIP embedding space is\ncorrelated with concept distance, i.e., close concepts have a less offset\nmisalignment. To address the limitations of the current approaches, as our\nsecond contribution, we propose Adaptation with Iterative Refinement (AIR)\nwhich is the first ZSGM approach to focus on improving target domain image\nquality based on our new insight on offset misalignment.Qualitative,\nquantitative, and user study in 26 experiment setups consistently demonstrate\nthe proposed AIR approach achieves SOTA performance. Additional experiments are\nin Supp.",
        "url": "http://arxiv.org/abs/2506.10895v1",
        "published_date": "2025-06-12T17:00:50+00:00",
        "updated_date": "2025-06-12T17:00:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Guimeng Liu",
            "Milad Abdollahzadeh",
            "Ngai-Man Cheung"
        ],
        "tldr": "This paper presents AIR, a novel zero-shot generative model adaptation approach that addresses misalignment issues in CLIP embeddings, leading to improved image quality in target domains without target domain samples.",
        "tldr_zh": "本文提出了AIR，一种新颖的零样本生成模型自适应方法，解决了CLIP嵌入中的错位问题，从而在没有目标域样本的情况下提高了目标域中的图像质量。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Symmetrical Flow Matching: Unified Image Generation, Segmentation, and Classification with Score-Based Generative Models",
        "summary": "Flow Matching has emerged as a powerful framework for learning continuous\ntransformations between distributions, enabling high-fidelity generative\nmodeling. This work introduces Symmetrical Flow Matching (SymmFlow), a new\nformulation that unifies semantic segmentation, classification, and image\ngeneration within a single model. Using a symmetric learning objective,\nSymmFlow models forward and reverse transformations jointly, ensuring\nbi-directional consistency, while preserving sufficient entropy for generative\ndiversity. A new training objective is introduced to explicitly retain semantic\ninformation across flows, featuring efficient sampling while preserving\nsemantic structure, allowing for one-step segmentation and classification\nwithout iterative refinement. Unlike previous approaches that impose strict\none-to-one mapping between masks and images, SymmFlow generalizes to flexible\nconditioning, supporting both pixel-level and image-level class labels.\nExperimental results on various benchmarks demonstrate that SymmFlow achieves\nstate-of-the-art performance on semantic image synthesis, obtaining FID scores\nof 11.9 on CelebAMask-HQ and 7.0 on COCO-Stuff with only 25 inference steps.\nAdditionally, it delivers competitive results on semantic segmentation and\nshows promising capabilities in classification tasks. The code will be publicly\navailable.",
        "url": "http://arxiv.org/abs/2506.10634v1",
        "published_date": "2025-06-12T12:19:28+00:00",
        "updated_date": "2025-06-12T12:19:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Francisco Caetano",
            "Christiaan Viviers",
            "Peter H. N. De With",
            "Fons van der Sommen"
        ],
        "tldr": "The paper introduces Symmetrical Flow Matching (SymmFlow), a novel formulation unifying image generation, semantic segmentation, and classification within a single score-based generative model using a symmetric learning objective, achieving state-of-the-art performance on semantic image synthesis.",
        "tldr_zh": "该论文介绍了一种名为对称流匹配（SymmFlow）的新方法，它使用对称学习目标，将图像生成、语义分割和分类统一在一个基于分数的生成模型中，并在语义图像合成上取得了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation",
        "summary": "Recent advances in large language models (LLMs) have enabled multimodal\nfoundation models to tackle both image understanding and generation within a\nunified framework. Despite these gains, unified models often underperform\ncompared to specialized models in either task. A key challenge in developing\nunified models lies in the inherent differences between the visual features\nneeded for image understanding versus generation, as well as the distinct\ntraining processes required for each modality. In this work, we introduce\nPisces, an auto-regressive multimodal foundation model that addresses this\nchallenge through a novel decoupled visual encoding architecture and tailored\ntraining techniques optimized for multimodal generation. Combined with\nmeticulous data curation, pretraining, and finetuning, Pisces achieves\ncompetitive performance in both image understanding and image generation. We\nevaluate Pisces on over 20 public benchmarks for image understanding, where it\ndemonstrates strong performance across a wide range of tasks. Additionally, on\nGenEval, a widely adopted benchmark for image generation, Pisces exhibits\nrobust generative capabilities. Our extensive analysis reveals the synergistic\nrelationship between image understanding and generation, and the benefits of\nusing separate visual encoders, advancing the field of unified multimodal\nmodels.",
        "url": "http://arxiv.org/abs/2506.10395v1",
        "published_date": "2025-06-12T06:37:34+00:00",
        "updated_date": "2025-06-12T06:37:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhiyang Xu",
            "Jiuhai Chen",
            "Zhaojiang Lin",
            "Xichen Pan",
            "Lifu Huang",
            "Tianyi Zhou",
            "Madian Khabsa",
            "Qifan Wang",
            "Di Jin",
            "Michihiro Yasunaga",
            "Lili Yu",
            "Xi Victoria Lin",
            "Shaoliang Nie"
        ],
        "tldr": "The paper introduces Pisces, an auto-regressive multimodal foundation model with a decoupled visual encoding architecture and tailored training techniques, showing competitive performance in both image understanding and generation tasks.",
        "tldr_zh": "该论文介绍了Pisces，一个具有解耦视觉编码架构和定制训练技术的自回归多模态基础模型，在图像理解和生成任务中都表现出竞争性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SPARKE: Scalable Prompt-Aware Diversity Guidance in Diffusion Models via RKE Score",
        "summary": "Diffusion models have demonstrated remarkable success in high-fidelity image\nsynthesis and prompt-guided generative modeling. However, ensuring adequate\ndiversity in generated samples of prompt-guided diffusion models remains a\nchallenge, particularly when the prompts span a broad semantic spectrum and the\ndiversity of generated data needs to be evaluated in a prompt-aware fashion\nacross semantically similar prompts. Recent methods have introduced guidance\nvia diversity measures to encourage more varied generations. In this work, we\nextend the diversity measure-based approaches by proposing the Scalable\nPrompt-Aware R\\'eny Kernel Entropy Diversity Guidance (SPARKE) method for\nprompt-aware diversity guidance. SPARKE utilizes conditional entropy for\ndiversity guidance, which dynamically conditions diversity measurement on\nsimilar prompts and enables prompt-aware diversity control. While the\nentropy-based guidance approach enhances prompt-aware diversity, its reliance\non the matrix-based entropy scores poses computational challenges in\nlarge-scale generation settings. To address this, we focus on the special case\nof Conditional latent RKE Score Guidance, reducing entropy computation and\ngradient-based optimization complexity from the $O(n^3)$ of general entropy\nmeasures to $O(n)$. The reduced computational complexity allows for\ndiversity-guided sampling over potentially thousands of generation rounds on\ndifferent prompts. We numerically test the SPARKE method on several\ntext-to-image diffusion models, demonstrating that the proposed method improves\nthe prompt-aware diversity of the generated data without incurring significant\ncomputational costs. We release our code on the project page:\nhttps://mjalali.github.io/SPARKE",
        "url": "http://arxiv.org/abs/2506.10173v1",
        "published_date": "2025-06-11T20:53:45+00:00",
        "updated_date": "2025-06-11T20:53:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Mohammad Jalali",
            "Haoyu Lei",
            "Amin Gohari",
            "Farzan Farnia"
        ]
    }
]