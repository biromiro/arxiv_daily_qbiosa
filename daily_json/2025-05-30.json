[
    {
        "title": "MAGREF: Masked Guidance for Any-Reference Video Generation",
        "summary": "Video generation has made substantial strides with the emergence of deep\ngenerative models, especially diffusion-based approaches. However, video\ngeneration based on multiple reference subjects still faces significant\nchallenges in maintaining multi-subject consistency and ensuring high\ngeneration quality. In this paper, we propose MAGREF, a unified framework for\nany-reference video generation that introduces masked guidance to enable\ncoherent multi-subject video synthesis conditioned on diverse reference images\nand a textual prompt. Specifically, we propose (1) a region-aware dynamic\nmasking mechanism that enables a single model to flexibly handle various\nsubject inference, including humans, objects, and backgrounds, without\narchitectural changes, and (2) a pixel-wise channel concatenation mechanism\nthat operates on the channel dimension to better preserve appearance features.\nOur model delivers state-of-the-art video generation quality, generalizing from\nsingle-subject training to complex multi-subject scenarios with coherent\nsynthesis and precise control over individual subjects, outperforming existing\nopen-source and commercial baselines. To facilitate evaluation, we also\nintroduce a comprehensive multi-subject video benchmark. Extensive experiments\ndemonstrate the effectiveness of our approach, paving the way for scalable,\ncontrollable, and high-fidelity multi-subject video synthesis. Code and model\ncan be found at: https://github.com/MAGREF-Video/MAGREF",
        "url": "http://arxiv.org/abs/2505.23742v1",
        "published_date": "2025-05-29T17:58:15+00:00",
        "updated_date": "2025-05-29T17:58:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yufan Deng",
            "Xun Guo",
            "Yuanyang Yin",
            "Jacob Zhiyuan Fang",
            "Yiding Yang",
            "Yizhi Wang",
            "Shenghai Yuan",
            "Angtian Wang",
            "Bo Liu",
            "Haibin Huang",
            "Chongyang Ma"
        ],
        "tldr": "The paper introduces MAGREF, a new framework for generating videos with multiple subjects based on reference images and text prompts, using masked guidance for improved subject consistency and generation quality.",
        "tldr_zh": "该论文介绍了MAGREF，一个新的视频生成框架，可以基于参考图像和文本提示生成包含多个主体的视频，通过使用掩码引导来提高主体一致性和生成质量。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models",
        "summary": "Recent advancements in text-to-video (T2V) diffusion models have enabled\nhigh-fidelity and realistic video synthesis. However, current T2V models often\nstruggle to generate physically plausible content due to their limited inherent\nability to accurately understand physics. We found that while the\nrepresentations within T2V models possess some capacity for physics\nunderstanding, they lag significantly behind those from recent video\nself-supervised learning methods. To this end, we propose a novel framework\ncalled VideoREPA, which distills physics understanding capability from video\nunderstanding foundation models into T2V models by aligning token-level\nrelations. This closes the physics understanding gap and enable more\nphysics-plausible generation. Specifically, we introduce the Token Relation\nDistillation (TRD) loss, leveraging spatio-temporal alignment to provide soft\nguidance suitable for finetuning powerful pre-trained T2V models, a critical\ndeparture from prior representation alignment (REPA) methods. To our knowledge,\nVideoREPA is the first REPA method designed for finetuning T2V models and\nspecifically for injecting physical knowledge. Empirical evaluations show that\nVideoREPA substantially enhances the physics commonsense of baseline method,\nCogVideoX, achieving significant improvement on relevant benchmarks and\ndemonstrating a strong capacity for generating videos consistent with intuitive\nphysics. More video results are available at https://videorepa.github.io/.",
        "url": "http://arxiv.org/abs/2505.23656v1",
        "published_date": "2025-05-29T17:06:44+00:00",
        "updated_date": "2025-05-29T17:06:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangdong Zhang",
            "Jiaqi Liao",
            "Shaofeng Zhang",
            "Fanqing Meng",
            "Xiangpeng Wan",
            "Junchi Yan",
            "Yu Cheng"
        ],
        "tldr": "The paper introduces VideoREPA, a novel framework that distills physics understanding from video understanding foundation models into text-to-video diffusion models by aligning token-level relations, leading to more physically plausible video generation.",
        "tldr_zh": "该论文介绍了VideoREPA，一种新颖的框架，通过对齐 token 级别的关系，将视频理解基础模型中的物理理解能力提炼到文本到视频的扩散模型中，从而生成更符合物理规律的视频。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Re-ttention: Ultra Sparse Visual Generation via Attention Statistical Reshape",
        "summary": "Diffusion Transformers (DiT) have become the de-facto model for generating\nhigh-quality visual content like videos and images. A huge bottleneck is the\nattention mechanism where complexity scales quadratically with resolution and\nvideo length. One logical way to lessen this burden is sparse attention, where\nonly a subset of tokens or patches are included in the calculation. However,\nexisting techniques fail to preserve visual quality at extremely high sparsity\nlevels and might even incur non-negligible compute overheads. % To address this\nconcern, we propose Re-ttention, which implements very high sparse attention\nfor visual generation models by leveraging the temporal redundancy of Diffusion\nModels to overcome the probabilistic normalization shift within the attention\nmechanism. Specifically, Re-ttention reshapes attention scores based on the\nprior softmax distribution history in order to preserve the visual quality of\nthe full quadratic attention at very high sparsity levels. % Experimental\nresults on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate\nthat Re-ttention requires as few as 3.1\\% of the tokens during inference,\noutperforming contemporary methods like FastDiTAttn, Sparse VideoGen and\nMInference. Further, we measure latency to show that our method can attain over\n45\\% end-to-end % and over 92\\% self-attention latency reduction on an H100 GPU\nat negligible overhead cost.\n  Code available online here:\n\\href{https://github.com/cccrrrccc/Re-ttention}{https://github.com/cccrrrccc/Re-ttention}",
        "url": "http://arxiv.org/abs/2505.22918v1",
        "published_date": "2025-05-28T22:39:12+00:00",
        "updated_date": "2025-05-28T22:39:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruichen Chen",
            "Keith G. Mills",
            "Liyao Jiang",
            "Chao Gao",
            "Di Niu"
        ],
        "tldr": "The paper introduces Re-ttention, a novel sparse attention mechanism for diffusion transformers that achieves significant latency reduction (up to 92% in self-attention) with minimal visual quality loss by reshaping attention scores based on prior softmax distributions, which improves upon existing sparsity methods.",
        "tldr_zh": "该论文介绍了 Re-ttention，一种用于扩散Transformer的新型稀疏注意力机制，通过基于先前的softmax分布重塑注意力得分，从而在最小化视觉质量损失的情况下显著降低延迟（自注意力方面高达92%），优于现有的稀疏方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation",
        "summary": "In this report, we present OpenUni, a simple, lightweight, and fully\nopen-source baseline for unifying multimodal understanding and generation.\nInspired by prevailing practices in unified model learning, we adopt an\nefficient training strategy that minimizes the training complexity and overhead\nby bridging the off-the-shelf multimodal large language models (LLMs) and\ndiffusion models through a set of learnable queries and a light-weight\ntransformer-based connector. With a minimalist choice of architecture, we\ndemonstrate that OpenUni can: 1) generate high-quality and instruction-aligned\nimages, and 2) achieve exceptional performance on standard benchmarks such as\nGenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To\nsupport open research and community advancement, we release all model weights,\ntraining code, and our curated training datasets (including 23M image-text\npairs) at https://github.com/wusize/OpenUni.",
        "url": "http://arxiv.org/abs/2505.23661v1",
        "published_date": "2025-05-29T17:09:44+00:00",
        "updated_date": "2025-05-29T17:09:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Size Wu",
            "Zhonghua Wu",
            "Zerui Gong",
            "Qingyi Tao",
            "Sheng Jin",
            "Qinyue Li",
            "Wei Li",
            "Chen Change Loy"
        ],
        "tldr": "OpenUni presents a simple and open-source approach to unified multimodal understanding and generation, leveraging existing LLMs and diffusion models with a lightweight connector, achieving strong performance on benchmarks with relatively few parameters.",
        "tldr_zh": "OpenUni 提出了一种简单且开源的统一多模态理解和生成方法，通过轻量级连接器利用现有的大语言模型和扩散模型，以相对较少的参数在基准测试中取得了优异的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "D-AR: Diffusion via Autoregressive Models",
        "summary": "This paper presents Diffusion via Autoregressive models (D-AR), a new\nparadigm recasting the image diffusion process as a vanilla autoregressive\nprocedure in the standard next-token-prediction fashion. We start by designing\nthe tokenizer that converts images into sequences of discrete tokens, where\ntokens in different positions can be decoded into different diffusion denoising\nsteps in the pixel space. Thanks to the diffusion properties, these tokens\nnaturally follow a coarse-to-fine order, which directly lends itself to\nautoregressive modeling. Therefore, we apply standard next-token prediction on\nthese tokens, without modifying any underlying designs (either causal masks or\ntraining/inference strategies), and such sequential autoregressive token\ngeneration directly mirrors the diffusion procedure in image space. That is,\nonce the autoregressive model generates an increment of tokens, we can directly\ndecode these tokens into the corresponding diffusion denoising step in the\nstreaming manner. Our pipeline naturally reveals several intriguing properties,\nfor example, it supports consistent previews when generating only a subset of\ntokens and enables zero-shot layout-controlled synthesis. On the standard\nImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone\nwith 256 discrete tokens. We hope our work can inspire future research on\nunified autoregressive architectures of visual synthesis, especially with large\nlanguage models. Code and models will be available at\nhttps://github.com/showlab/D-AR",
        "url": "http://arxiv.org/abs/2505.23660v1",
        "published_date": "2025-05-29T17:09:25+00:00",
        "updated_date": "2025-05-29T17:09:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziteng Gao",
            "Mike Zheng Shou"
        ],
        "tldr": "The paper introduces D-AR, a method that reframes image diffusion as an autoregressive token prediction task, achieving strong FID scores on ImageNet with a Llama backbone and enabling features like consistent previews and zero-shot layout control.",
        "tldr_zh": "该论文介绍了D-AR，一种将图像扩散重构为自回归token预测任务的方法，在ImageNet上使用Llama主干网络实现了很强的FID分数，并实现了诸如一致性预览和零样本布局控制等特性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model",
        "summary": "Unified generation models aim to handle diverse tasks across modalities --\nsuch as text generation, image generation, and vision-language reasoning --\nwithin a single architecture and decoding paradigm. Autoregressive unified\nmodels suffer from slow inference due to sequential decoding, and\nnon-autoregressive unified models suffer from weak generalization due to\nlimited pretrained backbones. We introduce Muddit, a unified discrete diffusion\ntransformer that enables fast and parallel generation across both text and\nimage modalities. Unlike prior unified diffusion models trained from scratch,\nMuddit integrates strong visual priors from a pretrained text-to-image backbone\nwith a lightweight text decoder, enabling flexible and high-quality multimodal\ngeneration under a unified architecture. Empirical results show that Muddit\nachieves competitive or superior performance compared to significantly larger\nautoregressive models in both quality and efficiency. The work highlights the\npotential of purely discrete diffusion, when equipped with strong visual\npriors, as a scalable and effective backbone for unified generation.",
        "url": "http://arxiv.org/abs/2505.23606v1",
        "published_date": "2025-05-29T16:15:48+00:00",
        "updated_date": "2025-05-29T16:15:48+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Qingyu Shi",
            "Jinbin Bai",
            "Zhuoran Zhao",
            "Wenhao Chai",
            "Kaidong Yu",
            "Jianzong Wu",
            "Shuangyong Song",
            "Yunhai Tong",
            "Xiangtai Li",
            "Xuelong Li",
            "Shuicheng Yan"
        ],
        "tldr": "Muddit, a unified discrete diffusion transformer, achieves fast and parallel text and image generation by incorporating strong visual priors from a pre-trained text-to-image backbone, surpassing autoregressive models in both quality and efficiency.",
        "tldr_zh": "Muddit是一个统一的离散扩散变换器，通过结合预训练的文本到图像主干网络的强大视觉先验，实现了快速且并行的文本和图像生成，在质量和效率上均优于自回归模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning",
        "summary": "Unified multimodal large language models such as Show-o and Janus have\nachieved strong performance across both generation and understanding tasks.\nHowever, these models typically rely on large-scale datasets and require\nsubstantial computation during the pretraining stage. In addition, several\npost-training methods have been proposed, but they often depend on external\ndata or are limited to task-specific customization. In this work, we introduce\nUniRL, a self-improving post-training approach. Our approach enables the model\nto generate images from prompts and use them as training data in each\niteration, without relying on any external image data. Moreover, it enables the\ntwo tasks to enhance each other: the generated images are used for\nunderstanding, and the understanding results are used to supervise generation.\nWe explore supervised fine-tuning (SFT) and Group Relative Policy Optimization\n(GRPO) to optimize the models. UniRL offers three key advantages: (1) it\nrequires no external image data, as all training samples are generated by the\nmodel itself during training; (2) it not only improves individual task\nperformance, but also reduces the imbalance between generation and\nunderstanding; and (3) it requires only several additional training steps\nduring the post-training stage. We evaluate UniRL on top of Show-o and Janus,\nachieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and\nmodels will be released in https://github.com/showlab/UniRL.",
        "url": "http://arxiv.org/abs/2505.23380v1",
        "published_date": "2025-05-29T12:00:15+00:00",
        "updated_date": "2025-05-29T12:00:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weijia Mao",
            "Zhenheng Yang",
            "Mike Zheng Shou"
        ],
        "tldr": "UniRL is a self-improving post-training method for multimodal models that generates its own training data to improve both image generation and understanding without external datasets, and reduces performance imbalance between tasks.",
        "tldr_zh": "UniRL 是一种自提升的多模态模型后训练方法，它生成自己的训练数据以提高图像生成和理解能力，无需外部数据集，并减少了任务之间的性能不平衡。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization",
        "summary": "Fine-tuning pre-trained generative models with Reinforcement Learning (RL)\nhas emerged as an effective approach for aligning outputs more closely with\nnuanced human preferences. In this paper, we investigate the application of\nGroup Relative Policy Optimization (GRPO) to fine-tune next-scale visual\nautoregressive (VAR) models. Our empirical results demonstrate that this\napproach enables alignment to intricate reward signals derived from aesthetic\npredictors and CLIP embeddings, significantly enhancing image quality and\nenabling precise control over the generation style. Interestingly, by\nleveraging CLIP, our method can help VAR models generalize beyond their initial\nImageNet distribution: through RL-driven exploration, these models can generate\nimages aligned with prompts referencing image styles that were absent during\npre-training. In summary, we show that RL-based fine-tuning is both efficient\nand effective for VAR models, benefiting particularly from their fast inference\nspeeds, which are advantageous for online sampling, an aspect that poses\nsignificant challenges for diffusion-based alternatives.",
        "url": "http://arxiv.org/abs/2505.23331v1",
        "published_date": "2025-05-29T10:45:38+00:00",
        "updated_date": "2025-05-29T10:45:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Matteo Gallici",
            "Haitz Sáez de Ocáriz Borde"
        ],
        "tldr": "This paper explores using Group Relative Policy Optimization (GRPO) for fine-tuning visual autoregressive models, demonstrating improved image quality, style control, and generalization beyond the pre-training dataset through RL-driven exploration.",
        "tldr_zh": "本文探讨了使用群体相对策略优化（GRPO）来微调视觉自回归模型，通过强化学习驱动的探索，展示了图像质量、风格控制以及超出预训练数据集的泛化能力的提升。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation",
        "summary": "Recent advancements in unified vision-language models (VLMs), which integrate\nboth visual understanding and generation capabilities, have attracted\nsignificant attention. The underlying hypothesis is that a unified architecture\nwith mixed training on both understanding and generation tasks can enable\nmutual enhancement between understanding and generation. However, this\nhypothesis remains underexplored in prior works on unified VLMs. To address\nthis gap, this paper systematically investigates the generalization across\nunderstanding and generation tasks in unified VLMs. Specifically, we design a\ndataset closely aligned with real-world scenarios to facilitate extensive\nexperiments and quantitative evaluations. We evaluate multiple unified VLM\narchitectures to validate our findings. Our key findings are as follows. First,\nunified VLMs trained with mixed data exhibit mutual benefits in understanding\nand generation tasks across various architectures, and this mutual benefits can\nscale up with increased data. Second, better alignment between multimodal input\nand output spaces will lead to better generalization. Third, the knowledge\nacquired during generation tasks can transfer to understanding tasks, and this\ncross-task generalization occurs within the base language model, beyond\nmodality adapters. Our findings underscore the critical necessity of unifying\nunderstanding and generation in VLMs, offering valuable insights for the design\nand optimization of unified VLMs.",
        "url": "http://arxiv.org/abs/2505.23043v1",
        "published_date": "2025-05-29T03:40:21+00:00",
        "updated_date": "2025-05-29T03:40:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jihai Zhang",
            "Tianle Li",
            "Linjie Li",
            "Zhengyuan Yang",
            "Yu Cheng"
        ],
        "tldr": "This paper investigates the generalization capabilities of unified vision-language models (VLMs) across understanding and generation tasks, finding mutual benefits and highlighting the importance of aligning input/output spaces.",
        "tldr_zh": "本文研究了统一视觉语言模型（VLM）在理解和生成任务中的泛化能力，发现它们之间存在互利关系，并强调了对齐输入/输出空间的重要性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Toward Memory-Aided World Models: Benchmarking via Spatial Consistency",
        "summary": "The ability to simulate the world in a spatially consistent manner is a\ncrucial requirements for effective world models. Such a model enables\nhigh-quality visual generation, and also ensures the reliability of world\nmodels for downstream tasks such as simulation and planning. Designing a memory\nmodule is a crucial component for addressing spatial consistency: such a model\nmust not only retain long-horizon observational information, but also enables\nthe construction of explicit or implicit internal spatial representations.\nHowever, there are no dataset designed to promote the development of memory\nmodules by explicitly enforcing spatial consistency constraints. Furthermore,\nmost existing benchmarks primarily emphasize visual coherence or generation\nquality, neglecting the requirement of long-range spatial consistency. To\nbridge this gap, we construct a dataset and corresponding benchmark by sampling\n150 distinct locations within the open-world environment of Minecraft,\ncollecting about 250 hours (20 million frames) of loop-based navigation videos\nwith actions. Our dataset follows a curriculum design of sequence lengths,\nallowing models to learn spatial consistency on increasingly complex navigation\ntrajectories. Furthermore, our data collection pipeline is easily extensible to\nnew Minecraft environments and modules. Four representative world model\nbaselines are evaluated on our benchmark. Dataset, benchmark, and code are\nopen-sourced to support future research.",
        "url": "http://arxiv.org/abs/2505.22976v1",
        "published_date": "2025-05-29T01:28:57+00:00",
        "updated_date": "2025-05-29T01:28:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kewei Lian",
            "Shaofei Cai",
            "Yilun Du",
            "Yitao Liang"
        ],
        "tldr": "This paper introduces a new Minecraft dataset and benchmark for evaluating spatial consistency in memory-aided world models, addressing a gap in current evaluation methods that primarily focus on visual coherence.",
        "tldr_zh": "本文介绍了一个新的Minecraft数据集和基准，用于评估记忆辅助世界模型中的空间一致性，解决了当前主要关注视觉连贯性的评估方法中的一个缺口。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]