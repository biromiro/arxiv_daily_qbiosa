[
    {
        "title": "MMIG-Bench: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models",
        "summary": "Recent multimodal image generators such as GPT-4o, Gemini 2.0 Flash, and\nGemini 2.5 Pro excel at following complex instructions, editing images and\nmaintaining concept consistency. However, they are still evaluated by disjoint\ntoolkits: text-to-image (T2I) benchmarks that lacks multi-modal conditioning,\nand customized image generation benchmarks that overlook compositional\nsemantics and common knowledge. We propose MMIG-Bench, a comprehensive\nMulti-Modal Image Generation Benchmark that unifies these tasks by pairing\n4,850 richly annotated text prompts with 1,750 multi-view reference images\nacross 380 subjects, spanning humans, animals, objects, and artistic styles.\nMMIG-Bench is equipped with a three-level evaluation framework: (1) low-level\nmetrics for visual artifacts and identity preservation of objects; (2) novel\nAspect Matching Score (AMS): a VQA-based mid-level metric that delivers\nfine-grained prompt-image alignment and shows strong correlation with human\njudgments; and (3) high-level metrics for aesthetics and human preference.\nUsing MMIG-Bench, we benchmark 17 state-of-the-art models, including Gemini 2.5\nPro, FLUX, DreamBooth, and IP-Adapter, and validate our metrics with 32k human\nratings, yielding in-depth insights into architecture and data design. We will\nrelease the dataset and evaluation code to foster rigorous, unified evaluation\nand accelerate future innovations in multi-modal image generation.",
        "url": "http://arxiv.org/abs/2505.19415v1",
        "published_date": "2025-05-26T02:07:24+00:00",
        "updated_date": "2025-05-26T02:07:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hang Hua",
            "Ziyun Zeng",
            "Yizhi Song",
            "Yunlong Tang",
            "Liu He",
            "Daniel Aliaga",
            "Wei Xiong",
            "Jiebo Luo"
        ],
        "tldr": "The paper introduces MMIG-Bench, a comprehensive benchmark for evaluating multi-modal image generation models, addressing the limitations of existing toolkits by incorporating diverse tasks and a three-level evaluation framework. They benchmarked 17 SOTA models and will release the dataset and evaluation code.",
        "tldr_zh": "该论文介绍了MMIG-Bench，一个综合性的多模态图像生成模型评估基准，通过整合多样化的任务和一个三级评估框架，解决了现有工具包的局限性。他们对17个最先进的模型进行了基准测试，并将发布数据集和评估代码。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities",
        "summary": "The rapid progress of large language models (LLMs) has catalyzed the\nemergence of multimodal large language models (MLLMs) that unify visual\nunderstanding and image generation within a single framework. However, most\nexisting MLLMs rely on autoregressive (AR) architectures, which impose inherent\nlimitations on future development, such as the raster-scan order in image\ngeneration and restricted reasoning abilities in causal context modeling. In\nthis work, we challenge the dominance of AR-based approaches by introducing\nFUDOKI, a unified multimodal model purely based on discrete flow matching, as\nan alternative to conventional AR paradigms. By leveraging metric-induced\nprobability paths with kinetic optimal velocities, our framework goes beyond\nthe previous masking-based corruption process, enabling iterative refinement\nwith self-correction capability and richer bidirectional context integration\nduring generation. To mitigate the high cost of training from scratch, we\ninitialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to\nthe discrete flow matching paradigm. Experimental results show that FUDOKI\nachieves performance comparable to state-of-the-art AR-based MLLMs across both\nvisual understanding and image generation tasks, highlighting its potential as\na foundation for next-generation unified multimodal models. Furthermore, we\nshow that applying test-time scaling techniques to FUDOKI yields significant\nperformance gains, further underscoring its promise for future enhancement\nthrough reinforcement learning.",
        "url": "http://arxiv.org/abs/2505.20147v1",
        "published_date": "2025-05-26T15:46:53+00:00",
        "updated_date": "2025-05-26T15:46:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jin Wang",
            "Yao Lai",
            "Aoxue Li",
            "Shifeng Zhang",
            "Jiacheng Sun",
            "Ning Kang",
            "Chengyue Wu",
            "Zhenguo Li",
            "Ping Luo"
        ],
        "tldr": "This paper introduces FUDOKI, a novel unified multimodal model based on discrete flow matching, offering an alternative to autoregressive models for visual understanding and image generation, achieving comparable performance with potential for future enhancement.",
        "tldr_zh": "本文介绍了FUDOKI，一种基于离散流匹配的新型统一多模态模型，为视觉理解和图像生成提供了除自回归模型之外的替代方案，实现了可比的性能，并具有未来增强的潜力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dynamic-I2V: Exploring Image-to-Video Generaion Models via Multimodal LLM",
        "summary": "Recent advancements in image-to-video (I2V) generation have shown promising\nperformance in conventional scenarios. However, these methods still encounter\nsignificant challenges when dealing with complex scenes that require a deep\nunderstanding of nuanced motion and intricate object-action relationships. To\naddress these challenges, we present Dynamic-I2V, an innovative framework that\nintegrates Multimodal Large Language Models (MLLMs) to jointly encode visual\nand textual conditions for a diffusion transformer (DiT) architecture. By\nleveraging the advanced multimodal understanding capabilities of MLLMs, our\nmodel significantly improves motion controllability and temporal coherence in\nsynthesized videos. The inherent multimodality of Dynamic-I2V further enables\nflexible support for diverse conditional inputs, extending its applicability to\nvarious downstream generation tasks. Through systematic analysis, we identify a\ncritical limitation in current I2V benchmarks: a significant bias towards\nfavoring low-dynamic videos, stemming from an inadequate balance between motion\ncomplexity and visual quality metrics. To resolve this evaluation gap, we\npropose DIVE - a novel assessment benchmark specifically designed for\ncomprehensive dynamic quality measurement in I2V generation. In conclusion,\nextensive quantitative and qualitative experiments confirm that Dynamic-I2V\nattains state-of-the-art performance in image-to-video generation, particularly\nrevealing significant improvements of 42.5%, 7.9%, and 11.8% in dynamic range,\ncontrollability, and quality, respectively, as assessed by the DIVE metric in\ncomparison to existing methods.",
        "url": "http://arxiv.org/abs/2505.19901v1",
        "published_date": "2025-05-26T12:29:34+00:00",
        "updated_date": "2025-05-26T12:29:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peng Liu",
            "Xiaoming Ren",
            "Fengkai Liu",
            "Qingsong Xie",
            "Quanlong Zheng",
            "Yanhao Zhang",
            "Haonan Lu",
            "Yujiu Yang"
        ],
        "tldr": "The paper introduces Dynamic-I2V, an image-to-video generation framework leveraging MLLMs and a custom benchmark, DIVE, to address limitations in generating complex, dynamic videos, achieving SotA results.",
        "tldr_zh": "该论文介绍了Dynamic-I2V，一个利用多模态LLM和自定义基准DIVE的图像到视频生成框架，旨在解决生成复杂动态视频的局限性，并取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ReDDiT: Rehashing Noise for Discrete Visual Generation",
        "summary": "Discrete diffusion models are gaining traction in the visual generative area\nfor their efficiency and compatibility. However, the pioneered attempts still\nfall behind the continuous counterparts, which we attribute to the noise\n(absorbing state) design and sampling heuristics. In this study, we propose the\nrehashing noise framework for discrete diffusion transformer, termed ReDDiT, to\nextend absorbing states and improve expressive capacity of discrete diffusion\nmodels. ReDDiT enriches the potential paths that latent variables can traverse\nduring training with randomized multi-index corruption. The derived rehash\nsampler, which reverses the randomized absorbing paths, guarantees the\ndiversity and low discrepancy of the generation process. These reformulations\nlead to more consistent and competitive generation quality, mitigating the need\nfor heavily tuned randomness. Experiments show that ReDDiT significantly\noutperforms the baseline (reducing gFID from 6.18 to 1.61) and is on par with\nthe continuous counterparts with higher efficiency.",
        "url": "http://arxiv.org/abs/2505.19656v1",
        "published_date": "2025-05-26T08:17:20+00:00",
        "updated_date": "2025-05-26T08:17:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianren Ma",
            "Xiaosong Zhang",
            "Boyu Yang",
            "Junlan Feng",
            "Qixiang Ye"
        ],
        "tldr": "The paper introduces ReDDiT, a new discrete diffusion transformer model that uses a rehashing noise framework to improve the efficiency and generation quality of discrete visual generation, achieving performance on par with continuous models.",
        "tldr_zh": "该论文介绍了ReDDiT，一种新的离散扩散Transformer模型，它使用重新哈希噪声框架来提高离散视觉生成的效率和生成质量，从而达到与连续模型相当的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Text-to-Image Diffusion Transformer via Split-Text Conditioning",
        "summary": "Current text-to-image diffusion generation typically employs complete-text\nconditioning. Due to the intricate syntax, diffusion transformers (DiTs)\ninherently suffer from a comprehension defect of complete-text captions.\nOne-fly complete-text input either overlooks critical semantic details or\ncauses semantic confusion by simultaneously modeling diverse semantic primitive\ntypes. To mitigate this defect of DiTs, we propose a novel split-text\nconditioning framework named DiT-ST. This framework converts a complete-text\ncaption into a split-text caption, a collection of simplified sentences, to\nexplicitly express various semantic primitives and their interconnections. The\nsplit-text caption is then injected into different denoising stages of DiT-ST\nin a hierarchical and incremental manner. Specifically, DiT-ST leverages Large\nLanguage Models to parse captions, extracting diverse primitives and\nhierarchically sorting out and constructing these primitives into a split-text\ninput. Moreover, we partition the diffusion denoising process according to its\ndifferential sensitivities to diverse semantic primitive types and determine\nthe appropriate timesteps to incrementally inject tokens of diverse semantic\nprimitive types into input tokens via cross-attention. In this way, DiT-ST\nenhances the representation learning of specific semantic primitive types\nacross different stages. Extensive experiments validate the effectiveness of\nour proposed DiT-ST in mitigating the complete-text comprehension defect.",
        "url": "http://arxiv.org/abs/2505.19261v1",
        "published_date": "2025-05-25T18:33:05+00:00",
        "updated_date": "2025-05-25T18:33:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yu Zhang",
            "Jialei Zhou",
            "Xinchen Li",
            "Qi Zhang",
            "Zhongwei Wan",
            "Tianyu Wang",
            "Duoqian Miao",
            "Changwei Wang",
            "Longbing Cao"
        ],
        "tldr": "The paper introduces DiT-ST, a novel split-text conditioning framework for Diffusion Transformers (DiTs) that converts complete-text captions into simplified sentences to improve semantic comprehension and ultimately enhance text-to-image generation.",
        "tldr_zh": "该论文介绍了一种名为DiT-ST的新型分割文本调节框架，用于扩散Transformer (DiTs)，该框架将完整的文本字幕转换为简化的句子，以提高语义理解，并最终增强文本到图像的生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]