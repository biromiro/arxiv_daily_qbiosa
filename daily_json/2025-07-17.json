[
    {
        "title": "Continued domain-specific pre-training of protein language models for pMHC-I binding prediction",
        "summary": "Predicting peptide--major histocompatibility complex I (pMHC-I) binding\naffinity remains challenging due to extreme allelic diversity ($\\sim$30,000 HLA\nalleles), severe data scarcity for most alleles, and noisy experimental\nmeasurements. Current methods particularly struggle with underrepresented\nalleles and quantitative binding prediction. We test whether domain-specific\ncontinued pre-training of protein language models is beneficial for their\napplication to pMHC-I binding affinity prediction. Starting from ESM Cambrian\n(300M parameters), we perform masked-language modeling (MLM)-based continued\npre-training on HLA-associated peptides (epitopes), testing two input formats:\nepitope sequences alone versus epitopes concatenated with HLA heavy chain\nsequences. We then fine-tune for functional IC$_{50}$ binding affinity\nprediction using only high-quality quantitative data, avoiding mass\nspectrometry biases that are inherited by existing methods.",
        "url": "http://arxiv.org/abs/2507.13077v1",
        "published_date": "2025-07-16T00:29:22+00:00",
        "updated_date": "2025-07-16T00:29:22+00:00",
        "categories": [
            "q-bio.QM"
        ],
        "authors": [
            "Sergio E. Mares",
            "Ariel Espinoza Weinberger",
            "Nilah M. Ioannidis"
        ],
        "pdf_url": "http://arxiv.org/pdf/2507.13077v1",
        "tldr": "This paper explores continued domain-specific pre-training of protein language models (PLMs) for improved pMHC-I binding affinity prediction, achieving state-of-the-art performance, especially for alleles with moderate data availability.",
        "explanation": "The paper addresses the challenging problem of predicting pMHC-I binding affinity, which is crucial for vaccine design and personalized immunotherapy. The authors propose a two-stage training protocol using the ESM Cambrian model: continued masked-language modeling (MLM) pre-training on HLA-associated peptides, followed by supervised fine-tuning for IC50 binding affinity prediction. They demonstrate that this approach, termed ESMCBA, outperforms existing methods, particularly for alleles with moderate amounts of available binding data. The study also acknowledges limitations regarding data-scarce alleles and computational resources.",
        "interests_alignment": "While the paper focuses on protein-peptide binding rather than peptide self-assembly or aggregation directly, the techniques used, namely protein language models and machine learning, are highly relevant to predicting peptide structure and function. The general idea of using pre-trained models and domain-specific fine-tuning could be applicable to peptide self-assembly problems.",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    }
]