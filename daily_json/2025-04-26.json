[
    {
        "title": "NoiseController: Towards Consistent Multi-view Video Generation via Noise Decomposition and Collaboration",
        "summary": "High-quality video generation is crucial for many fields, including the film\nindustry and autonomous driving. However, generating videos with spatiotemporal\nconsistencies remains challenging. Current methods typically utilize attention\nmechanisms or modify noise to achieve consistent videos, neglecting global\nspatiotemporal information that could help ensure spatial and temporal\nconsistency during video generation. In this paper, we propose the\nNoiseController, consisting of Multi-Level Noise Decomposition, Multi-Frame\nNoise Collaboration, and Joint Denoising, to enhance spatiotemporal\nconsistencies in video generation. In multi-level noise decomposition, we first\ndecompose initial noises into scene-level foreground/background noises,\ncapturing distinct motion properties to model multi-view foreground/background\nvariations. Furthermore, each scene-level noise is further decomposed into\nindividual-level shared and residual components. The shared noise preserves\nconsistency, while the residual component maintains diversity. In multi-frame\nnoise collaboration, we introduce an inter-view spatiotemporal collaboration\nmatrix and an intra-view impact collaboration matrix , which captures mutual\ncross-view effects and historical cross-frame impacts to enhance video quality.\nThe joint denoising contains two parallel denoising U-Nets to remove each\nscene-level noise, mutually enhancing video generation. We evaluate our\nNoiseController on public datasets focusing on video generation and downstream\ntasks, demonstrating its state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2504.18448v1",
        "published_date": "2025-04-25T16:01:48+00:00",
        "updated_date": "2025-04-25T16:01:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haotian Dong",
            "Xin Wang",
            "Di Lin",
            "Yipeng Wu",
            "Qin Chen",
            "Ruonan Liu",
            "Kairui Yang",
            "Ping Li",
            "Qing Guo"
        ],
        "tldr": "the paper introduces noisecontroller, a novel approach for consistent multi-view video generation by decomposing and collaborating noises at multiple levels, achieving state-of-the-art performance.",
        "tldr_zh": "该论文介绍了noisecontroller，一种新颖的多视角视频生成方法，通过在多个层级上分解和协作噪声，实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "STP4D: Spatio-Temporal-Prompt Consistent Modeling for Text-to-4D Gaussian Splatting",
        "summary": "Text-to-4D generation is rapidly developing and widely applied in various\nscenarios. However, existing methods often fail to incorporate adequate\nspatio-temporal modeling and prompt alignment within a unified framework,\nresulting in temporal inconsistencies, geometric distortions, or low-quality 4D\ncontent that deviates from the provided texts. Therefore, we propose STP4D, a\nnovel approach that aims to integrate comprehensive spatio-temporal-prompt\nconsistency modeling for high-quality text-to-4D generation. Specifically,\nSTP4D employs three carefully designed modules: Time-varying Prompt Embedding,\nGeometric Information Enhancement, and Temporal Extension Deformation, which\ncollaborate to accomplish this goal. Furthermore, STP4D is among the first\nmethods to exploit the Diffusion model to generate 4D Gaussians, combining the\nfine-grained modeling capabilities and the real-time rendering process of 4DGS\nwith the rapid inference speed of the Diffusion model. Extensive experiments\ndemonstrate that STP4D excels in generating high-fidelity 4D content with\nexceptional efficiency (approximately 4.6s per asset), surpassing existing\nmethods in both quality and speed.",
        "url": "http://arxiv.org/abs/2504.18318v1",
        "published_date": "2025-04-25T12:53:15+00:00",
        "updated_date": "2025-04-25T12:53:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunze Deng",
            "Haijun Xiong",
            "Bin Feng",
            "Xinggang Wang",
            "Wenyu Liu"
        ],
        "tldr": "the paper introduces stp4d, a novel method for text-to-4d gaussian splatting that leverages a diffusion model and spatio-temporal-prompt consistency modeling to generate high-fidelity 4d content with exceptional speed.",
        "tldr_zh": "该论文介绍了stp4d，一种新颖的文本到4d高斯溅射方法，利用扩散模型和时空提示一致性建模，以极高的速度生成高保真度的4d内容。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Seeing Soundscapes: Audio-Visual Generation and Separation from Soundscapes Using Audio-Visual Separator",
        "summary": "Recent audio-visual generative models have made substantial progress in\ngenerating images from audio. However, existing approaches focus on generating\nimages from single-class audio and fail to generate images from mixed audio. To\naddress this, we propose an Audio-Visual Generation and Separation model\n(AV-GAS) for generating images from soundscapes (mixed audio containing\nmultiple classes). Our contribution is threefold: First, we propose a new\nchallenge in the audio-visual generation task, which is to generate an image\ngiven a multi-class audio input, and we propose a method that solves this task\nusing an audio-visual separator. Second, we introduce a new audio-visual\nseparation task, which involves generating separate images for each class\npresent in a mixed audio input. Lastly, we propose new evaluation metrics for\nthe audio-visual generation task: Class Representation Score (CRS) and a\nmodified R@K. Our model is trained and evaluated on the VGGSound dataset. We\nshow that our method outperforms the state-of-the-art, achieving 7% higher CRS\nand 4% higher R@2* in generating plausible images with mixed audio.",
        "url": "http://arxiv.org/abs/2504.18283v1",
        "published_date": "2025-04-25T11:51:04+00:00",
        "updated_date": "2025-04-25T11:51:04+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM",
            "cs.SD",
            "eess.AS"
        ],
        "authors": [
            "Minjae Kang",
            "Martim Brandão"
        ],
        "tldr": "the paper introduces av-gas, a model for generating images from mixed audio soundscapes and separating the visual representations of individual sound classes. it also proposes new evaluation metrics tailored for this task.",
        "tldr_zh": "本文介绍了一种名为av-gas的模型，该模型可以从混合音频声景中生成图像，并分离各个声音类别的视觉表示。此外，文章还提出了适用于此任务的新评估指标。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation",
        "summary": "Generating images from prompts containing specific entities requires models\nto retain as much entity-specific knowledge as possible. However, fully\nmemorizing such knowledge is impractical due to the vast number of entities and\ntheir continuous emergence. To address this, we propose Text-based Intelligent\nGeneration with Entity prompt Refinement (TextTIGER), which augments knowledge\non entities included in the prompts and then summarizes the augmented\ndescriptions using Large Language Models (LLMs) to mitigate performance\ndegradation from longer inputs. To evaluate our method, we introduce WiT-Cub\n(WiT with Captions and Uncomplicated Background-explanations), a dataset\ncomprising captions, images, and an entity list. Experiments on four image\ngeneration models and five LLMs show that TextTIGER improves image generation\nperformance in standard metrics (IS, FID, and CLIPScore) compared to\ncaption-only prompts. Additionally, multiple annotators' evaluation confirms\nthat the summarized descriptions are more informative, validating LLMs' ability\nto generate concise yet rich descriptions. These findings demonstrate that\nrefining prompts with augmented and summarized entity-related descriptions\nenhances image generation capabilities. The code and dataset will be available\nupon acceptance.",
        "url": "http://arxiv.org/abs/2504.18269v1",
        "published_date": "2025-04-25T11:27:44+00:00",
        "updated_date": "2025-04-25T11:27:44+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Shintaro Ozaki",
            "Kazuki Hayashi",
            "Yusuke Sakai",
            "Jingun Kwon",
            "Hidetaka Kamigaito",
            "Katsuhiko Hayashi",
            "Manabu Okumura",
            "Taro Watanabe"
        ],
        "tldr": "the paper introduces texttiger, a method that enhances text-to-image generation by augmenting prompts with entity-specific knowledge and summarizing them using llms, achieving improved image quality and informativeness. they also contribute a new dataset wit-cub.",
        "tldr_zh": "该论文介绍了一种名为texttiger的方法，它通过增强提示词中的实体特定知识，并使用大型语言模型进行总结，从而提高文本到图像生成的效果，并改善图像质量和信息量。 他们还贡献了一个新的数据集wit-cub。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Optimizing Multi-Round Enhanced Training in Diffusion Models for Improved Preference Understanding",
        "summary": "Generative AI has significantly changed industries by enabling text-driven\nimage generation, yet challenges remain in achieving high-resolution outputs\nthat align with fine-grained user preferences. Consequently, multi-round\ninteractions are necessary to ensure the generated images meet expectations.\nPrevious methods enhanced prompts via reward feedback but did not optimize over\na multi-round dialogue dataset. In this work, we present a Visual Co-Adaptation\n(VCA) framework incorporating human-in-the-loop feedback, leveraging a\nwell-trained reward model aligned with human preferences. Using a diverse\nmulti-turn dialogue dataset, our framework applies multiple reward functions,\nsuch as diversity, consistency, and preference feedback, while fine-tuning the\ndiffusion model through LoRA, thus optimizing image generation based on user\ninput. We also construct multi-round dialogue datasets of prompts and image\npairs aligned with user intent. Experiments demonstrate that our method\noutperforms state-of-the-art baselines, significantly improving image\nconsistency and alignment with user intent. Our approach consistently surpasses\ncompeting models in user satisfaction, especially in multi-turn dialogue\nscenarios.",
        "url": "http://arxiv.org/abs/2504.18204v1",
        "published_date": "2025-04-25T09:35:02+00:00",
        "updated_date": "2025-04-25T09:35:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kun Li",
            "Jianhui Wang",
            "Yangfan He",
            "Xinyuan Song",
            "Ruoyu Wang",
            "Hongyang He",
            "Wenxin Zhang",
            "Jiaqi Chen",
            "Keqin Li",
            "Sida Li",
            "Miao Zhang",
            "Tianyu Shi",
            "Xueqian Wang"
        ],
        "tldr": "this paper introduces a visual co-adaptation (vca) framework that optimizes diffusion models for image generation through multi-round human feedback, using a well-trained reward model and lora fine-tuning. the results demonstrate improved image consistency and user satisfaction in multi-turn dialogue scenarios.",
        "tldr_zh": "本文介绍了一种视觉协同适应（vca）框架，通过多轮人工反馈优化扩散模型以生成图像，该框架使用经过良好训练的奖励模型和lora微调。结果表明，在多轮对话场景中，图像一致性和用户满意度得到了提高。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation",
        "summary": "Recent advances in Talking Head Generation (THG) have achieved impressive lip\nsynchronization and visual quality through diffusion models; yet existing\nmethods struggle to generate emotionally expressive portraits while preserving\nspeaker identity. We identify three critical limitations in current emotional\ntalking head generation: insufficient utilization of audio's inherent emotional\ncues, identity leakage in emotion representations, and isolated learning of\nemotion correlations. To address these challenges, we propose a novel framework\ndubbed as DICE-Talk, following the idea of disentangling identity with emotion,\nand then cooperating emotions with similar characteristics. First, we develop a\ndisentangled emotion embedder that jointly models audio-visual emotional cues\nthrough cross-modal attention, representing emotions as identity-agnostic\nGaussian distributions. Second, we introduce a correlation-enhanced emotion\nconditioning module with learnable Emotion Banks that explicitly capture\ninter-emotion relationships through vector quantization and attention-based\nfeature aggregation. Third, we design an emotion discrimination objective that\nenforces affective consistency during the diffusion process through\nlatent-space classification. Extensive experiments on MEAD and HDTF datasets\ndemonstrate our method's superiority, outperforming state-of-the-art approaches\nin emotion accuracy while maintaining competitive lip-sync performance.\nQualitative results and user studies further confirm our method's ability to\ngenerate identity-preserving portraits with rich, correlated emotional\nexpressions that naturally adapt to unseen identities.",
        "url": "http://arxiv.org/abs/2504.18087v1",
        "published_date": "2025-04-25T05:28:21+00:00",
        "updated_date": "2025-04-25T05:28:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weipeng Tan",
            "Chuming Lin",
            "Chengming Xu",
            "FeiFan Xu",
            "Xiaobin Hu",
            "Xiaozhong Ji",
            "Junwei Zhu",
            "Chengjie Wang",
            "Yanwei Fu"
        ],
        "tldr": "the paper introduces dice-talk, a novel framework for emotionally expressive talking head generation that disentangles identity and emotion, leveraging cross-modal attention and emotion correlation modeling to achieve improved emotion accuracy and identity preservation.",
        "tldr_zh": "本文介绍了一种名为 dice-talk 的新型情感表达式说话人头部生成框架，该框架分离了身份和情感，利用跨模态注意力和情感相关性建模，以提高情感准确性和身份保持。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models",
        "summary": "Text-to-image diffusion models have demonstrated remarkable capabilities in\ncreating images highly aligned with user prompts, yet their proclivity for\nmemorizing training set images has sparked concerns about the originality of\nthe generated images and privacy issues, potentially leading to legal\ncomplications for both model owners and users, particularly when the memorized\nimages contain proprietary content. Although methods to mitigate these issues\nhave been suggested, enhancing privacy often results in a significant decrease\nin the utility of the outputs, as indicated by text-alignment scores. To bridge\nthe research gap, we introduce a novel method, PRSS, which refines the\nclassifier-free guidance approach in diffusion models by integrating prompt\nre-anchoring (PR) to improve privacy and incorporating semantic prompt search\n(SS) to enhance utility. Extensive experiments across various privacy levels\ndemonstrate that our approach consistently improves the privacy-utility\ntrade-off, establishing a new state-of-the-art.",
        "url": "http://arxiv.org/abs/2504.18032v1",
        "published_date": "2025-04-25T02:51:23+00:00",
        "updated_date": "2025-04-25T02:51:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chen Chen",
            "Daochang Liu",
            "Mubarak Shah",
            "Chang Xu"
        ],
        "tldr": "this paper introduces a novel method, prss, to improve the privacy-utility trade-off in text-to-image diffusion models, mitigating memorization issues while maintaining output quality through prompt re-anchoring and semantic prompt search.",
        "tldr_zh": "该论文提出了一种名为prss的新方法，通过提示重定位和语义提示搜索来改善文本到图像扩散模型中的隐私-效用权衡，减轻记忆化问题，同时保持输出质量。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Token Sequence Compression for Efficient Multimodal Computing",
        "summary": "The exponential growth of Large Multimodal Models (LMMs) has driven\nadvancements in cross-modal reasoning but at significant computational costs.\nIn this work, we focus on visual language models. We highlight the redundancy\nand inefficiency in current vision encoders, and seek to construct an adaptive\ncompression method for multimodal data. In this work, we characterize a panoply\nof visual token selection and merging approaches through both benchmarking and\nqualitative analysis. In particular, we demonstrate that simple cluster-level\ntoken aggregation outperforms prior state-of-the-art works in token selection\nand merging, including merging at the vision encoder level and attention-based\napproaches. We underline the redundancy in current vision encoders, and shed\nlight on several puzzling trends regarding principles of visual token selection\nthrough cross-modal attention visualizations. This work is a first effort\ntowards more effective encoding and processing of high-dimensional data, and\npaves the way for more scalable and sustainable multimodal systems.",
        "url": "http://arxiv.org/abs/2504.17892v1",
        "published_date": "2025-04-24T19:11:10+00:00",
        "updated_date": "2025-04-24T19:11:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Yasmine Omri",
            "Parth Shroff",
            "Thierry Tambe"
        ],
        "tldr": "this paper introduces a token sequence compression method for vision encoders in large multimodal models (lmms) to reduce computational costs, demonstrating that simple cluster-level token aggregation outperforms existing methods.",
        "tldr_zh": "本文提出了一种针对大型多模态模型（lmm）中视觉编码器的令牌序列压缩方法，以降低计算成本，并证明简单的集群级别令牌聚合优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Augmenting Perceptual Super-Resolution via Image Quality Predictors",
        "summary": "Super-resolution (SR), a classical inverse problem in computer vision, is\ninherently ill-posed, inducing a distribution of plausible solutions for every\ninput. However, the desired result is not simply the expectation of this\ndistribution, which is the blurry image obtained by minimizing pixelwise error,\nbut rather the sample with the highest image quality. A variety of techniques,\nfrom perceptual metrics to adversarial losses, are employed to this end. In\nthis work, we explore an alternative: utilizing powerful non-reference image\nquality assessment (NR-IQA) models in the SR context. We begin with a\ncomprehensive analysis of NR-IQA metrics on human-derived SR data, identifying\nboth the accuracy (human alignment) and complementarity of different metrics.\nThen, we explore two methods of applying NR-IQA models to SR learning: (i)\naltering data sampling, by building on an existing multi-ground-truth SR\nframework, and (ii) directly optimizing a differentiable quality score. Our\nresults demonstrate a more human-centric perception-distortion tradeoff,\nfocusing less on non-perceptual pixel-wise distortion, instead improving the\nbalance between perceptual fidelity and human-tuned NR-IQA measures.",
        "url": "http://arxiv.org/abs/2504.18524v1",
        "published_date": "2025-04-25T17:47:38+00:00",
        "updated_date": "2025-04-25T17:47:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fengjia Zhang",
            "Samrudhdhi B. Rangrej",
            "Tristan Aumentado-Armstrong",
            "Afsaneh Fazly",
            "Alex Levinshtein"
        ],
        "tldr": "this paper explores how to leverage non-reference image quality assessment (nr-iqa) models to improve perceptual quality in super-resolution (sr) image generation, achieving a better balance between perceptual fidelity and nr-iqa measures.",
        "tldr_zh": "该论文探讨了如何利用无参考图像质量评估（nr-iqa）模型来提高超分辨率（sr）图像生成中的感知质量，从而在感知保真度和nr-iqa测量之间取得更好的平衡。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "HepatoGEN: Generating Hepatobiliary Phase MRI with Perceptual and Adversarial Models",
        "summary": "Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays a\ncrucial role in the detection and characterization of focal liver lesions, with\nthe hepatobiliary phase (HBP) providing essential diagnostic information.\nHowever, acquiring HBP images requires prolonged scan times, which may\ncompromise patient comfort and scanner throughput. In this study, we propose a\ndeep learning based approach for synthesizing HBP images from earlier contrast\nphases (precontrast and transitional) and compare three generative models: a\nperceptual U-Net, a perceptual GAN (pGAN), and a denoising diffusion\nprobabilistic model (DDPM). We curated a multi-site DCE-MRI dataset from\ndiverse clinical settings and introduced a contrast evolution score (CES) to\nassess training data quality, enhancing model performance. Quantitative\nevaluation using pixel-wise and perceptual metrics, combined with qualitative\nassessment through blinded radiologist reviews, showed that pGAN achieved the\nbest quantitative performance but introduced heterogeneous contrast in\nout-of-distribution cases. In contrast, the U-Net produced consistent liver\nenhancement with fewer artifacts, while DDPM underperformed due to limited\npreservation of fine structural details. These findings demonstrate the\nfeasibility of synthetic HBP image generation as a means to reduce scan time\nwithout compromising diagnostic utility, highlighting the clinical potential of\ndeep learning for dynamic contrast enhancement in liver MRI. A project demo is\navailable at: https://jhooge.github.io/hepatogen",
        "url": "http://arxiv.org/abs/2504.18405v1",
        "published_date": "2025-04-25T15:01:09+00:00",
        "updated_date": "2025-04-25T15:01:09+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Jens Hooge",
            "Gerard Sanroma-Guell",
            "Faidra Stavropoulou",
            "Alexander Ullmann",
            "Gesine Knobloch",
            "Mark Klemens",
            "Carola Schmidt",
            "Sabine Weckbach",
            "Andreas Bolz"
        ],
        "tldr": "the paper explores deep learning methods (u-net, pgan, ddpm) for generating hepatobiliary phase mri images from earlier contrast phases to reduce scan time, finding that pgan performs best quantitatively while u-net provides more consistent results.",
        "tldr_zh": "该论文探索了深度学习方法（u-net、pgan、ddpm），用于从早期对比阶段生成肝胆期 mri 图像以减少扫描时间，发现 pgan 在定量方面表现最佳，而 u-net 提供更一致的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Fast-Slow Thinking for Large Vision-Language Model Reasoning",
        "summary": "Recent advances in large vision-language models (LVLMs) have revealed an\n\\textit{overthinking} phenomenon, where models generate verbose reasoning\nacross all tasks regardless of questions. To address this issue, we present\n\\textbf{FAST}, a novel \\textbf{Fa}st-\\textbf{S}low \\textbf{T}hinking framework\nthat dynamically adapts reasoning depth based on question characteristics.\nThrough empirical analysis, we establish the feasibility of fast-slow thinking\nin LVLMs by investigating how response length and data distribution affect\nperformance. We develop FAST-GRPO with three components: model-based metrics\nfor question characterization, an adaptive thinking reward mechanism, and\ndifficulty-aware KL regularization. Experiments across seven reasoning\nbenchmarks demonstrate that FAST achieves state-of-the-art accuracy with over\n10\\% relative improvement compared to the base model, while reducing token\nusage by 32.7-67.3\\% compared to previous slow-thinking approaches, effectively\nbalancing reasoning length and accuracy.",
        "url": "http://arxiv.org/abs/2504.18458v1",
        "published_date": "2025-04-25T16:11:23+00:00",
        "updated_date": "2025-04-25T16:11:23+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Wenyi Xiao",
            "Leilei Gan",
            "Weilong Dai",
            "Wanggui He",
            "Ziwei Huang",
            "Haoyuan Li",
            "Fangxun Shu",
            "Zhelun Yu",
            "Peng Zhang",
            "Hao Jiang",
            "Fei Wu"
        ],
        "tldr": "this paper introduces fast, a framework for dynamically adjusting the reasoning depth of large vision-language models (lvlms) based on the question characteristics, improving accuracy and reducing token usage.",
        "tldr_zh": "该论文介绍了 fast，一个根据问题特征动态调整大型视觉-语言模型（lvlm）推理深度的框架，从而提高准确性并减少 token 使用量。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "A Multimodal Hybrid Late-Cascade Fusion Network for Enhanced 3D Object Detection",
        "summary": "We present a new way to detect 3D objects from multimodal inputs, leveraging\nboth LiDAR and RGB cameras in a hybrid late-cascade scheme, that combines an\nRGB detection network and a 3D LiDAR detector. We exploit late fusion\nprinciples to reduce LiDAR False Positives, matching LiDAR detections with RGB\nones by projecting the LiDAR bounding boxes on the image. We rely on cascade\nfusion principles to recover LiDAR False Negatives leveraging epipolar\nconstraints and frustums generated by RGB detections of separate views. Our\nsolution can be plugged on top of any underlying single-modal detectors,\nenabling a flexible training process that can take advantage of pre-trained\nLiDAR and RGB detectors, or train the two branches separately. We evaluate our\nresults on the KITTI object detection benchmark, showing significant\nperformance improvements, especially for the detection of Pedestrians and\nCyclists.",
        "url": "http://arxiv.org/abs/2504.18419v1",
        "published_date": "2025-04-25T15:28:53+00:00",
        "updated_date": "2025-04-25T15:28:53+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Carlo Sgaravatti",
            "Roberto Basla",
            "Riccardo Pieroni",
            "Matteo Corno",
            "Sergio M. Savaresi",
            "Luca Magri",
            "Giacomo Boracchi"
        ],
        "tldr": "this paper introduces a multimodal 3d object detection network that fuses lidar and rgb data using a hybrid late-cascade scheme, improving performance on the kitti benchmark, particularly for pedestrians and cyclists.",
        "tldr_zh": "这篇论文介绍了一种多模态3d物体检测网络，该网络使用混合延迟级联方案融合激光雷达和rgb数据，从而提高了kitti基准测试的性能，尤其是在行人和骑自行车者方面。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    },
    {
        "title": "TSCL:Multi-party loss Balancing scheme for deep learning Image steganography based on Curriculum learning",
        "summary": "For deep learning-based image steganography frameworks, in order to ensure\nthe invisibility and recoverability of the information embedding, the loss\nfunction usually contains several losses such as embedding loss, recovery loss\nand steganalysis loss. In previous research works, fixed loss weights are\nusually chosen for training optimization, and this setting is not linked to the\nimportance of the steganography task itself and the training process. In this\npaper, we propose a Two-stage Curriculum Learning loss scheduler (TSCL) for\nbalancing multinomial losses in deep learning image steganography algorithms.\nTSCL consists of two phases: a priori curriculum control and loss dynamics\ncontrol. The first phase firstly focuses the model on learning the information\nembedding of the original image by controlling the loss weights in the\nmulti-party adversarial training; secondly, it makes the model shift its\nlearning focus to improving the decoding accuracy; and finally, it makes the\nmodel learn to generate a steganographic image that is resistant to\nsteganalysis. In the second stage, the learning speed of each training task is\nevaluated by calculating the loss drop of the before and after iteration rounds\nto balance the learning of each task. Experimental results on three large\npublic datasets, ALASKA2, VOC2012 and ImageNet, show that the proposed TSCL\nstrategy improves the quality of steganography, decoding accuracy and security.",
        "url": "http://arxiv.org/abs/2504.18348v1",
        "published_date": "2025-04-25T13:36:50+00:00",
        "updated_date": "2025-04-25T13:36:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CR"
        ],
        "authors": [
            "Fengchun Liu. Tong Zhang",
            "Chunying Zhang"
        ],
        "tldr": "this paper introduces a two-stage curriculum learning loss scheduler (tscl) to balance different loss functions in deep learning-based image steganography, achieving better invisibility, recoverability and security.",
        "tldr_zh": "本文提出了一种两阶段课程学习损失调度器(tscl)，用于平衡基于深度学习的图像隐写术中不同的损失函数，从而实现更好的不可见性、可恢复性和安全性。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    },
    {
        "title": "Salient Region-Guided Spacecraft Image Arbitrary-Scale Super-Resolution Network",
        "summary": "Spacecraft image super-resolution seeks to enhance low-resolution spacecraft\nimages into high-resolution ones. Although existing arbitrary-scale\nsuper-resolution methods perform well on general images, they tend to overlook\nthe difference in features between the spacecraft core region and the large\nblack space background, introducing irrelevant noise. In this paper, we propose\na salient region-guided spacecraft image arbitrary-scale super-resolution\nnetwork (SGSASR), which uses features from the spacecraft core salient regions\nto guide latent modulation and achieve arbitrary-scale super-resolution.\nSpecifically, we design a spacecraft core region recognition block (SCRRB) that\nidentifies the core salient regions in spacecraft images using a pre-trained\nsaliency detection model. Furthermore, we present an adaptive-weighted feature\nfusion enhancement mechanism (AFFEM) to selectively aggregate the spacecraft\ncore region features with general image features by dynamic weight parameter to\nenhance the response of the core salient regions. Experimental results\ndemonstrate that the proposed SGSASR outperforms state-of-the-art approaches.",
        "url": "http://arxiv.org/abs/2504.18127v1",
        "published_date": "2025-04-25T07:23:13+00:00",
        "updated_date": "2025-04-25T07:23:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingfan Yang",
            "Hu Gao",
            "Ying Zhang",
            "Depeng Dang"
        ],
        "tldr": "this paper proposes a novel super-resolution network (sgsasr) specifically designed for spacecraft images, utilizing a saliency detection model to focus on the spacecraft core region and enhance feature fusion. it claims to outperform state-of-the-art methods.",
        "tldr_zh": "本文提出了一种新的超分辨率网络 (sgsasr)，专门为航天器图像设计，利用显著性检测模型关注航天器核心区域并增强特征融合。它声称优于最先进的方法。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    },
    {
        "title": "Masked strategies for images with small objects",
        "summary": "The hematology analytics used for detection and classification of small blood\ncomponents is a significant challenge. In particular, when objects exists as\nsmall pixel-sized entities in a large context of similar objects. Deep learning\napproaches using supervised models with pre-trained weights, such as residual\nnetworks and vision transformers have demonstrated success for many\napplications. Unfortunately, when applied to images outside the domain of\nlearned representations, these methods often result with less than acceptable\nperformance. A strategy to overcome this can be achieved by using\nself-supervised models, where representations are learned and weights are then\napplied for downstream applications. Recently, masked autoencoders have proven\nto be effective to obtain representations that captures global context\ninformation. By masking regions of an image and having the model learn to\nreconstruct both the masked and non-masked regions, weights can be used for\nvarious applications. However, if the sizes of the objects in images are less\nthan the size of the mask, the global context information is lost, making it\nalmost impossible to reconstruct the image. In this study, we investigated the\neffect of mask ratios and patch sizes for blood components using a MAE to\nobtain learned ViT encoder representations. We then applied the encoder weights\nto train a U-Net Transformer for semantic segmentation to obtain both local and\nglobal contextual information. Our experimental results demonstrates that both\nsmaller mask ratios and patch sizes improve the reconstruction of images using\na MAE. We also show the results of semantic segmentation with and without\npre-trained weights, where smaller-sized blood components benefited with\npre-training. Overall, our proposed method offers an efficient and effective\nstrategy for the segmentation and classification of small objects.",
        "url": "http://arxiv.org/abs/2504.17935v1",
        "published_date": "2025-04-24T20:52:23+00:00",
        "updated_date": "2025-04-24T20:52:23+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "H. Martin Gillis",
            "Ming Hill",
            "Paul Hollensen",
            "Alan Fine",
            "Thomas Trappenberg"
        ],
        "tldr": "this paper investigates the effect of mask ratios and patch sizes for small object (blood component) detection using masked autoencoders and a u-net transformer, finding that smaller mask ratios and patch sizes improve reconstruction and segmentation, especially when pre-training.",
        "tldr_zh": "该论文研究了使用掩码自动编码器和 u-net transformer 检测小物体(血细胞)时，掩码比例和图像块大小的影响，发现较小的掩码比例和图像块大小可以提高重建和分割效果，尤其是在预训练的情况下。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4
    }
]