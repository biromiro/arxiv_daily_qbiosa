[
    {
        "title": "Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances",
        "summary": "Generative models have made significant progress in synthesizing visual\ncontent, including images, videos, and 3D/4D structures. However, they are\ntypically trained with surrogate objectives such as likelihood or\nreconstruction loss, which often misalign with perceptual quality, semantic\naccuracy, or physical realism. Reinforcement learning (RL) offers a principled\nframework for optimizing non-differentiable, preference-driven, and temporally\nstructured objectives. Recent advances demonstrate its effectiveness in\nenhancing controllability, consistency, and human alignment across generative\ntasks. This survey provides a systematic overview of RL-based methods for\nvisual content generation. We review the evolution of RL from classical control\nto its role as a general-purpose optimization tool, and examine its integration\ninto image, video, and 3D/4D generation. Across these domains, RL serves not\nonly as a fine-tuning mechanism but also as a structural component for aligning\ngeneration with complex, high-level goals. We conclude with open challenges and\nfuture research directions at the intersection of RL and generative modeling.",
        "url": "http://arxiv.org/abs/2508.10316v1",
        "published_date": "2025-08-14T03:44:03+00:00",
        "updated_date": "2025-08-14T03:44:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanzhi Liang",
            "Yijie Fang",
            "Rui Li",
            "Ziqi Ni",
            "Ruijie Su",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "tldr": "This survey paper reviews the integration of Reinforcement Learning (RL) techniques into visual generative models (images, videos, 3D/4D), highlighting RL's role in aligning these models with complex, high-level goals and perceptual qualities.",
        "tldr_zh": "该综述论文回顾了强化学习（RL）技术与视觉生成模型（图像、视频、3D/4D）的结合，强调了RL在使这些模型与复杂、高级目标和感知质量对齐方面的作用。",
        "relevance_score": 10,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Video-BLADE: Block-Sparse Attention Meets Step Distillation for Efficient Video Generation",
        "summary": "Diffusion transformers currently lead the field in high-quality video\ngeneration, but their slow iterative denoising process and prohibitive\nquadratic attention costs for long sequences create significant inference\nbottlenecks. While both step distillation and sparse attention mechanisms have\nshown promise as independent acceleration strategies, effectively combining\nthese approaches presents critical challenges -- training-free integration\nyields suboptimal results, while separately training sparse attention after\nstep distillation requires prohibitively expensive high-quality video data. To\novercome these limitations, we propose BLADE, an innovative data-free joint\ntraining framework that introduces: (1) an Adaptive Block-Sparse Attention\n(ASA) mechanism for dynamically generating content-aware sparsity masks to\nfocus computation on salient spatiotemporal features, and (2) a sparsity-aware\nstep distillation paradigm built upon Trajectory Distribution Matching (TDM)\nthat directly incorporates sparsity into the distillation process rather than\ntreating it as a separate compression step, with fast convergence. We validate\nBLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework\ndemonstrates remarkable efficiency gains across different scales. On\nWan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over a\n50-step baseline. Moreover, on models such as CogVideoX-5B with short video\nsequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the\nacceleration is accompanied by a consistent quality improvement. On the\nVBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from\n0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further\ncorroborated by superior ratings in human evaluations. Our code and model\nweights are publicly available at: http://ziplab.co/BLADE-Homepage/.",
        "url": "http://arxiv.org/abs/2508.10774v1",
        "published_date": "2025-08-14T15:58:59+00:00",
        "updated_date": "2025-08-14T15:58:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Youping Gu",
            "Xiaolong Li",
            "Yuhao Hu",
            "Bohan Zhuang"
        ],
        "tldr": "The paper introduces Video-BLADE, a data-free joint training framework using Adaptive Block-Sparse Attention and sparsity-aware step distillation to accelerate video generation while improving quality, achieving significant speedups on text-to-video models like CogVideoX-5B and Wan2.1-1.3B.",
        "tldr_zh": "该论文介绍了 Video-BLADE，一个无数据的联合训练框架，它使用自适应块稀疏注意力和感知稀疏性的步骤蒸馏来加速视频生成并提高质量，在 CogVideoX-5B 和 Wan2.1-1.3B 等文本到视频模型上实现了显著的加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale",
        "summary": "Prevailing autoregressive (AR) models for text-to-image generation either\nrely on heavy, computationally-intensive diffusion models to process continuous\nimage tokens, or employ vector quantization (VQ) to obtain discrete tokens with\nquantization loss. In this paper, we push the autoregressive paradigm forward\nwith NextStep-1, a 14B autoregressive model paired with a 157M flow matching\nhead, training on discrete text tokens and continuous image tokens with\nnext-token prediction objectives. NextStep-1 achieves state-of-the-art\nperformance for autoregressive models in text-to-image generation tasks,\nexhibiting strong capabilities in high-fidelity image synthesis. Furthermore,\nour method shows strong performance in image editing, highlighting the power\nand versatility of our unified approach. To facilitate open research, we will\nrelease our code and models to the community.",
        "url": "http://arxiv.org/abs/2508.10711v1",
        "published_date": "2025-08-14T14:54:22+00:00",
        "updated_date": "2025-08-14T14:54:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "NextStep Team",
            "Chunrui Han",
            "Guopeng Li",
            "Jingwei Wu",
            "Quan Sun",
            "Yan Cai",
            "Yuang Peng",
            "Zheng Ge",
            "Deyu Zhou",
            "Haomiao Tang",
            "Hongyu Zhou",
            "Kenkun Liu",
            "Ailin Huang",
            "Bin Wang",
            "Changxin Miao",
            "Deshan Sun",
            "En Yu",
            "Fukun Yin",
            "Gang Yu",
            "Hao Nie",
            "Haoran Lv",
            "Hanpeng Hu",
            "Jia Wang",
            "Jian Zhou",
            "Jianjian Sun",
            "Kaijun Tan",
            "Kang An",
            "Kangheng Lin",
            "Liang Zhao",
            "Mei Chen",
            "Peng Xing",
            "Rui Wang",
            "Shiyu Liu",
            "Shutao Xia",
            "Tianhao You",
            "Wei Ji",
            "Xianfang Zeng",
            "Xin Han",
            "Xuelin Zhang",
            "Yana Wei",
            "Yanming Xu",
            "Yimin Jiang",
            "Yingming Wang",
            "Yu Zhou",
            "Yucheng Han",
            "Ziyang Meng",
            "Binxing Jiao",
            "Daxin Jiang",
            "Xiangyu Zhang",
            "Yibo Zhu"
        ],
        "tldr": "NextStep-1, a 14B autoregressive model with a flow matching head, achieves state-of-the-art text-to-image generation performance by training on discrete text and continuous image tokens, surpassing existing AR models.",
        "tldr_zh": "NextStep-1是一个拥有140亿参数的自回归模型，配备了流匹配头，通过在离散文本和连续图像token上训练，实现了最先进的文本到图像生成性能，超越了现有的AR模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]