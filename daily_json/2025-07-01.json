[
    {
        "title": "SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation",
        "summary": "Diffusion-based video motion customization facilitates the acquisition of\nhuman motion representations from a few video samples, while achieving\narbitrary subjects transfer through precise textual conditioning. Existing\napproaches often rely on semantic-level alignment, expecting the model to learn\nnew motion concepts and combine them with other entities (e.g., ''cats'' or\n''dogs'') to produce visually appealing results. However, video data involve\ncomplex spatio-temporal patterns, and focusing solely on semantics cause the\nmodel to overlook the visual complexity of motion. Conversely, tuning only the\nvisual representation leads to semantic confusion in representing the intended\naction. To address these limitations, we propose SynMotion, a new\nmotion-customized video generation model that jointly leverages semantic\nguidance and visual adaptation. At the semantic level, we introduce the\ndual-embedding semantic comprehension mechanism which disentangles subject and\nmotion representations, allowing the model to learn customized motion features\nwhile preserving its generative capabilities for diverse subjects. At the\nvisual level, we integrate parameter-efficient motion adapters into a\npre-trained video generation model to enhance motion fidelity and temporal\ncoherence. Furthermore, we introduce a new embedding-specific training strategy\nwhich \\textbf{alternately optimizes} subject and motion embeddings, supported\nby the manually constructed Subject Prior Video (SPV) training dataset. This\nstrategy promotes motion specificity while preserving generalization across\ndiverse subjects. Lastly, we introduce MotionBench, a newly curated benchmark\nwith diverse motion patterns. Experimental results across both T2V and I2V\nsettings demonstrate that \\method outperforms existing baselines. Project page:\nhttps://lucaria-academy.github.io/SynMotion/",
        "url": "http://arxiv.org/abs/2506.23690v1",
        "published_date": "2025-06-30T10:09:32+00:00",
        "updated_date": "2025-06-30T10:09:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuai Tan",
            "Biao Gong",
            "Yujie Wei",
            "Shiwei Zhang",
            "Zhuoxin Liu",
            "Dandan Zheng",
            "Jingdong Chen",
            "Yan Wang",
            "Hao Ouyang",
            "Kecheng Zheng",
            "Yujun Shen"
        ],
        "tldr": "The paper introduces SynMotion, a video generation model that combines semantic guidance with visual adaptation to improve motion customization and temporal coherence, using a dual-embedding mechanism and motion adapters.",
        "tldr_zh": "该论文介绍了SynMotion，一种结合语义指导和视觉适应的视频生成模型，通过使用双重嵌入机制和运动适配器，来改进运动定制和时间连贯性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Blending Concepts with Text-to-Image Diffusion Models",
        "summary": "Diffusion models have dramatically advanced text-to-image generation in\nrecent years, translating abstract concepts into high-fidelity images with\nremarkable ease. In this work, we examine whether they can also blend distinct\nconcepts, ranging from concrete objects to intangible ideas, into coherent new\nvisual entities under a zero-shot framework. Specifically, concept blending\nmerges the key attributes of multiple concepts (expressed as textual prompts)\ninto a single, novel image that captures the essence of each concept. We\ninvestigate four blending methods, each exploiting different aspects of the\ndiffusion pipeline (e.g., prompt scheduling, embedding interpolation, or\nlayer-wise conditioning). Through systematic experimentation across diverse\nconcept categories, such as merging concrete concepts, synthesizing compound\nwords, transferring artistic styles, and blending architectural landmarks, we\nshow that modern diffusion models indeed exhibit creative blending capabilities\nwithout further training or fine-tuning. Our extensive user study, involving\n100 participants, reveals that no single approach dominates in all scenarios:\neach blending technique excels under certain conditions, with factors like\nprompt ordering, conceptual distance, and random seed affecting the outcome.\nThese findings highlight the remarkable compositional potential of diffusion\nmodels while exposing their sensitivity to seemingly minor input variations.",
        "url": "http://arxiv.org/abs/2506.23630v1",
        "published_date": "2025-06-30T08:53:30+00:00",
        "updated_date": "2025-06-30T08:53:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lorenzo Olearo",
            "Giorgio Longari",
            "Alessandro Raganato",
            "Rafael Peñaloza",
            "Simone Melzi"
        ],
        "tldr": "This paper explores the zero-shot blending capabilities of diffusion models, demonstrating their ability to merge distinct concepts into novel images using various techniques but also highlighting their sensitivity to input variations.",
        "tldr_zh": "本文研究了扩散模型的零样本概念融合能力，展示了它们使用各种技术将不同概念合并成新颖图像的能力，同时也强调了它们对输入变化的敏感性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Pyramidal Patchification Flow for Visual Generation",
        "summary": "Diffusion transformers (DiTs) adopt Patchify, mapping patch representations\nto token representations through linear projections, to adjust the number of\ntokens input to DiT blocks and thus the computation cost. Instead of a single\npatch size for all the timesteps, we introduce a Pyramidal Patchification Flow\n(PPFlow) approach: Large patch sizes are used for high noise timesteps and\nsmall patch sizes for low noise timesteps; Linear projections are learned for\neach patch size; and Unpatchify is accordingly modified. Unlike Pyramidal Flow,\nour approach operates over full latent representations other than pyramid\nrepresentations, and adopts the normal denoising process without requiring the\nrenoising trick. We demonstrate the effectiveness of our approach through two\ntraining manners. Training from scratch achieves a $1.6\\times$ ($2.0\\times$)\ninference speed over SiT-B/2 for 2-level (3-level) pyramid patchification with\nslightly lower training FLOPs and similar image generation performance.\nTraining from pretrained normal DiTs achieves even better performance with\nsmall training time. The code and checkpoint are at\nhttps://github.com/fudan-generative-vision/PPFlow.",
        "url": "http://arxiv.org/abs/2506.23543v1",
        "published_date": "2025-06-30T06:29:24+00:00",
        "updated_date": "2025-06-30T06:29:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hui Li",
            "Baoyou Chen",
            "Liwei Zhang",
            "Jiaye Li",
            "Jingdong Wang",
            "Siyu Zhu"
        ],
        "tldr": "The paper introduces Pyramidal Patchification Flow (PPFlow) for diffusion transformers, utilizing varying patch sizes at different noise timesteps to improve inference speed with comparable or better image generation performance compared to standard DiTs.",
        "tldr_zh": "该论文介绍了金字塔式分片流 (PPFlow)，用于扩散 Transformer，在不同的噪声时间步长利用不同的分片大小，从而提高推理速度，同时保持与标准 DiT 相当或更好的图像生成性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]