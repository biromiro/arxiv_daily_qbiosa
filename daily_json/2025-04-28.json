[
    {
        "title": "VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?",
        "summary": "Visual storytelling is an interdisciplinary field combining computer vision\nand natural language processing to generate cohesive narratives from sequences\nof images. This paper presents a novel approach that leverages recent\nadvancements in multimodal models, specifically adapting transformer-based\narchitectures and large multimodal models, for the visual storytelling task.\nLeveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT\nmodel produces visually grounded, contextually appropriate narratives. We\naddress the limitations of traditional evaluation metrics, such as BLEU,\nMETEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we\nutilize RoViST and GROOVIST, novel reference-free metrics designed to assess\nvisual storytelling, focusing on visual grounding, coherence, and\nnon-redundancy. These metrics provide a more nuanced evaluation of narrative\nquality, aligning closely with human judgment.",
        "url": "http://arxiv.org/abs/2504.19267v1",
        "published_date": "2025-04-27T14:55:51+00:00",
        "updated_date": "2025-04-27T14:55:51+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Mohamed Gado",
            "Towhid Taliee",
            "Muhammad Memon",
            "Dmitry Ignatov",
            "Radu Timofte"
        ],
        "tldr": "this paper introduces vist-gpt, a transformer-based large multimodal model for visual storytelling using the vist dataset, and proposes rovist and groovist, new reference-free metrics for evaluating visual storytelling quality.",
        "tldr_zh": "本文介绍了vist-gpt，一个基于transformer的大型多模态模型，用于使用vist数据集进行视觉故事讲述，并提出了rovist和groovist，用于评估视觉故事讲述质量的新型无参考指标。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation",
        "summary": "Storyboarding is widely used for creating 3D animations. Animators use the 2D\nsketches in storyboards as references to craft the desired 3D animations\nthrough a trial-and-error process. The traditional approach requires\nexceptional expertise and is both labor-intensive and time-consuming.\nConsequently, there is a high demand for automated methods that can directly\ntranslate 2D storyboard sketches into 3D animations. This task is\nunder-explored to date and inspired by the significant advancements of motion\ndiffusion models, we propose to address it from the perspective of conditional\nmotion synthesis. We thus present Sketch2Anim, composed of two key modules for\nsketch constraint understanding and motion generation. Specifically, due to the\nlarge domain gap between the 2D sketch and 3D motion, instead of directly\nconditioning on 2D inputs, we design a 3D conditional motion generator that\nsimultaneously leverages 3D keyposes, joint trajectories, and action words, to\nachieve precise and fine-grained motion control. Then, we invent a neural\nmapper dedicated to aligning user-provided 2D sketches with their corresponding\n3D keyposes and trajectories in a shared embedding space, enabling, for the\nfirst time, direct 2D control of motion generation. Our approach successfully\ntransfers storyboards into high-quality 3D motions and inherently supports\ndirect 3D animation editing, thanks to the flexibility of our multi-conditional\nmotion generator. Comprehensive experiments and evaluations, and a user\nperceptual study demonstrate the effectiveness of our approach.",
        "url": "http://arxiv.org/abs/2504.19189v1",
        "published_date": "2025-04-27T10:38:17+00:00",
        "updated_date": "2025-04-27T10:38:17+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Lei Zhong",
            "Chuan Guo",
            "Yiming Xie",
            "Jiawei Wang",
            "Changjian Li"
        ],
        "tldr": "the paper introduces sketch2anim, a novel approach using conditional motion synthesis and a neural mapper to translate 2d storyboard sketches into 3d animations, enabling direct 2d control and 3d editing of motion generation.",
        "tldr_zh": "本文介绍 sketch2anim，一种新颖的方法，使用条件运动合成和神经映射器将 2d 故事板草图转换为 3d 动画，从而实现对运动生成的直接 2d 控制和 3d 编辑。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IM-Portrait: Learning 3D-aware Video Diffusion for PhotorealisticTalking Heads from Monocular Videos",
        "summary": "We propose a novel 3D-aware diffusion-based method for generating\nphotorealistic talking head videos directly from a single identity image and\nexplicit control signals (e.g., expressions). Our method generates Multiplane\nImages (MPIs) that ensure geometric consistency, making them ideal for\nimmersive viewing experiences like binocular videos for VR headsets. Unlike\nexisting methods that often require a separate stage or joint optimization to\nreconstruct a 3D representation (such as NeRF or 3D Gaussians), our approach\ndirectly generates the final output through a single denoising process,\neliminating the need for post-processing steps to render novel views\nefficiently. To effectively learn from monocular videos, we introduce a\ntraining mechanism that reconstructs the output MPI randomly in either the\ntarget or the reference camera space. This approach enables the model to\nsimultaneously learn sharp image details and underlying 3D information.\nExtensive experiments demonstrate the effectiveness of our method, which\nachieves competitive avatar quality and novel-view rendering capabilities, even\nwithout explicit 3D reconstruction or high-quality multi-view training data.",
        "url": "http://arxiv.org/abs/2504.19165v1",
        "published_date": "2025-04-27T08:56:02+00:00",
        "updated_date": "2025-04-27T08:56:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuan Li",
            "Ziqian Bai",
            "Feitong Tan",
            "Zhaopeng Cui",
            "Sean Fanello",
            "Yinda Zhang"
        ],
        "tldr": "the paper introduces a novel 3d-aware diffusion method, im-portrait, for photorealistic talking head video generation from monocular videos using mpis, eliminating the need for explicit 3d reconstruction and post-processing.",
        "tldr_zh": "该论文提出了一种新的基于扩散的3d感知方法im-portrait，用于从单目视频生成逼真的说话人头部视频，使用mpis，无需显式的3d重建和后处理。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions",
        "summary": "Generative AI is reshaping art, gaming, and most notably animation. Recent\nbreakthroughs in foundation and diffusion models have reduced the time and cost\nof producing animated content. Characters are central animation components,\ninvolving motion, emotions, gestures, and facial expressions. The pace and\nbreadth of advances in recent months make it difficult to maintain a coherent\nview of the field, motivating the need for an integrative review. Unlike\nearlier overviews that treat avatars, gestures, or facial animation in\nisolation, this survey offers a single, comprehensive perspective on all the\nmain generative AI applications for character animation. We begin by examining\nthe state-of-the-art in facial animation, expression rendering, image\nsynthesis, avatar creation, gesture modeling, motion synthesis, object\ngeneration, and texture synthesis. We highlight leading research, practical\ndeployments, commonly used datasets, and emerging trends for each area. To\nsupport newcomers, we also provide a comprehensive background section that\nintroduces foundational models and evaluation metrics, equipping readers with\nthe knowledge needed to enter the field. We discuss open challenges and map\nfuture research directions, providing a roadmap to advance AI-driven\ncharacter-animation technologies. This survey is intended as a resource for\nresearchers and developers entering the field of generative AI animation or\nadjacent fields. Resources are available at:\nhttps://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.",
        "url": "http://arxiv.org/abs/2504.19056v1",
        "published_date": "2025-04-27T00:09:31+00:00",
        "updated_date": "2025-04-27T00:09:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG",
            "cs.MM"
        ],
        "authors": [
            "Mohammad Mahdi Abootorabi",
            "Omid Ghahroodi",
            "Pardis Sadat Zahraei",
            "Hossein Behzadasl",
            "Alireza Mirrokni",
            "Mobina Salimipanah",
            "Arash Rasouli",
            "Bahar Behzadipour",
            "Sara Azarnoush",
            "Benyamin Maleki",
            "Erfan Sadraiye",
            "Kiarash Kiani Feriz",
            "Mahdi Teymouri Nahad",
            "Ali Moghadasi",
            "Abolfazl Eshagh Abianeh",
            "Nizi Nazar",
            "Hamid R. Rabiee",
            "Mahdieh Soleymani Baghshah",
            "Meisam Ahmadi",
            "Ehsaneddin Asgari"
        ],
        "tldr": "this paper presents a comprehensive survey of generative ai techniques for character animation, covering various aspects from facial animation to motion synthesis and offering a roadmap for future research.",
        "tldr_zh": "该论文全面调研了用于角色动画的生成式人工智能技术，涵盖了从面部动画到运动合成的各个方面，并为未来的研究提供了路线图。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VI3NR: Variance Informed Initialization for Implicit Neural Representations",
        "summary": "Implicit Neural Representations (INRs) are a versatile and powerful tool for\nencoding various forms of data, including images, videos, sound, and 3D shapes.\nA critical factor in the success of INRs is the initialization of the network,\nwhich can significantly impact the convergence and accuracy of the learned\nmodel. Unfortunately, commonly used neural network initializations are not\nwidely applicable for many activation functions, especially those used by INRs.\nIn this paper, we improve upon previous initialization methods by deriving an\ninitialization that has stable variance across layers, and applies to any\nactivation function. We show that this generalizes many previous initialization\nmethods, and has even better stability for well studied activations. We also\nshow that our initialization leads to improved results with INR activation\nfunctions in multiple signal modalities. Our approach is particularly effective\nfor Gaussian INRs, where we demonstrate that the theory of our initialization\nmatches with task performance in multiple experiments, allowing us to achieve\nimprovements in image, audio, and 3D surface reconstruction.",
        "url": "http://arxiv.org/abs/2504.19270v1",
        "published_date": "2025-04-27T14:58:27+00:00",
        "updated_date": "2025-04-27T14:58:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chamin Hewa Koneputugodage",
            "Yizhak Ben-Shabat",
            "Sameera Ramasinghe",
            "Stephen Gould"
        ],
        "tldr": "this paper proposes a novel initialization method for implicit neural representations (inrs) that ensures stable variance across layers and applies to various activation functions, leading to improved performance in image, audio, and 3d surface reconstruction.",
        "tldr_zh": "本文提出了一种新的隐式神经表示（inr）初始化方法，该方法可确保跨层稳定的方差，并适用于各种激活函数，从而提高了图像、音频和3d表面重建的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "REED-VAE: RE-Encode Decode Training for Iterative Image Editing with Diffusion Models",
        "summary": "While latent diffusion models achieve impressive image editing results, their\napplication to iterative editing of the same image is severely restricted. When\ntrying to apply consecutive edit operations using current models, they\naccumulate artifacts and noise due to repeated transitions between pixel and\nlatent spaces. Some methods have attempted to address this limitation by\nperforming the entire edit chain within the latent space, sacrificing\nflexibility by supporting only a limited, predetermined set of diffusion\nediting operations. We present a RE-encode decode (REED) training scheme for\nvariational autoencoders (VAEs), which promotes image quality preservation even\nafter many iterations. Our work enables multi-method iterative image editing:\nusers can perform a variety of iterative edit operations, with each operation\nbuilding on the output of the previous one using both diffusion-based\noperations and conventional editing techniques. We demonstrate the advantage of\nREED-VAE across a range of image editing scenarios, including text-based and\nmask-based editing frameworks. In addition, we show how REED-VAE enhances the\noverall editability of images, increasing the likelihood of successful and\nprecise edit operations. We hope that this work will serve as a benchmark for\nthe newly introduced task of multi-method image editing. Our code and models\nwill be available at https://github.com/galmog/REED-VAE",
        "url": "http://arxiv.org/abs/2504.18989v1",
        "published_date": "2025-04-26T18:26:54+00:00",
        "updated_date": "2025-04-26T18:26:54+00:00",
        "categories": [
            "cs.GR",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Gal Almog",
            "Ariel Shamir",
            "Ohad Fried"
        ],
        "tldr": "the paper introduces reed-vae, a training scheme for vaes that improves iterative image editing with diffusion models by preserving image quality and enabling multi-method editing.",
        "tldr_zh": "该论文介绍了 reed-vae，一种 vae 的训练方案，通过保持图像质量和支持多方法编辑，改进了扩散模型的迭代图像编辑。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FusionNet: Multi-model Linear Fusion Framework for Low-light Image Enhancement",
        "summary": "The advent of Deep Neural Networks (DNNs) has driven remarkable progress in\nlow-light image enhancement (LLIE), with diverse architectures (e.g., CNNs and\nTransformers) and color spaces (e.g., sRGB, HSV, HVI) yielding impressive\nresults. Recent efforts have sought to leverage the complementary strengths of\nthese paradigms, offering promising solutions to enhance performance across\nvarying degradation scenarios. However, existing fusion strategies are hindered\nby challenges such as parameter explosion, optimization instability, and\nfeature misalignment, limiting further improvements. To overcome these issues,\nwe introduce FusionNet, a novel multi-model linear fusion framework that\noperates in parallel to effectively capture global and local features across\ndiverse color spaces. By incorporating a linear fusion strategy underpinned by\nHilbert space theoretical guarantees, FusionNet mitigates network collapse and\nreduces excessive training costs. Our method achieved 1st place in the CVPR2025\nNTIRE Low Light Enhancement Challenge. Extensive experiments conducted on\nsynthetic and real-world benchmark datasets demonstrate that the proposed\nmethod significantly outperforms state-of-the-art methods in terms of both\nquantitative and qualitative results, delivering robust enhancement under\ndiverse low-light conditions.",
        "url": "http://arxiv.org/abs/2504.19295v1",
        "published_date": "2025-04-27T16:22:03+00:00",
        "updated_date": "2025-04-27T16:22:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kangbiao Shi",
            "Yixu Feng",
            "Tao Hu",
            "Yu Cao",
            "Peng Wu",
            "Yijin Liang",
            "Yanning Zhang",
            "Qingsen Yan"
        ],
        "tldr": "fusionnet, a novel multi-model linear fusion framework, enhances low-light images by capturing global and local features across diverse color spaces, achieving state-of-the-art results and winning the cvpr2025 ntire challenge.",
        "tldr_zh": "fusionnet 是一种新颖的多模型线性融合框架，通过捕获不同色彩空间中的全局和局部特征来增强弱光图像，实现了最先进的结果，并赢得了 cvpr2025 ntire 挑战赛。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "Improving Generalization in MRI-Based Deep Learning Models for Total Knee Replacement Prediction",
        "summary": "Knee osteoarthritis (KOA) is a common joint disease that causes pain and\nmobility issues. While MRI-based deep learning models have demonstrated\nsuperior performance in predicting total knee replacement (TKR) and disease\nprogression, their generalizability remains challenging, particularly when\napplied to imaging data from different sources. In this study, we have shown\nthat replacing batch normalization with instance normalization, using data\naugmentation, and applying contrastive loss improves model generalization in a\nbaseline deep learning model for knee osteoarthritis (KOA) prediction. We\ntrained and evaluated our model using MRI data from the Osteoarthritis\nInitiative (OAI) database, considering sagittal fat-suppressed\nintermediate-weighted turbo spin-echo (FS-IW-TSE) images as the source domain\nand sagittal fat-suppressed three-dimensional (3D) dual-echo in steady state\n(DESS) images as the target domain. The results demonstrate a statistically\nsignificant improvement in classification accuracy across both domains, with\nour approach outperforming the baseline model.",
        "url": "http://arxiv.org/abs/2504.19203v1",
        "published_date": "2025-04-27T11:41:19+00:00",
        "updated_date": "2025-04-27T11:41:19+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Ehsan Karami",
            "Hamid Soltanian-Zadeh"
        ],
        "tldr": "this paper improves the generalization of deep learning models for total knee replacement prediction using mri data by implementing instance normalization, data augmentation, and contrastive loss, demonstrating statistically significant accuracy improvements across different mri domains.",
        "tldr_zh": "本文通过使用实例归一化、数据增强和对比损失，提高了基于mri的深度学习模型在全膝关节置换预测中的泛化能力，并在不同mri领域中展示了统计学上显著的准确性提升。",
        "relevance_score": 2,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4
    },
    {
        "title": "Blind Source Separation Based on Sparsity",
        "summary": "Blind source separation (BSS) is a key technique in array processing and data\nanalysis, aiming to recover unknown sources from observed mixtures without\nknowledge of the mixing matrix. Classical independent component analysis (ICA)\nmethods rely on the assumption that sources are mutually independent. To\naddress limitations of ICA, sparsity-based methods have been introduced, which\ndecompose source signals sparsely in a predefined dictionary. Morphological\nComponent Analysis (MCA), based on sparse representation theory, assumes that a\nsignal is a linear combination of components with distinct geometries, each\nsparsely representable in one dictionary and not in others. This approach has\nrecently been applied to BSS with promising results.\n  This report reviews key approaches derived from classical ICA and explores\nsparsity-based methods for BSS. It introduces the theory of sparse\nrepresentation and decomposition, followed by a block coordinate relaxation MCA\nalgorithm, whose variants are used in Multichannel MCA (MMCA) and Generalized\nMCA (GMCA). A local dictionary learning method using K-SVD is then presented.\nFinally, we propose an improved algorithm, SAC+BK-SVD, which enhances K-SVD by\nlearning a block-sparsifying dictionary that clusters and updates similar atoms\nin blocks.\n  The implementation includes experiments on image segmentation and blind image\nsource separation using the discussed techniques. We also compare the proposed\nblock-sparse dictionary learning algorithm with K-SVD. Simulation results\ndemonstrate that our method yields improved blind image separation quality.",
        "url": "http://arxiv.org/abs/2504.19124v1",
        "published_date": "2025-04-27T06:42:06+00:00",
        "updated_date": "2025-04-27T06:42:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhongxuan Li"
        ],
        "tldr": "this paper reviews sparsity-based blind source separation (bss) methods, proposes an improved dictionary learning algorithm (sac+bk-svd), and demonstrates its effectiveness in image segmentation and blind image separation.",
        "tldr_zh": "本文回顾了基于稀疏性的盲源分离(bss)方法，提出了一种改进的字典学习算法(sac+bk-svd)，并证明了其在图像分割和盲图像分离中的有效性。",
        "relevance_score": 2,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 4,
        "overall_priority_score": 3
    }
]