[
    {
        "title": "Spatial Knowledge Graph-Guided Multimodal Synthesis",
        "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced their capabilities; however, their spatial perception\nabilities remain a notable limitation. To address this challenge, multimodal\ndata synthesis offers a promising solution. Yet, ensuring that synthesized data\nadhere to spatial common sense is a non-trivial task. In this work, we\nintroduce SKG2Data, a novel multimodal synthesis approach guided by spatial\nknowledge graphs, grounded in the concept of knowledge-to-data generation.\nSKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate\nhuman-like perception of spatial directions and distances, which is\nsubsequently utilized to guide multimodal data synthesis. Extensive experiments\ndemonstrate that data synthesized from diverse types of spatial knowledge,\nincluding direction and distance, not only enhance the spatial perception and\nreasoning abilities of MLLMs but also exhibit strong generalization\ncapabilities. We hope that the idea of knowledge-based data synthesis can\nadvance the development of spatial intelligence.",
        "url": "http://arxiv.org/abs/2505.22633v1",
        "published_date": "2025-05-28T17:50:21+00:00",
        "updated_date": "2025-05-28T17:50:21+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.MM"
        ],
        "authors": [
            "Yida Xue",
            "Zhen Bi",
            "Jinnan Yang",
            "Jungang Lou",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "tldr": "The paper introduces SKG2Data, a spatial knowledge graph-guided multimodal data synthesis approach to improve spatial perception and reasoning in MLLMs. It shows promising results in enhancing spatial intelligence through knowledge-based data synthesis.",
        "tldr_zh": "该论文介绍了一种名为SKG2Data的空间知识图引导的多模态数据合成方法，旨在提高多模态大型语言模型（MLLM）中的空间感知和推理能力。研究表明，通过基于知识的数据合成，该方法在增强空间智能方面具有良好的潜力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning World Models for Interactive Video Generation",
        "summary": "Foundational world models must be both interactive and preserve\nspatiotemporal coherence for effective future planning with action choices.\nHowever, present models for long video generation have limited inherent world\nmodeling capabilities due to two main challenges: compounding errors and\ninsufficient memory mechanisms. We enhance image-to-video models with\ninteractive capabilities through additional action conditioning and\nautoregressive framework, and reveal that compounding error is inherently\nirreducible in autoregressive video generation, while insufficient memory\nmechanism leads to incoherence of world models. We propose video retrieval\naugmented generation (VRAG) with explicit global state conditioning, which\nsignificantly reduces long-term compounding errors and increases spatiotemporal\nconsistency of world models. In contrast, naive autoregressive generation with\nextended context windows and retrieval-augmented generation prove less\neffective for video generation, primarily due to the limited in-context\nlearning capabilities of current video models. Our work illuminates the\nfundamental challenges in video world models and establishes a comprehensive\nbenchmark for improving video generation models with internal world modeling\ncapabilities.",
        "url": "http://arxiv.org/abs/2505.21996v1",
        "published_date": "2025-05-28T05:55:44+00:00",
        "updated_date": "2025-05-28T05:55:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Taiye Chen",
            "Xun Hu",
            "Zihan Ding",
            "Chi Jin"
        ],
        "tldr": "This paper addresses the challenges of compounding errors and insufficient memory in interactive video generation by introducing Video Retrieval Augmented Generation (VRAG), which significantly improves spatiotemporal consistency. It also analyzes the limitations of autoregressive approaches for complex video generation tasks.",
        "tldr_zh": "本文通过引入视频检索增强生成 (VRAG) 来解决交互式视频生成中复合误差和内存不足的挑战，该方法显着提高了时空一致性。此外，本文还分析了自回归方法在复杂视频生成任务中的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "One-Way Ticket:Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion Models",
        "summary": "Text-to-Image (T2I) diffusion models have made remarkable advancements in\ngenerative modeling; however, they face a trade-off between inference speed and\nimage quality, posing challenges for efficient deployment. Existing distilled\nT2I models can generate high-fidelity images with fewer sampling steps, but\noften struggle with diversity and quality, especially in one-step models. From\nour analysis, we observe redundant computations in the UNet encoders. Our\nfindings suggest that, for T2I diffusion models, decoders are more adept at\ncapturing richer and more explicit semantic information, while encoders can be\neffectively shared across decoders from diverse time steps. Based on these\nobservations, we introduce the first Time-independent Unified Encoder TiUE for\nthe student model UNet architecture, which is a loop-free image generation\napproach for distilling T2I diffusion models. Using a one-pass scheme, TiUE\nshares encoder features across multiple decoder time steps, enabling parallel\nsampling and significantly reducing inference time complexity. In addition, we\nincorporate a KL divergence term to regularize noise prediction, which enhances\nthe perceptual realism and diversity of the generated images. Experimental\nresults demonstrate that TiUE outperforms state-of-the-art methods, including\nLCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results\nwhile maintaining the computational efficiency.",
        "url": "http://arxiv.org/abs/2505.21960v1",
        "published_date": "2025-05-28T04:23:22+00:00",
        "updated_date": "2025-05-28T04:23:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Senmao Li",
            "Lei Wang",
            "Kai Wang",
            "Tao Liu",
            "Jiehang Xie",
            "Joost van de Weijer",
            "Fahad Shahbaz Khan",
            "Shiqi Yang",
            "Yaxing Wang",
            "Jian Yang"
        ],
        "tldr": "This paper introduces TiUE, a time-independent unified encoder for distilling text-to-image diffusion models, which improves inference speed and image quality by sharing encoder features across decoder time steps and regularizing noise prediction with KL divergence.",
        "tldr_zh": "本文介绍了一种用于蒸馏文本到图像扩散模型的时间独立统一编码器TiUE，通过在解码器时间步长上共享编码器特征，并利用KL散度正则化噪声预测，从而提高推理速度和图像质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]