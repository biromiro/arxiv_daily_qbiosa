[
    {
        "title": "Marrying Autoregressive Transformer and Diffusion with Multi-Reference Autoregression",
        "summary": "We introduce TransDiff, the first image generation model that marries\nAutoregressive (AR) Transformer with diffusion models. In this joint modeling\nframework, TransDiff encodes labels and images into high-level semantic\nfeatures and employs a diffusion model to estimate the distribution of image\nsamples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms\nother image generation models based on standalone AR Transformer or diffusion\nmodels. Specifically, TransDiff achieves a Fr\\'echet Inception Distance (FID)\nof 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster\ninference latency compared to state-of-the-art methods based on AR Transformer\nand x112 faster inference compared to diffusion-only models. Furthermore,\nbuilding on the TransDiff model, we introduce a novel image generation paradigm\ncalled Multi-Reference Autoregression (MRAR), which performs autoregressive\ngeneration by predicting the next image. MRAR enables the model to reference\nmultiple previously generated images, thereby facilitating the learning of more\ndiverse representations and improving the quality of generated images in\nsubsequent iterations. By applying MRAR, the performance of TransDiff is\nimproved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open\nup a new frontier in the field of image generation.",
        "url": "http://arxiv.org/abs/2506.09482v1",
        "published_date": "2025-06-11T07:50:31+00:00",
        "updated_date": "2025-06-11T07:50:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dingcheng Zhen",
            "Qian Qiao",
            "Tan Yu",
            "Kangxi Wu",
            "Ziwei Zhang",
            "Siyuan Liu",
            "Shunshun Yin",
            "Ming Tao"
        ],
        "tldr": "The paper introduces TransDiff, a novel image generation model combining Autoregressive Transformers and diffusion models, achieving state-of-the-art FID and inference speed on ImageNet, further improved by a Multi-Reference Autoregression paradigm.",
        "tldr_zh": "该论文介绍了TransDiff，一种结合了自回归Transformer和扩散模型的新型图像生成模型，在ImageNet上实现了最先进的FID和推理速度，并通过多参考自回归范式进一步改进。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation",
        "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
        "url": "http://arxiv.org/abs/2506.09350v1",
        "published_date": "2025-06-11T03:04:23+00:00",
        "updated_date": "2025-06-11T03:04:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Shanchuan Lin",
            "Ceyuan Yang",
            "Hao He",
            "Jianwen Jiang",
            "Yuxi Ren",
            "Xin Xia",
            "Yang Zhao",
            "Xuefeng Xiao",
            "Lu Jiang"
        ],
        "tldr": "The paper introduces Autoregressive Adversarial Post-Training (AAPT), a method to transform pre-trained latent video diffusion models into real-time, interactive video generators, achieving 24fps generation on a single H100 GPU.",
        "tldr_zh": "该论文介绍了一种名为自回归对抗后训练（AAPT）的方法，可以将预训练的潜在视频扩散模型转换为实时、交互式视频生成器，在单个H100 GPU上实现24fps的生成速度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation",
        "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.",
        "url": "http://arxiv.org/abs/2506.09344v1",
        "published_date": "2025-06-11T02:50:49+00:00",
        "updated_date": "2025-06-11T02:50:49+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.LG",
            "cs.SD",
            "eess.AS"
        ],
        "authors": [
            "Inclusion AI",
            "Biao Gong",
            "Cheng Zou",
            "Chuanyang Zheng",
            "Chunluan Zhou",
            "Canxiang Yan",
            "Chunxiang Jin",
            "Chunjie Shen",
            "Dandan Zheng",
            "Fudong Wang",
            "Furong Xu",
            "GuangMing Yao",
            "Jun Zhou",
            "Jingdong Chen",
            "Jianxin Sun",
            "Jiajia Liu",
            "Jianjiang Zhu",
            "Jun Peng",
            "Kaixiang Ji",
            "Kaiyou Song",
            "Kaimeng Ren",
            "Libin Wang",
            "Lixiang Ru",
            "Lele Xie",
            "Longhua Tan",
            "Lyuxin Xue",
            "Lan Wang",
            "Mochen Bai",
            "Ning Gao",
            "Pei Chen",
            "Qingpei Guo",
            "Qinglong Zhang",
            "Qiang Xu",
            "Rui Liu",
            "Ruijie Xiong",
            "Sirui Gao",
            "Tinghao Liu",
            "Taisong Li",
            "Weilong Chai",
            "Xinyu Xiao",
            "Xiaomei Wang",
            "Xiaoxue Chen",
            "Xiao Lu",
            "Xiaoyu Li",
            "Xingning Dong",
            "Xuzheng Yu",
            "Yi Yuan",
            "Yuting Gao",
            "Yunxiao Sun",
            "Yipeng Chen",
            "Yifei Wu",
            "Yongjie Lyu",
            "Ziping Ma",
            "Zipeng Feng",
            "Zhijiang Fang",
            "Zhihao Qiu",
            "Ziyuan Huang",
            "Zhengyu He"
        ],
        "tldr": "Ming-Omni is a newly proposed open-source unified multimodal model capable of perception and generation across image, text, audio, and video modalities, claiming to match GPT-4o in modality support.",
        "tldr_zh": "Ming-Omni是一个新提出的开源统一多模态模型，能够跨图像、文本、音频和视频等模态进行感知和生成，并声称在模态支持方面与GPT-4o相媲美。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    }
]